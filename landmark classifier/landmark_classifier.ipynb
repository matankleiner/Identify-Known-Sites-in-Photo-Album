{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre trained YOLO model as a landmark classifier  \n",
    "\n",
    "As we mentioned in the test.ipynb notebook (in the data directory), the test set we use is full of out of domain images. We would like to discard as many of them as possible. In order to do that we will use an object detector and further processing to create our own landmark classifier. \n",
    "\n",
    "We will use YOLO (You Only Look Once) pre trained model as an object detctor. YOLO is a state of the art object detctor that achived great results on various data sets. We used [darknet](https://github.com/AlexeyAB/darknet) implemntation which allow to use pre-trained models easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cloning the darknet repo, we changed the Makefile in order it to use the GPU and preform faster (as described in the [readme](\"https://github.com/AlexeyAB/darknet/blob/master/README.md\") file). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use the YOLOv3 that was pre trained on the [Open Images dataset](\"https://storage.googleapis.com/openimages/web/index.html\"). Open Images dataset is a big, diverse data set with ~9M images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. We used it's detection part that contain 15,851,536 boxes on 600 categories. The bounding box annotation made by people and not by computers and therefore are higly accurate.  \n",
    "\n",
    "Out of the 600 categories we chose 5 categroies that could indicate that the image contain a landmark: Tower, Fountain, Skyscraper, Building and Castle.  \n",
    "\n",
    "Another 8 categories could inidicate that the image may contain a landmark Bronze sculpture, Sculpture, Lighthouse, House, Tree, Palm tree, Watercraft, Hiking equipment. \n",
    "\n",
    "All the other classes can indicate that it is not a landmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Images dataset examples:\n",
    "\n",
    "Landmarks: \n",
    "\n",
    "<img src=\"example_images/openimages_landmark_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"example_images/openimages_landmark_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Maybe landmarks: \n",
    "\n",
    "<img src=\"example_images/openimages_maybe_landmark_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"example_images/openimages_maybe_landmark_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Non-landmarks: \n",
    "\n",
    "<img src=\"example_images/openimages_non_landmark_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"example_images/openimages_non_landmark_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passed all the test set images inside the yolo-darknet implementation. The network produced a json file as a result. In this file each image is connected to its filename and the objects that detected in it. For each detected object there will be the corresponding class_id, name and the realtive coordinates, \n",
    "\n",
    "Some examples of the network result (on test set images) as images with bounding box: \n",
    "\n",
    "<img src=\"example_images/predictions1.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"example_images/predictions2.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"example_images/predictions3.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"example_images/predictions9.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "The results may not be perfect but they are good and the best that we can achive with that method. \n",
    "\n",
    "We will further process the results file in oreder to clean the test set as much as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for code \n",
    "import json\n",
    "import pandas as pd \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results file from the yolo v3 object detector\n",
    "f = open('result_yolov3_openimages.json')  \n",
    "data = json.load(f) \n",
    "\n",
    "# pars the results file so every image will be connected to its filename and objects that detected in it.\n",
    "# for each object that was detected there will be the corresponding class name, confidence level \n",
    "# and size (realtive to the image)\n",
    "for i in range(len(data)): \n",
    "    file_name = re.search('[a-z & 0-9]{16}', data[i]['filename']).group(0)\n",
    "    data[i]['filename'] = file_name\n",
    "    if len(data[i]['objects']) != 0: \n",
    "        for j in range(len(data[i]['objects'])): \n",
    "            data[i]['objects'][j].pop('class_id', None)\n",
    "            new_key1 = \"class_name\"\n",
    "            old_key1 = \"name\"\n",
    "            data[i]['objects'][j][new_key1] = data[i]['objects'][j].pop(old_key1)\n",
    "            relative_size = data[i]['objects'][j]['relative_coordinates']['width'] * data[i]['objects'][j]['relative_coordinates']['height']\n",
    "            data[i]['objects'][j].pop('relative_coordinates', None)\n",
    "            data[i]['objects'][j]['realtive_size'] = relative_size\n",
    "            new_key2 = \"confidence_val\"\n",
    "            old_key2 = \"confidence\"\n",
    "            data[i]['objects'][j][new_key2] = data[i]['objects'][j].pop(old_key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the objects that detcted in the test set images can be landmarks  \n",
    "landmark = ['Tower', 'Fountain', 'Skyscraper', 'Building', 'Castle']\n",
    "maybe_landmark_1 = ['Bronze sculpture', 'Sculpture', 'Lighthouse', 'House']\n",
    "maybe_landmark_2 = ['Tree', 'Palm tree', 'Watercraft', 'Hiking equipment']\n",
    "keep = []\n",
    "throw = []\n",
    "\n",
    "for i in range(len(data)): \n",
    "    if len(data[i]['objects']) != 0: \n",
    "        for j in range(len(data[i]['objects'])): \n",
    "            # if any of the dtected objects can be defined as one of the landmark list we'll keep it   \n",
    "            if data[i]['objects'][j]['class_name'] in landmark: \n",
    "                keep.append(data[i])\n",
    "            # if any of the detected objects can be defined as one of the maybe_landmark lists we will check its \n",
    "            # confidence value and realtive size.\n",
    "            # for classes in maybe_landmarks_1 list we'll use large threshold, because those classes are of big objects.\n",
    "            # for classes in maybe_landmarks_2 list we'll use small threshold, because those classes are of small objects. \n",
    "            elif data[i]['objects'][j]['class_name'] in maybe_landmark_1:\n",
    "                if data[i]['objects'][j]['confidence_val'] > 0.5 and data[i]['objects'][j]['realtive_size'] > 0.6:\n",
    "                    keep.append(data[i])\n",
    "            elif data[i]['objects'][j]['class_name'] in maybe_landmark_2:\n",
    "                if data[i]['objects'][j]['confidence_val'] > 0.5 and data[i]['objects'][j]['realtive_size'] < 0.2:\n",
    "                    keep.append(data[i])\n",
    "            # if the objects detected cannot be defined as landmark or maybe_landmark we will check its confidence value \n",
    "            # and its realtive size (to make sure its the major part of the image) to make sure we want to throw it as \n",
    "            # this image is not of a landmark. \n",
    "            else: \n",
    "                if data[i]['objects'][j]['confidence_val'] > 0.3 and data[i]['objects'][j]['realtive_size'] > 0.6: \n",
    "                    throw.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates rows\n",
    "keep_df_tmp = pd.DataFrame(keep)\n",
    "keep_df = keep_df_tmp.drop_duplicates(subset=['frame_id'])\n",
    "keep_df = keep_df.reset_index()\n",
    "keep_df = keep_df.drop('index', axis=1)\n",
    "\n",
    "throw_df_tmp = pd.DataFrame(throw)\n",
    "throw_df = throw_df_tmp.drop_duplicates(subset=['frame_id'])\n",
    "throw_df = throw_df.reset_index()\n",
    "throw_df = throw_df.drop('index', axis=1)\n",
    "\n",
    "# remove from \"throw\" rows that are also in \"keep\"\n",
    "keep_series = keep_df['frame_id']\n",
    "throw_df = throw_df[~throw_df[\"frame_id\"].isin(keep_series)]\n",
    "throw_df = throw_df.reset_index()\n",
    "throw_df = throw_df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e324e0f3e6d9e504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d9e17c5f3e0c47b3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a748a755ed67512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>537bf9bdfccdafea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13f4c974274ee08b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117222</th>\n",
       "      <td>e351c3e672c25fbd</td>\n",
       "      <td>190441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117223</th>\n",
       "      <td>5426472625271a4d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117224</th>\n",
       "      <td>7b6a585405978398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117225</th>\n",
       "      <td>d885235ba249cf5d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117226</th>\n",
       "      <td>c7f657e8d0f7fafb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117227 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id landmarks\n",
       "0       e324e0f3e6d9e504         0\n",
       "1       d9e17c5f3e0c47b3         0\n",
       "2       1a748a755ed67512         0\n",
       "3       537bf9bdfccdafea         0\n",
       "4       13f4c974274ee08b         0\n",
       "...                  ...       ...\n",
       "117222  e351c3e672c25fbd    190441\n",
       "117223  5426472625271a4d         0\n",
       "117224  7b6a585405978398         0\n",
       "117225  d885235ba249cf5d         0\n",
       "117226  c7f657e8d0f7fafb         0\n",
       "\n",
       "[117227 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"C:/Users/Matan/Desktop/projectB/data/test/test.csv\") \n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['id'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117227"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
