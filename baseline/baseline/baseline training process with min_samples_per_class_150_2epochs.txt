total batches: 3682
Epoch: 1
----------------------------------------------------------------------
batch: [0/3682] batch time: 0.568 trainign loss: 6.8999 avg training loss: 6.8999
batch: [10/3682] batch time: 0.056 trainign loss: 6.7936 avg training loss: 6.8463
batch: [20/3682] batch time: 0.217 trainign loss: 6.7961 avg training loss: 6.8293
batch: [30/3682] batch time: 0.057 trainign loss: 6.8274 avg training loss: 6.8385
batch: [40/3682] batch time: 0.239 trainign loss: 6.8369 avg training loss: 6.8217
batch: [50/3682] batch time: 0.057 trainign loss: 6.8556 avg training loss: 6.8202
batch: [60/3682] batch time: 0.241 trainign loss: 6.8342 avg training loss: 6.8190
batch: [70/3682] batch time: 0.057 trainign loss: 6.8648 avg training loss: 6.8189
batch: [80/3682] batch time: 0.295 trainign loss: 6.7188 avg training loss: 6.8101
batch: [90/3682] batch time: 0.059 trainign loss: 6.8568 avg training loss: 6.8146
batch: [100/3682] batch time: 0.251 trainign loss: 6.8251 avg training loss: 6.8172
batch: [110/3682] batch time: 0.058 trainign loss: 6.8321 avg training loss: 6.8179
batch: [120/3682] batch time: 0.270 trainign loss: 6.8324 avg training loss: 6.8152
batch: [130/3682] batch time: 0.058 trainign loss: 6.7571 avg training loss: 6.8146
batch: [140/3682] batch time: 0.220 trainign loss: 6.8199 avg training loss: 6.8116
batch: [150/3682] batch time: 0.058 trainign loss: 6.6792 avg training loss: 6.8086
batch: [160/3682] batch time: 0.243 trainign loss: 6.8785 avg training loss: 6.7832
batch: [170/3682] batch time: 0.058 trainign loss: 6.8667 avg training loss: 6.7859
batch: [180/3682] batch time: 0.253 trainign loss: 6.1273 avg training loss: 6.7699
batch: [190/3682] batch time: 0.057 trainign loss: 6.8573 avg training loss: 6.7709
batch: [200/3682] batch time: 0.266 trainign loss: 6.7713 avg training loss: 6.7758
batch: [210/3682] batch time: 0.057 trainign loss: 6.8560 avg training loss: 6.7742
batch: [220/3682] batch time: 0.215 trainign loss: 6.8282 avg training loss: 6.7741
batch: [230/3682] batch time: 0.058 trainign loss: 6.6883 avg training loss: 6.7768
batch: [240/3682] batch time: 0.245 trainign loss: 6.7379 avg training loss: 6.7576
batch: [250/3682] batch time: 0.057 trainign loss: 6.5297 avg training loss: 6.7582
batch: [260/3682] batch time: 0.257 trainign loss: 6.8375 avg training loss: 6.7599
batch: [270/3682] batch time: 0.048 trainign loss: 6.9628 avg training loss: 6.7640
batch: [280/3682] batch time: 0.046 trainign loss: 6.8486 avg training loss: 6.7666
batch: [290/3682] batch time: 0.388 trainign loss: 6.9489 avg training loss: 6.7695
batch: [300/3682] batch time: 0.048 trainign loss: 6.4548 avg training loss: 6.7710
batch: [310/3682] batch time: 0.794 trainign loss: 6.9366 avg training loss: 6.7682
batch: [320/3682] batch time: 0.045 trainign loss: 6.2989 avg training loss: 6.7655
batch: [330/3682] batch time: 0.047 trainign loss: 6.9861 avg training loss: 6.7439
batch: [340/3682] batch time: 0.046 trainign loss: 5.5154 avg training loss: 6.7328
batch: [350/3682] batch time: 0.056 trainign loss: 2.9386 avg training loss: 6.6605
batch: [360/3682] batch time: 0.047 trainign loss: 8.6378 avg training loss: 6.5821
batch: [370/3682] batch time: 0.941 trainign loss: 7.2999 avg training loss: 6.6292
batch: [380/3682] batch time: 0.046 trainign loss: 6.9546 avg training loss: 6.6450
batch: [390/3682] batch time: 2.111 trainign loss: 6.7276 avg training loss: 6.6500
batch: [400/3682] batch time: 0.045 trainign loss: 6.8955 avg training loss: 6.6554
batch: [410/3682] batch time: 2.184 trainign loss: 6.8347 avg training loss: 6.6601
batch: [420/3682] batch time: 0.057 trainign loss: 6.8581 avg training loss: 6.6629
batch: [430/3682] batch time: 1.594 trainign loss: 6.1206 avg training loss: 6.6544
batch: [440/3682] batch time: 0.046 trainign loss: 6.6242 avg training loss: 6.6558
batch: [450/3682] batch time: 1.031 trainign loss: 6.5755 avg training loss: 6.6558
batch: [460/3682] batch time: 0.056 trainign loss: 6.7819 avg training loss: 6.6527
batch: [470/3682] batch time: 1.865 trainign loss: 6.6038 avg training loss: 6.6414
batch: [480/3682] batch time: 0.047 trainign loss: 6.7385 avg training loss: 6.6404
batch: [490/3682] batch time: 1.626 trainign loss: 5.2054 avg training loss: 6.6345
batch: [500/3682] batch time: 0.046 trainign loss: 6.5815 avg training loss: 6.6319
batch: [510/3682] batch time: 1.052 trainign loss: 6.8964 avg training loss: 6.6279
batch: [520/3682] batch time: 0.046 trainign loss: 6.9062 avg training loss: 6.6232
batch: [530/3682] batch time: 0.464 trainign loss: 6.5856 avg training loss: 6.6256
batch: [540/3682] batch time: 0.047 trainign loss: 6.0011 avg training loss: 6.6249
batch: [550/3682] batch time: 0.047 trainign loss: 6.6282 avg training loss: 6.6258
batch: [560/3682] batch time: 0.056 trainign loss: 7.0578 avg training loss: 6.6207
batch: [570/3682] batch time: 0.370 trainign loss: 6.6364 avg training loss: 6.6220
batch: [580/3682] batch time: 0.046 trainign loss: 6.5567 avg training loss: 6.6220
batch: [590/3682] batch time: 0.043 trainign loss: 6.8586 avg training loss: 6.6238
batch: [600/3682] batch time: 0.046 trainign loss: 6.6597 avg training loss: 6.6254
batch: [610/3682] batch time: 0.057 trainign loss: 6.7560 avg training loss: 6.6249
batch: [620/3682] batch time: 1.453 trainign loss: 6.8881 avg training loss: 6.6283
batch: [630/3682] batch time: 0.045 trainign loss: 6.7907 avg training loss: 6.6296
batch: [640/3682] batch time: 2.791 trainign loss: 6.4519 avg training loss: 6.6312
batch: [650/3682] batch time: 0.046 trainign loss: 4.4475 avg training loss: 6.6028
batch: [660/3682] batch time: 2.634 trainign loss: 7.0984 avg training loss: 6.6116
batch: [670/3682] batch time: 0.046 trainign loss: 5.4485 avg training loss: 6.6100
batch: [680/3682] batch time: 2.357 trainign loss: 6.0470 avg training loss: 6.5886
batch: [690/3682] batch time: 0.048 trainign loss: 6.6989 avg training loss: 6.5913
batch: [700/3682] batch time: 2.504 trainign loss: 6.9746 avg training loss: 6.5901
batch: [710/3682] batch time: 0.048 trainign loss: 6.9987 avg training loss: 6.5836
batch: [720/3682] batch time: 2.159 trainign loss: 4.8007 avg training loss: 6.5774
batch: [730/3682] batch time: 0.057 trainign loss: 7.3935 avg training loss: 6.5661
batch: [740/3682] batch time: 2.564 trainign loss: 6.3740 avg training loss: 6.5705
batch: [750/3682] batch time: 0.046 trainign loss: 6.6421 avg training loss: 6.5692
batch: [760/3682] batch time: 2.726 trainign loss: 2.2143 avg training loss: 6.5448
batch: [770/3682] batch time: 0.056 trainign loss: 5.7930 avg training loss: 6.5479
batch: [780/3682] batch time: 2.237 trainign loss: 5.9489 avg training loss: 6.5417
batch: [790/3682] batch time: 0.046 trainign loss: 6.7067 avg training loss: 6.5445
batch: [800/3682] batch time: 2.284 trainign loss: 6.7296 avg training loss: 6.5469
batch: [810/3682] batch time: 0.048 trainign loss: 6.7149 avg training loss: 6.5476
batch: [820/3682] batch time: 1.472 trainign loss: 6.5314 avg training loss: 6.5492
batch: [830/3682] batch time: 0.672 trainign loss: 6.0498 avg training loss: 6.5463
batch: [840/3682] batch time: 1.119 trainign loss: 7.8586 avg training loss: 6.5224
batch: [850/3682] batch time: 1.272 trainign loss: 6.5229 avg training loss: 6.5332
batch: [860/3682] batch time: 1.006 trainign loss: 6.1127 avg training loss: 6.5318
batch: [870/3682] batch time: 1.082 trainign loss: 0.5344 avg training loss: 6.4923
batch: [880/3682] batch time: 1.445 trainign loss: 6.6335 avg training loss: 6.5091
batch: [890/3682] batch time: 0.780 trainign loss: 6.7211 avg training loss: 6.5096
batch: [900/3682] batch time: 1.425 trainign loss: 2.9321 avg training loss: 6.4949
batch: [910/3682] batch time: 0.735 trainign loss: 6.0829 avg training loss: 6.4966
batch: [920/3682] batch time: 0.801 trainign loss: 7.1569 avg training loss: 6.4978
batch: [930/3682] batch time: 0.791 trainign loss: 6.8857 avg training loss: 6.5002
batch: [940/3682] batch time: 2.229 trainign loss: 5.9980 avg training loss: 6.5016
batch: [950/3682] batch time: 0.046 trainign loss: 4.8729 avg training loss: 6.4983
batch: [960/3682] batch time: 2.408 trainign loss: 6.3696 avg training loss: 6.4999
batch: [970/3682] batch time: 0.056 trainign loss: 6.9613 avg training loss: 6.4917
batch: [980/3682] batch time: 2.246 trainign loss: 6.4240 avg training loss: 6.4938
batch: [990/3682] batch time: 1.148 trainign loss: 5.6098 avg training loss: 6.4930
batch: [1000/3682] batch time: 1.283 trainign loss: 5.6578 avg training loss: 6.4894
batch: [1010/3682] batch time: 1.078 trainign loss: 6.3184 avg training loss: 6.4917
batch: [1020/3682] batch time: 1.429 trainign loss: 6.9298 avg training loss: 6.4926
batch: [1030/3682] batch time: 2.032 trainign loss: 5.7244 avg training loss: 6.4922
batch: [1040/3682] batch time: 0.393 trainign loss: 6.3335 avg training loss: 6.4909
batch: [1050/3682] batch time: 2.138 trainign loss: 6.6547 avg training loss: 6.4918
batch: [1060/3682] batch time: 1.095 trainign loss: 5.9881 avg training loss: 6.4910
batch: [1070/3682] batch time: 1.494 trainign loss: 6.7343 avg training loss: 6.4929
batch: [1080/3682] batch time: 0.272 trainign loss: 6.5765 avg training loss: 6.4947
batch: [1090/3682] batch time: 2.479 trainign loss: 6.5589 avg training loss: 6.4965
batch: [1100/3682] batch time: 0.057 trainign loss: 6.6634 avg training loss: 6.4980
batch: [1110/3682] batch time: 2.169 trainign loss: 6.9225 avg training loss: 6.4989
batch: [1120/3682] batch time: 0.243 trainign loss: 6.7833 avg training loss: 6.5003
batch: [1130/3682] batch time: 2.024 trainign loss: 6.0953 avg training loss: 6.5010
batch: [1140/3682] batch time: 0.047 trainign loss: 6.7963 avg training loss: 6.5028
batch: [1150/3682] batch time: 2.213 trainign loss: 6.5350 avg training loss: 6.5044
batch: [1160/3682] batch time: 0.475 trainign loss: 6.9240 avg training loss: 6.4994
batch: [1170/3682] batch time: 2.114 trainign loss: 6.4921 avg training loss: 6.4997
batch: [1180/3682] batch time: 1.044 trainign loss: 5.5841 avg training loss: 6.4980
batch: [1190/3682] batch time: 0.583 trainign loss: 6.2573 avg training loss: 6.4994
batch: [1200/3682] batch time: 2.266 trainign loss: 6.1221 avg training loss: 6.5006
batch: [1210/3682] batch time: 0.046 trainign loss: 6.8149 avg training loss: 6.5002
batch: [1220/3682] batch time: 2.120 trainign loss: 5.8469 avg training loss: 6.5007
batch: [1230/3682] batch time: 0.203 trainign loss: 6.4040 avg training loss: 6.5008
batch: [1240/3682] batch time: 1.479 trainign loss: 6.0411 avg training loss: 6.5012
batch: [1250/3682] batch time: 1.016 trainign loss: 6.6678 avg training loss: 6.5005
batch: [1260/3682] batch time: 1.871 trainign loss: 6.5760 avg training loss: 6.4980
batch: [1270/3682] batch time: 0.565 trainign loss: 7.0955 avg training loss: 6.4933
batch: [1280/3682] batch time: 1.538 trainign loss: 6.2751 avg training loss: 6.4946
batch: [1290/3682] batch time: 0.690 trainign loss: 6.3026 avg training loss: 6.4947
batch: [1300/3682] batch time: 2.385 trainign loss: 4.1703 avg training loss: 6.4880
batch: [1310/3682] batch time: 0.047 trainign loss: 7.2245 avg training loss: 6.4900
batch: [1320/3682] batch time: 2.925 trainign loss: 6.5178 avg training loss: 6.4917
batch: [1330/3682] batch time: 0.057 trainign loss: 6.5003 avg training loss: 6.4890
batch: [1340/3682] batch time: 2.214 trainign loss: 5.1066 avg training loss: 6.4888
batch: [1350/3682] batch time: 0.050 trainign loss: 7.0600 avg training loss: 6.4835
batch: [1360/3682] batch time: 2.327 trainign loss: 6.7424 avg training loss: 6.4853
batch: [1370/3682] batch time: 0.046 trainign loss: 6.5212 avg training loss: 6.4855
batch: [1380/3682] batch time: 2.333 trainign loss: 5.4278 avg training loss: 6.4840
batch: [1390/3682] batch time: 0.046 trainign loss: 6.8374 avg training loss: 6.4801
batch: [1400/3682] batch time: 2.274 trainign loss: 6.6307 avg training loss: 6.4769
batch: [1410/3682] batch time: 0.046 trainign loss: 6.8190 avg training loss: 6.4782
batch: [1420/3682] batch time: 2.050 trainign loss: 6.4853 avg training loss: 6.4792
batch: [1430/3682] batch time: 0.046 trainign loss: 6.8376 avg training loss: 6.4808
batch: [1440/3682] batch time: 2.232 trainign loss: 6.7633 avg training loss: 6.4821
batch: [1450/3682] batch time: 0.046 trainign loss: 4.6623 avg training loss: 6.4800
batch: [1460/3682] batch time: 2.529 trainign loss: 0.0378 avg training loss: 6.4463
batch: [1470/3682] batch time: 0.046 trainign loss: 0.0006 avg training loss: 6.4026
batch: [1480/3682] batch time: 2.523 trainign loss: 6.7673 avg training loss: 6.4229
batch: [1490/3682] batch time: 0.046 trainign loss: 6.6570 avg training loss: 6.4236
batch: [1500/3682] batch time: 2.030 trainign loss: 7.0236 avg training loss: 6.4229
batch: [1510/3682] batch time: 0.047 trainign loss: 5.5214 avg training loss: 6.4176
batch: [1520/3682] batch time: 2.455 trainign loss: 6.8734 avg training loss: 6.4211
batch: [1530/3682] batch time: 0.047 trainign loss: 6.2739 avg training loss: 6.4219
batch: [1540/3682] batch time: 2.506 trainign loss: 4.5329 avg training loss: 6.4202
batch: [1550/3682] batch time: 0.046 trainign loss: 5.8136 avg training loss: 6.4213
batch: [1560/3682] batch time: 2.494 trainign loss: 6.9713 avg training loss: 6.4222
batch: [1570/3682] batch time: 0.051 trainign loss: 6.3806 avg training loss: 6.4229
batch: [1580/3682] batch time: 2.468 trainign loss: 6.5464 avg training loss: 6.4249
batch: [1590/3682] batch time: 0.049 trainign loss: 6.5836 avg training loss: 6.4226
batch: [1600/3682] batch time: 2.479 trainign loss: 6.6649 avg training loss: 6.4229
batch: [1610/3682] batch time: 0.046 trainign loss: 6.2923 avg training loss: 6.4211
batch: [1620/3682] batch time: 2.145 trainign loss: 6.5667 avg training loss: 6.4200
batch: [1630/3682] batch time: 0.045 trainign loss: 6.8142 avg training loss: 6.4214
batch: [1640/3682] batch time: 1.484 trainign loss: 6.6617 avg training loss: 6.4216
batch: [1650/3682] batch time: 0.056 trainign loss: 6.4219 avg training loss: 6.4217
batch: [1660/3682] batch time: 0.979 trainign loss: 6.7333 avg training loss: 6.4216
batch: [1670/3682] batch time: 0.046 trainign loss: 6.3107 avg training loss: 6.4219
batch: [1680/3682] batch time: 0.048 trainign loss: 4.7709 avg training loss: 6.4214
batch: [1690/3682] batch time: 0.046 trainign loss: 6.3097 avg training loss: 6.4210
batch: [1700/3682] batch time: 0.057 trainign loss: 5.9625 avg training loss: 6.4212
batch: [1710/3682] batch time: 0.240 trainign loss: 3.3209 avg training loss: 6.4171
batch: [1720/3682] batch time: 0.046 trainign loss: 6.3949 avg training loss: 6.4187
batch: [1730/3682] batch time: 0.048 trainign loss: 6.3632 avg training loss: 6.4175
batch: [1740/3682] batch time: 0.054 trainign loss: 6.9621 avg training loss: 6.4172
batch: [1750/3682] batch time: 0.045 trainign loss: 7.0614 avg training loss: 6.4111
batch: [1760/3682] batch time: 0.755 trainign loss: 6.7805 avg training loss: 6.4130
batch: [1770/3682] batch time: 0.046 trainign loss: 6.4056 avg training loss: 6.4142
batch: [1780/3682] batch time: 0.859 trainign loss: 6.6833 avg training loss: 6.4133
batch: [1790/3682] batch time: 0.428 trainign loss: 6.6777 avg training loss: 6.4138
batch: [1800/3682] batch time: 0.320 trainign loss: 3.8368 avg training loss: 6.4105
batch: [1810/3682] batch time: 0.046 trainign loss: 6.7638 avg training loss: 6.4123
batch: [1820/3682] batch time: 0.046 trainign loss: 6.7563 avg training loss: 6.4123
batch: [1830/3682] batch time: 0.046 trainign loss: 7.0428 avg training loss: 6.4128
batch: [1840/3682] batch time: 0.045 trainign loss: 6.8926 avg training loss: 6.4122
batch: [1850/3682] batch time: 0.046 trainign loss: 6.3003 avg training loss: 6.4129
batch: [1860/3682] batch time: 0.055 trainign loss: 3.8307 avg training loss: 6.4100
batch: [1870/3682] batch time: 0.047 trainign loss: 6.1635 avg training loss: 6.4082
batch: [1880/3682] batch time: 0.057 trainign loss: 6.9912 avg training loss: 6.4061
batch: [1890/3682] batch time: 0.046 trainign loss: 6.9014 avg training loss: 6.4061
batch: [1900/3682] batch time: 0.049 trainign loss: 6.3446 avg training loss: 6.4072
batch: [1910/3682] batch time: 0.051 trainign loss: 6.8815 avg training loss: 6.4009
batch: [1920/3682] batch time: 0.046 trainign loss: 6.4536 avg training loss: 6.4016
batch: [1930/3682] batch time: 0.047 trainign loss: 5.9286 avg training loss: 6.4030
batch: [1940/3682] batch time: 0.047 trainign loss: 6.9466 avg training loss: 6.4056
batch: [1950/3682] batch time: 0.048 trainign loss: 6.5763 avg training loss: 6.4072
batch: [1960/3682] batch time: 0.050 trainign loss: 6.5926 avg training loss: 6.4076
batch: [1970/3682] batch time: 0.045 trainign loss: 0.3459 avg training loss: 6.3908
batch: [1980/3682] batch time: 0.043 trainign loss: 10.3020 avg training loss: 6.3878
batch: [1990/3682] batch time: 0.046 trainign loss: 6.3419 avg training loss: 6.3913
batch: [2000/3682] batch time: 0.044 trainign loss: 7.1889 avg training loss: 6.3916
batch: [2010/3682] batch time: 0.045 trainign loss: 7.0788 avg training loss: 6.3893
batch: [2020/3682] batch time: 0.056 trainign loss: 6.3602 avg training loss: 6.3897
batch: [2030/3682] batch time: 0.046 trainign loss: 7.1696 avg training loss: 6.3909
batch: [2040/3682] batch time: 0.043 trainign loss: 7.1750 avg training loss: 6.3881
batch: [2050/3682] batch time: 0.046 trainign loss: 6.9671 avg training loss: 6.3900
batch: [2060/3682] batch time: 0.046 trainign loss: 7.1883 avg training loss: 6.3888
batch: [2070/3682] batch time: 0.047 trainign loss: 6.8454 avg training loss: 6.3906
batch: [2080/3682] batch time: 0.046 trainign loss: 6.3702 avg training loss: 6.3919
batch: [2090/3682] batch time: 1.094 trainign loss: 5.5940 avg training loss: 6.3927
batch: [2100/3682] batch time: 0.056 trainign loss: 5.7441 avg training loss: 6.3934
batch: [2110/3682] batch time: 0.127 trainign loss: 11.4120 avg training loss: 6.3797
batch: [2120/3682] batch time: 0.046 trainign loss: 6.3010 avg training loss: 6.3854
batch: [2130/3682] batch time: 0.575 trainign loss: 6.4987 avg training loss: 6.3856
batch: [2140/3682] batch time: 0.054 trainign loss: 7.0828 avg training loss: 6.3885
batch: [2150/3682] batch time: 1.169 trainign loss: 5.1907 avg training loss: 6.3880
batch: [2160/3682] batch time: 0.046 trainign loss: 6.7235 avg training loss: 6.3902
batch: [2170/3682] batch time: 0.046 trainign loss: 6.5390 avg training loss: 6.3909
batch: [2180/3682] batch time: 0.044 trainign loss: 7.1617 avg training loss: 6.3909
batch: [2190/3682] batch time: 0.049 trainign loss: 6.5222 avg training loss: 6.3924
batch: [2200/3682] batch time: 0.049 trainign loss: 5.4132 avg training loss: 6.3914
batch: [2210/3682] batch time: 0.047 trainign loss: 6.9924 avg training loss: 6.3910
batch: [2220/3682] batch time: 0.046 trainign loss: 6.7875 avg training loss: 6.3908
batch: [2230/3682] batch time: 0.057 trainign loss: 3.3660 avg training loss: 6.3882
batch: [2240/3682] batch time: 0.057 trainign loss: 0.0058 avg training loss: 6.3621
batch: [2250/3682] batch time: 0.057 trainign loss: 0.0011 avg training loss: 6.3338
batch: [2260/3682] batch time: 0.043 trainign loss: 8.2261 avg training loss: 6.3187
batch: [2270/3682] batch time: 0.046 trainign loss: 7.0560 avg training loss: 6.3204
batch: [2280/3682] batch time: 0.047 trainign loss: 6.9322 avg training loss: 6.3218
batch: [2290/3682] batch time: 0.046 trainign loss: 6.0609 avg training loss: 6.3233
batch: [2300/3682] batch time: 0.043 trainign loss: 6.9363 avg training loss: 6.3239
batch: [2310/3682] batch time: 0.046 trainign loss: 6.3938 avg training loss: 6.3254
batch: [2320/3682] batch time: 0.046 trainign loss: 6.7055 avg training loss: 6.3271
batch: [2330/3682] batch time: 0.046 trainign loss: 7.1137 avg training loss: 6.3293
batch: [2340/3682] batch time: 0.047 trainign loss: 5.3023 avg training loss: 6.3296
batch: [2350/3682] batch time: 0.048 trainign loss: 6.8749 avg training loss: 6.3290
batch: [2360/3682] batch time: 0.047 trainign loss: 6.8716 avg training loss: 6.3289
batch: [2370/3682] batch time: 0.049 trainign loss: 7.2224 avg training loss: 6.3302
batch: [2380/3682] batch time: 0.055 trainign loss: 5.9974 avg training loss: 6.3315
batch: [2390/3682] batch time: 0.486 trainign loss: 6.1466 avg training loss: 6.3325
batch: [2400/3682] batch time: 0.047 trainign loss: 5.9038 avg training loss: 6.3338
batch: [2410/3682] batch time: 0.593 trainign loss: 6.0506 avg training loss: 6.3347
batch: [2420/3682] batch time: 0.056 trainign loss: 5.4168 avg training loss: 6.3355
batch: [2430/3682] batch time: 0.201 trainign loss: 7.0319 avg training loss: 6.3342
batch: [2440/3682] batch time: 0.055 trainign loss: 5.3669 avg training loss: 6.3357
batch: [2450/3682] batch time: 0.047 trainign loss: 7.3179 avg training loss: 6.3360
batch: [2460/3682] batch time: 0.058 trainign loss: 5.2074 avg training loss: 6.3360
batch: [2470/3682] batch time: 0.058 trainign loss: 0.0255 avg training loss: 6.3167
batch: [2480/3682] batch time: 0.058 trainign loss: 0.0002 avg training loss: 6.2912
batch: [2490/3682] batch time: 0.057 trainign loss: 0.0001 avg training loss: 6.2660
batch: [2500/3682] batch time: 0.057 trainign loss: 0.0000 avg training loss: 6.2409
batch: [2510/3682] batch time: 0.057 trainign loss: 0.0000 avg training loss: 6.2161
batch: [2520/3682] batch time: 0.057 trainign loss: 0.0000 avg training loss: 6.1914
batch: [2530/3682] batch time: 0.057 trainign loss: 0.0000 avg training loss: 6.1669
batch: [2540/3682] batch time: 0.057 trainign loss: 0.0000 avg training loss: 6.1427
batch: [2550/3682] batch time: 0.045 trainign loss: 0.0000 avg training loss: 6.1186
batch: [2560/3682] batch time: 0.051 trainign loss: 7.6476 avg training loss: 6.1247
batch: [2570/3682] batch time: 0.565 trainign loss: 5.1133 avg training loss: 6.1258
batch: [2580/3682] batch time: 0.046 trainign loss: 8.4807 avg training loss: 6.1104
batch: [2590/3682] batch time: 0.794 trainign loss: 6.5198 avg training loss: 6.1170
batch: [2600/3682] batch time: 0.354 trainign loss: 6.9487 avg training loss: 6.1192
batch: [2610/3682] batch time: 0.047 trainign loss: 5.3163 avg training loss: 6.1202
batch: [2620/3682] batch time: 1.652 trainign loss: 7.0833 avg training loss: 6.1231
batch: [2630/3682] batch time: 0.056 trainign loss: 7.0072 avg training loss: 6.1246
batch: [2640/3682] batch time: 2.268 trainign loss: 8.0570 avg training loss: 6.1231
batch: [2650/3682] batch time: 0.045 trainign loss: 6.2073 avg training loss: 6.1237
batch: [2660/3682] batch time: 2.121 trainign loss: 7.0011 avg training loss: 6.1265
batch: [2670/3682] batch time: 0.047 trainign loss: 6.2494 avg training loss: 6.1278
batch: [2680/3682] batch time: 1.245 trainign loss: 7.0505 avg training loss: 6.1297
batch: [2690/3682] batch time: 0.702 trainign loss: 6.8862 avg training loss: 6.1318
batch: [2700/3682] batch time: 1.040 trainign loss: 6.2142 avg training loss: 6.1326
batch: [2710/3682] batch time: 0.302 trainign loss: 3.9872 avg training loss: 6.1319
batch: [2720/3682] batch time: 1.332 trainign loss: 6.9697 avg training loss: 6.1332
batch: [2730/3682] batch time: 1.164 trainign loss: 5.1877 avg training loss: 6.1346
batch: [2740/3682] batch time: 1.215 trainign loss: 9.0634 avg training loss: 6.1214
batch: [2750/3682] batch time: 0.633 trainign loss: 6.6442 avg training loss: 6.1292
batch: [2760/3682] batch time: 2.664 trainign loss: 7.3538 avg training loss: 6.1308
batch: [2770/3682] batch time: 0.190 trainign loss: 7.3572 avg training loss: 6.1305
batch: [2780/3682] batch time: 1.701 trainign loss: 6.5494 avg training loss: 6.1342
batch: [2790/3682] batch time: 0.239 trainign loss: 6.6224 avg training loss: 6.1361
batch: [2800/3682] batch time: 2.018 trainign loss: 6.2279 avg training loss: 6.1385
batch: [2810/3682] batch time: 0.487 trainign loss: 6.8389 avg training loss: 6.1406
batch: [2820/3682] batch time: 2.088 trainign loss: 6.6372 avg training loss: 6.1431
batch: [2830/3682] batch time: 0.349 trainign loss: 5.7331 avg training loss: 6.1440
batch: [2840/3682] batch time: 1.560 trainign loss: 7.3344 avg training loss: 6.1463
batch: [2850/3682] batch time: 0.223 trainign loss: 6.4813 avg training loss: 6.1485
batch: [2860/3682] batch time: 2.349 trainign loss: 7.3133 avg training loss: 6.1495
batch: [2870/3682] batch time: 0.045 trainign loss: 6.6849 avg training loss: 6.1521
batch: [2880/3682] batch time: 2.466 trainign loss: 0.1117 avg training loss: 6.1406
batch: [2890/3682] batch time: 0.047 trainign loss: 7.5201 avg training loss: 6.1491
batch: [2900/3682] batch time: 2.030 trainign loss: 5.6894 avg training loss: 6.1504
batch: [2910/3682] batch time: 0.049 trainign loss: 7.1259 avg training loss: 6.1519
batch: [2920/3682] batch time: 2.572 trainign loss: 4.5331 avg training loss: 6.1477
batch: [2930/3682] batch time: 0.045 trainign loss: 6.7881 avg training loss: 6.1518
batch: [2940/3682] batch time: 2.330 trainign loss: 7.0040 avg training loss: 6.1520
batch: [2950/3682] batch time: 0.055 trainign loss: 6.6766 avg training loss: 6.1524
batch: [2960/3682] batch time: 2.007 trainign loss: 6.8784 avg training loss: 6.1543
batch: [2970/3682] batch time: 0.048 trainign loss: 6.5813 avg training loss: 6.1561
batch: [2980/3682] batch time: 1.666 trainign loss: 6.6045 avg training loss: 6.1579
batch: [2990/3682] batch time: 0.057 trainign loss: 1.3963 avg training loss: 6.1531
batch: [3000/3682] batch time: 0.474 trainign loss: 7.1697 avg training loss: 6.1532
batch: [3010/3682] batch time: 0.049 trainign loss: 6.7989 avg training loss: 6.1553
batch: [3020/3682] batch time: 0.370 trainign loss: 7.4732 avg training loss: 6.1576
batch: [3030/3682] batch time: 1.101 trainign loss: 6.3654 avg training loss: 6.1602
batch: [3040/3682] batch time: 0.048 trainign loss: 1.6655 avg training loss: 6.1557
batch: [3050/3682] batch time: 1.519 trainign loss: 5.6604 avg training loss: 6.1541
batch: [3060/3682] batch time: 0.058 trainign loss: 6.5367 avg training loss: 6.1568
batch: [3070/3682] batch time: 1.579 trainign loss: 6.9884 avg training loss: 6.1584
batch: [3080/3682] batch time: 0.046 trainign loss: 7.1272 avg training loss: 6.1601
batch: [3090/3682] batch time: 1.919 trainign loss: 5.9391 avg training loss: 6.1615
batch: [3100/3682] batch time: 0.057 trainign loss: 7.4738 avg training loss: 6.1617
batch: [3110/3682] batch time: 1.372 trainign loss: 6.2500 avg training loss: 6.1633
batch: [3120/3682] batch time: 0.046 trainign loss: 7.3328 avg training loss: 6.1647
batch: [3130/3682] batch time: 1.081 trainign loss: 7.8646 avg training loss: 6.1611
batch: [3140/3682] batch time: 0.057 trainign loss: 7.2689 avg training loss: 6.1647
batch: [3150/3682] batch time: 1.349 trainign loss: 7.2073 avg training loss: 6.1662
batch: [3160/3682] batch time: 0.048 trainign loss: 6.1273 avg training loss: 6.1681
batch: [3170/3682] batch time: 1.080 trainign loss: 6.9368 avg training loss: 6.1698
batch: [3180/3682] batch time: 0.056 trainign loss: 3.7753 avg training loss: 6.1696
batch: [3190/3682] batch time: 1.228 trainign loss: 10.9142 avg training loss: 6.1669
batch: [3200/3682] batch time: 0.046 trainign loss: 3.4588 avg training loss: 6.1674
batch: [3210/3682] batch time: 0.044 trainign loss: 10.0670 avg training loss: 6.1640
batch: [3220/3682] batch time: 0.050 trainign loss: 7.3565 avg training loss: 6.1672
batch: [3230/3682] batch time: 0.043 trainign loss: 6.8850 avg training loss: 6.1683
batch: [3240/3682] batch time: 0.057 trainign loss: 0.0926 avg training loss: 6.1584
batch: [3250/3682] batch time: 0.051 trainign loss: 6.7624 avg training loss: 6.1566
batch: [3260/3682] batch time: 0.046 trainign loss: 7.2493 avg training loss: 6.1582
batch: [3270/3682] batch time: 0.043 trainign loss: 6.9427 avg training loss: 6.1602
batch: [3280/3682] batch time: 0.046 trainign loss: 7.5113 avg training loss: 6.1617
batch: [3290/3682] batch time: 0.056 trainign loss: 4.5380 avg training loss: 6.1619
batch: [3300/3682] batch time: 0.047 trainign loss: 6.3278 avg training loss: 6.1641
batch: [3310/3682] batch time: 0.047 trainign loss: 6.6640 avg training loss: 6.1659
batch: [3320/3682] batch time: 0.462 trainign loss: 7.2185 avg training loss: 6.1679
batch: [3330/3682] batch time: 0.048 trainign loss: 7.3337 avg training loss: 6.1700
batch: [3340/3682] batch time: 0.047 trainign loss: 6.7512 avg training loss: 6.1714
batch: [3350/3682] batch time: 0.046 trainign loss: 6.9107 avg training loss: 6.1734
batch: [3360/3682] batch time: 0.046 trainign loss: 7.4567 avg training loss: 6.1764
batch: [3370/3682] batch time: 0.045 trainign loss: 7.3551 avg training loss: 6.1783
batch: [3380/3682] batch time: 0.046 trainign loss: 2.6947 avg training loss: 6.1764
batch: [3390/3682] batch time: 0.046 trainign loss: 7.9231 avg training loss: 6.1782
batch: [3400/3682] batch time: 0.114 trainign loss: 6.8006 avg training loss: 6.1795
batch: [3410/3682] batch time: 0.047 trainign loss: 7.6657 avg training loss: 6.1778
batch: [3420/3682] batch time: 1.185 trainign loss: 7.0801 avg training loss: 6.1760
batch: [3430/3682] batch time: 0.046 trainign loss: 6.4916 avg training loss: 6.1773
batch: [3440/3682] batch time: 1.099 trainign loss: 2.8534 avg training loss: 6.1760
batch: [3450/3682] batch time: 0.046 trainign loss: 7.0497 avg training loss: 6.1783
batch: [3460/3682] batch time: 2.070 trainign loss: 6.2339 avg training loss: 6.1791
batch: [3470/3682] batch time: 0.048 trainign loss: 5.4841 avg training loss: 6.1775
batch: [3480/3682] batch time: 2.220 trainign loss: 6.5669 avg training loss: 6.1808
batch: [3490/3682] batch time: 0.122 trainign loss: 7.0552 avg training loss: 6.1825
batch: [3500/3682] batch time: 2.135 trainign loss: 4.9203 avg training loss: 6.1834
batch: [3510/3682] batch time: 0.093 trainign loss: 6.8705 avg training loss: 6.1836
batch: [3520/3682] batch time: 0.869 trainign loss: 6.4931 avg training loss: 6.1844
batch: [3530/3682] batch time: 0.048 trainign loss: 7.1992 avg training loss: 6.1865
batch: [3540/3682] batch time: 0.382 trainign loss: 7.2788 avg training loss: 6.1871
batch: [3550/3682] batch time: 0.048 trainign loss: 6.2976 avg training loss: 6.1893
batch: [3560/3682] batch time: 0.048 trainign loss: 0.1290 avg training loss: 6.1791
batch: [3570/3682] batch time: 0.056 trainign loss: 7.0312 avg training loss: 6.1837
batch: [3580/3682] batch time: 0.449 trainign loss: 6.8086 avg training loss: 6.1841
batch: [3590/3682] batch time: 0.171 trainign loss: 7.2739 avg training loss: 6.1868
batch: [3600/3682] batch time: 1.539 trainign loss: 6.8386 avg training loss: 6.1890
batch: [3610/3682] batch time: 0.047 trainign loss: 6.8595 avg training loss: 6.1913
batch: [3620/3682] batch time: 0.152 trainign loss: 6.5953 avg training loss: 6.1915
batch: [3630/3682] batch time: 0.046 trainign loss: 6.9994 avg training loss: 6.1931
batch: [3640/3682] batch time: 0.056 trainign loss: 7.2084 avg training loss: 6.1956
batch: [3650/3682] batch time: 1.589 trainign loss: 7.3415 avg training loss: 6.1976
batch: [3660/3682] batch time: 0.046 trainign loss: 7.0675 avg training loss: 6.1999
batch: [3670/3682] batch time: 1.218 trainign loss: 7.5259 avg training loss: 6.2006
batch: [3680/3682] batch time: 0.046 trainign loss: 6.7422 avg training loss: 6.2027
Epoch: 2
----------------------------------------------------------------------
batch: [0/3682] batch time: 0.599 trainign loss: 6.8166 avg training loss: 6.2032
batch: [10/3682] batch time: 0.057 trainign loss: 6.4036 avg training loss: 6.2008
batch: [20/3682] batch time: 0.220 trainign loss: 6.0216 avg training loss: 6.2016
batch: [30/3682] batch time: 0.057 trainign loss: 3.5712 avg training loss: 6.1995
batch: [40/3682] batch time: 0.237 trainign loss: 11.4356 avg training loss: 6.1908
batch: [50/3682] batch time: 0.057 trainign loss: 6.4206 avg training loss: 6.1969
batch: [60/3682] batch time: 0.240 trainign loss: 5.9748 avg training loss: 6.1971
batch: [70/3682] batch time: 0.057 trainign loss: 6.4660 avg training loss: 6.1975
batch: [80/3682] batch time: 0.294 trainign loss: 6.2388 avg training loss: 6.1974
batch: [90/3682] batch time: 0.057 trainign loss: 5.3241 avg training loss: 6.1964
batch: [100/3682] batch time: 0.256 trainign loss: 5.4328 avg training loss: 6.1962
batch: [110/3682] batch time: 0.057 trainign loss: 6.0346 avg training loss: 6.1957
batch: [120/3682] batch time: 0.269 trainign loss: 6.3656 avg training loss: 6.1948
batch: [130/3682] batch time: 0.057 trainign loss: 6.1967 avg training loss: 6.1958
batch: [140/3682] batch time: 0.226 trainign loss: 6.1887 avg training loss: 6.1957
batch: [150/3682] batch time: 0.057 trainign loss: 4.0257 avg training loss: 6.1940
batch: [160/3682] batch time: 0.246 trainign loss: 11.2582 avg training loss: 6.1847
batch: [170/3682] batch time: 0.057 trainign loss: 6.5809 avg training loss: 6.1887
batch: [180/3682] batch time: 0.261 trainign loss: 0.2379 avg training loss: 6.1809
batch: [190/3682] batch time: 0.057 trainign loss: 6.3067 avg training loss: 6.1858
batch: [200/3682] batch time: 0.268 trainign loss: 6.0216 avg training loss: 6.1857
batch: [210/3682] batch time: 0.058 trainign loss: 6.4186 avg training loss: 6.1849
batch: [220/3682] batch time: 0.215 trainign loss: 6.4676 avg training loss: 6.1850
batch: [230/3682] batch time: 0.057 trainign loss: 4.7518 avg training loss: 6.1846
batch: [240/3682] batch time: 0.250 trainign loss: 9.4651 avg training loss: 6.1787
batch: [250/3682] batch time: 0.057 trainign loss: 4.4287 avg training loss: 6.1799
batch: [260/3682] batch time: 0.250 trainign loss: 5.9009 avg training loss: 6.1793
batch: [270/3682] batch time: 0.057 trainign loss: 6.2798 avg training loss: 6.1789
batch: [280/3682] batch time: 0.233 trainign loss: 6.0003 avg training loss: 6.1780
batch: [290/3682] batch time: 0.057 trainign loss: 6.1021 avg training loss: 6.1775
batch: [300/3682] batch time: 0.240 trainign loss: 3.5064 avg training loss: 6.1757
batch: [310/3682] batch time: 0.057 trainign loss: 6.2594 avg training loss: 6.1742
batch: [320/3682] batch time: 0.263 trainign loss: 3.3199 avg training loss: 6.1712
batch: [330/3682] batch time: 0.056 trainign loss: 9.9578 avg training loss: 6.1652
batch: [340/3682] batch time: 0.256 trainign loss: 0.6718 avg training loss: 6.1601
batch: [350/3682] batch time: 0.057 trainign loss: 0.0039 avg training loss: 6.1450
batch: [360/3682] batch time: 0.289 trainign loss: 13.0242 avg training loss: 6.1408
batch: [370/3682] batch time: 0.057 trainign loss: 5.2716 avg training loss: 6.1427
batch: [380/3682] batch time: 0.269 trainign loss: 6.0150 avg training loss: 6.1429
batch: [390/3682] batch time: 0.057 trainign loss: 5.2251 avg training loss: 6.1421
batch: [400/3682] batch time: 0.288 trainign loss: 5.8711 avg training loss: 6.1417
batch: [410/3682] batch time: 0.057 trainign loss: 6.2233 avg training loss: 6.1412
batch: [420/3682] batch time: 0.281 trainign loss: 5.9817 avg training loss: 6.1404
batch: [430/3682] batch time: 0.057 trainign loss: 4.1981 avg training loss: 6.1347
batch: [440/3682] batch time: 0.240 trainign loss: 5.9669 avg training loss: 6.1355
batch: [450/3682] batch time: 0.057 trainign loss: 5.8690 avg training loss: 6.1351
batch: [460/3682] batch time: 0.272 trainign loss: 6.0235 avg training loss: 6.1338
batch: [470/3682] batch time: 0.057 trainign loss: 6.0483 avg training loss: 6.1311
batch: [480/3682] batch time: 0.230 trainign loss: 6.2518 avg training loss: 6.1308
batch: [490/3682] batch time: 0.057 trainign loss: 3.7205 avg training loss: 6.1292
batch: [500/3682] batch time: 0.251 trainign loss: 5.7266 avg training loss: 6.1289
batch: [510/3682] batch time: 0.057 trainign loss: 5.9846 avg training loss: 6.1276
batch: [520/3682] batch time: 0.239 trainign loss: 6.2742 avg training loss: 6.1255
batch: [530/3682] batch time: 0.057 trainign loss: 5.5451 avg training loss: 6.1252
batch: [540/3682] batch time: 0.258 trainign loss: 4.1246 avg training loss: 6.1238
batch: [550/3682] batch time: 0.057 trainign loss: 5.4112 avg training loss: 6.1233
batch: [560/3682] batch time: 0.253 trainign loss: 6.0404 avg training loss: 6.1206
batch: [570/3682] batch time: 0.057 trainign loss: 5.2866 avg training loss: 6.1195
batch: [580/3682] batch time: 0.262 trainign loss: 5.5693 avg training loss: 6.1181
batch: [590/3682] batch time: 0.057 trainign loss: 6.2427 avg training loss: 6.1179
batch: [600/3682] batch time: 0.266 trainign loss: 5.1856 avg training loss: 6.1163
batch: [610/3682] batch time: 0.057 trainign loss: 6.0455 avg training loss: 6.1146
batch: [620/3682] batch time: 0.259 trainign loss: 6.0670 avg training loss: 6.1144
batch: [630/3682] batch time: 0.056 trainign loss: 5.8265 avg training loss: 6.1134
batch: [640/3682] batch time: 0.268 trainign loss: 4.5554 avg training loss: 6.1123
batch: [650/3682] batch time: 0.057 trainign loss: 3.7592 avg training loss: 6.1029
batch: [660/3682] batch time: 0.255 trainign loss: 6.0681 avg training loss: 6.1057
batch: [670/3682] batch time: 0.056 trainign loss: 4.0571 avg training loss: 6.1039
batch: [680/3682] batch time: 0.253 trainign loss: 5.6174 avg training loss: 6.0994
batch: [690/3682] batch time: 0.056 trainign loss: 6.0208 avg training loss: 6.0992
batch: [700/3682] batch time: 0.252 trainign loss: 6.1254 avg training loss: 6.0984
batch: [710/3682] batch time: 0.057 trainign loss: 6.0320 avg training loss: 6.0960
batch: [720/3682] batch time: 0.243 trainign loss: 3.4027 avg training loss: 6.0935
batch: [730/3682] batch time: 0.056 trainign loss: 6.9292 avg training loss: 6.0910
batch: [740/3682] batch time: 0.234 trainign loss: 5.5441 avg training loss: 6.0907
batch: [750/3682] batch time: 0.057 trainign loss: 5.5025 avg training loss: 6.0893
batch: [760/3682] batch time: 0.271 trainign loss: 0.7809 avg training loss: 6.0830
batch: [770/3682] batch time: 0.057 trainign loss: 2.8074 avg training loss: 6.0811
batch: [780/3682] batch time: 0.270 trainign loss: 4.3285 avg training loss: 6.0781
batch: [790/3682] batch time: 0.057 trainign loss: 6.0449 avg training loss: 6.0774
batch: [800/3682] batch time: 0.247 trainign loss: 5.4013 avg training loss: 6.0769
batch: [810/3682] batch time: 0.058 trainign loss: 5.6207 avg training loss: 6.0750
batch: [820/3682] batch time: 0.237 trainign loss: 4.9634 avg training loss: 6.0734
batch: [830/3682] batch time: 0.057 trainign loss: 5.0471 avg training loss: 6.0711
batch: [840/3682] batch time: 0.254 trainign loss: 7.9656 avg training loss: 6.0652
batch: [850/3682] batch time: 0.057 trainign loss: 5.9335 avg training loss: 6.0667
batch: [860/3682] batch time: 0.251 trainign loss: 5.4231 avg training loss: 6.0661
batch: [870/3682] batch time: 0.057 trainign loss: 0.5365 avg training loss: 6.0589
batch: [880/3682] batch time: 0.240 trainign loss: 5.5067 avg training loss: 6.0604
batch: [890/3682] batch time: 0.057 trainign loss: 5.7833 avg training loss: 6.0595
batch: [900/3682] batch time: 0.247 trainign loss: 1.5184 avg training loss: 6.0546
batch: [910/3682] batch time: 0.057 trainign loss: 4.9257 avg training loss: 6.0537
batch: [920/3682] batch time: 0.270 trainign loss: 6.1996 avg training loss: 6.0526
batch: [930/3682] batch time: 0.057 trainign loss: 6.3190 avg training loss: 6.0526
batch: [940/3682] batch time: 0.251 trainign loss: 5.1976 avg training loss: 6.0524
batch: [950/3682] batch time: 0.057 trainign loss: 3.7837 avg training loss: 6.0505
batch: [960/3682] batch time: 0.332 trainign loss: 5.0189 avg training loss: 6.0496
batch: [970/3682] batch time: 0.057 trainign loss: 6.3264 avg training loss: 6.0468
batch: [980/3682] batch time: 0.294 trainign loss: 5.8310 avg training loss: 6.0470
batch: [990/3682] batch time: 0.057 trainign loss: 5.3282 avg training loss: 6.0467
batch: [1000/3682] batch time: 0.250 trainign loss: 4.8058 avg training loss: 6.0453
batch: [1010/3682] batch time: 0.057 trainign loss: 4.5715 avg training loss: 6.0433
batch: [1020/3682] batch time: 0.251 trainign loss: 5.8925 avg training loss: 6.0417
batch: [1030/3682] batch time: 0.057 trainign loss: 4.1991 avg training loss: 6.0401
batch: [1040/3682] batch time: 0.250 trainign loss: 5.0976 avg training loss: 6.0381
batch: [1050/3682] batch time: 0.057 trainign loss: 5.5940 avg training loss: 6.0367
batch: [1060/3682] batch time: 0.280 trainign loss: 4.9085 avg training loss: 6.0349
batch: [1070/3682] batch time: 0.057 trainign loss: 5.9288 avg training loss: 6.0345
batch: [1080/3682] batch time: 0.241 trainign loss: 5.3323 avg training loss: 6.0333
batch: [1090/3682] batch time: 0.057 trainign loss: 5.1533 avg training loss: 6.0317
batch: [1100/3682] batch time: 0.243 trainign loss: 4.7428 avg training loss: 6.0295
batch: [1110/3682] batch time: 0.057 trainign loss: 5.2333 avg training loss: 6.0273
batch: [1120/3682] batch time: 0.206 trainign loss: 5.7732 avg training loss: 6.0252
batch: [1130/3682] batch time: 0.057 trainign loss: 4.5159 avg training loss: 6.0243
batch: [1140/3682] batch time: 0.220 trainign loss: 5.5520 avg training loss: 6.0236
batch: [1150/3682] batch time: 0.057 trainign loss: 5.4245 avg training loss: 6.0226
batch: [1160/3682] batch time: 0.265 trainign loss: 5.5128 avg training loss: 6.0185
batch: [1170/3682] batch time: 0.057 trainign loss: 5.1698 avg training loss: 6.0172
batch: [1180/3682] batch time: 0.242 trainign loss: 4.5212 avg training loss: 6.0154
batch: [1190/3682] batch time: 0.057 trainign loss: 4.9543 avg training loss: 6.0145
batch: [1200/3682] batch time: 0.245 trainign loss: 4.5531 avg training loss: 6.0132
batch: [1210/3682] batch time: 0.057 trainign loss: 5.7405 avg training loss: 6.0116
batch: [1220/3682] batch time: 0.226 trainign loss: 4.5127 avg training loss: 6.0105
batch: [1230/3682] batch time: 0.057 trainign loss: 5.2400 avg training loss: 6.0092
batch: [1240/3682] batch time: 0.252 trainign loss: 4.3766 avg training loss: 6.0074
batch: [1250/3682] batch time: 0.057 trainign loss: 5.0060 avg training loss: 6.0049
batch: [1260/3682] batch time: 0.297 trainign loss: 4.2417 avg training loss: 6.0009
batch: [1270/3682] batch time: 0.057 trainign loss: 5.9133 avg training loss: 5.9961
batch: [1280/3682] batch time: 0.233 trainign loss: 5.5615 avg training loss: 5.9963
batch: [1290/3682] batch time: 0.057 trainign loss: 5.0576 avg training loss: 5.9956
batch: [1300/3682] batch time: 0.247 trainign loss: 2.1025 avg training loss: 5.9913
batch: [1310/3682] batch time: 0.057 trainign loss: 5.6584 avg training loss: 5.9892
batch: [1320/3682] batch time: 0.306 trainign loss: 5.2782 avg training loss: 5.9885
batch: [1330/3682] batch time: 0.057 trainign loss: 5.6371 avg training loss: 5.9864
batch: [1340/3682] batch time: 0.268 trainign loss: 3.8605 avg training loss: 5.9850
batch: [1350/3682] batch time: 0.057 trainign loss: 5.8380 avg training loss: 5.9824
batch: [1360/3682] batch time: 0.269 trainign loss: 6.3239 avg training loss: 5.9826
batch: [1370/3682] batch time: 0.057 trainign loss: 5.7647 avg training loss: 5.9828
batch: [1380/3682] batch time: 0.237 trainign loss: 3.5312 avg training loss: 5.9804
batch: [1390/3682] batch time: 0.057 trainign loss: 4.5970 avg training loss: 5.9771
batch: [1400/3682] batch time: 0.243 trainign loss: 6.2349 avg training loss: 5.9747
batch: [1410/3682] batch time: 0.057 trainign loss: 6.0385 avg training loss: 5.9748
batch: [1420/3682] batch time: 0.239 trainign loss: 5.2943 avg training loss: 5.9742
batch: [1430/3682] batch time: 0.057 trainign loss: 4.9227 avg training loss: 5.9728
batch: [1440/3682] batch time: 0.254 trainign loss: 5.7876 avg training loss: 5.9712
batch: [1450/3682] batch time: 0.057 trainign loss: 1.9544 avg training loss: 5.9680
batch: [1460/3682] batch time: 0.267 trainign loss: 0.0044 avg training loss: 5.9569
batch: [1470/3682] batch time: 0.057 trainign loss: 0.0002 avg training loss: 5.9454
batch: [1480/3682] batch time: 0.255 trainign loss: 6.7590 avg training loss: 5.9519
batch: [1490/3682] batch time: 0.057 trainign loss: 6.3165 avg training loss: 5.9529
batch: [1500/3682] batch time: 0.258 trainign loss: 5.9439 avg training loss: 5.9525
batch: [1510/3682] batch time: 0.057 trainign loss: 4.6172 avg training loss: 5.9495
batch: [1520/3682] batch time: 0.277 trainign loss: 6.1319 avg training loss: 5.9497
batch: [1530/3682] batch time: 0.057 trainign loss: 5.5133 avg training loss: 5.9496
batch: [1540/3682] batch time: 0.257 trainign loss: 3.5609 avg training loss: 5.9477
batch: [1550/3682] batch time: 0.058 trainign loss: 4.8959 avg training loss: 5.9466
batch: [1560/3682] batch time: 0.285 trainign loss: 5.3575 avg training loss: 5.9456
batch: [1570/3682] batch time: 0.057 trainign loss: 4.9806 avg training loss: 5.9434
batch: [1580/3682] batch time: 0.248 trainign loss: 5.2545 avg training loss: 5.9429
batch: [1590/3682] batch time: 0.057 trainign loss: 5.1030 avg training loss: 5.9405
batch: [1600/3682] batch time: 0.266 trainign loss: 5.3691 avg training loss: 5.9391
batch: [1610/3682] batch time: 0.057 trainign loss: 5.3396 avg training loss: 5.9374
batch: [1620/3682] batch time: 0.286 trainign loss: 5.6185 avg training loss: 5.9363
batch: [1630/3682] batch time: 0.057 trainign loss: 5.9343 avg training loss: 5.9358
batch: [1640/3682] batch time: 0.226 trainign loss: 5.9170 avg training loss: 5.9352
batch: [1650/3682] batch time: 0.057 trainign loss: 5.7174 avg training loss: 5.9344
batch: [1660/3682] batch time: 0.268 trainign loss: 6.1533 avg training loss: 5.9342
batch: [1670/3682] batch time: 0.057 trainign loss: 5.8513 avg training loss: 5.9341
batch: [1680/3682] batch time: 0.225 trainign loss: 3.9765 avg training loss: 5.9333
batch: [1690/3682] batch time: 0.057 trainign loss: 5.6782 avg training loss: 5.9328
batch: [1700/3682] batch time: 0.282 trainign loss: 5.4916 avg training loss: 5.9326
batch: [1710/3682] batch time: 0.057 trainign loss: 3.2605 avg training loss: 5.9311
batch: [1720/3682] batch time: 0.249 trainign loss: 5.9162 avg training loss: 5.9306
batch: [1730/3682] batch time: 0.057 trainign loss: 4.9709 avg training loss: 5.9296
batch: [1740/3682] batch time: 0.205 trainign loss: 6.4376 avg training loss: 5.9286
batch: [1750/3682] batch time: 0.057 trainign loss: 5.0650 avg training loss: 5.9260
batch: [1760/3682] batch time: 0.273 trainign loss: 6.0126 avg training loss: 5.9251
batch: [1770/3682] batch time: 0.057 trainign loss: 5.7672 avg training loss: 5.9249
batch: [1780/3682] batch time: 0.242 trainign loss: 5.3698 avg training loss: 5.9234
batch: [1790/3682] batch time: 0.057 trainign loss: 6.0135 avg training loss: 5.9227
batch: [1800/3682] batch time: 0.245 trainign loss: 3.9700 avg training loss: 5.9218
batch: [1810/3682] batch time: 0.057 trainign loss: 6.4018 avg training loss: 5.9219
batch: [1820/3682] batch time: 0.238 trainign loss: 5.7260 avg training loss: 5.9218
batch: [1830/3682] batch time: 0.057 trainign loss: 5.9618 avg training loss: 5.9209
batch: [1840/3682] batch time: 0.270 trainign loss: 6.0231 avg training loss: 5.9199
batch: [1850/3682] batch time: 0.056 trainign loss: 5.6097 avg training loss: 5.9194
batch: [1860/3682] batch time: 0.262 trainign loss: 3.2496 avg training loss: 5.9179
batch: [1870/3682] batch time: 0.056 trainign loss: 5.7746 avg training loss: 5.9161
batch: [1880/3682] batch time: 0.235 trainign loss: 5.4108 avg training loss: 5.9149
batch: [1890/3682] batch time: 0.056 trainign loss: 6.1675 avg training loss: 5.9132
batch: [1900/3682] batch time: 0.264 trainign loss: 5.0913 avg training loss: 5.9127
batch: [1910/3682] batch time: 0.056 trainign loss: 5.3210 avg training loss: 5.9089
batch: [1920/3682] batch time: 0.259 trainign loss: 5.3583 avg training loss: 5.9072
batch: [1930/3682] batch time: 0.056 trainign loss: 4.9243 avg training loss: 5.9066
batch: [1940/3682] batch time: 0.252 trainign loss: 5.7558 avg training loss: 5.9063
batch: [1950/3682] batch time: 0.056 trainign loss: 5.5531 avg training loss: 5.9058
batch: [1960/3682] batch time: 0.247 trainign loss: 4.6237 avg training loss: 5.9046
batch: [1970/3682] batch time: 0.056 trainign loss: 0.0998 avg training loss: 5.8972
batch: [1980/3682] batch time: 0.242 trainign loss: 6.2247 avg training loss: 5.8932
batch: [1990/3682] batch time: 0.057 trainign loss: 5.3132 avg training loss: 5.8924
batch: [2000/3682] batch time: 0.253 trainign loss: 5.2265 avg training loss: 5.8910
batch: [2010/3682] batch time: 0.057 trainign loss: 6.3550 avg training loss: 5.8878
batch: [2020/3682] batch time: 0.235 trainign loss: 4.7700 avg training loss: 5.8863
batch: [2030/3682] batch time: 0.057 trainign loss: 5.3280 avg training loss: 5.8849
batch: [2040/3682] batch time: 0.270 trainign loss: 6.0580 avg training loss: 5.8826
batch: [2050/3682] batch time: 0.057 trainign loss: 6.0786 avg training loss: 5.8824
batch: [2060/3682] batch time: 0.269 trainign loss: 5.9823 avg training loss: 5.8811
batch: [2070/3682] batch time: 0.057 trainign loss: 6.0633 avg training loss: 5.8809
batch: [2080/3682] batch time: 0.234 trainign loss: 5.0608 avg training loss: 5.8804
batch: [2090/3682] batch time: 0.057 trainign loss: 3.2287 avg training loss: 5.8786
batch: [2100/3682] batch time: 0.209 trainign loss: 4.0346 avg training loss: 5.8778
batch: [2110/3682] batch time: 0.057 trainign loss: 8.2825 avg training loss: 5.8713
batch: [2120/3682] batch time: 0.297 trainign loss: 6.0085 avg training loss: 5.8722
batch: [2130/3682] batch time: 0.057 trainign loss: 5.9678 avg training loss: 5.8726
batch: [2140/3682] batch time: 0.295 trainign loss: 5.8287 avg training loss: 5.8730
batch: [2150/3682] batch time: 0.057 trainign loss: 3.7541 avg training loss: 5.8714
batch: [2160/3682] batch time: 0.284 trainign loss: 5.2796 avg training loss: 5.8708
batch: [2170/3682] batch time: 0.057 trainign loss: 4.5841 avg training loss: 5.8691
batch: [2180/3682] batch time: 0.262 trainign loss: 6.0838 avg training loss: 5.8669
batch: [2190/3682] batch time: 0.057 trainign loss: 5.3607 avg training loss: 5.8663
batch: [2200/3682] batch time: 0.215 trainign loss: 3.8070 avg training loss: 5.8646
batch: [2210/3682] batch time: 0.057 trainign loss: 5.0704 avg training loss: 5.8624
batch: [2220/3682] batch time: 0.257 trainign loss: 5.3541 avg training loss: 5.8607
batch: [2230/3682] batch time: 0.057 trainign loss: 0.7432 avg training loss: 5.8568
batch: [2240/3682] batch time: 0.263 trainign loss: 0.0021 avg training loss: 5.8470
batch: [2250/3682] batch time: 0.057 trainign loss: 0.0001 avg training loss: 5.8371
batch: [2260/3682] batch time: 0.278 trainign loss: 14.3744 avg training loss: 5.8330
batch: [2270/3682] batch time: 0.057 trainign loss: 6.9282 avg training loss: 5.8386
batch: [2280/3682] batch time: 0.238 trainign loss: 5.9451 avg training loss: 5.8386
batch: [2290/3682] batch time: 0.057 trainign loss: 5.0669 avg training loss: 5.8386
batch: [2300/3682] batch time: 0.250 trainign loss: 5.8958 avg training loss: 5.8378
batch: [2310/3682] batch time: 0.057 trainign loss: 5.6717 avg training loss: 5.8372
batch: [2320/3682] batch time: 0.221 trainign loss: 5.8764 avg training loss: 5.8375
batch: [2330/3682] batch time: 0.058 trainign loss: 6.0897 avg training loss: 5.8377
batch: [2340/3682] batch time: 0.234 trainign loss: 3.8258 avg training loss: 5.8367
batch: [2350/3682] batch time: 0.058 trainign loss: 6.1898 avg training loss: 5.8353
batch: [2360/3682] batch time: 0.269 trainign loss: 5.5041 avg training loss: 5.8346
batch: [2370/3682] batch time: 0.059 trainign loss: 6.8068 avg training loss: 5.8336
batch: [2380/3682] batch time: 0.241 trainign loss: 4.8298 avg training loss: 5.8336
batch: [2390/3682] batch time: 0.059 trainign loss: 5.1133 avg training loss: 5.8331
batch: [2400/3682] batch time: 0.282 trainign loss: 4.5070 avg training loss: 5.8322
batch: [2410/3682] batch time: 0.057 trainign loss: 4.1288 avg training loss: 5.8306
batch: [2420/3682] batch time: 0.263 trainign loss: 3.6324 avg training loss: 5.8292
batch: [2430/3682] batch time: 0.057 trainign loss: 5.9958 avg training loss: 5.8262
batch: [2440/3682] batch time: 0.234 trainign loss: 3.8661 avg training loss: 5.8252
batch: [2450/3682] batch time: 0.057 trainign loss: 6.0123 avg training loss: 5.8239
batch: [2460/3682] batch time: 0.300 trainign loss: 3.3612 avg training loss: 5.8223
batch: [2470/3682] batch time: 0.057 trainign loss: 0.0422 avg training loss: 5.8140
batch: [2480/3682] batch time: 0.305 trainign loss: 0.0005 avg training loss: 5.8046
batch: [2490/3682] batch time: 0.058 trainign loss: 0.0002 avg training loss: 5.7952
batch: [2500/3682] batch time: 0.305 trainign loss: 0.0001 avg training loss: 5.7858
batch: [2510/3682] batch time: 0.058 trainign loss: 0.0001 avg training loss: 5.7765
batch: [2520/3682] batch time: 0.293 trainign loss: 0.0001 avg training loss: 5.7672
batch: [2530/3682] batch time: 0.058 trainign loss: 0.0007 avg training loss: 5.7579
batch: [2540/3682] batch time: 0.308 trainign loss: 0.0001 avg training loss: 5.7486
batch: [2550/3682] batch time: 0.058 trainign loss: 0.0001 avg training loss: 5.7394
batch: [2560/3682] batch time: 0.266 trainign loss: 7.0752 avg training loss: 5.7421
batch: [2570/3682] batch time: 0.058 trainign loss: 3.6860 avg training loss: 5.7418
batch: [2580/3682] batch time: 0.248 trainign loss: 6.6603 avg training loss: 5.7350
batch: [2590/3682] batch time: 0.058 trainign loss: 6.5593 avg training loss: 5.7390
batch: [2600/3682] batch time: 0.259 trainign loss: 6.1212 avg training loss: 5.7401
batch: [2610/3682] batch time: 0.058 trainign loss: 3.9808 avg training loss: 5.7393
batch: [2620/3682] batch time: 0.281 trainign loss: 6.4337 avg training loss: 5.7396
batch: [2630/3682] batch time: 0.058 trainign loss: 5.3822 avg training loss: 5.7393
batch: [2640/3682] batch time: 0.258 trainign loss: 6.4914 avg training loss: 5.7375
batch: [2650/3682] batch time: 0.057 trainign loss: 5.9453 avg training loss: 5.7375
batch: [2660/3682] batch time: 0.240 trainign loss: 5.7753 avg training loss: 5.7379
batch: [2670/3682] batch time: 0.057 trainign loss: 5.3002 avg training loss: 5.7374
batch: [2680/3682] batch time: 0.236 trainign loss: 6.1140 avg training loss: 5.7375
batch: [2690/3682] batch time: 0.057 trainign loss: 5.7894 avg training loss: 5.7376
batch: [2700/3682] batch time: 0.236 trainign loss: 5.4425 avg training loss: 5.7372
batch: [2710/3682] batch time: 0.057 trainign loss: 3.4273 avg training loss: 5.7362
batch: [2720/3682] batch time: 0.233 trainign loss: 6.3138 avg training loss: 5.7355
batch: [2730/3682] batch time: 0.057 trainign loss: 3.8469 avg training loss: 5.7352
batch: [2740/3682] batch time: 0.264 trainign loss: 5.9609 avg training loss: 5.7289
batch: [2750/3682] batch time: 0.057 trainign loss: 6.0552 avg training loss: 5.7306
batch: [2760/3682] batch time: 0.289 trainign loss: 5.8469 avg training loss: 5.7308
batch: [2770/3682] batch time: 0.057 trainign loss: 6.0650 avg training loss: 5.7292
batch: [2780/3682] batch time: 0.268 trainign loss: 5.5063 avg training loss: 5.7296
batch: [2790/3682] batch time: 0.057 trainign loss: 5.9724 avg training loss: 5.7299
batch: [2800/3682] batch time: 0.237 trainign loss: 4.3350 avg training loss: 5.7292
batch: [2810/3682] batch time: 0.058 trainign loss: 5.6380 avg training loss: 5.7284
batch: [2820/3682] batch time: 0.269 trainign loss: 5.8880 avg training loss: 5.7285
batch: [2830/3682] batch time: 0.058 trainign loss: 4.6193 avg training loss: 5.7277
batch: [2840/3682] batch time: 0.222 trainign loss: 5.5299 avg training loss: 5.7271
batch: [2850/3682] batch time: 0.057 trainign loss: 5.2169 avg training loss: 5.7267
batch: [2860/3682] batch time: 0.233 trainign loss: 6.2415 avg training loss: 5.7262
batch: [2870/3682] batch time: 0.057 trainign loss: 5.4304 avg training loss: 5.7262
batch: [2880/3682] batch time: 0.241 trainign loss: 0.4728 avg training loss: 5.7215
batch: [2890/3682] batch time: 0.057 trainign loss: 6.9703 avg training loss: 5.7226
batch: [2900/3682] batch time: 0.248 trainign loss: 4.4730 avg training loss: 5.7225
batch: [2910/3682] batch time: 0.058 trainign loss: 5.2373 avg training loss: 5.7215
batch: [2920/3682] batch time: 0.250 trainign loss: 3.5766 avg training loss: 5.7172
batch: [2930/3682] batch time: 0.058 trainign loss: 6.2935 avg training loss: 5.7180
batch: [2940/3682] batch time: 0.285 trainign loss: 6.0986 avg training loss: 5.7179
batch: [2950/3682] batch time: 0.058 trainign loss: 5.5032 avg training loss: 5.7169
batch: [2960/3682] batch time: 0.231 trainign loss: 6.2556 avg training loss: 5.7175
batch: [2970/3682] batch time: 0.058 trainign loss: 5.0382 avg training loss: 5.7173
batch: [2980/3682] batch time: 0.241 trainign loss: 5.8860 avg training loss: 5.7167
batch: [2990/3682] batch time: 0.058 trainign loss: 0.3358 avg training loss: 5.7134
batch: [3000/3682] batch time: 0.217 trainign loss: 6.6701 avg training loss: 5.7132
batch: [3010/3682] batch time: 0.058 trainign loss: 5.9816 avg training loss: 5.7139
batch: [3020/3682] batch time: 0.207 trainign loss: 6.4404 avg training loss: 5.7138
batch: [3030/3682] batch time: 0.057 trainign loss: 5.4041 avg training loss: 5.7145
batch: [3040/3682] batch time: 0.217 trainign loss: 1.3814 avg training loss: 5.7118
batch: [3050/3682] batch time: 0.057 trainign loss: 4.0266 avg training loss: 5.7090
batch: [3060/3682] batch time: 0.288 trainign loss: 6.2892 avg training loss: 5.7106
batch: [3070/3682] batch time: 0.057 trainign loss: 6.1388 avg training loss: 5.7112
batch: [3080/3682] batch time: 0.129 trainign loss: 5.5591 avg training loss: 5.7109
batch: [3090/3682] batch time: 0.057 trainign loss: 4.5506 avg training loss: 5.7101
batch: [3100/3682] batch time: 0.262 trainign loss: 6.6229 avg training loss: 5.7094
batch: [3110/3682] batch time: 0.057 trainign loss: 5.3589 avg training loss: 5.7098
batch: [3120/3682] batch time: 0.255 trainign loss: 6.3420 avg training loss: 5.7096
batch: [3130/3682] batch time: 0.057 trainign loss: 5.6584 avg training loss: 5.7071
batch: [3140/3682] batch time: 0.268 trainign loss: 6.8122 avg training loss: 5.7074
batch: [3150/3682] batch time: 0.057 trainign loss: 6.4287 avg training loss: 5.7081
batch: [3160/3682] batch time: 0.269 trainign loss: 4.9007 avg training loss: 5.7080
batch: [3170/3682] batch time: 0.057 trainign loss: 6.0447 avg training loss: 5.7080
batch: [3180/3682] batch time: 0.277 trainign loss: 4.1448 avg training loss: 5.7078
batch: [3190/3682] batch time: 0.057 trainign loss: 6.9207 avg training loss: 5.7054
batch: [3200/3682] batch time: 0.289 trainign loss: 2.8942 avg training loss: 5.7043
batch: [3210/3682] batch time: 0.057 trainign loss: 7.5825 avg training loss: 5.7016
batch: [3220/3682] batch time: 0.264 trainign loss: 6.1590 avg training loss: 5.7024
batch: [3230/3682] batch time: 0.057 trainign loss: 4.8965 avg training loss: 5.7020
batch: [3240/3682] batch time: 0.266 trainign loss: 0.1762 avg training loss: 5.6967
batch: [3250/3682] batch time: 0.057 trainign loss: 11.0144 avg training loss: 5.6973
batch: [3260/3682] batch time: 0.264 trainign loss: 6.7897 avg training loss: 5.7001
batch: [3270/3682] batch time: 0.057 trainign loss: 5.7949 avg training loss: 5.7006
batch: [3280/3682] batch time: 0.273 trainign loss: 6.0223 avg training loss: 5.7003
batch: [3290/3682] batch time: 0.057 trainign loss: 4.4664 avg training loss: 5.6999
batch: [3300/3682] batch time: 0.287 trainign loss: 6.0420 avg training loss: 5.7007
batch: [3310/3682] batch time: 0.057 trainign loss: 6.4490 avg training loss: 5.7019
batch: [3320/3682] batch time: 0.284 trainign loss: 6.5082 avg training loss: 5.7029
batch: [3330/3682] batch time: 0.058 trainign loss: 6.2101 avg training loss: 5.7035
batch: [3340/3682] batch time: 0.207 trainign loss: 5.9583 avg training loss: 5.7038
batch: [3350/3682] batch time: 0.058 trainign loss: 6.7019 avg training loss: 5.7046
batch: [3360/3682] batch time: 0.268 trainign loss: 6.4834 avg training loss: 5.7058
batch: [3370/3682] batch time: 0.058 trainign loss: 6.6323 avg training loss: 5.7065
batch: [3380/3682] batch time: 0.263 trainign loss: 3.5184 avg training loss: 5.7058
batch: [3390/3682] batch time: 0.058 trainign loss: 7.1998 avg training loss: 5.7057
batch: [3400/3682] batch time: 0.263 trainign loss: 6.7289 avg training loss: 5.7072
batch: [3410/3682] batch time: 0.058 trainign loss: 5.8558 avg training loss: 5.7071
batch: [3420/3682] batch time: 0.240 trainign loss: 6.4860 avg training loss: 5.7057
batch: [3430/3682] batch time: 0.058 trainign loss: 6.8866 avg training loss: 5.7072
batch: [3440/3682] batch time: 0.238 trainign loss: 4.8058 avg training loss: 5.7078
batch: [3450/3682] batch time: 0.058 trainign loss: 6.1924 avg training loss: 5.7079
batch: [3460/3682] batch time: 0.248 trainign loss: 5.8336 avg training loss: 5.7082
batch: [3470/3682] batch time: 0.058 trainign loss: 4.5864 avg training loss: 5.7064
batch: [3480/3682] batch time: 0.260 trainign loss: 6.2230 avg training loss: 5.7073
batch: [3490/3682] batch time: 0.059 trainign loss: 6.7351 avg training loss: 5.7084
batch: [3500/3682] batch time: 0.256 trainign loss: 4.1403 avg training loss: 5.7084
batch: [3510/3682] batch time: 0.059 trainign loss: 6.4152 avg training loss: 5.7081
batch: [3520/3682] batch time: 0.260 trainign loss: 6.2883 avg training loss: 5.7088
batch: [3530/3682] batch time: 0.058 trainign loss: 6.6013 avg training loss: 5.7097
batch: [3540/3682] batch time: 0.257 trainign loss: 6.2073 avg training loss: 5.7101
batch: [3550/3682] batch time: 0.059 trainign loss: 5.3697 avg training loss: 5.7104
batch: [3560/3682] batch time: 0.155 trainign loss: 1.4525 avg training loss: 5.7070
batch: [3570/3682] batch time: 0.058 trainign loss: 6.7558 avg training loss: 5.7069
batch: [3580/3682] batch time: 0.280 trainign loss: 6.8815 avg training loss: 5.7081
batch: [3590/3682] batch time: 0.057 trainign loss: 6.8796 avg training loss: 5.7097
batch: [3600/3682] batch time: 0.246 trainign loss: 6.6262 avg training loss: 5.7112
batch: [3610/3682] batch time: 0.058 trainign loss: 6.3108 avg training loss: 5.7124
batch: [3620/3682] batch time: 0.281 trainign loss: 5.1069 avg training loss: 5.7120
batch: [3630/3682] batch time: 0.058 trainign loss: 6.1726 avg training loss: 5.7114
batch: [3640/3682] batch time: 0.241 trainign loss: 6.8117 avg training loss: 5.7129
batch: [3650/3682] batch time: 0.057 trainign loss: 6.4060 avg training loss: 5.7137
batch: [3660/3682] batch time: 0.273 trainign loss: 5.5975 avg training loss: 5.7139
batch: [3670/3682] batch time: 0.058 trainign loss: 6.9042 avg training loss: 5.7136
batch: [3680/3682] batch time: 0.186 trainign loss: 6.5206 avg training loss: 5.7144
