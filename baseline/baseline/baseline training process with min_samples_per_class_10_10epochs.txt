total batches: 21305
Epoch: 1
----------------------------------------------------------------------
batch: [0/21305] batch time: 0.560 trainign loss: 10.6094 avg training loss: 10.6094
batch: [10/21305] batch time: 0.063 trainign loss: 10.6181 avg training loss: 10.5810
batch: [20/21305] batch time: 0.256 trainign loss: 10.6428 avg training loss: 10.5970
batch: [30/21305] batch time: 0.063 trainign loss: 10.6128 avg training loss: 10.6092
batch: [40/21305] batch time: 0.218 trainign loss: 10.5734 avg training loss: 10.6110
batch: [50/21305] batch time: 0.064 trainign loss: 10.5394 avg training loss: 10.6062
batch: [60/21305] batch time: 0.213 trainign loss: 10.5716 avg training loss: 10.6083
batch: [70/21305] batch time: 0.064 trainign loss: 10.6108 avg training loss: 10.6142
batch: [80/21305] batch time: 0.231 trainign loss: 10.6056 avg training loss: 10.6146
batch: [90/21305] batch time: 0.063 trainign loss: 10.6711 avg training loss: 10.6167
batch: [100/21305] batch time: 0.217 trainign loss: 10.6050 avg training loss: 10.6177
batch: [110/21305] batch time: 0.063 trainign loss: 10.6766 avg training loss: 10.6164
batch: [120/21305] batch time: 0.242 trainign loss: 10.6086 avg training loss: 10.6138
batch: [130/21305] batch time: 0.063 trainign loss: 10.6258 avg training loss: 10.6158
batch: [140/21305] batch time: 0.170 trainign loss: 10.5784 avg training loss: 10.6162
batch: [150/21305] batch time: 0.064 trainign loss: 10.7159 avg training loss: 10.6163
batch: [160/21305] batch time: 0.242 trainign loss: 10.6811 avg training loss: 10.6199
batch: [170/21305] batch time: 0.062 trainign loss: 10.6597 avg training loss: 10.6208
batch: [180/21305] batch time: 0.239 trainign loss: 10.6341 avg training loss: 10.6213
batch: [190/21305] batch time: 0.064 trainign loss: 10.6411 avg training loss: 10.6216
batch: [200/21305] batch time: 0.214 trainign loss: 9.9634 avg training loss: 10.6075
batch: [210/21305] batch time: 0.064 trainign loss: 10.6399 avg training loss: 10.5898
batch: [220/21305] batch time: 0.211 trainign loss: 10.5754 avg training loss: 10.5917
batch: [230/21305] batch time: 0.063 trainign loss: 10.7097 avg training loss: 10.5928
batch: [240/21305] batch time: 0.217 trainign loss: 10.7234 avg training loss: 10.5945
batch: [250/21305] batch time: 0.063 trainign loss: 10.6330 avg training loss: 10.5964
batch: [260/21305] batch time: 0.062 trainign loss: 10.6348 avg training loss: 10.5981
batch: [270/21305] batch time: 0.062 trainign loss: 10.6332 avg training loss: 10.5994
batch: [280/21305] batch time: 0.119 trainign loss: 10.6280 avg training loss: 10.6013
batch: [290/21305] batch time: 0.076 trainign loss: 10.5960 avg training loss: 10.6002
batch: [300/21305] batch time: 0.154 trainign loss: 10.7108 avg training loss: 10.6016
batch: [310/21305] batch time: 0.160 trainign loss: 10.6601 avg training loss: 10.6029
batch: [320/21305] batch time: 0.063 trainign loss: 10.6587 avg training loss: 10.6037
batch: [330/21305] batch time: 0.170 trainign loss: 10.5777 avg training loss: 10.6028
batch: [340/21305] batch time: 0.063 trainign loss: 10.6500 avg training loss: 10.6031
batch: [350/21305] batch time: 0.123 trainign loss: 10.6794 avg training loss: 10.6042
batch: [360/21305] batch time: 0.081 trainign loss: 10.6017 avg training loss: 10.6049
batch: [370/21305] batch time: 0.120 trainign loss: 10.6106 avg training loss: 10.6058
batch: [380/21305] batch time: 0.145 trainign loss: 10.6300 avg training loss: 10.6061
batch: [390/21305] batch time: 0.157 trainign loss: 10.4778 avg training loss: 10.6065
batch: [400/21305] batch time: 0.108 trainign loss: 10.5622 avg training loss: 10.6057
batch: [410/21305] batch time: 0.130 trainign loss: 10.6347 avg training loss: 10.6050
batch: [420/21305] batch time: 0.200 trainign loss: 10.5591 avg training loss: 10.6049
batch: [430/21305] batch time: 0.067 trainign loss: 10.5293 avg training loss: 10.6053
batch: [440/21305] batch time: 0.257 trainign loss: 10.6150 avg training loss: 10.6039
batch: [450/21305] batch time: 0.063 trainign loss: 10.7102 avg training loss: 10.6040
batch: [460/21305] batch time: 0.220 trainign loss: 10.5105 avg training loss: 10.6031
batch: [470/21305] batch time: 0.078 trainign loss: 10.5010 avg training loss: 10.6029
batch: [480/21305] batch time: 0.234 trainign loss: 10.7517 avg training loss: 10.6035
batch: [490/21305] batch time: 0.063 trainign loss: 10.6340 avg training loss: 10.6039
batch: [500/21305] batch time: 0.063 trainign loss: 10.4406 avg training loss: 10.6036
batch: [510/21305] batch time: 0.063 trainign loss: 10.5458 avg training loss: 10.6040
batch: [520/21305] batch time: 0.063 trainign loss: 10.4302 avg training loss: 10.6040
batch: [530/21305] batch time: 0.063 trainign loss: 10.6667 avg training loss: 10.6023
batch: [540/21305] batch time: 0.063 trainign loss: 10.5906 avg training loss: 10.6021
batch: [550/21305] batch time: 0.063 trainign loss: 10.4632 avg training loss: 10.5989
batch: [560/21305] batch time: 0.063 trainign loss: 10.4569 avg training loss: 10.5988
batch: [570/21305] batch time: 0.063 trainign loss: 10.5424 avg training loss: 10.5972
batch: [580/21305] batch time: 0.063 trainign loss: 10.5918 avg training loss: 10.5946
batch: [590/21305] batch time: 0.063 trainign loss: 10.6926 avg training loss: 10.5944
batch: [600/21305] batch time: 0.127 trainign loss: 10.5827 avg training loss: 10.5949
batch: [610/21305] batch time: 0.063 trainign loss: 10.6779 avg training loss: 10.5964
batch: [620/21305] batch time: 0.065 trainign loss: 10.7240 avg training loss: 10.5971
batch: [630/21305] batch time: 0.063 trainign loss: 10.6407 avg training loss: 10.5972
batch: [640/21305] batch time: 0.063 trainign loss: 10.6120 avg training loss: 10.5977
batch: [650/21305] batch time: 0.063 trainign loss: 10.5917 avg training loss: 10.5979
batch: [660/21305] batch time: 0.062 trainign loss: 10.5832 avg training loss: 10.5951
batch: [670/21305] batch time: 0.062 trainign loss: 10.7039 avg training loss: 10.5916
batch: [680/21305] batch time: 0.062 trainign loss: 10.5859 avg training loss: 10.5921
batch: [690/21305] batch time: 0.062 trainign loss: 10.5574 avg training loss: 10.5929
batch: [700/21305] batch time: 0.062 trainign loss: 10.6381 avg training loss: 10.5919
batch: [710/21305] batch time: 0.062 trainign loss: 10.6371 avg training loss: 10.5916
batch: [720/21305] batch time: 0.062 trainign loss: 10.5494 avg training loss: 10.5912
batch: [730/21305] batch time: 0.062 trainign loss: 10.6640 avg training loss: 10.5918
batch: [740/21305] batch time: 0.065 trainign loss: 10.6337 avg training loss: 10.5921
batch: [750/21305] batch time: 0.062 trainign loss: 10.5597 avg training loss: 10.5919
batch: [760/21305] batch time: 0.062 trainign loss: 10.6378 avg training loss: 10.5922
batch: [770/21305] batch time: 0.063 trainign loss: 10.5058 avg training loss: 10.5917
batch: [780/21305] batch time: 0.077 trainign loss: 10.6275 avg training loss: 10.5918
batch: [790/21305] batch time: 0.063 trainign loss: 10.6583 avg training loss: 10.5925
batch: [800/21305] batch time: 0.063 trainign loss: 10.5615 avg training loss: 10.5928
batch: [810/21305] batch time: 0.063 trainign loss: 10.5998 avg training loss: 10.5923
batch: [820/21305] batch time: 0.063 trainign loss: 10.5758 avg training loss: 10.5925
batch: [830/21305] batch time: 0.063 trainign loss: 10.5594 avg training loss: 10.5926
batch: [840/21305] batch time: 0.064 trainign loss: 10.6890 avg training loss: 10.5929
batch: [850/21305] batch time: 0.063 trainign loss: 10.6532 avg training loss: 10.5927
batch: [860/21305] batch time: 0.063 trainign loss: 10.6543 avg training loss: 10.5928
batch: [870/21305] batch time: 0.063 trainign loss: 10.5851 avg training loss: 10.5927
batch: [880/21305] batch time: 0.063 trainign loss: 10.6295 avg training loss: 10.5898
batch: [890/21305] batch time: 0.063 trainign loss: 10.6983 avg training loss: 10.5904
batch: [900/21305] batch time: 0.062 trainign loss: 10.5772 avg training loss: 10.5908
batch: [910/21305] batch time: 0.063 trainign loss: 10.5452 avg training loss: 10.5908
batch: [920/21305] batch time: 0.062 trainign loss: 10.4233 avg training loss: 10.5905
batch: [930/21305] batch time: 0.062 trainign loss: 10.6417 avg training loss: 10.5907
batch: [940/21305] batch time: 0.062 trainign loss: 10.4752 avg training loss: 10.5910
batch: [950/21305] batch time: 0.062 trainign loss: 5.6422 avg training loss: 10.5673
batch: [960/21305] batch time: 0.063 trainign loss: 11.3487 avg training loss: 10.5592
batch: [970/21305] batch time: 0.062 trainign loss: 10.6075 avg training loss: 10.5613
batch: [980/21305] batch time: 0.062 trainign loss: 10.5776 avg training loss: 10.5618
batch: [990/21305] batch time: 0.063 trainign loss: 10.6395 avg training loss: 10.5620
batch: [1000/21305] batch time: 0.062 trainign loss: 10.1064 avg training loss: 10.5615
batch: [1010/21305] batch time: 0.057 trainign loss: 10.6255 avg training loss: 10.5582
batch: [1020/21305] batch time: 0.052 trainign loss: 10.6188 avg training loss: 10.5587
batch: [1030/21305] batch time: 0.061 trainign loss: 10.7490 avg training loss: 10.5586
batch: [1040/21305] batch time: 0.053 trainign loss: 10.5336 avg training loss: 10.5596
batch: [1050/21305] batch time: 0.165 trainign loss: 10.6446 avg training loss: 10.5596
batch: [1060/21305] batch time: 0.063 trainign loss: 10.6512 avg training loss: 10.5592
batch: [1070/21305] batch time: 0.418 trainign loss: 10.3795 avg training loss: 10.5595
batch: [1080/21305] batch time: 0.063 trainign loss: 6.0132 avg training loss: 10.5447
batch: [1090/21305] batch time: 0.056 trainign loss: 11.9860 avg training loss: 10.5283
batch: [1100/21305] batch time: 0.053 trainign loss: 10.3037 avg training loss: 10.5276
batch: [1110/21305] batch time: 0.056 trainign loss: 10.4986 avg training loss: 10.5278
batch: [1120/21305] batch time: 0.062 trainign loss: 10.5892 avg training loss: 10.5277
batch: [1130/21305] batch time: 0.940 trainign loss: 10.6667 avg training loss: 10.5282
batch: [1140/21305] batch time: 0.054 trainign loss: 10.5642 avg training loss: 10.5286
batch: [1150/21305] batch time: 2.253 trainign loss: 10.6303 avg training loss: 10.5287
batch: [1160/21305] batch time: 0.062 trainign loss: 10.5246 avg training loss: 10.5288
batch: [1170/21305] batch time: 2.337 trainign loss: 10.5763 avg training loss: 10.5292
batch: [1180/21305] batch time: 0.056 trainign loss: 10.6964 avg training loss: 10.5294
batch: [1190/21305] batch time: 2.151 trainign loss: 10.6173 avg training loss: 10.5296
batch: [1200/21305] batch time: 0.062 trainign loss: 10.1955 avg training loss: 10.5281
batch: [1210/21305] batch time: 1.752 trainign loss: 10.6317 avg training loss: 10.5280
batch: [1220/21305] batch time: 0.056 trainign loss: 10.6678 avg training loss: 10.5291
batch: [1230/21305] batch time: 2.358 trainign loss: 10.6941 avg training loss: 10.5292
batch: [1240/21305] batch time: 0.062 trainign loss: 10.4987 avg training loss: 10.5287
batch: [1250/21305] batch time: 2.353 trainign loss: 10.5808 avg training loss: 10.5292
batch: [1260/21305] batch time: 0.056 trainign loss: 10.3659 avg training loss: 10.5293
batch: [1270/21305] batch time: 2.526 trainign loss: 10.5687 avg training loss: 10.5293
batch: [1280/21305] batch time: 0.061 trainign loss: 10.5734 avg training loss: 10.5296
batch: [1290/21305] batch time: 2.177 trainign loss: 10.6082 avg training loss: 10.5302
batch: [1300/21305] batch time: 0.063 trainign loss: 10.5701 avg training loss: 10.5302
batch: [1310/21305] batch time: 1.662 trainign loss: 10.6630 avg training loss: 10.5306
batch: [1320/21305] batch time: 0.056 trainign loss: 10.5796 avg training loss: 10.5304
batch: [1330/21305] batch time: 1.514 trainign loss: 10.6398 avg training loss: 10.5308
batch: [1340/21305] batch time: 0.061 trainign loss: 10.6326 avg training loss: 10.5314
batch: [1350/21305] batch time: 1.969 trainign loss: 10.6896 avg training loss: 10.5319
batch: [1360/21305] batch time: 0.185 trainign loss: 9.8998 avg training loss: 10.5319
batch: [1370/21305] batch time: 2.038 trainign loss: 10.7012 avg training loss: 10.5276
batch: [1380/21305] batch time: 0.221 trainign loss: 10.4064 avg training loss: 10.5281
batch: [1390/21305] batch time: 1.201 trainign loss: 10.7045 avg training loss: 10.5278
batch: [1400/21305] batch time: 0.632 trainign loss: 10.4857 avg training loss: 10.5278
batch: [1410/21305] batch time: 1.171 trainign loss: 10.6316 avg training loss: 10.5233
batch: [1420/21305] batch time: 0.600 trainign loss: 10.7466 avg training loss: 10.5230
batch: [1430/21305] batch time: 1.601 trainign loss: 10.3459 avg training loss: 10.5232
batch: [1440/21305] batch time: 0.463 trainign loss: 10.2255 avg training loss: 10.5230
batch: [1450/21305] batch time: 1.210 trainign loss: 10.7410 avg training loss: 10.5222
batch: [1460/21305] batch time: 0.057 trainign loss: 10.7363 avg training loss: 10.5229
batch: [1470/21305] batch time: 0.713 trainign loss: 10.6354 avg training loss: 10.5222
batch: [1480/21305] batch time: 0.435 trainign loss: 10.6435 avg training loss: 10.5223
batch: [1490/21305] batch time: 0.056 trainign loss: 10.5720 avg training loss: 10.5224
batch: [1500/21305] batch time: 0.782 trainign loss: 10.5842 avg training loss: 10.5226
batch: [1510/21305] batch time: 0.105 trainign loss: 10.6696 avg training loss: 10.5227
batch: [1520/21305] batch time: 0.681 trainign loss: 9.2435 avg training loss: 10.5212
batch: [1530/21305] batch time: 0.414 trainign loss: 11.9621 avg training loss: 10.4939
batch: [1540/21305] batch time: 1.418 trainign loss: 10.5714 avg training loss: 10.4990
batch: [1550/21305] batch time: 0.288 trainign loss: 9.7907 avg training loss: 10.4991
batch: [1560/21305] batch time: 1.221 trainign loss: 10.5795 avg training loss: 10.4989
batch: [1570/21305] batch time: 0.467 trainign loss: 10.7768 avg training loss: 10.4998
batch: [1580/21305] batch time: 1.833 trainign loss: 10.6906 avg training loss: 10.4984
batch: [1590/21305] batch time: 0.055 trainign loss: 10.6604 avg training loss: 10.4899
batch: [1600/21305] batch time: 1.933 trainign loss: 10.6725 avg training loss: 10.4917
batch: [1610/21305] batch time: 0.053 trainign loss: 10.6447 avg training loss: 10.4921
batch: [1620/21305] batch time: 2.091 trainign loss: 10.5999 avg training loss: 10.4924
batch: [1630/21305] batch time: 0.052 trainign loss: 10.6206 avg training loss: 10.4917
batch: [1640/21305] batch time: 1.332 trainign loss: 10.5867 avg training loss: 10.4924
batch: [1650/21305] batch time: 0.073 trainign loss: 10.7353 avg training loss: 10.4930
batch: [1660/21305] batch time: 0.664 trainign loss: 10.4068 avg training loss: 10.4932
batch: [1670/21305] batch time: 0.632 trainign loss: 10.5995 avg training loss: 10.4939
batch: [1680/21305] batch time: 0.632 trainign loss: 10.3215 avg training loss: 10.4941
batch: [1690/21305] batch time: 0.055 trainign loss: 10.5785 avg training loss: 10.4947
batch: [1700/21305] batch time: 0.867 trainign loss: 10.4682 avg training loss: 10.4950
batch: [1710/21305] batch time: 0.051 trainign loss: 10.3503 avg training loss: 10.4945
batch: [1720/21305] batch time: 1.473 trainign loss: 10.6287 avg training loss: 10.4935
batch: [1730/21305] batch time: 0.057 trainign loss: 10.4654 avg training loss: 10.4940
batch: [1740/21305] batch time: 2.453 trainign loss: 10.5375 avg training loss: 10.4942
batch: [1750/21305] batch time: 0.059 trainign loss: 10.6992 avg training loss: 10.4947
batch: [1760/21305] batch time: 2.177 trainign loss: 10.5982 avg training loss: 10.4952
batch: [1770/21305] batch time: 0.056 trainign loss: 10.5606 avg training loss: 10.4953
batch: [1780/21305] batch time: 2.491 trainign loss: 10.4743 avg training loss: 10.4954
batch: [1790/21305] batch time: 0.062 trainign loss: 10.7039 avg training loss: 10.4956
batch: [1800/21305] batch time: 2.736 trainign loss: 10.2421 avg training loss: 10.4961
batch: [1810/21305] batch time: 0.054 trainign loss: 10.6499 avg training loss: 10.4967
batch: [1820/21305] batch time: 2.296 trainign loss: 9.4994 avg training loss: 10.4953
batch: [1830/21305] batch time: 0.056 trainign loss: 10.6305 avg training loss: 10.4958
batch: [1840/21305] batch time: 2.358 trainign loss: 10.5710 avg training loss: 10.4967
batch: [1850/21305] batch time: 0.053 trainign loss: 10.5610 avg training loss: 10.4973
batch: [1860/21305] batch time: 2.419 trainign loss: 10.4254 avg training loss: 10.4977
batch: [1870/21305] batch time: 0.058 trainign loss: 10.4694 avg training loss: 10.4982
batch: [1880/21305] batch time: 2.410 trainign loss: 10.1724 avg training loss: 10.4983
batch: [1890/21305] batch time: 0.056 trainign loss: 9.7152 avg training loss: 10.4941
batch: [1900/21305] batch time: 2.379 trainign loss: 10.6435 avg training loss: 10.4942
batch: [1910/21305] batch time: 0.056 trainign loss: 10.5514 avg training loss: 10.4949
batch: [1920/21305] batch time: 2.238 trainign loss: 10.4389 avg training loss: 10.4946
batch: [1930/21305] batch time: 0.057 trainign loss: 8.3449 avg training loss: 10.4925
batch: [1940/21305] batch time: 2.173 trainign loss: 10.5647 avg training loss: 10.4923
batch: [1950/21305] batch time: 0.062 trainign loss: 10.4187 avg training loss: 10.4932
batch: [1960/21305] batch time: 2.630 trainign loss: 9.6883 avg training loss: 10.4929
batch: [1970/21305] batch time: 0.059 trainign loss: 10.5649 avg training loss: 10.4925
batch: [1980/21305] batch time: 2.337 trainign loss: 10.0155 avg training loss: 10.4925
batch: [1990/21305] batch time: 0.053 trainign loss: 10.6636 avg training loss: 10.4931
batch: [2000/21305] batch time: 2.460 trainign loss: 10.0321 avg training loss: 10.4923
batch: [2010/21305] batch time: 0.057 trainign loss: 10.6634 avg training loss: 10.4919
batch: [2020/21305] batch time: 1.973 trainign loss: 8.5644 avg training loss: 10.4904
batch: [2030/21305] batch time: 0.056 trainign loss: 10.6297 avg training loss: 10.4840
batch: [2040/21305] batch time: 2.102 trainign loss: 10.5670 avg training loss: 10.4854
batch: [2050/21305] batch time: 0.056 trainign loss: 10.3034 avg training loss: 10.4858
batch: [2060/21305] batch time: 2.393 trainign loss: 7.9907 avg training loss: 10.4833
batch: [2070/21305] batch time: 0.051 trainign loss: 7.3498 avg training loss: 10.4783
batch: [2080/21305] batch time: 2.089 trainign loss: 2.9476 avg training loss: 10.4646
batch: [2090/21305] batch time: 0.052 trainign loss: 12.6734 avg training loss: 10.4503
batch: [2100/21305] batch time: 2.331 trainign loss: 10.6165 avg training loss: 10.4518
batch: [2110/21305] batch time: 0.056 trainign loss: 10.5380 avg training loss: 10.4526
batch: [2120/21305] batch time: 2.240 trainign loss: 7.9413 avg training loss: 10.4500
batch: [2130/21305] batch time: 0.055 trainign loss: 0.0096 avg training loss: 10.4138
batch: [2140/21305] batch time: 2.660 trainign loss: 0.0001 avg training loss: 10.3651
batch: [2150/21305] batch time: 0.052 trainign loss: 10.3862 avg training loss: 10.3744
batch: [2160/21305] batch time: 2.381 trainign loss: 10.7179 avg training loss: 10.3754
batch: [2170/21305] batch time: 0.056 trainign loss: 10.6371 avg training loss: 10.3763
batch: [2180/21305] batch time: 2.199 trainign loss: 10.6546 avg training loss: 10.3765
batch: [2190/21305] batch time: 0.056 trainign loss: 10.6196 avg training loss: 10.3778
batch: [2200/21305] batch time: 2.359 trainign loss: 10.5248 avg training loss: 10.3786
batch: [2210/21305] batch time: 0.056 trainign loss: 10.5766 avg training loss: 10.3796
batch: [2220/21305] batch time: 2.781 trainign loss: 10.5823 avg training loss: 10.3805
batch: [2230/21305] batch time: 0.061 trainign loss: 10.7201 avg training loss: 10.3804
batch: [2240/21305] batch time: 2.549 trainign loss: 10.6457 avg training loss: 10.3788
batch: [2250/21305] batch time: 0.056 trainign loss: 10.5936 avg training loss: 10.3797
batch: [2260/21305] batch time: 2.626 trainign loss: 10.6942 avg training loss: 10.3801
batch: [2270/21305] batch time: 0.056 trainign loss: 9.8453 avg training loss: 10.3784
batch: [2280/21305] batch time: 2.476 trainign loss: 10.6995 avg training loss: 10.3750
batch: [2290/21305] batch time: 0.054 trainign loss: 9.4377 avg training loss: 10.3718
batch: [2300/21305] batch time: 2.299 trainign loss: 9.5764 avg training loss: 10.3726
batch: [2310/21305] batch time: 0.056 trainign loss: 10.7293 avg training loss: 10.3706
batch: [2320/21305] batch time: 2.305 trainign loss: 10.7831 avg training loss: 10.3717
batch: [2330/21305] batch time: 0.052 trainign loss: 10.6548 avg training loss: 10.3723
batch: [2340/21305] batch time: 2.126 trainign loss: 10.4295 avg training loss: 10.3732
batch: [2350/21305] batch time: 0.054 trainign loss: 10.0413 avg training loss: 10.3738
batch: [2360/21305] batch time: 1.895 trainign loss: 10.7513 avg training loss: 10.3747
batch: [2370/21305] batch time: 0.063 trainign loss: 9.9036 avg training loss: 10.3747
batch: [2380/21305] batch time: 1.099 trainign loss: 10.2633 avg training loss: 10.3752
batch: [2390/21305] batch time: 0.056 trainign loss: 10.6297 avg training loss: 10.3764
batch: [2400/21305] batch time: 0.061 trainign loss: 10.3590 avg training loss: 10.3768
batch: [2410/21305] batch time: 0.060 trainign loss: 10.6331 avg training loss: 10.3775
batch: [2420/21305] batch time: 1.104 trainign loss: 10.6357 avg training loss: 10.3784
batch: [2430/21305] batch time: 0.063 trainign loss: 10.4193 avg training loss: 10.3791
batch: [2440/21305] batch time: 2.201 trainign loss: 10.6061 avg training loss: 10.3798
batch: [2450/21305] batch time: 0.056 trainign loss: 10.5993 avg training loss: 10.3804
batch: [2460/21305] batch time: 2.419 trainign loss: 10.6106 avg training loss: 10.3812
batch: [2470/21305] batch time: 0.062 trainign loss: 10.6051 avg training loss: 10.3810
batch: [2480/21305] batch time: 1.722 trainign loss: 10.6931 avg training loss: 10.3817
batch: [2490/21305] batch time: 0.063 trainign loss: 10.6145 avg training loss: 10.3803
batch: [2500/21305] batch time: 1.476 trainign loss: 10.6759 avg training loss: 10.3814
batch: [2510/21305] batch time: 0.062 trainign loss: 10.2169 avg training loss: 10.3821
batch: [2520/21305] batch time: 0.472 trainign loss: 10.3426 avg training loss: 10.3824
batch: [2530/21305] batch time: 0.055 trainign loss: 10.4226 avg training loss: 10.3828
batch: [2540/21305] batch time: 1.333 trainign loss: 10.5684 avg training loss: 10.3832
batch: [2550/21305] batch time: 0.056 trainign loss: 10.4578 avg training loss: 10.3836
batch: [2560/21305] batch time: 1.711 trainign loss: 10.6421 avg training loss: 10.3846
batch: [2570/21305] batch time: 0.063 trainign loss: 10.6338 avg training loss: 10.3854
batch: [2580/21305] batch time: 0.598 trainign loss: 10.4045 avg training loss: 10.3852
batch: [2590/21305] batch time: 0.056 trainign loss: 10.5841 avg training loss: 10.3858
batch: [2600/21305] batch time: 0.196 trainign loss: 4.6760 avg training loss: 10.3788
batch: [2610/21305] batch time: 0.056 trainign loss: 10.5188 avg training loss: 10.3729
batch: [2620/21305] batch time: 0.350 trainign loss: 10.6894 avg training loss: 10.3739
batch: [2630/21305] batch time: 0.057 trainign loss: 10.5416 avg training loss: 10.3745
batch: [2640/21305] batch time: 0.075 trainign loss: 9.5252 avg training loss: 10.3747
batch: [2650/21305] batch time: 0.058 trainign loss: 10.7274 avg training loss: 10.3739
batch: [2660/21305] batch time: 0.057 trainign loss: 10.6829 avg training loss: 10.3747
batch: [2670/21305] batch time: 0.056 trainign loss: 10.4992 avg training loss: 10.3753
batch: [2680/21305] batch time: 0.056 trainign loss: 10.3945 avg training loss: 10.3755
batch: [2690/21305] batch time: 0.056 trainign loss: 10.6732 avg training loss: 10.3762
batch: [2700/21305] batch time: 0.051 trainign loss: 10.5995 avg training loss: 10.3766
batch: [2710/21305] batch time: 0.062 trainign loss: 10.4817 avg training loss: 10.3763
batch: [2720/21305] batch time: 0.056 trainign loss: 8.3738 avg training loss: 10.3746
batch: [2730/21305] batch time: 0.061 trainign loss: 10.7780 avg training loss: 10.3753
batch: [2740/21305] batch time: 0.056 trainign loss: 10.5992 avg training loss: 10.3757
batch: [2750/21305] batch time: 0.063 trainign loss: 10.6174 avg training loss: 10.3763
batch: [2760/21305] batch time: 0.056 trainign loss: 10.5298 avg training loss: 10.3770
batch: [2770/21305] batch time: 0.056 trainign loss: 10.6730 avg training loss: 10.3776
batch: [2780/21305] batch time: 0.056 trainign loss: 10.5231 avg training loss: 10.3782
batch: [2790/21305] batch time: 0.062 trainign loss: 10.5704 avg training loss: 10.3786
batch: [2800/21305] batch time: 0.056 trainign loss: 10.6843 avg training loss: 10.3796
batch: [2810/21305] batch time: 0.056 trainign loss: 10.6524 avg training loss: 10.3799
batch: [2820/21305] batch time: 0.056 trainign loss: 10.7717 avg training loss: 10.3787
batch: [2830/21305] batch time: 0.058 trainign loss: 10.7929 avg training loss: 10.3796
batch: [2840/21305] batch time: 0.056 trainign loss: 6.6260 avg training loss: 10.3766
batch: [2850/21305] batch time: 0.063 trainign loss: 10.6396 avg training loss: 10.3747
batch: [2860/21305] batch time: 0.050 trainign loss: 10.4932 avg training loss: 10.3752
batch: [2870/21305] batch time: 0.059 trainign loss: 10.6589 avg training loss: 10.3738
batch: [2880/21305] batch time: 0.055 trainign loss: 10.8006 avg training loss: 10.3745
batch: [2890/21305] batch time: 0.056 trainign loss: 10.4929 avg training loss: 10.3751
batch: [2900/21305] batch time: 0.056 trainign loss: 10.5161 avg training loss: 10.3748
batch: [2910/21305] batch time: 0.057 trainign loss: 10.7803 avg training loss: 10.3758
batch: [2920/21305] batch time: 0.051 trainign loss: 10.5026 avg training loss: 10.3764
batch: [2930/21305] batch time: 0.056 trainign loss: 10.3469 avg training loss: 10.3769
batch: [2940/21305] batch time: 0.054 trainign loss: 10.0391 avg training loss: 10.3770
batch: [2950/21305] batch time: 0.244 trainign loss: 5.7411 avg training loss: 10.3724
batch: [2960/21305] batch time: 0.056 trainign loss: 10.1694 avg training loss: 10.3729
batch: [2970/21305] batch time: 0.063 trainign loss: 10.5454 avg training loss: 10.3729
batch: [2980/21305] batch time: 0.052 trainign loss: 10.7036 avg training loss: 10.3734
batch: [2990/21305] batch time: 0.063 trainign loss: 10.6109 avg training loss: 10.3743
batch: [3000/21305] batch time: 0.059 trainign loss: 9.9840 avg training loss: 10.3746
batch: [3010/21305] batch time: 0.063 trainign loss: 10.8064 avg training loss: 10.3722
batch: [3020/21305] batch time: 0.051 trainign loss: 9.9317 avg training loss: 10.3730
batch: [3030/21305] batch time: 0.056 trainign loss: 10.5411 avg training loss: 10.3737
batch: [3040/21305] batch time: 0.056 trainign loss: 10.7257 avg training loss: 10.3741
batch: [3050/21305] batch time: 0.056 trainign loss: 10.4811 avg training loss: 10.3747
batch: [3060/21305] batch time: 0.056 trainign loss: 10.3933 avg training loss: 10.3751
batch: [3070/21305] batch time: 0.056 trainign loss: 10.5661 avg training loss: 10.3752
batch: [3080/21305] batch time: 0.056 trainign loss: 10.5427 avg training loss: 10.3756
batch: [3090/21305] batch time: 0.059 trainign loss: 10.5906 avg training loss: 10.3760
batch: [3100/21305] batch time: 0.050 trainign loss: 10.6557 avg training loss: 10.3765
batch: [3110/21305] batch time: 0.059 trainign loss: 10.5882 avg training loss: 10.3772
batch: [3120/21305] batch time: 0.057 trainign loss: 10.5972 avg training loss: 10.3778
batch: [3130/21305] batch time: 0.056 trainign loss: 6.9227 avg training loss: 10.3749
batch: [3140/21305] batch time: 0.051 trainign loss: 10.9048 avg training loss: 10.3753
batch: [3150/21305] batch time: 0.062 trainign loss: 10.6333 avg training loss: 10.3764
batch: [3160/21305] batch time: 0.051 trainign loss: 10.6750 avg training loss: 10.3759
batch: [3170/21305] batch time: 0.056 trainign loss: 10.6095 avg training loss: 10.3765
batch: [3180/21305] batch time: 0.052 trainign loss: 10.5101 avg training loss: 10.3772
batch: [3190/21305] batch time: 0.056 trainign loss: 10.5177 avg training loss: 10.3777
batch: [3200/21305] batch time: 0.056 trainign loss: 10.6332 avg training loss: 10.3783
batch: [3210/21305] batch time: 0.056 trainign loss: 10.6070 avg training loss: 10.3790
batch: [3220/21305] batch time: 0.052 trainign loss: 10.6550 avg training loss: 10.3791
batch: [3230/21305] batch time: 0.056 trainign loss: 10.6597 avg training loss: 10.3796
batch: [3240/21305] batch time: 0.051 trainign loss: 10.4459 avg training loss: 10.3803
batch: [3250/21305] batch time: 0.062 trainign loss: 10.3232 avg training loss: 10.3797
batch: [3260/21305] batch time: 0.062 trainign loss: 10.7323 avg training loss: 10.3805
batch: [3270/21305] batch time: 0.056 trainign loss: 10.6643 avg training loss: 10.3812
batch: [3280/21305] batch time: 0.060 trainign loss: 9.2859 avg training loss: 10.3809
batch: [3290/21305] batch time: 0.063 trainign loss: 10.7949 avg training loss: 10.3786
batch: [3300/21305] batch time: 0.056 trainign loss: 10.4638 avg training loss: 10.3791
batch: [3310/21305] batch time: 0.060 trainign loss: 6.7207 avg training loss: 10.3763
batch: [3320/21305] batch time: 0.061 trainign loss: 10.9686 avg training loss: 10.3758
batch: [3330/21305] batch time: 0.062 trainign loss: 10.4997 avg training loss: 10.3765
batch: [3340/21305] batch time: 0.055 trainign loss: 9.5990 avg training loss: 10.3765
batch: [3350/21305] batch time: 0.057 trainign loss: 10.4607 avg training loss: 10.3766
batch: [3360/21305] batch time: 0.058 trainign loss: 10.6337 avg training loss: 10.3754
batch: [3370/21305] batch time: 0.056 trainign loss: 9.1445 avg training loss: 10.3742
batch: [3380/21305] batch time: 0.057 trainign loss: 10.6731 avg training loss: 10.3749
batch: [3390/21305] batch time: 0.058 trainign loss: 10.4663 avg training loss: 10.3755
batch: [3400/21305] batch time: 0.053 trainign loss: 10.5688 avg training loss: 10.3760
batch: [3410/21305] batch time: 0.063 trainign loss: 10.4788 avg training loss: 10.3765
batch: [3420/21305] batch time: 0.266 trainign loss: 10.6167 avg training loss: 10.3760
batch: [3430/21305] batch time: 0.056 trainign loss: 10.6150 avg training loss: 10.3766
batch: [3440/21305] batch time: 0.054 trainign loss: 10.6939 avg training loss: 10.3772
batch: [3450/21305] batch time: 0.056 trainign loss: 10.6655 avg training loss: 10.3774
batch: [3460/21305] batch time: 0.058 trainign loss: 10.6181 avg training loss: 10.3778
batch: [3470/21305] batch time: 0.635 trainign loss: 10.6680 avg training loss: 10.3784
batch: [3480/21305] batch time: 0.056 trainign loss: 10.6331 avg training loss: 10.3791
batch: [3490/21305] batch time: 0.376 trainign loss: 10.4769 avg training loss: 10.3792
batch: [3500/21305] batch time: 0.052 trainign loss: 10.4136 avg training loss: 10.3797
batch: [3510/21305] batch time: 0.624 trainign loss: 10.6283 avg training loss: 10.3803
batch: [3520/21305] batch time: 0.054 trainign loss: 10.5403 avg training loss: 10.3808
batch: [3530/21305] batch time: 0.058 trainign loss: 10.6048 avg training loss: 10.3813
batch: [3540/21305] batch time: 0.062 trainign loss: 10.4020 avg training loss: 10.3815
batch: [3550/21305] batch time: 0.062 trainign loss: 10.5904 avg training loss: 10.3808
batch: [3560/21305] batch time: 0.052 trainign loss: 10.6804 avg training loss: 10.3800
batch: [3570/21305] batch time: 0.058 trainign loss: 10.6654 avg training loss: 10.3809
batch: [3580/21305] batch time: 0.055 trainign loss: 10.5962 avg training loss: 10.3815
batch: [3590/21305] batch time: 0.056 trainign loss: 10.0683 avg training loss: 10.3819
batch: [3600/21305] batch time: 0.057 trainign loss: 10.5892 avg training loss: 10.3812
batch: [3610/21305] batch time: 0.056 trainign loss: 10.7624 avg training loss: 10.3817
batch: [3620/21305] batch time: 0.050 trainign loss: 10.7112 avg training loss: 10.3824
batch: [3630/21305] batch time: 0.056 trainign loss: 10.6594 avg training loss: 10.3828
batch: [3640/21305] batch time: 0.057 trainign loss: 10.5972 avg training loss: 10.3832
batch: [3650/21305] batch time: 0.056 trainign loss: 10.4294 avg training loss: 10.3834
batch: [3660/21305] batch time: 0.054 trainign loss: 10.6497 avg training loss: 10.3838
batch: [3670/21305] batch time: 0.062 trainign loss: 10.5243 avg training loss: 10.3842
batch: [3680/21305] batch time: 0.052 trainign loss: 10.6859 avg training loss: 10.3846
batch: [3690/21305] batch time: 0.056 trainign loss: 10.7300 avg training loss: 10.3851
batch: [3700/21305] batch time: 0.052 trainign loss: 10.3803 avg training loss: 10.3856
batch: [3710/21305] batch time: 0.062 trainign loss: 10.5113 avg training loss: 10.3860
batch: [3720/21305] batch time: 0.051 trainign loss: 10.6370 avg training loss: 10.3862
batch: [3730/21305] batch time: 0.057 trainign loss: 10.4738 avg training loss: 10.3865
batch: [3740/21305] batch time: 0.052 trainign loss: 10.6262 avg training loss: 10.3871
batch: [3750/21305] batch time: 0.056 trainign loss: 10.6448 avg training loss: 10.3875
batch: [3760/21305] batch time: 0.057 trainign loss: 10.5113 avg training loss: 10.3871
batch: [3770/21305] batch time: 0.062 trainign loss: 9.6321 avg training loss: 10.3870
batch: [3780/21305] batch time: 0.053 trainign loss: 10.5737 avg training loss: 10.3875
batch: [3790/21305] batch time: 0.060 trainign loss: 9.9919 avg training loss: 10.3875
batch: [3800/21305] batch time: 0.053 trainign loss: 10.5407 avg training loss: 10.3879
batch: [3810/21305] batch time: 0.061 trainign loss: 10.5650 avg training loss: 10.3884
batch: [3820/21305] batch time: 0.056 trainign loss: 10.0332 avg training loss: 10.3886
batch: [3830/21305] batch time: 0.056 trainign loss: 10.6917 avg training loss: 10.3892
batch: [3840/21305] batch time: 0.052 trainign loss: 10.4880 avg training loss: 10.3895
batch: [3850/21305] batch time: 0.056 trainign loss: 10.5090 avg training loss: 10.3899
batch: [3860/21305] batch time: 0.056 trainign loss: 10.5343 avg training loss: 10.3904
batch: [3870/21305] batch time: 0.540 trainign loss: 10.3953 avg training loss: 10.3909
batch: [3880/21305] batch time: 0.063 trainign loss: 5.9632 avg training loss: 10.3875
batch: [3890/21305] batch time: 0.165 trainign loss: 13.3951 avg training loss: 10.3810
batch: [3900/21305] batch time: 0.056 trainign loss: 10.6717 avg training loss: 10.3817
batch: [3910/21305] batch time: 0.062 trainign loss: 10.5113 avg training loss: 10.3822
batch: [3920/21305] batch time: 0.061 trainign loss: 9.5529 avg training loss: 10.3825
batch: [3930/21305] batch time: 0.056 trainign loss: 10.8054 avg training loss: 10.3830
batch: [3940/21305] batch time: 0.053 trainign loss: 10.5034 avg training loss: 10.3834
batch: [3950/21305] batch time: 0.057 trainign loss: 10.6253 avg training loss: 10.3839
batch: [3960/21305] batch time: 0.051 trainign loss: 10.5535 avg training loss: 10.3845
batch: [3970/21305] batch time: 0.056 trainign loss: 10.6307 avg training loss: 10.3849
batch: [3980/21305] batch time: 0.056 trainign loss: 10.4489 avg training loss: 10.3853
batch: [3990/21305] batch time: 0.056 trainign loss: 9.8001 avg training loss: 10.3848
batch: [4000/21305] batch time: 0.052 trainign loss: 10.5957 avg training loss: 10.3850
batch: [4010/21305] batch time: 0.056 trainign loss: 10.5652 avg training loss: 10.3855
batch: [4020/21305] batch time: 0.056 trainign loss: 9.6702 avg training loss: 10.3854
batch: [4030/21305] batch time: 0.056 trainign loss: 10.5937 avg training loss: 10.3860
batch: [4040/21305] batch time: 0.057 trainign loss: 10.6549 avg training loss: 10.3864
batch: [4050/21305] batch time: 0.056 trainign loss: 10.3000 avg training loss: 10.3868
batch: [4060/21305] batch time: 0.059 trainign loss: 11.6416 avg training loss: 10.3782
batch: [4070/21305] batch time: 0.063 trainign loss: 10.6469 avg training loss: 10.3778
batch: [4080/21305] batch time: 0.056 trainign loss: 10.6700 avg training loss: 10.3780
batch: [4090/21305] batch time: 0.059 trainign loss: 10.5345 avg training loss: 10.3779
batch: [4100/21305] batch time: 0.056 trainign loss: 10.5361 avg training loss: 10.3786
batch: [4110/21305] batch time: 0.062 trainign loss: 10.2577 avg training loss: 10.3788
batch: [4120/21305] batch time: 0.056 trainign loss: 10.4422 avg training loss: 10.3791
batch: [4130/21305] batch time: 0.056 trainign loss: 10.5965 avg training loss: 10.3793
batch: [4140/21305] batch time: 0.056 trainign loss: 10.2502 avg training loss: 10.3781
batch: [4150/21305] batch time: 0.056 trainign loss: 10.7774 avg training loss: 10.3787
batch: [4160/21305] batch time: 0.058 trainign loss: 10.5797 avg training loss: 10.3791
batch: [4170/21305] batch time: 0.062 trainign loss: 10.2473 avg training loss: 10.3794
batch: [4180/21305] batch time: 0.061 trainign loss: 10.6329 avg training loss: 10.3798
batch: [4190/21305] batch time: 0.056 trainign loss: 10.6998 avg training loss: 10.3799
batch: [4200/21305] batch time: 0.063 trainign loss: 10.6004 avg training loss: 10.3803
batch: [4210/21305] batch time: 0.056 trainign loss: 7.4793 avg training loss: 10.3789
batch: [4220/21305] batch time: 0.060 trainign loss: 10.9499 avg training loss: 10.3786
batch: [4230/21305] batch time: 0.057 trainign loss: 10.5686 avg training loss: 10.3781
batch: [4240/21305] batch time: 0.053 trainign loss: 10.5146 avg training loss: 10.3784
batch: [4250/21305] batch time: 0.063 trainign loss: 2.2534 avg training loss: 10.3700
batch: [4260/21305] batch time: 0.060 trainign loss: 10.7347 avg training loss: 10.3730
batch: [4270/21305] batch time: 0.062 trainign loss: 10.6443 avg training loss: 10.3733
batch: [4280/21305] batch time: 0.055 trainign loss: 10.5859 avg training loss: 10.3740
batch: [4290/21305] batch time: 0.056 trainign loss: 10.5371 avg training loss: 10.3742
batch: [4300/21305] batch time: 0.051 trainign loss: 10.6711 avg training loss: 10.3747
batch: [4310/21305] batch time: 0.055 trainign loss: 10.4764 avg training loss: 10.3747
batch: [4320/21305] batch time: 0.056 trainign loss: 10.3307 avg training loss: 10.3751
batch: [4330/21305] batch time: 0.056 trainign loss: 9.9492 avg training loss: 10.3753
batch: [4340/21305] batch time: 0.051 trainign loss: 10.2396 avg training loss: 10.3753
batch: [4350/21305] batch time: 0.057 trainign loss: 10.3990 avg training loss: 10.3757
batch: [4360/21305] batch time: 0.056 trainign loss: 10.4548 avg training loss: 10.3756
batch: [4370/21305] batch time: 0.056 trainign loss: 10.8470 avg training loss: 10.3747
batch: [4380/21305] batch time: 0.057 trainign loss: 10.5987 avg training loss: 10.3753
batch: [4390/21305] batch time: 0.056 trainign loss: 10.5154 avg training loss: 10.3747
batch: [4400/21305] batch time: 0.051 trainign loss: 10.6836 avg training loss: 10.3752
batch: [4410/21305] batch time: 0.056 trainign loss: 10.5006 avg training loss: 10.3757
batch: [4420/21305] batch time: 0.056 trainign loss: 10.5938 avg training loss: 10.3761
batch: [4430/21305] batch time: 0.062 trainign loss: 8.2002 avg training loss: 10.3752
batch: [4440/21305] batch time: 0.051 trainign loss: 12.7396 avg training loss: 10.3690
batch: [4450/21305] batch time: 0.057 trainign loss: 10.7177 avg training loss: 10.3696
batch: [4460/21305] batch time: 0.056 trainign loss: 9.8786 avg training loss: 10.3698
batch: [4470/21305] batch time: 0.059 trainign loss: 11.3732 avg training loss: 10.3661
batch: [4480/21305] batch time: 0.062 trainign loss: 10.3462 avg training loss: 10.3670
batch: [4490/21305] batch time: 0.056 trainign loss: 8.7571 avg training loss: 10.3662
batch: [4500/21305] batch time: 0.056 trainign loss: 10.7760 avg training loss: 10.3665
batch: [4510/21305] batch time: 0.059 trainign loss: 10.6187 avg training loss: 10.3660
batch: [4520/21305] batch time: 0.056 trainign loss: 10.6280 avg training loss: 10.3665
batch: [4530/21305] batch time: 0.056 trainign loss: 10.6133 avg training loss: 10.3668
batch: [4540/21305] batch time: 0.056 trainign loss: 10.4838 avg training loss: 10.3673
batch: [4550/21305] batch time: 0.060 trainign loss: 10.5154 avg training loss: 10.3674
batch: [4560/21305] batch time: 0.050 trainign loss: 10.6220 avg training loss: 10.3676
batch: [4570/21305] batch time: 0.057 trainign loss: 10.6029 avg training loss: 10.3680
batch: [4580/21305] batch time: 0.053 trainign loss: 10.5760 avg training loss: 10.3685
batch: [4590/21305] batch time: 0.051 trainign loss: 10.1508 avg training loss: 10.3684
batch: [4600/21305] batch time: 0.052 trainign loss: 10.5173 avg training loss: 10.3688
batch: [4610/21305] batch time: 0.057 trainign loss: 10.5643 avg training loss: 10.3692
batch: [4620/21305] batch time: 0.057 trainign loss: 10.3741 avg training loss: 10.3688
batch: [4630/21305] batch time: 0.057 trainign loss: 10.4969 avg training loss: 10.3693
batch: [4640/21305] batch time: 0.057 trainign loss: 10.4193 avg training loss: 10.3697
batch: [4650/21305] batch time: 0.057 trainign loss: 10.6002 avg training loss: 10.3702
batch: [4660/21305] batch time: 0.061 trainign loss: 10.6104 avg training loss: 10.3705
batch: [4670/21305] batch time: 0.056 trainign loss: 10.6213 avg training loss: 10.3698
batch: [4680/21305] batch time: 0.057 trainign loss: 10.6950 avg training loss: 10.3702
batch: [4690/21305] batch time: 0.058 trainign loss: 10.4323 avg training loss: 10.3707
batch: [4700/21305] batch time: 0.052 trainign loss: 10.5777 avg training loss: 10.3706
batch: [4710/21305] batch time: 0.056 trainign loss: 10.6226 avg training loss: 10.3711
batch: [4720/21305] batch time: 0.050 trainign loss: 10.6132 avg training loss: 10.3716
batch: [4730/21305] batch time: 0.061 trainign loss: 10.6507 avg training loss: 10.3720
batch: [4740/21305] batch time: 0.053 trainign loss: 10.5997 avg training loss: 10.3720
batch: [4750/21305] batch time: 1.036 trainign loss: 10.6662 avg training loss: 10.3725
batch: [4760/21305] batch time: 0.057 trainign loss: 9.9423 avg training loss: 10.3727
batch: [4770/21305] batch time: 1.451 trainign loss: 10.4564 avg training loss: 10.3729
batch: [4780/21305] batch time: 0.055 trainign loss: 9.2806 avg training loss: 10.3729
batch: [4790/21305] batch time: 1.201 trainign loss: 10.6488 avg training loss: 10.3720
batch: [4800/21305] batch time: 0.062 trainign loss: 10.5620 avg training loss: 10.3724
batch: [4810/21305] batch time: 1.154 trainign loss: 10.1366 avg training loss: 10.3728
batch: [4820/21305] batch time: 0.056 trainign loss: 10.6577 avg training loss: 10.3730
batch: [4830/21305] batch time: 0.400 trainign loss: 10.4067 avg training loss: 10.3733
batch: [4840/21305] batch time: 0.061 trainign loss: 7.5064 avg training loss: 10.3722
batch: [4850/21305] batch time: 0.057 trainign loss: 14.9926 avg training loss: 10.3657
batch: [4860/21305] batch time: 0.051 trainign loss: 9.7202 avg training loss: 10.3672
batch: [4870/21305] batch time: 0.057 trainign loss: 10.6286 avg training loss: 10.3675
batch: [4880/21305] batch time: 0.063 trainign loss: 10.6503 avg training loss: 10.3680
batch: [4890/21305] batch time: 0.058 trainign loss: 10.6232 avg training loss: 10.3683
batch: [4900/21305] batch time: 0.051 trainign loss: 10.6769 avg training loss: 10.3686
batch: [4910/21305] batch time: 0.056 trainign loss: 10.2082 avg training loss: 10.3682
batch: [4920/21305] batch time: 0.054 trainign loss: 10.7342 avg training loss: 10.3687
batch: [4930/21305] batch time: 0.062 trainign loss: 10.4429 avg training loss: 10.3683
batch: [4940/21305] batch time: 0.051 trainign loss: 10.6587 avg training loss: 10.3686
batch: [4950/21305] batch time: 0.056 trainign loss: 10.2175 avg training loss: 10.3689
batch: [4960/21305] batch time: 0.053 trainign loss: 10.4791 avg training loss: 10.3685
batch: [4970/21305] batch time: 0.061 trainign loss: 10.7431 avg training loss: 10.3690
batch: [4980/21305] batch time: 0.053 trainign loss: 10.4819 avg training loss: 10.3694
batch: [4990/21305] batch time: 0.057 trainign loss: 1.6149 avg training loss: 10.3615
batch: [5000/21305] batch time: 0.062 trainign loss: 11.0495 avg training loss: 10.3594
batch: [5010/21305] batch time: 0.056 trainign loss: 10.6349 avg training loss: 10.3600
batch: [5020/21305] batch time: 0.062 trainign loss: 10.5929 avg training loss: 10.3599
batch: [5030/21305] batch time: 0.058 trainign loss: 10.7329 avg training loss: 10.3598
batch: [5040/21305] batch time: 0.056 trainign loss: 9.1308 avg training loss: 10.3599
batch: [5050/21305] batch time: 0.056 trainign loss: 10.3797 avg training loss: 10.3593
batch: [5060/21305] batch time: 0.054 trainign loss: 10.6385 avg training loss: 10.3598
batch: [5070/21305] batch time: 0.058 trainign loss: 10.2653 avg training loss: 10.3603
batch: [5080/21305] batch time: 0.056 trainign loss: 1.1891 avg training loss: 10.3493
batch: [5090/21305] batch time: 0.056 trainign loss: 10.6621 avg training loss: 10.3531
batch: [5100/21305] batch time: 0.050 trainign loss: 10.5927 avg training loss: 10.3536
batch: [5110/21305] batch time: 0.061 trainign loss: 10.5234 avg training loss: 10.3539
batch: [5120/21305] batch time: 0.057 trainign loss: 10.6396 avg training loss: 10.3544
batch: [5130/21305] batch time: 0.062 trainign loss: 10.5306 avg training loss: 10.3547
batch: [5140/21305] batch time: 0.051 trainign loss: 10.5362 avg training loss: 10.3541
batch: [5150/21305] batch time: 0.063 trainign loss: 10.8215 avg training loss: 10.3546
batch: [5160/21305] batch time: 0.053 trainign loss: 10.6510 avg training loss: 10.3552
batch: [5170/21305] batch time: 0.058 trainign loss: 10.6299 avg training loss: 10.3556
batch: [5180/21305] batch time: 0.057 trainign loss: 10.6055 avg training loss: 10.3560
batch: [5190/21305] batch time: 0.058 trainign loss: 10.5901 avg training loss: 10.3564
batch: [5200/21305] batch time: 0.062 trainign loss: 10.4473 avg training loss: 10.3568
batch: [5210/21305] batch time: 0.063 trainign loss: 10.5540 avg training loss: 10.3572
batch: [5220/21305] batch time: 0.057 trainign loss: 10.5940 avg training loss: 10.3575
batch: [5230/21305] batch time: 0.058 trainign loss: 10.7145 avg training loss: 10.3579
batch: [5240/21305] batch time: 0.051 trainign loss: 10.3660 avg training loss: 10.3583
batch: [5250/21305] batch time: 0.062 trainign loss: 10.4003 avg training loss: 10.3578
batch: [5260/21305] batch time: 0.051 trainign loss: 10.6577 avg training loss: 10.3584
batch: [5270/21305] batch time: 0.058 trainign loss: 10.6314 avg training loss: 10.3589
batch: [5280/21305] batch time: 0.057 trainign loss: 10.6165 avg training loss: 10.3594
batch: [5290/21305] batch time: 0.056 trainign loss: 10.3778 avg training loss: 10.3594
batch: [5300/21305] batch time: 0.056 trainign loss: 10.6293 avg training loss: 10.3598
batch: [5310/21305] batch time: 0.062 trainign loss: 10.6740 avg training loss: 10.3601
batch: [5320/21305] batch time: 0.056 trainign loss: 10.7277 avg training loss: 10.3606
batch: [5330/21305] batch time: 0.057 trainign loss: 10.6169 avg training loss: 10.3610
batch: [5340/21305] batch time: 0.052 trainign loss: 10.5738 avg training loss: 10.3606
batch: [5350/21305] batch time: 0.056 trainign loss: 10.5711 avg training loss: 10.3603
batch: [5360/21305] batch time: 0.050 trainign loss: 10.8148 avg training loss: 10.3606
batch: [5370/21305] batch time: 0.825 trainign loss: 10.6578 avg training loss: 10.3610
batch: [5380/21305] batch time: 0.056 trainign loss: 10.6120 avg training loss: 10.3614
batch: [5390/21305] batch time: 1.624 trainign loss: 10.2151 avg training loss: 10.3616
batch: [5400/21305] batch time: 0.062 trainign loss: 10.5877 avg training loss: 10.3620
batch: [5410/21305] batch time: 0.885 trainign loss: 10.5817 avg training loss: 10.3624
batch: [5420/21305] batch time: 0.063 trainign loss: 10.5783 avg training loss: 10.3625
batch: [5430/21305] batch time: 0.701 trainign loss: 10.6935 avg training loss: 10.3630
batch: [5440/21305] batch time: 0.056 trainign loss: 10.4631 avg training loss: 10.3630
batch: [5450/21305] batch time: 1.147 trainign loss: 10.1802 avg training loss: 10.3627
batch: [5460/21305] batch time: 0.062 trainign loss: 10.6527 avg training loss: 10.3631
batch: [5470/21305] batch time: 0.797 trainign loss: 10.3931 avg training loss: 10.3632
batch: [5480/21305] batch time: 0.062 trainign loss: 10.6410 avg training loss: 10.3635
batch: [5490/21305] batch time: 0.056 trainign loss: 10.3993 avg training loss: 10.3639
batch: [5500/21305] batch time: 0.059 trainign loss: 10.5390 avg training loss: 10.3643
batch: [5510/21305] batch time: 0.056 trainign loss: 8.4318 avg training loss: 10.3638
batch: [5520/21305] batch time: 0.062 trainign loss: 10.8566 avg training loss: 10.3631
batch: [5530/21305] batch time: 0.247 trainign loss: 10.6096 avg training loss: 10.3636
batch: [5540/21305] batch time: 0.056 trainign loss: 10.5758 avg training loss: 10.3639
batch: [5550/21305] batch time: 0.552 trainign loss: 10.6310 avg training loss: 10.3644
batch: [5560/21305] batch time: 0.062 trainign loss: 10.2832 avg training loss: 10.3644
batch: [5570/21305] batch time: 0.529 trainign loss: 8.9689 avg training loss: 10.3642
batch: [5580/21305] batch time: 0.056 trainign loss: 11.4149 avg training loss: 10.3619
batch: [5590/21305] batch time: 0.840 trainign loss: 10.6048 avg training loss: 10.3630
batch: [5600/21305] batch time: 0.058 trainign loss: 10.6103 avg training loss: 10.3632
batch: [5610/21305] batch time: 0.054 trainign loss: 10.5931 avg training loss: 10.3635
batch: [5620/21305] batch time: 0.059 trainign loss: 10.6002 avg training loss: 10.3639
batch: [5630/21305] batch time: 0.063 trainign loss: 10.7278 avg training loss: 10.3642
batch: [5640/21305] batch time: 0.056 trainign loss: 10.3921 avg training loss: 10.3644
batch: [5650/21305] batch time: 0.051 trainign loss: 10.6528 avg training loss: 10.3648
batch: [5660/21305] batch time: 0.062 trainign loss: 10.0136 avg training loss: 10.3651
batch: [5670/21305] batch time: 0.052 trainign loss: 10.6347 avg training loss: 10.3655
batch: [5680/21305] batch time: 0.062 trainign loss: 10.4634 avg training loss: 10.3657
batch: [5690/21305] batch time: 0.057 trainign loss: 10.4723 avg training loss: 10.3662
batch: [5700/21305] batch time: 0.056 trainign loss: 10.4999 avg training loss: 10.3663
batch: [5710/21305] batch time: 0.050 trainign loss: 10.4108 avg training loss: 10.3667
batch: [5720/21305] batch time: 0.050 trainign loss: 10.6290 avg training loss: 10.3662
batch: [5730/21305] batch time: 0.062 trainign loss: 10.5221 avg training loss: 10.3666
batch: [5740/21305] batch time: 0.059 trainign loss: 10.5638 avg training loss: 10.3670
batch: [5750/21305] batch time: 0.056 trainign loss: 10.6218 avg training loss: 10.3675
batch: [5760/21305] batch time: 0.051 trainign loss: 10.3691 avg training loss: 10.3679
batch: [5770/21305] batch time: 0.056 trainign loss: 10.4926 avg training loss: 10.3682
batch: [5780/21305] batch time: 0.062 trainign loss: 10.5923 avg training loss: 10.3685
batch: [5790/21305] batch time: 0.056 trainign loss: 10.6604 avg training loss: 10.3689
batch: [5800/21305] batch time: 0.055 trainign loss: 10.4453 avg training loss: 10.3692
batch: [5810/21305] batch time: 0.062 trainign loss: 10.6347 avg training loss: 10.3696
batch: [5820/21305] batch time: 0.056 trainign loss: 10.5938 avg training loss: 10.3700
batch: [5830/21305] batch time: 0.062 trainign loss: 10.4239 avg training loss: 10.3703
batch: [5840/21305] batch time: 0.061 trainign loss: 10.6504 avg training loss: 10.3706
batch: [5850/21305] batch time: 0.056 trainign loss: 10.6375 avg training loss: 10.3710
batch: [5860/21305] batch time: 0.052 trainign loss: 10.6425 avg training loss: 10.3714
batch: [5870/21305] batch time: 0.059 trainign loss: 10.5738 avg training loss: 10.3717
batch: [5880/21305] batch time: 0.059 trainign loss: 10.4516 avg training loss: 10.3721
batch: [5890/21305] batch time: 0.056 trainign loss: 10.7081 avg training loss: 10.3725
batch: [5900/21305] batch time: 0.057 trainign loss: 10.3671 avg training loss: 10.3728
batch: [5910/21305] batch time: 0.211 trainign loss: 10.1337 avg training loss: 10.3723
batch: [5920/21305] batch time: 0.057 trainign loss: 9.7055 avg training loss: 10.3719
batch: [5930/21305] batch time: 0.814 trainign loss: 10.6525 avg training loss: 10.3723
batch: [5940/21305] batch time: 0.060 trainign loss: 10.4136 avg training loss: 10.3726
batch: [5950/21305] batch time: 0.062 trainign loss: 10.3383 avg training loss: 10.3723
batch: [5960/21305] batch time: 0.060 trainign loss: 9.8219 avg training loss: 10.3723
batch: [5970/21305] batch time: 0.059 trainign loss: 10.6219 avg training loss: 10.3726
batch: [5980/21305] batch time: 0.051 trainign loss: 10.4567 avg training loss: 10.3730
batch: [5990/21305] batch time: 0.056 trainign loss: 10.5902 avg training loss: 10.3735
batch: [6000/21305] batch time: 0.051 trainign loss: 10.6438 avg training loss: 10.3739
batch: [6010/21305] batch time: 0.056 trainign loss: 10.5723 avg training loss: 10.3742
batch: [6020/21305] batch time: 0.063 trainign loss: 10.5985 avg training loss: 10.3746
batch: [6030/21305] batch time: 0.462 trainign loss: 10.6481 avg training loss: 10.3747
batch: [6040/21305] batch time: 0.056 trainign loss: 8.9488 avg training loss: 10.3743
batch: [6050/21305] batch time: 0.058 trainign loss: 10.6443 avg training loss: 10.3742
batch: [6060/21305] batch time: 0.063 trainign loss: 10.7214 avg training loss: 10.3741
batch: [6070/21305] batch time: 0.122 trainign loss: 9.2361 avg training loss: 10.3741
batch: [6080/21305] batch time: 0.061 trainign loss: 10.0709 avg training loss: 10.3740
batch: [6090/21305] batch time: 0.740 trainign loss: 10.6800 avg training loss: 10.3740
batch: [6100/21305] batch time: 0.054 trainign loss: 10.5918 avg training loss: 10.3744
batch: [6110/21305] batch time: 1.085 trainign loss: 10.6645 avg training loss: 10.3745
batch: [6120/21305] batch time: 0.057 trainign loss: 10.5842 avg training loss: 10.3748
batch: [6130/21305] batch time: 0.841 trainign loss: 10.6488 avg training loss: 10.3744
batch: [6140/21305] batch time: 0.056 trainign loss: 10.7140 avg training loss: 10.3747
batch: [6150/21305] batch time: 1.034 trainign loss: 10.4694 avg training loss: 10.3751
batch: [6160/21305] batch time: 0.056 trainign loss: 10.6528 avg training loss: 10.3755
batch: [6170/21305] batch time: 0.793 trainign loss: 10.3159 avg training loss: 10.3758
batch: [6180/21305] batch time: 0.060 trainign loss: 10.6449 avg training loss: 10.3757
batch: [6190/21305] batch time: 0.062 trainign loss: 10.6413 avg training loss: 10.3761
batch: [6200/21305] batch time: 0.055 trainign loss: 10.5541 avg training loss: 10.3765
batch: [6210/21305] batch time: 0.057 trainign loss: 10.5998 avg training loss: 10.3768
batch: [6220/21305] batch time: 0.056 trainign loss: 10.6035 avg training loss: 10.3771
batch: [6230/21305] batch time: 0.392 trainign loss: 10.5912 avg training loss: 10.3774
batch: [6240/21305] batch time: 0.056 trainign loss: 10.6121 avg training loss: 10.3777
batch: [6250/21305] batch time: 0.989 trainign loss: 10.6184 avg training loss: 10.3780
batch: [6260/21305] batch time: 0.056 trainign loss: 10.5684 avg training loss: 10.3782
batch: [6270/21305] batch time: 1.291 trainign loss: 10.6496 avg training loss: 10.3786
batch: [6280/21305] batch time: 0.056 trainign loss: 10.6653 avg training loss: 10.3789
batch: [6290/21305] batch time: 1.300 trainign loss: 9.8313 avg training loss: 10.3790
batch: [6300/21305] batch time: 0.051 trainign loss: 10.6301 avg training loss: 10.3793
batch: [6310/21305] batch time: 1.169 trainign loss: 10.5880 avg training loss: 10.3796
batch: [6320/21305] batch time: 0.062 trainign loss: 10.6497 avg training loss: 10.3800
batch: [6330/21305] batch time: 1.316 trainign loss: 10.6163 avg training loss: 10.3801
batch: [6340/21305] batch time: 0.056 trainign loss: 9.9181 avg training loss: 10.3802
batch: [6350/21305] batch time: 0.833 trainign loss: 10.5983 avg training loss: 10.3804
batch: [6360/21305] batch time: 0.056 trainign loss: 10.5531 avg training loss: 10.3807
batch: [6370/21305] batch time: 0.551 trainign loss: 10.6231 avg training loss: 10.3811
batch: [6380/21305] batch time: 0.056 trainign loss: 10.5118 avg training loss: 10.3812
batch: [6390/21305] batch time: 0.592 trainign loss: 10.0510 avg training loss: 10.3813
batch: [6400/21305] batch time: 0.063 trainign loss: 10.7133 avg training loss: 10.3813
batch: [6410/21305] batch time: 0.063 trainign loss: 10.6619 avg training loss: 10.3817
batch: [6420/21305] batch time: 0.056 trainign loss: 10.3141 avg training loss: 10.3816
batch: [6430/21305] batch time: 0.681 trainign loss: 10.3283 avg training loss: 10.3817
batch: [6440/21305] batch time: 0.062 trainign loss: 10.6591 avg training loss: 10.3815
batch: [6450/21305] batch time: 0.698 trainign loss: 10.7811 avg training loss: 10.3810
batch: [6460/21305] batch time: 0.063 trainign loss: 10.6418 avg training loss: 10.3816
batch: [6470/21305] batch time: 0.511 trainign loss: 10.6154 avg training loss: 10.3818
batch: [6480/21305] batch time: 0.062 trainign loss: 10.5278 avg training loss: 10.3821
batch: [6490/21305] batch time: 0.813 trainign loss: 10.5910 avg training loss: 10.3825
batch: [6500/21305] batch time: 0.057 trainign loss: 10.5896 avg training loss: 10.3828
batch: [6510/21305] batch time: 0.848 trainign loss: 10.6208 avg training loss: 10.3830
batch: [6520/21305] batch time: 0.062 trainign loss: 10.7484 avg training loss: 10.3833
batch: [6530/21305] batch time: 1.581 trainign loss: 10.5957 avg training loss: 10.3835
batch: [6540/21305] batch time: 0.056 trainign loss: 10.6537 avg training loss: 10.3836
batch: [6550/21305] batch time: 1.655 trainign loss: 10.4254 avg training loss: 10.3838
batch: [6560/21305] batch time: 0.056 trainign loss: 10.6894 avg training loss: 10.3836
batch: [6570/21305] batch time: 0.893 trainign loss: 10.2328 avg training loss: 10.3838
batch: [6580/21305] batch time: 0.056 trainign loss: 9.9196 avg training loss: 10.3841
batch: [6590/21305] batch time: 0.678 trainign loss: 10.4531 avg training loss: 10.3843
batch: [6600/21305] batch time: 0.062 trainign loss: 10.5892 avg training loss: 10.3846
batch: [6610/21305] batch time: 0.056 trainign loss: 10.6183 avg training loss: 10.3848
batch: [6620/21305] batch time: 0.056 trainign loss: 10.6158 avg training loss: 10.3851
batch: [6630/21305] batch time: 0.052 trainign loss: 10.6634 avg training loss: 10.3850
batch: [6640/21305] batch time: 0.056 trainign loss: 9.9737 avg training loss: 10.3852
batch: [6650/21305] batch time: 0.051 trainign loss: 10.5464 avg training loss: 10.3854
batch: [6660/21305] batch time: 0.057 trainign loss: 10.5853 avg training loss: 10.3857
batch: [6670/21305] batch time: 0.056 trainign loss: 10.5315 avg training loss: 10.3857
batch: [6680/21305] batch time: 0.056 trainign loss: 10.6019 avg training loss: 10.3861
batch: [6690/21305] batch time: 0.053 trainign loss: 10.6130 avg training loss: 10.3864
batch: [6700/21305] batch time: 0.056 trainign loss: 10.2657 avg training loss: 10.3863
batch: [6710/21305] batch time: 0.053 trainign loss: 10.0002 avg training loss: 10.3863
batch: [6720/21305] batch time: 0.059 trainign loss: 10.5550 avg training loss: 10.3864
batch: [6730/21305] batch time: 1.052 trainign loss: 10.3350 avg training loss: 10.3865
batch: [6740/21305] batch time: 0.056 trainign loss: 10.6448 avg training loss: 10.3869
batch: [6750/21305] batch time: 1.710 trainign loss: 10.6388 avg training loss: 10.3872
batch: [6760/21305] batch time: 0.055 trainign loss: 10.6134 avg training loss: 10.3875
batch: [6770/21305] batch time: 1.267 trainign loss: 10.4241 avg training loss: 10.3877
batch: [6780/21305] batch time: 0.056 trainign loss: 10.6000 avg training loss: 10.3879
batch: [6790/21305] batch time: 1.218 trainign loss: 10.5595 avg training loss: 10.3881
batch: [6800/21305] batch time: 0.056 trainign loss: 10.4280 avg training loss: 10.3883
batch: [6810/21305] batch time: 1.090 trainign loss: 10.6784 avg training loss: 10.3884
batch: [6820/21305] batch time: 0.056 trainign loss: 10.1859 avg training loss: 10.3886
batch: [6830/21305] batch time: 0.917 trainign loss: 10.6459 avg training loss: 10.3890
batch: [6840/21305] batch time: 0.062 trainign loss: 10.3726 avg training loss: 10.3858
batch: [6850/21305] batch time: 0.272 trainign loss: 10.3900 avg training loss: 10.3865
batch: [6860/21305] batch time: 0.056 trainign loss: 10.5801 avg training loss: 10.3867
batch: [6870/21305] batch time: 1.710 trainign loss: 10.4127 avg training loss: 10.3859
batch: [6880/21305] batch time: 0.056 trainign loss: 10.6119 avg training loss: 10.3861
batch: [6890/21305] batch time: 1.622 trainign loss: 10.5938 avg training loss: 10.3859
batch: [6900/21305] batch time: 0.056 trainign loss: 10.6769 avg training loss: 10.3861
batch: [6910/21305] batch time: 1.597 trainign loss: 10.6343 avg training loss: 10.3864
batch: [6920/21305] batch time: 0.056 trainign loss: 10.4807 avg training loss: 10.3866
batch: [6930/21305] batch time: 1.695 trainign loss: 10.5655 avg training loss: 10.3867
batch: [6940/21305] batch time: 0.126 trainign loss: 10.5127 avg training loss: 10.3860
batch: [6950/21305] batch time: 0.890 trainign loss: 10.5123 avg training loss: 10.3861
batch: [6960/21305] batch time: 0.058 trainign loss: 9.2939 avg training loss: 10.3856
batch: [6970/21305] batch time: 1.548 trainign loss: 8.9554 avg training loss: 10.3855
batch: [6980/21305] batch time: 0.307 trainign loss: 10.6820 avg training loss: 10.3858
batch: [6990/21305] batch time: 1.864 trainign loss: 10.6894 avg training loss: 10.3861
batch: [7000/21305] batch time: 0.062 trainign loss: 10.4201 avg training loss: 10.3863
batch: [7010/21305] batch time: 2.322 trainign loss: 10.2496 avg training loss: 10.3863
batch: [7020/21305] batch time: 0.061 trainign loss: 10.6237 avg training loss: 10.3866
batch: [7030/21305] batch time: 2.343 trainign loss: 10.6969 avg training loss: 10.3869
batch: [7040/21305] batch time: 0.052 trainign loss: 10.6583 avg training loss: 10.3872
batch: [7050/21305] batch time: 2.153 trainign loss: 10.6160 avg training loss: 10.3870
batch: [7060/21305] batch time: 0.056 trainign loss: 10.1504 avg training loss: 10.3871
batch: [7070/21305] batch time: 1.335 trainign loss: 10.7691 avg training loss: 10.3875
batch: [7080/21305] batch time: 0.057 trainign loss: 10.6417 avg training loss: 10.3878
batch: [7090/21305] batch time: 1.978 trainign loss: 10.4665 avg training loss: 10.3879
batch: [7100/21305] batch time: 0.055 trainign loss: 10.4695 avg training loss: 10.3882
batch: [7110/21305] batch time: 2.260 trainign loss: 10.6860 avg training loss: 10.3881
batch: [7120/21305] batch time: 0.056 trainign loss: 9.1568 avg training loss: 10.3871
batch: [7130/21305] batch time: 2.490 trainign loss: 10.7100 avg training loss: 10.3873
batch: [7140/21305] batch time: 0.056 trainign loss: 10.5966 avg training loss: 10.3872
batch: [7150/21305] batch time: 2.293 trainign loss: 9.9808 avg training loss: 10.3874
batch: [7160/21305] batch time: 0.056 trainign loss: 10.5748 avg training loss: 10.3874
batch: [7170/21305] batch time: 2.340 trainign loss: 10.6703 avg training loss: 10.3875
batch: [7180/21305] batch time: 0.062 trainign loss: 10.5896 avg training loss: 10.3878
batch: [7190/21305] batch time: 2.351 trainign loss: 10.1004 avg training loss: 10.3880
batch: [7200/21305] batch time: 0.062 trainign loss: 10.3342 avg training loss: 10.3876
batch: [7210/21305] batch time: 2.070 trainign loss: 10.5090 avg training loss: 10.3879
batch: [7220/21305] batch time: 1.121 trainign loss: 10.5005 avg training loss: 10.3880
batch: [7230/21305] batch time: 0.587 trainign loss: 10.2791 avg training loss: 10.3882
batch: [7240/21305] batch time: 1.518 trainign loss: 10.7171 avg training loss: 10.3882
batch: [7250/21305] batch time: 1.656 trainign loss: 10.3692 avg training loss: 10.3886
batch: [7260/21305] batch time: 0.761 trainign loss: 10.2987 avg training loss: 10.3886
batch: [7270/21305] batch time: 2.287 trainign loss: 10.6260 avg training loss: 10.3889
batch: [7280/21305] batch time: 0.059 trainign loss: 10.4717 avg training loss: 10.3892
batch: [7290/21305] batch time: 2.163 trainign loss: 10.3451 avg training loss: 10.3893
batch: [7300/21305] batch time: 0.054 trainign loss: 10.7342 avg training loss: 10.3892
batch: [7310/21305] batch time: 2.500 trainign loss: 10.6183 avg training loss: 10.3895
batch: [7320/21305] batch time: 0.056 trainign loss: 10.7148 avg training loss: 10.3898
batch: [7330/21305] batch time: 2.126 trainign loss: 10.5416 avg training loss: 10.3900
batch: [7340/21305] batch time: 0.056 trainign loss: 10.6227 avg training loss: 10.3900
batch: [7350/21305] batch time: 2.686 trainign loss: 10.5448 avg training loss: 10.3902
batch: [7360/21305] batch time: 0.056 trainign loss: 10.5915 avg training loss: 10.3902
batch: [7370/21305] batch time: 1.985 trainign loss: 8.8108 avg training loss: 10.3900
batch: [7380/21305] batch time: 0.057 trainign loss: 10.4486 avg training loss: 10.3899
batch: [7390/21305] batch time: 2.386 trainign loss: 10.6538 avg training loss: 10.3899
batch: [7400/21305] batch time: 0.056 trainign loss: 10.1438 avg training loss: 10.3901
batch: [7410/21305] batch time: 2.166 trainign loss: 10.7577 avg training loss: 10.3889
batch: [7420/21305] batch time: 0.056 trainign loss: 10.3902 avg training loss: 10.3894
batch: [7430/21305] batch time: 2.182 trainign loss: 10.5972 avg training loss: 10.3897
batch: [7440/21305] batch time: 0.062 trainign loss: 10.5139 avg training loss: 10.3898
batch: [7450/21305] batch time: 2.309 trainign loss: 10.5731 avg training loss: 10.3901
batch: [7460/21305] batch time: 0.060 trainign loss: 8.0263 avg training loss: 10.3889
batch: [7470/21305] batch time: 2.423 trainign loss: 9.3464 avg training loss: 10.3890
batch: [7480/21305] batch time: 0.056 trainign loss: 10.5712 avg training loss: 10.3888
batch: [7490/21305] batch time: 2.338 trainign loss: 10.5076 avg training loss: 10.3890
batch: [7500/21305] batch time: 0.062 trainign loss: 10.6229 avg training loss: 10.3892
batch: [7510/21305] batch time: 2.109 trainign loss: 10.6025 avg training loss: 10.3892
batch: [7520/21305] batch time: 0.057 trainign loss: 10.6422 avg training loss: 10.3895
batch: [7530/21305] batch time: 2.111 trainign loss: 10.2901 avg training loss: 10.3898
batch: [7540/21305] batch time: 0.062 trainign loss: 10.6967 avg training loss: 10.3899
batch: [7550/21305] batch time: 1.849 trainign loss: 10.6272 avg training loss: 10.3902
batch: [7560/21305] batch time: 0.063 trainign loss: 10.7212 avg training loss: 10.3905
batch: [7570/21305] batch time: 1.630 trainign loss: 10.5028 avg training loss: 10.3906
batch: [7580/21305] batch time: 0.059 trainign loss: 10.5254 avg training loss: 10.3909
batch: [7590/21305] batch time: 1.294 trainign loss: 10.6423 avg training loss: 10.3910
batch: [7600/21305] batch time: 0.056 trainign loss: 10.6812 avg training loss: 10.3912
batch: [7610/21305] batch time: 1.354 trainign loss: 10.5823 avg training loss: 10.3915
batch: [7620/21305] batch time: 0.057 trainign loss: 10.4307 avg training loss: 10.3915
batch: [7630/21305] batch time: 1.373 trainign loss: 10.6150 avg training loss: 10.3916
batch: [7640/21305] batch time: 0.058 trainign loss: 10.5422 avg training loss: 10.3918
batch: [7650/21305] batch time: 2.364 trainign loss: 10.3495 avg training loss: 10.3919
batch: [7660/21305] batch time: 0.056 trainign loss: 10.7176 avg training loss: 10.3917
batch: [7670/21305] batch time: 2.068 trainign loss: 10.6264 avg training loss: 10.3920
batch: [7680/21305] batch time: 0.058 trainign loss: 10.2920 avg training loss: 10.3922
batch: [7690/21305] batch time: 2.366 trainign loss: 10.1209 avg training loss: 10.3923
batch: [7700/21305] batch time: 0.056 trainign loss: 10.6250 avg training loss: 10.3924
batch: [7710/21305] batch time: 2.629 trainign loss: 10.3496 avg training loss: 10.3917
batch: [7720/21305] batch time: 0.059 trainign loss: 10.7554 avg training loss: 10.3921
batch: [7730/21305] batch time: 2.098 trainign loss: 9.5562 avg training loss: 10.3900
batch: [7740/21305] batch time: 0.057 trainign loss: 10.7724 avg training loss: 10.3902
batch: [7750/21305] batch time: 2.461 trainign loss: 10.5619 avg training loss: 10.3905
batch: [7760/21305] batch time: 0.056 trainign loss: 10.7136 avg training loss: 10.3907
batch: [7770/21305] batch time: 2.648 trainign loss: 9.8947 avg training loss: 10.3909
batch: [7780/21305] batch time: 0.057 trainign loss: 10.7261 avg training loss: 10.3911
batch: [7790/21305] batch time: 2.529 trainign loss: 10.5900 avg training loss: 10.3914
batch: [7800/21305] batch time: 0.053 trainign loss: 10.5999 avg training loss: 10.3914
batch: [7810/21305] batch time: 2.170 trainign loss: 10.3799 avg training loss: 10.3915
batch: [7820/21305] batch time: 0.056 trainign loss: 10.6305 avg training loss: 10.3918
batch: [7830/21305] batch time: 1.997 trainign loss: 10.4671 avg training loss: 10.3920
batch: [7840/21305] batch time: 0.056 trainign loss: 10.6862 avg training loss: 10.3923
batch: [7850/21305] batch time: 2.237 trainign loss: 10.4032 avg training loss: 10.3924
batch: [7860/21305] batch time: 0.062 trainign loss: 10.3141 avg training loss: 10.3924
batch: [7870/21305] batch time: 2.281 trainign loss: 10.5431 avg training loss: 10.3926
batch: [7880/21305] batch time: 0.057 trainign loss: 9.8769 avg training loss: 10.3927
batch: [7890/21305] batch time: 2.482 trainign loss: 10.6204 avg training loss: 10.3929
batch: [7900/21305] batch time: 0.058 trainign loss: 10.3551 avg training loss: 10.3920
batch: [7910/21305] batch time: 2.476 trainign loss: 10.6580 avg training loss: 10.3924
batch: [7920/21305] batch time: 0.055 trainign loss: 10.3011 avg training loss: 10.3926
batch: [7930/21305] batch time: 2.280 trainign loss: 10.6189 avg training loss: 10.3928
batch: [7940/21305] batch time: 0.056 trainign loss: 10.6264 avg training loss: 10.3931
batch: [7950/21305] batch time: 2.308 trainign loss: 10.6850 avg training loss: 10.3934
batch: [7960/21305] batch time: 0.062 trainign loss: 10.4013 avg training loss: 10.3936
batch: [7970/21305] batch time: 2.677 trainign loss: 10.6090 avg training loss: 10.3939
batch: [7980/21305] batch time: 0.062 trainign loss: 10.6408 avg training loss: 10.3938
batch: [7990/21305] batch time: 2.192 trainign loss: 10.5978 avg training loss: 10.3941
batch: [8000/21305] batch time: 0.057 trainign loss: 10.7029 avg training loss: 10.3941
batch: [8010/21305] batch time: 2.206 trainign loss: 10.1905 avg training loss: 10.3942
batch: [8020/21305] batch time: 0.063 trainign loss: 8.4531 avg training loss: 10.3938
batch: [8030/21305] batch time: 2.009 trainign loss: 10.8153 avg training loss: 10.3912
batch: [8040/21305] batch time: 0.056 trainign loss: 10.6117 avg training loss: 10.3916
batch: [8050/21305] batch time: 2.441 trainign loss: 10.5718 avg training loss: 10.3918
batch: [8060/21305] batch time: 0.063 trainign loss: 10.6355 avg training loss: 10.3921
batch: [8070/21305] batch time: 2.268 trainign loss: 10.5330 avg training loss: 10.3923
batch: [8080/21305] batch time: 0.056 trainign loss: 10.5844 avg training loss: 10.3925
batch: [8090/21305] batch time: 1.237 trainign loss: 10.5762 avg training loss: 10.3927
batch: [8100/21305] batch time: 1.110 trainign loss: 10.5869 avg training loss: 10.3929
batch: [8110/21305] batch time: 0.761 trainign loss: 10.6550 avg training loss: 10.3931
batch: [8120/21305] batch time: 0.063 trainign loss: 10.4757 avg training loss: 10.3933
batch: [8130/21305] batch time: 0.408 trainign loss: 10.4722 avg training loss: 10.3934
batch: [8140/21305] batch time: 0.060 trainign loss: 10.4224 avg training loss: 10.3936
batch: [8150/21305] batch time: 0.058 trainign loss: 10.5907 avg training loss: 10.3938
batch: [8160/21305] batch time: 0.053 trainign loss: 10.6613 avg training loss: 10.3941
batch: [8170/21305] batch time: 0.056 trainign loss: 10.6265 avg training loss: 10.3944
batch: [8180/21305] batch time: 0.061 trainign loss: 10.5019 avg training loss: 10.3944
batch: [8190/21305] batch time: 0.565 trainign loss: 10.5197 avg training loss: 10.3946
batch: [8200/21305] batch time: 0.056 trainign loss: 10.5853 avg training loss: 10.3949
batch: [8210/21305] batch time: 0.406 trainign loss: 10.4076 avg training loss: 10.3951
batch: [8220/21305] batch time: 0.056 trainign loss: 10.4170 avg training loss: 10.3954
batch: [8230/21305] batch time: 0.936 trainign loss: 10.6229 avg training loss: 10.3954
batch: [8240/21305] batch time: 0.063 trainign loss: 10.5856 avg training loss: 10.3957
batch: [8250/21305] batch time: 0.056 trainign loss: 10.5777 avg training loss: 10.3959
batch: [8260/21305] batch time: 0.056 trainign loss: 10.6100 avg training loss: 10.3958
batch: [8270/21305] batch time: 0.058 trainign loss: 10.6925 avg training loss: 10.3960
batch: [8280/21305] batch time: 0.051 trainign loss: 10.5661 avg training loss: 10.3963
batch: [8290/21305] batch time: 0.056 trainign loss: 10.6402 avg training loss: 10.3965
batch: [8300/21305] batch time: 0.056 trainign loss: 10.3997 avg training loss: 10.3967
batch: [8310/21305] batch time: 0.062 trainign loss: 10.5693 avg training loss: 10.3969
batch: [8320/21305] batch time: 0.056 trainign loss: 10.6319 avg training loss: 10.3971
batch: [8330/21305] batch time: 0.056 trainign loss: 10.6133 avg training loss: 10.3974
batch: [8340/21305] batch time: 0.054 trainign loss: 10.6459 avg training loss: 10.3975
batch: [8350/21305] batch time: 0.062 trainign loss: 10.5061 avg training loss: 10.3976
batch: [8360/21305] batch time: 0.056 trainign loss: 10.3575 avg training loss: 10.3978
batch: [8370/21305] batch time: 0.056 trainign loss: 10.6733 avg training loss: 10.3977
batch: [8380/21305] batch time: 0.056 trainign loss: 10.4323 avg training loss: 10.3979
batch: [8390/21305] batch time: 0.058 trainign loss: 10.3211 avg training loss: 10.3975
batch: [8400/21305] batch time: 0.057 trainign loss: 11.1220 avg training loss: 10.3952
batch: [8410/21305] batch time: 0.062 trainign loss: 10.1742 avg training loss: 10.3953
batch: [8420/21305] batch time: 0.051 trainign loss: 9.5740 avg training loss: 10.3936
batch: [8430/21305] batch time: 0.056 trainign loss: 10.7952 avg training loss: 10.3941
batch: [8440/21305] batch time: 0.056 trainign loss: 10.6164 avg training loss: 10.3942
batch: [8450/21305] batch time: 0.056 trainign loss: 10.6768 avg training loss: 10.3944
batch: [8460/21305] batch time: 0.059 trainign loss: 10.6283 avg training loss: 10.3947
batch: [8470/21305] batch time: 0.323 trainign loss: 10.6051 avg training loss: 10.3947
batch: [8480/21305] batch time: 0.056 trainign loss: 10.6231 avg training loss: 10.3950
batch: [8490/21305] batch time: 0.784 trainign loss: 10.6339 avg training loss: 10.3952
batch: [8500/21305] batch time: 0.056 trainign loss: 10.5360 avg training loss: 10.3952
batch: [8510/21305] batch time: 1.584 trainign loss: 10.3505 avg training loss: 10.3953
batch: [8520/21305] batch time: 0.061 trainign loss: 10.7095 avg training loss: 10.3954
batch: [8530/21305] batch time: 1.957 trainign loss: 10.5901 avg training loss: 10.3955
batch: [8540/21305] batch time: 0.058 trainign loss: 10.5053 avg training loss: 10.3956
batch: [8550/21305] batch time: 1.765 trainign loss: 10.6314 avg training loss: 10.3958
batch: [8560/21305] batch time: 0.063 trainign loss: 10.6464 avg training loss: 10.3961
batch: [8570/21305] batch time: 1.656 trainign loss: 10.4138 avg training loss: 10.3961
batch: [8580/21305] batch time: 0.056 trainign loss: 10.5598 avg training loss: 10.3964
batch: [8590/21305] batch time: 1.503 trainign loss: 10.7200 avg training loss: 10.3965
batch: [8600/21305] batch time: 0.054 trainign loss: 10.6748 avg training loss: 10.3966
batch: [8610/21305] batch time: 1.461 trainign loss: 10.1928 avg training loss: 10.3966
batch: [8620/21305] batch time: 0.062 trainign loss: 10.7129 avg training loss: 10.3969
batch: [8630/21305] batch time: 1.817 trainign loss: 10.3312 avg training loss: 10.3969
batch: [8640/21305] batch time: 0.062 trainign loss: 10.6414 avg training loss: 10.3971
batch: [8650/21305] batch time: 1.394 trainign loss: 9.9739 avg training loss: 10.3971
batch: [8660/21305] batch time: 0.055 trainign loss: 9.9934 avg training loss: 10.3970
batch: [8670/21305] batch time: 2.190 trainign loss: 10.3325 avg training loss: 10.3971
batch: [8680/21305] batch time: 0.063 trainign loss: 10.6284 avg training loss: 10.3972
batch: [8690/21305] batch time: 2.376 trainign loss: 10.1742 avg training loss: 10.3973
batch: [8700/21305] batch time: 0.061 trainign loss: 10.0803 avg training loss: 10.3973
batch: [8710/21305] batch time: 2.626 trainign loss: 8.4899 avg training loss: 10.3971
batch: [8720/21305] batch time: 0.053 trainign loss: 0.0125 avg training loss: 10.3884
batch: [8730/21305] batch time: 2.494 trainign loss: 0.0000 avg training loss: 10.3765
batch: [8740/21305] batch time: 0.056 trainign loss: 10.5088 avg training loss: 10.3756
batch: [8750/21305] batch time: 2.401 trainign loss: 10.5753 avg training loss: 10.3757
batch: [8760/21305] batch time: 0.056 trainign loss: 10.6650 avg training loss: 10.3757
batch: [8770/21305] batch time: 2.382 trainign loss: 10.6797 avg training loss: 10.3758
batch: [8780/21305] batch time: 0.062 trainign loss: 10.4047 avg training loss: 10.3759
batch: [8790/21305] batch time: 2.421 trainign loss: 10.3763 avg training loss: 10.3761
batch: [8800/21305] batch time: 0.063 trainign loss: 10.5655 avg training loss: 10.3759
batch: [8810/21305] batch time: 2.157 trainign loss: 10.7370 avg training loss: 10.3761
batch: [8820/21305] batch time: 0.056 trainign loss: 10.5935 avg training loss: 10.3764
batch: [8830/21305] batch time: 1.861 trainign loss: 10.6063 avg training loss: 10.3766
batch: [8840/21305] batch time: 0.059 trainign loss: 10.6427 avg training loss: 10.3767
batch: [8850/21305] batch time: 2.076 trainign loss: 10.5560 avg training loss: 10.3769
batch: [8860/21305] batch time: 0.061 trainign loss: 10.4948 avg training loss: 10.3771
batch: [8870/21305] batch time: 1.546 trainign loss: 10.6652 avg training loss: 10.3774
batch: [8880/21305] batch time: 0.733 trainign loss: 10.3649 avg training loss: 10.3775
batch: [8890/21305] batch time: 0.433 trainign loss: 10.6653 avg training loss: 10.3778
batch: [8900/21305] batch time: 1.055 trainign loss: 10.6728 avg training loss: 10.3779
batch: [8910/21305] batch time: 0.494 trainign loss: 10.4900 avg training loss: 10.3781
batch: [8920/21305] batch time: 0.933 trainign loss: 8.2904 avg training loss: 10.3777
batch: [8930/21305] batch time: 0.134 trainign loss: 10.7625 avg training loss: 10.3779
batch: [8940/21305] batch time: 1.074 trainign loss: 10.6314 avg training loss: 10.3781
batch: [8950/21305] batch time: 0.057 trainign loss: 10.3939 avg training loss: 10.3783
batch: [8960/21305] batch time: 1.127 trainign loss: 10.6054 avg training loss: 10.3785
batch: [8970/21305] batch time: 0.234 trainign loss: 10.6217 avg training loss: 10.3788
batch: [8980/21305] batch time: 0.890 trainign loss: 10.5121 avg training loss: 10.3788
batch: [8990/21305] batch time: 0.057 trainign loss: 10.6890 avg training loss: 10.3789
batch: [9000/21305] batch time: 0.659 trainign loss: 6.3256 avg training loss: 10.3778
batch: [9010/21305] batch time: 0.056 trainign loss: 10.7617 avg training loss: 10.3777
batch: [9020/21305] batch time: 0.607 trainign loss: 10.4889 avg training loss: 10.3778
batch: [9030/21305] batch time: 0.056 trainign loss: 10.5331 avg training loss: 10.3778
batch: [9040/21305] batch time: 0.386 trainign loss: 10.4105 avg training loss: 10.3780
batch: [9050/21305] batch time: 0.056 trainign loss: 10.6561 avg training loss: 10.3781
batch: [9060/21305] batch time: 0.602 trainign loss: 10.4384 avg training loss: 10.3782
batch: [9070/21305] batch time: 0.056 trainign loss: 9.3757 avg training loss: 10.3781
batch: [9080/21305] batch time: 0.234 trainign loss: 10.7035 avg training loss: 10.3784
batch: [9090/21305] batch time: 0.058 trainign loss: 10.5129 avg training loss: 10.3784
batch: [9100/21305] batch time: 0.111 trainign loss: 10.7631 avg training loss: 10.3787
batch: [9110/21305] batch time: 0.062 trainign loss: 10.4882 avg training loss: 10.3789
batch: [9120/21305] batch time: 0.116 trainign loss: 10.6761 avg training loss: 10.3789
batch: [9130/21305] batch time: 0.056 trainign loss: 9.6217 avg training loss: 10.3782
batch: [9140/21305] batch time: 0.063 trainign loss: 10.8294 avg training loss: 10.3783
batch: [9150/21305] batch time: 0.056 trainign loss: 10.5295 avg training loss: 10.3786
batch: [9160/21305] batch time: 0.056 trainign loss: 10.5755 avg training loss: 10.3788
batch: [9170/21305] batch time: 0.057 trainign loss: 10.5492 avg training loss: 10.3785
batch: [9180/21305] batch time: 0.255 trainign loss: 10.6341 avg training loss: 10.3787
batch: [9190/21305] batch time: 0.056 trainign loss: 10.5869 avg training loss: 10.3790
batch: [9200/21305] batch time: 0.053 trainign loss: 10.6489 avg training loss: 10.3792
batch: [9210/21305] batch time: 0.348 trainign loss: 10.6698 avg training loss: 10.3794
batch: [9220/21305] batch time: 0.056 trainign loss: 10.1017 avg training loss: 10.3794
batch: [9230/21305] batch time: 0.806 trainign loss: 10.6736 avg training loss: 10.3797
batch: [9240/21305] batch time: 0.053 trainign loss: 9.8525 avg training loss: 10.3798
batch: [9250/21305] batch time: 1.168 trainign loss: 10.6171 avg training loss: 10.3799
batch: [9260/21305] batch time: 0.056 trainign loss: 10.4756 avg training loss: 10.3801
batch: [9270/21305] batch time: 1.410 trainign loss: 10.5916 avg training loss: 10.3803
batch: [9280/21305] batch time: 0.056 trainign loss: 10.6912 avg training loss: 10.3804
batch: [9290/21305] batch time: 1.590 trainign loss: 10.0227 avg training loss: 10.3804
batch: [9300/21305] batch time: 0.057 trainign loss: 10.6922 avg training loss: 10.3801
batch: [9310/21305] batch time: 0.878 trainign loss: 10.7228 avg training loss: 10.3803
batch: [9320/21305] batch time: 0.060 trainign loss: 10.3610 avg training loss: 10.3805
batch: [9330/21305] batch time: 0.056 trainign loss: 10.5311 avg training loss: 10.3805
batch: [9340/21305] batch time: 0.057 trainign loss: 10.5656 avg training loss: 10.3807
batch: [9350/21305] batch time: 0.063 trainign loss: 10.6575 avg training loss: 10.3810
batch: [9360/21305] batch time: 0.056 trainign loss: 9.7614 avg training loss: 10.3810
batch: [9370/21305] batch time: 0.063 trainign loss: 10.3480 avg training loss: 10.3812
batch: [9380/21305] batch time: 0.051 trainign loss: 10.4791 avg training loss: 10.3813
batch: [9390/21305] batch time: 0.061 trainign loss: 10.6349 avg training loss: 10.3815
batch: [9400/21305] batch time: 0.051 trainign loss: 10.5864 avg training loss: 10.3817
batch: [9410/21305] batch time: 0.056 trainign loss: 10.6629 avg training loss: 10.3818
batch: [9420/21305] batch time: 0.057 trainign loss: 9.5086 avg training loss: 10.3818
batch: [9430/21305] batch time: 0.058 trainign loss: 10.7738 avg training loss: 10.3818
batch: [9440/21305] batch time: 0.053 trainign loss: 10.6585 avg training loss: 10.3821
batch: [9450/21305] batch time: 0.056 trainign loss: 10.3444 avg training loss: 10.3815
batch: [9460/21305] batch time: 0.061 trainign loss: 10.8771 avg training loss: 10.3818
batch: [9470/21305] batch time: 0.062 trainign loss: 10.6070 avg training loss: 10.3820
batch: [9480/21305] batch time: 0.055 trainign loss: 10.5642 avg training loss: 10.3821
batch: [9490/21305] batch time: 0.062 trainign loss: 10.5811 avg training loss: 10.3819
batch: [9500/21305] batch time: 0.056 trainign loss: 10.5954 avg training loss: 10.3821
batch: [9510/21305] batch time: 0.061 trainign loss: 10.4219 avg training loss: 10.3820
batch: [9520/21305] batch time: 0.052 trainign loss: 10.6022 avg training loss: 10.3822
batch: [9530/21305] batch time: 0.056 trainign loss: 10.7412 avg training loss: 10.3825
batch: [9540/21305] batch time: 0.056 trainign loss: 10.6606 avg training loss: 10.3822
batch: [9550/21305] batch time: 0.056 trainign loss: 10.2504 avg training loss: 10.3825
batch: [9560/21305] batch time: 0.054 trainign loss: 10.3615 avg training loss: 10.3825
batch: [9570/21305] batch time: 0.058 trainign loss: 10.6102 avg training loss: 10.3827
batch: [9580/21305] batch time: 0.053 trainign loss: 10.7747 avg training loss: 10.3821
batch: [9590/21305] batch time: 0.057 trainign loss: 10.7675 avg training loss: 10.3826
batch: [9600/21305] batch time: 0.052 trainign loss: 10.6726 avg training loss: 10.3828
batch: [9610/21305] batch time: 0.056 trainign loss: 10.5854 avg training loss: 10.3828
batch: [9620/21305] batch time: 0.052 trainign loss: 10.7272 avg training loss: 10.3830
batch: [9630/21305] batch time: 0.056 trainign loss: 10.6863 avg training loss: 10.3832
batch: [9640/21305] batch time: 0.056 trainign loss: 10.6093 avg training loss: 10.3834
batch: [9650/21305] batch time: 0.055 trainign loss: 10.6311 avg training loss: 10.3836
batch: [9660/21305] batch time: 0.051 trainign loss: 9.9811 avg training loss: 10.3836
batch: [9670/21305] batch time: 0.062 trainign loss: 10.7847 avg training loss: 10.3838
batch: [9680/21305] batch time: 0.055 trainign loss: 10.6695 avg training loss: 10.3840
batch: [9690/21305] batch time: 0.057 trainign loss: 10.5968 avg training loss: 10.3843
batch: [9700/21305] batch time: 0.051 trainign loss: 10.5523 avg training loss: 10.3845
batch: [9710/21305] batch time: 0.056 trainign loss: 10.6025 avg training loss: 10.3847
batch: [9720/21305] batch time: 0.056 trainign loss: 10.6229 avg training loss: 10.3849
batch: [9730/21305] batch time: 0.058 trainign loss: 10.6180 avg training loss: 10.3851
batch: [9740/21305] batch time: 0.057 trainign loss: 10.7092 avg training loss: 10.3851
batch: [9750/21305] batch time: 0.062 trainign loss: 10.6830 avg training loss: 10.3853
batch: [9760/21305] batch time: 0.053 trainign loss: 10.6078 avg training loss: 10.3855
batch: [9770/21305] batch time: 0.059 trainign loss: 10.6687 avg training loss: 10.3856
batch: [9780/21305] batch time: 0.054 trainign loss: 10.7201 avg training loss: 10.3859
batch: [9790/21305] batch time: 0.063 trainign loss: 10.6602 avg training loss: 10.3860
batch: [9800/21305] batch time: 0.053 trainign loss: 10.5960 avg training loss: 10.3862
batch: [9810/21305] batch time: 0.056 trainign loss: 10.7171 avg training loss: 10.3862
batch: [9820/21305] batch time: 0.051 trainign loss: 10.4436 avg training loss: 10.3863
batch: [9830/21305] batch time: 0.056 trainign loss: 10.5908 avg training loss: 10.3865
batch: [9840/21305] batch time: 0.055 trainign loss: 9.8210 avg training loss: 10.3864
batch: [9850/21305] batch time: 0.056 trainign loss: 10.6951 avg training loss: 10.3866
batch: [9860/21305] batch time: 0.056 trainign loss: 8.7945 avg training loss: 10.3865
batch: [9870/21305] batch time: 0.056 trainign loss: 10.2298 avg training loss: 10.3866
batch: [9880/21305] batch time: 0.050 trainign loss: 10.7242 avg training loss: 10.3863
batch: [9890/21305] batch time: 0.057 trainign loss: 10.7002 avg training loss: 10.3864
batch: [9900/21305] batch time: 0.056 trainign loss: 10.5373 avg training loss: 10.3867
batch: [9910/21305] batch time: 0.058 trainign loss: 10.6573 avg training loss: 10.3868
batch: [9920/21305] batch time: 0.056 trainign loss: 10.5758 avg training loss: 10.3869
batch: [9930/21305] batch time: 0.058 trainign loss: 10.2872 avg training loss: 10.3871
batch: [9940/21305] batch time: 0.056 trainign loss: 10.7118 avg training loss: 10.3872
batch: [9950/21305] batch time: 0.056 trainign loss: 10.5561 avg training loss: 10.3873
batch: [9960/21305] batch time: 0.050 trainign loss: 10.6566 avg training loss: 10.3875
batch: [9970/21305] batch time: 0.054 trainign loss: 10.6688 avg training loss: 10.3877
batch: [9980/21305] batch time: 0.062 trainign loss: 10.4588 avg training loss: 10.3879
batch: [9990/21305] batch time: 0.058 trainign loss: 10.5967 avg training loss: 10.3881
batch: [10000/21305] batch time: 0.056 trainign loss: 10.2996 avg training loss: 10.3883
batch: [10010/21305] batch time: 0.053 trainign loss: 10.5903 avg training loss: 10.3884
batch: [10020/21305] batch time: 0.063 trainign loss: 10.5834 avg training loss: 10.3886
batch: [10030/21305] batch time: 0.063 trainign loss: 10.5582 avg training loss: 10.3888
batch: [10040/21305] batch time: 0.058 trainign loss: 10.6229 avg training loss: 10.3890
batch: [10050/21305] batch time: 0.054 trainign loss: 10.6253 avg training loss: 10.3892
batch: [10060/21305] batch time: 0.057 trainign loss: 10.2874 avg training loss: 10.3892
batch: [10070/21305] batch time: 0.056 trainign loss: 10.4230 avg training loss: 10.3892
batch: [10080/21305] batch time: 0.057 trainign loss: 10.4037 avg training loss: 10.3894
batch: [10090/21305] batch time: 0.051 trainign loss: 10.6951 avg training loss: 10.3890
batch: [10100/21305] batch time: 0.056 trainign loss: 10.7031 avg training loss: 10.3892
batch: [10110/21305] batch time: 0.052 trainign loss: 10.5411 avg training loss: 10.3893
batch: [10120/21305] batch time: 0.056 trainign loss: 10.5436 avg training loss: 10.3895
batch: [10130/21305] batch time: 0.051 trainign loss: 10.5079 avg training loss: 10.3897
batch: [10140/21305] batch time: 0.056 trainign loss: 10.6141 avg training loss: 10.3898
batch: [10150/21305] batch time: 0.562 trainign loss: 10.5215 avg training loss: 10.3901
batch: [10160/21305] batch time: 0.056 trainign loss: 10.4351 avg training loss: 10.3903
batch: [10170/21305] batch time: 1.666 trainign loss: 10.6542 avg training loss: 10.3905
batch: [10180/21305] batch time: 0.062 trainign loss: 10.6001 avg training loss: 10.3907
batch: [10190/21305] batch time: 1.609 trainign loss: 10.6853 avg training loss: 10.3908
batch: [10200/21305] batch time: 0.056 trainign loss: 10.3943 avg training loss: 10.3909
batch: [10210/21305] batch time: 2.529 trainign loss: 10.6427 avg training loss: 10.3911
batch: [10220/21305] batch time: 0.054 trainign loss: 10.4885 avg training loss: 10.3909
batch: [10230/21305] batch time: 2.184 trainign loss: 10.2126 avg training loss: 10.3911
batch: [10240/21305] batch time: 0.063 trainign loss: 10.6436 avg training loss: 10.3912
batch: [10250/21305] batch time: 2.214 trainign loss: 10.5992 avg training loss: 10.3912
batch: [10260/21305] batch time: 0.052 trainign loss: 10.7229 avg training loss: 10.3914
batch: [10270/21305] batch time: 2.446 trainign loss: 10.6574 avg training loss: 10.3914
batch: [10280/21305] batch time: 0.057 trainign loss: 10.6274 avg training loss: 10.3916
batch: [10290/21305] batch time: 2.151 trainign loss: 10.5869 avg training loss: 10.3915
batch: [10300/21305] batch time: 0.059 trainign loss: 10.5281 avg training loss: 10.3917
batch: [10310/21305] batch time: 2.173 trainign loss: 10.5152 avg training loss: 10.3917
batch: [10320/21305] batch time: 0.056 trainign loss: 7.9702 avg training loss: 10.3914
batch: [10330/21305] batch time: 2.392 trainign loss: 11.0351 avg training loss: 10.3911
batch: [10340/21305] batch time: 0.054 trainign loss: 10.7229 avg training loss: 10.3914
batch: [10350/21305] batch time: 2.472 trainign loss: 10.6078 avg training loss: 10.3915
batch: [10360/21305] batch time: 0.056 trainign loss: 10.5603 avg training loss: 10.3917
batch: [10370/21305] batch time: 2.533 trainign loss: 10.5717 avg training loss: 10.3919
batch: [10380/21305] batch time: 0.061 trainign loss: 10.6355 avg training loss: 10.3921
batch: [10390/21305] batch time: 2.622 trainign loss: 10.5844 avg training loss: 10.3920
batch: [10400/21305] batch time: 0.055 trainign loss: 10.7833 avg training loss: 10.3921
batch: [10410/21305] batch time: 2.085 trainign loss: 10.6352 avg training loss: 10.3921
batch: [10420/21305] batch time: 0.056 trainign loss: 10.6644 avg training loss: 10.3923
batch: [10430/21305] batch time: 1.767 trainign loss: 10.3981 avg training loss: 10.3925
batch: [10440/21305] batch time: 0.062 trainign loss: 10.5838 avg training loss: 10.3924
batch: [10450/21305] batch time: 1.172 trainign loss: 10.6772 avg training loss: 10.3925
batch: [10460/21305] batch time: 0.060 trainign loss: 10.6322 avg training loss: 10.3927
batch: [10470/21305] batch time: 0.685 trainign loss: 10.5209 avg training loss: 10.3929
batch: [10480/21305] batch time: 0.057 trainign loss: 10.6483 avg training loss: 10.3926
batch: [10490/21305] batch time: 0.131 trainign loss: 10.6744 avg training loss: 10.3928
batch: [10500/21305] batch time: 0.059 trainign loss: 10.5499 avg training loss: 10.3930
batch: [10510/21305] batch time: 0.422 trainign loss: 10.3126 avg training loss: 10.3930
batch: [10520/21305] batch time: 0.056 trainign loss: 10.6434 avg training loss: 10.3931
batch: [10530/21305] batch time: 1.469 trainign loss: 10.5748 avg training loss: 10.3933
batch: [10540/21305] batch time: 0.059 trainign loss: 10.4105 avg training loss: 10.3934
batch: [10550/21305] batch time: 0.420 trainign loss: 10.4133 avg training loss: 10.3936
batch: [10560/21305] batch time: 0.062 trainign loss: 10.8653 avg training loss: 10.3919
batch: [10570/21305] batch time: 0.621 trainign loss: 10.7051 avg training loss: 10.3923
batch: [10580/21305] batch time: 0.256 trainign loss: 10.6515 avg training loss: 10.3925
batch: [10590/21305] batch time: 1.203 trainign loss: 10.5988 avg training loss: 10.3924
batch: [10600/21305] batch time: 0.530 trainign loss: 10.6942 avg training loss: 10.3927
batch: [10610/21305] batch time: 0.461 trainign loss: 9.7570 avg training loss: 10.3926
batch: [10620/21305] batch time: 0.059 trainign loss: 10.6518 avg training loss: 10.3928
batch: [10630/21305] batch time: 0.207 trainign loss: 10.5611 avg training loss: 10.3930
batch: [10640/21305] batch time: 0.063 trainign loss: 10.1375 avg training loss: 10.3928
batch: [10650/21305] batch time: 0.670 trainign loss: 10.6920 avg training loss: 10.3929
batch: [10660/21305] batch time: 0.062 trainign loss: 10.6738 avg training loss: 10.3932
batch: [10670/21305] batch time: 1.223 trainign loss: 9.8198 avg training loss: 10.3933
batch: [10680/21305] batch time: 0.062 trainign loss: 10.6589 avg training loss: 10.3934
batch: [10690/21305] batch time: 2.567 trainign loss: 10.3805 avg training loss: 10.3933
batch: [10700/21305] batch time: 0.056 trainign loss: 10.6726 avg training loss: 10.3934
batch: [10710/21305] batch time: 2.407 trainign loss: 10.6048 avg training loss: 10.3930
batch: [10720/21305] batch time: 0.060 trainign loss: 10.8371 avg training loss: 10.3927
batch: [10730/21305] batch time: 2.377 trainign loss: 10.5376 avg training loss: 10.3929
batch: [10740/21305] batch time: 0.056 trainign loss: 10.7387 avg training loss: 10.3931
batch: [10750/21305] batch time: 2.174 trainign loss: 10.5060 avg training loss: 10.3932
batch: [10760/21305] batch time: 0.061 trainign loss: 10.0446 avg training loss: 10.3933
batch: [10770/21305] batch time: 2.178 trainign loss: 10.6039 avg training loss: 10.3932
batch: [10780/21305] batch time: 0.056 trainign loss: 10.6194 avg training loss: 10.3935
batch: [10790/21305] batch time: 2.058 trainign loss: 10.6862 avg training loss: 10.3936
batch: [10800/21305] batch time: 0.368 trainign loss: 10.4288 avg training loss: 10.3938
batch: [10810/21305] batch time: 1.907 trainign loss: 10.5377 avg training loss: 10.3940
batch: [10820/21305] batch time: 0.539 trainign loss: 9.3901 avg training loss: 10.3939
batch: [10830/21305] batch time: 1.500 trainign loss: 7.0655 avg training loss: 10.3930
batch: [10840/21305] batch time: 0.984 trainign loss: 10.7962 avg training loss: 10.3935
batch: [10850/21305] batch time: 0.764 trainign loss: 10.6340 avg training loss: 10.3936
batch: [10860/21305] batch time: 0.566 trainign loss: 10.4273 avg training loss: 10.3938
batch: [10870/21305] batch time: 2.225 trainign loss: 10.4875 avg training loss: 10.3939
batch: [10880/21305] batch time: 0.338 trainign loss: 10.4710 avg training loss: 10.3940
batch: [10890/21305] batch time: 1.462 trainign loss: 10.5697 avg training loss: 10.3941
batch: [10900/21305] batch time: 0.284 trainign loss: 10.7519 avg training loss: 10.3943
batch: [10910/21305] batch time: 0.639 trainign loss: 10.6023 avg training loss: 10.3942
batch: [10920/21305] batch time: 0.058 trainign loss: 10.7233 avg training loss: 10.3944
batch: [10930/21305] batch time: 1.028 trainign loss: 10.5060 avg training loss: 10.3945
batch: [10940/21305] batch time: 0.864 trainign loss: 10.5660 avg training loss: 10.3945
batch: [10950/21305] batch time: 1.336 trainign loss: 10.5701 avg training loss: 10.3944
batch: [10960/21305] batch time: 1.200 trainign loss: 10.7297 avg training loss: 10.3946
batch: [10970/21305] batch time: 2.280 trainign loss: 10.6289 avg training loss: 10.3948
batch: [10980/21305] batch time: 0.056 trainign loss: 10.5565 avg training loss: 10.3947
batch: [10990/21305] batch time: 2.179 trainign loss: 10.6508 avg training loss: 10.3948
batch: [11000/21305] batch time: 0.061 trainign loss: 10.5778 avg training loss: 10.3949
batch: [11010/21305] batch time: 2.055 trainign loss: 10.3896 avg training loss: 10.3946
batch: [11020/21305] batch time: 1.040 trainign loss: 10.7821 avg training loss: 10.3949
batch: [11030/21305] batch time: 0.635 trainign loss: 10.6678 avg training loss: 10.3950
batch: [11040/21305] batch time: 1.117 trainign loss: 10.6589 avg training loss: 10.3951
batch: [11050/21305] batch time: 2.085 trainign loss: 10.5434 avg training loss: 10.3952
batch: [11060/21305] batch time: 0.063 trainign loss: 10.7155 avg training loss: 10.3955
batch: [11070/21305] batch time: 2.387 trainign loss: 10.7641 avg training loss: 10.3956
batch: [11080/21305] batch time: 0.088 trainign loss: 10.6853 avg training loss: 10.3957
batch: [11090/21305] batch time: 2.242 trainign loss: 10.6549 avg training loss: 10.3959
batch: [11100/21305] batch time: 0.975 trainign loss: 10.5169 avg training loss: 10.3961
batch: [11110/21305] batch time: 1.618 trainign loss: 10.6530 avg training loss: 10.3962
batch: [11120/21305] batch time: 0.603 trainign loss: 10.5669 avg training loss: 10.3964
batch: [11130/21305] batch time: 1.518 trainign loss: 10.1201 avg training loss: 10.3966
batch: [11140/21305] batch time: 0.683 trainign loss: 9.7656 avg training loss: 10.3966
batch: [11150/21305] batch time: 1.415 trainign loss: 10.7463 avg training loss: 10.3966
batch: [11160/21305] batch time: 1.018 trainign loss: 10.6319 avg training loss: 10.3967
batch: [11170/21305] batch time: 0.916 trainign loss: 9.8677 avg training loss: 10.3968
batch: [11180/21305] batch time: 2.070 trainign loss: 10.9424 avg training loss: 10.3955
batch: [11190/21305] batch time: 0.588 trainign loss: 10.6554 avg training loss: 10.3954
batch: [11200/21305] batch time: 2.039 trainign loss: 10.6260 avg training loss: 10.3955
batch: [11210/21305] batch time: 0.056 trainign loss: 10.3937 avg training loss: 10.3957
batch: [11220/21305] batch time: 2.212 trainign loss: 10.5647 avg training loss: 10.3958
batch: [11230/21305] batch time: 0.061 trainign loss: 10.2336 avg training loss: 10.3959
batch: [11240/21305] batch time: 2.077 trainign loss: 10.6407 avg training loss: 10.3961
batch: [11250/21305] batch time: 0.054 trainign loss: 6.6600 avg training loss: 10.3952
batch: [11260/21305] batch time: 2.158 trainign loss: 11.1095 avg training loss: 10.3953
batch: [11270/21305] batch time: 0.056 trainign loss: 10.4602 avg training loss: 10.3955
batch: [11280/21305] batch time: 2.471 trainign loss: 10.0546 avg training loss: 10.3955
batch: [11290/21305] batch time: 0.061 trainign loss: 10.7385 avg training loss: 10.3957
batch: [11300/21305] batch time: 2.477 trainign loss: 7.6736 avg training loss: 10.3953
batch: [11310/21305] batch time: 0.063 trainign loss: 10.8190 avg training loss: 10.3955
batch: [11320/21305] batch time: 2.443 trainign loss: 10.6320 avg training loss: 10.3955
batch: [11330/21305] batch time: 0.063 trainign loss: 10.6384 avg training loss: 10.3955
batch: [11340/21305] batch time: 2.088 trainign loss: 10.6861 avg training loss: 10.3957
batch: [11350/21305] batch time: 0.249 trainign loss: 10.5784 avg training loss: 10.3959
batch: [11360/21305] batch time: 2.661 trainign loss: 10.6060 avg training loss: 10.3961
batch: [11370/21305] batch time: 0.063 trainign loss: 10.7089 avg training loss: 10.3962
batch: [11380/21305] batch time: 2.672 trainign loss: 10.1779 avg training loss: 10.3963
batch: [11390/21305] batch time: 0.056 trainign loss: 9.6942 avg training loss: 10.3940
batch: [11400/21305] batch time: 2.035 trainign loss: 10.6068 avg training loss: 10.3946
batch: [11410/21305] batch time: 0.056 trainign loss: 10.6212 avg training loss: 10.3948
batch: [11420/21305] batch time: 2.247 trainign loss: 10.5412 avg training loss: 10.3949
batch: [11430/21305] batch time: 0.053 trainign loss: 10.6027 avg training loss: 10.3951
batch: [11440/21305] batch time: 2.094 trainign loss: 10.5147 avg training loss: 10.3953
batch: [11450/21305] batch time: 0.057 trainign loss: 10.5170 avg training loss: 10.3953
batch: [11460/21305] batch time: 2.392 trainign loss: 10.6722 avg training loss: 10.3955
batch: [11470/21305] batch time: 0.054 trainign loss: 10.6302 avg training loss: 10.3954
batch: [11480/21305] batch time: 2.521 trainign loss: 10.6587 avg training loss: 10.3956
batch: [11490/21305] batch time: 0.057 trainign loss: 10.6442 avg training loss: 10.3949
batch: [11500/21305] batch time: 2.471 trainign loss: 10.5607 avg training loss: 10.3952
batch: [11510/21305] batch time: 0.054 trainign loss: 10.6296 avg training loss: 10.3951
batch: [11520/21305] batch time: 2.228 trainign loss: 10.6449 avg training loss: 10.3953
batch: [11530/21305] batch time: 0.056 trainign loss: 10.7551 avg training loss: 10.3954
batch: [11540/21305] batch time: 2.192 trainign loss: 10.6682 avg training loss: 10.3956
batch: [11550/21305] batch time: 0.060 trainign loss: 9.8322 avg training loss: 10.3957
batch: [11560/21305] batch time: 2.387 trainign loss: 10.7120 avg training loss: 10.3957
batch: [11570/21305] batch time: 0.052 trainign loss: 10.3578 avg training loss: 10.3957
batch: [11580/21305] batch time: 2.178 trainign loss: 10.6756 avg training loss: 10.3959
batch: [11590/21305] batch time: 0.061 trainign loss: 10.5721 avg training loss: 10.3960
batch: [11600/21305] batch time: 2.058 trainign loss: 9.9654 avg training loss: 10.3961
batch: [11610/21305] batch time: 0.053 trainign loss: 10.6238 avg training loss: 10.3962
batch: [11620/21305] batch time: 2.231 trainign loss: 10.6922 avg training loss: 10.3962
batch: [11630/21305] batch time: 0.056 trainign loss: 10.7436 avg training loss: 10.3964
batch: [11640/21305] batch time: 2.262 trainign loss: 10.6873 avg training loss: 10.3965
batch: [11650/21305] batch time: 0.056 trainign loss: 10.6596 avg training loss: 10.3967
batch: [11660/21305] batch time: 2.246 trainign loss: 10.4673 avg training loss: 10.3968
batch: [11670/21305] batch time: 0.054 trainign loss: 10.6476 avg training loss: 10.3970
batch: [11680/21305] batch time: 2.230 trainign loss: 10.6785 avg training loss: 10.3972
batch: [11690/21305] batch time: 0.062 trainign loss: 10.5529 avg training loss: 10.3973
batch: [11700/21305] batch time: 2.725 trainign loss: 10.7598 avg training loss: 10.3972
batch: [11710/21305] batch time: 0.057 trainign loss: 9.7647 avg training loss: 10.3973
batch: [11720/21305] batch time: 1.762 trainign loss: 10.7338 avg training loss: 10.3973
batch: [11730/21305] batch time: 0.056 trainign loss: 10.4871 avg training loss: 10.3973
batch: [11740/21305] batch time: 2.179 trainign loss: 10.5146 avg training loss: 10.3973
batch: [11750/21305] batch time: 0.065 trainign loss: 10.6130 avg training loss: 10.3975
batch: [11760/21305] batch time: 2.510 trainign loss: 10.6171 avg training loss: 10.3977
batch: [11770/21305] batch time: 0.056 trainign loss: 10.6227 avg training loss: 10.3979
batch: [11780/21305] batch time: 2.742 trainign loss: 10.4661 avg training loss: 10.3980
batch: [11790/21305] batch time: 0.053 trainign loss: 8.0941 avg training loss: 10.3976
batch: [11800/21305] batch time: 2.335 trainign loss: 0.0145 avg training loss: 10.3912
batch: [11810/21305] batch time: 0.056 trainign loss: 10.6584 avg training loss: 10.3906
batch: [11820/21305] batch time: 1.901 trainign loss: 10.5051 avg training loss: 10.3907
batch: [11830/21305] batch time: 0.057 trainign loss: 10.6245 avg training loss: 10.3905
batch: [11840/21305] batch time: 2.314 trainign loss: 10.6695 avg training loss: 10.3906
batch: [11850/21305] batch time: 0.055 trainign loss: 10.7294 avg training loss: 10.3904
batch: [11860/21305] batch time: 2.306 trainign loss: 8.9923 avg training loss: 10.3904
batch: [11870/21305] batch time: 0.052 trainign loss: 10.7619 avg training loss: 10.3903
batch: [11880/21305] batch time: 2.106 trainign loss: 10.7212 avg training loss: 10.3888
batch: [11890/21305] batch time: 0.055 trainign loss: 10.4609 avg training loss: 10.3893
batch: [11900/21305] batch time: 2.296 trainign loss: 10.3250 avg training loss: 10.3895
batch: [11910/21305] batch time: 0.062 trainign loss: 10.7173 avg training loss: 10.3895
batch: [11920/21305] batch time: 2.497 trainign loss: 10.5422 avg training loss: 10.3897
batch: [11930/21305] batch time: 0.057 trainign loss: 9.6548 avg training loss: 10.3893
batch: [11940/21305] batch time: 2.490 trainign loss: 10.8252 avg training loss: 10.3896
batch: [11950/21305] batch time: 0.057 trainign loss: 10.7095 avg training loss: 10.3899
batch: [11960/21305] batch time: 2.289 trainign loss: 10.6165 avg training loss: 10.3898
batch: [11970/21305] batch time: 0.061 trainign loss: 10.6167 avg training loss: 10.3900
batch: [11980/21305] batch time: 2.355 trainign loss: 10.4916 avg training loss: 10.3901
batch: [11990/21305] batch time: 0.056 trainign loss: 10.4622 avg training loss: 10.3903
batch: [12000/21305] batch time: 2.427 trainign loss: 10.6448 avg training loss: 10.3904
batch: [12010/21305] batch time: 0.056 trainign loss: 10.6613 avg training loss: 10.3906
batch: [12020/21305] batch time: 1.881 trainign loss: 10.6952 avg training loss: 10.3907
batch: [12030/21305] batch time: 0.075 trainign loss: 10.5814 avg training loss: 10.3909
batch: [12040/21305] batch time: 2.209 trainign loss: 10.5848 avg training loss: 10.3911
batch: [12050/21305] batch time: 0.579 trainign loss: 10.4916 avg training loss: 10.3912
batch: [12060/21305] batch time: 2.025 trainign loss: 10.6870 avg training loss: 10.3914
batch: [12070/21305] batch time: 0.053 trainign loss: 9.3489 avg training loss: 10.3914
batch: [12080/21305] batch time: 2.478 trainign loss: 10.4400 avg training loss: 10.3902
batch: [12090/21305] batch time: 0.058 trainign loss: 10.2121 avg training loss: 10.3903
batch: [12100/21305] batch time: 1.763 trainign loss: 10.5881 avg training loss: 10.3903
batch: [12110/21305] batch time: 0.062 trainign loss: 10.6603 avg training loss: 10.3905
batch: [12120/21305] batch time: 1.309 trainign loss: 10.4200 avg training loss: 10.3904
batch: [12130/21305] batch time: 0.254 trainign loss: 10.6037 avg training loss: 10.3901
batch: [12140/21305] batch time: 1.145 trainign loss: 10.0943 avg training loss: 10.3898
batch: [12150/21305] batch time: 0.812 trainign loss: 10.6647 avg training loss: 10.3900
batch: [12160/21305] batch time: 0.348 trainign loss: 10.6658 avg training loss: 10.3902
batch: [12170/21305] batch time: 0.634 trainign loss: 10.6485 avg training loss: 10.3904
batch: [12180/21305] batch time: 0.056 trainign loss: 10.5148 avg training loss: 10.3905
batch: [12190/21305] batch time: 0.294 trainign loss: 10.6534 avg training loss: 10.3907
batch: [12200/21305] batch time: 0.056 trainign loss: 10.6066 avg training loss: 10.3909
batch: [12210/21305] batch time: 0.055 trainign loss: 10.6791 avg training loss: 10.3911
batch: [12220/21305] batch time: 0.058 trainign loss: 10.6934 avg training loss: 10.3913
batch: [12230/21305] batch time: 0.051 trainign loss: 10.6507 avg training loss: 10.3915
batch: [12240/21305] batch time: 0.061 trainign loss: 10.5870 avg training loss: 10.3916
batch: [12250/21305] batch time: 0.051 trainign loss: 10.6257 avg training loss: 10.3918
batch: [12260/21305] batch time: 0.063 trainign loss: 10.7085 avg training loss: 10.3920
batch: [12270/21305] batch time: 0.063 trainign loss: 10.4232 avg training loss: 10.3922
batch: [12280/21305] batch time: 0.056 trainign loss: 10.3026 avg training loss: 10.3921
batch: [12290/21305] batch time: 0.553 trainign loss: 10.6097 avg training loss: 10.3922
batch: [12300/21305] batch time: 0.056 trainign loss: 10.6711 avg training loss: 10.3924
batch: [12310/21305] batch time: 0.062 trainign loss: 10.6421 avg training loss: 10.3927
batch: [12320/21305] batch time: 0.057 trainign loss: 10.5564 avg training loss: 10.3928
batch: [12330/21305] batch time: 0.660 trainign loss: 10.7753 avg training loss: 10.3930
batch: [12340/21305] batch time: 0.056 trainign loss: 10.7714 avg training loss: 10.3932
batch: [12350/21305] batch time: 1.812 trainign loss: 10.6857 avg training loss: 10.3934
batch: [12360/21305] batch time: 0.056 trainign loss: 10.4052 avg training loss: 10.3936
batch: [12370/21305] batch time: 1.441 trainign loss: 10.4124 avg training loss: 10.3936
batch: [12380/21305] batch time: 0.055 trainign loss: 10.5985 avg training loss: 10.3937
batch: [12390/21305] batch time: 1.066 trainign loss: 10.7067 avg training loss: 10.3939
batch: [12400/21305] batch time: 0.056 trainign loss: 10.6653 avg training loss: 10.3940
batch: [12410/21305] batch time: 1.002 trainign loss: 10.6752 avg training loss: 10.3941
batch: [12420/21305] batch time: 0.056 trainign loss: 10.4615 avg training loss: 10.3943
batch: [12430/21305] batch time: 1.186 trainign loss: 10.5239 avg training loss: 10.3944
batch: [12440/21305] batch time: 0.056 trainign loss: 10.6713 avg training loss: 10.3944
batch: [12450/21305] batch time: 0.632 trainign loss: 10.7402 avg training loss: 10.3946
batch: [12460/21305] batch time: 0.054 trainign loss: 10.6553 avg training loss: 10.3948
batch: [12470/21305] batch time: 0.058 trainign loss: 10.4152 avg training loss: 10.3948
batch: [12480/21305] batch time: 0.056 trainign loss: 10.6707 avg training loss: 10.3950
batch: [12490/21305] batch time: 0.062 trainign loss: 10.8096 avg training loss: 10.3951
batch: [12500/21305] batch time: 0.056 trainign loss: 10.6538 avg training loss: 10.3950
batch: [12510/21305] batch time: 0.062 trainign loss: 10.7659 avg training loss: 10.3951
batch: [12520/21305] batch time: 0.053 trainign loss: 10.7529 avg training loss: 10.3953
batch: [12530/21305] batch time: 0.063 trainign loss: 10.7205 avg training loss: 10.3955
batch: [12540/21305] batch time: 0.062 trainign loss: 10.5309 avg training loss: 10.3955
batch: [12550/21305] batch time: 0.263 trainign loss: 10.5169 avg training loss: 10.3956
batch: [12560/21305] batch time: 0.056 trainign loss: 9.9315 avg training loss: 10.3957
batch: [12570/21305] batch time: 1.000 trainign loss: 0.6278 avg training loss: 10.3916
batch: [12580/21305] batch time: 0.062 trainign loss: 10.7101 avg training loss: 10.3925
batch: [12590/21305] batch time: 0.602 trainign loss: 9.5802 avg training loss: 10.3926
batch: [12600/21305] batch time: 0.056 trainign loss: 10.6877 avg training loss: 10.3926
batch: [12610/21305] batch time: 1.313 trainign loss: 9.5318 avg training loss: 10.3925
batch: [12620/21305] batch time: 0.057 trainign loss: 10.6796 avg training loss: 10.3927
batch: [12630/21305] batch time: 0.839 trainign loss: 10.5527 avg training loss: 10.3929
batch: [12640/21305] batch time: 0.061 trainign loss: 10.6350 avg training loss: 10.3931
batch: [12650/21305] batch time: 0.807 trainign loss: 10.3987 avg training loss: 10.3932
batch: [12660/21305] batch time: 0.056 trainign loss: 10.5096 avg training loss: 10.3934
batch: [12670/21305] batch time: 1.272 trainign loss: 10.6211 avg training loss: 10.3935
batch: [12680/21305] batch time: 0.058 trainign loss: 10.0219 avg training loss: 10.3936
batch: [12690/21305] batch time: 1.276 trainign loss: 10.6826 avg training loss: 10.3935
batch: [12700/21305] batch time: 0.056 trainign loss: 10.6286 avg training loss: 10.3936
batch: [12710/21305] batch time: 0.062 trainign loss: 10.6907 avg training loss: 10.3937
batch: [12720/21305] batch time: 0.056 trainign loss: 10.3030 avg training loss: 10.3939
batch: [12730/21305] batch time: 0.900 trainign loss: 10.4961 avg training loss: 10.3938
batch: [12740/21305] batch time: 0.061 trainign loss: 9.9603 avg training loss: 10.3939
batch: [12750/21305] batch time: 0.061 trainign loss: 8.9140 avg training loss: 10.3935
batch: [12760/21305] batch time: 0.070 trainign loss: 10.5067 avg training loss: 10.3937
batch: [12770/21305] batch time: 0.056 trainign loss: 10.5795 avg training loss: 10.3938
batch: [12780/21305] batch time: 0.105 trainign loss: 10.5401 avg training loss: 10.3939
batch: [12790/21305] batch time: 0.050 trainign loss: 10.5094 avg training loss: 10.3940
batch: [12800/21305] batch time: 0.062 trainign loss: 10.7585 avg training loss: 10.3941
batch: [12810/21305] batch time: 0.055 trainign loss: 10.6802 avg training loss: 10.3943
batch: [12820/21305] batch time: 0.062 trainign loss: 10.6212 avg training loss: 10.3944
batch: [12830/21305] batch time: 0.061 trainign loss: 9.8350 avg training loss: 10.3945
batch: [12840/21305] batch time: 0.063 trainign loss: 10.6826 avg training loss: 10.3945
batch: [12850/21305] batch time: 0.060 trainign loss: 10.6427 avg training loss: 10.3947
batch: [12860/21305] batch time: 0.233 trainign loss: 10.6176 avg training loss: 10.3949
batch: [12870/21305] batch time: 0.062 trainign loss: 10.6754 avg training loss: 10.3951
batch: [12880/21305] batch time: 1.179 trainign loss: 10.4377 avg training loss: 10.3951
batch: [12890/21305] batch time: 0.056 trainign loss: 10.5965 avg training loss: 10.3953
batch: [12900/21305] batch time: 1.586 trainign loss: 10.6864 avg training loss: 10.3954
batch: [12910/21305] batch time: 0.063 trainign loss: 10.4418 avg training loss: 10.3956
batch: [12920/21305] batch time: 1.800 trainign loss: 10.6392 avg training loss: 10.3958
batch: [12930/21305] batch time: 0.063 trainign loss: 10.5368 avg training loss: 10.3959
batch: [12940/21305] batch time: 1.651 trainign loss: 10.6042 avg training loss: 10.3960
batch: [12950/21305] batch time: 0.062 trainign loss: 10.1518 avg training loss: 10.3956
batch: [12960/21305] batch time: 1.699 trainign loss: 10.7527 avg training loss: 10.3958
batch: [12970/21305] batch time: 0.242 trainign loss: 10.5692 avg training loss: 10.3960
batch: [12980/21305] batch time: 1.595 trainign loss: 10.6489 avg training loss: 10.3958
batch: [12990/21305] batch time: 0.198 trainign loss: 10.5090 avg training loss: 10.3959
batch: [13000/21305] batch time: 2.125 trainign loss: 10.6257 avg training loss: 10.3961
batch: [13010/21305] batch time: 0.056 trainign loss: 10.5889 avg training loss: 10.3960
batch: [13020/21305] batch time: 1.808 trainign loss: 10.6395 avg training loss: 10.3962
batch: [13030/21305] batch time: 0.311 trainign loss: 10.6202 avg training loss: 10.3964
batch: [13040/21305] batch time: 2.147 trainign loss: 10.6997 avg training loss: 10.3964
batch: [13050/21305] batch time: 0.238 trainign loss: 8.1009 avg training loss: 10.3961
batch: [13060/21305] batch time: 1.514 trainign loss: 10.7354 avg training loss: 10.3962
batch: [13070/21305] batch time: 0.377 trainign loss: 10.6885 avg training loss: 10.3964
batch: [13080/21305] batch time: 1.793 trainign loss: 10.7133 avg training loss: 10.3965
batch: [13090/21305] batch time: 0.063 trainign loss: 10.3385 avg training loss: 10.3966
batch: [13100/21305] batch time: 2.262 trainign loss: 10.6483 avg training loss: 10.3961
batch: [13110/21305] batch time: 0.062 trainign loss: 10.7224 avg training loss: 10.3960
batch: [13120/21305] batch time: 2.098 trainign loss: 10.4935 avg training loss: 10.3962
batch: [13130/21305] batch time: 0.063 trainign loss: 10.3411 avg training loss: 10.3963
batch: [13140/21305] batch time: 2.605 trainign loss: 8.7666 avg training loss: 10.3962
batch: [13150/21305] batch time: 0.056 trainign loss: 10.7174 avg training loss: 10.3961
batch: [13160/21305] batch time: 2.061 trainign loss: 10.4386 avg training loss: 10.3964
batch: [13170/21305] batch time: 0.062 trainign loss: 10.6763 avg training loss: 10.3965
batch: [13180/21305] batch time: 1.483 trainign loss: 10.3584 avg training loss: 10.3966
batch: [13190/21305] batch time: 1.036 trainign loss: 10.6693 avg training loss: 10.3967
batch: [13200/21305] batch time: 0.490 trainign loss: 10.3491 avg training loss: 10.3967
batch: [13210/21305] batch time: 1.183 trainign loss: 1.5967 avg training loss: 10.3935
batch: [13220/21305] batch time: 0.297 trainign loss: 0.0001 avg training loss: 10.3857
batch: [13230/21305] batch time: 1.018 trainign loss: 0.0000 avg training loss: 10.3779
batch: [13240/21305] batch time: 0.469 trainign loss: 10.7445 avg training loss: 10.3793
batch: [13250/21305] batch time: 0.927 trainign loss: 10.6705 avg training loss: 10.3795
batch: [13260/21305] batch time: 0.941 trainign loss: 10.6555 avg training loss: 10.3797
batch: [13270/21305] batch time: 0.587 trainign loss: 10.6879 avg training loss: 10.3798
batch: [13280/21305] batch time: 1.200 trainign loss: 10.6629 avg training loss: 10.3800
batch: [13290/21305] batch time: 0.893 trainign loss: 10.6644 avg training loss: 10.3801
batch: [13300/21305] batch time: 1.025 trainign loss: 10.6562 avg training loss: 10.3803
batch: [13310/21305] batch time: 1.216 trainign loss: 10.4687 avg training loss: 10.3805
batch: [13320/21305] batch time: 1.527 trainign loss: 10.6898 avg training loss: 10.3800
batch: [13330/21305] batch time: 1.017 trainign loss: 10.6723 avg training loss: 10.3802
batch: [13340/21305] batch time: 2.055 trainign loss: 10.6575 avg training loss: 10.3804
batch: [13350/21305] batch time: 0.124 trainign loss: 10.6008 avg training loss: 10.3805
batch: [13360/21305] batch time: 2.308 trainign loss: 10.7258 avg training loss: 10.3807
batch: [13370/21305] batch time: 0.486 trainign loss: 10.7401 avg training loss: 10.3809
batch: [13380/21305] batch time: 1.805 trainign loss: 10.4937 avg training loss: 10.3810
batch: [13390/21305] batch time: 0.104 trainign loss: 10.5872 avg training loss: 10.3810
batch: [13400/21305] batch time: 1.839 trainign loss: 9.8579 avg training loss: 10.3810
batch: [13410/21305] batch time: 0.288 trainign loss: 10.6231 avg training loss: 10.3812
batch: [13420/21305] batch time: 2.225 trainign loss: 10.7635 avg training loss: 10.3814
batch: [13430/21305] batch time: 0.062 trainign loss: 10.6276 avg training loss: 10.3815
batch: [13440/21305] batch time: 2.212 trainign loss: 10.3996 avg training loss: 10.3816
batch: [13450/21305] batch time: 0.056 trainign loss: 10.6117 avg training loss: 10.3818
batch: [13460/21305] batch time: 2.606 trainign loss: 10.5914 avg training loss: 10.3820
batch: [13470/21305] batch time: 0.063 trainign loss: 9.4090 avg training loss: 10.3820
batch: [13480/21305] batch time: 2.286 trainign loss: 9.9688 avg training loss: 10.3820
batch: [13490/21305] batch time: 0.052 trainign loss: 10.6729 avg training loss: 10.3820
batch: [13500/21305] batch time: 1.854 trainign loss: 10.6084 avg training loss: 10.3820
batch: [13510/21305] batch time: 1.054 trainign loss: 10.5641 avg training loss: 10.3821
batch: [13520/21305] batch time: 1.281 trainign loss: 10.2803 avg training loss: 10.3822
batch: [13530/21305] batch time: 0.837 trainign loss: 10.6162 avg training loss: 10.3823
batch: [13540/21305] batch time: 0.108 trainign loss: 9.2385 avg training loss: 10.3823
batch: [13550/21305] batch time: 1.247 trainign loss: 10.6945 avg training loss: 10.3823
batch: [13560/21305] batch time: 0.056 trainign loss: 10.6300 avg training loss: 10.3825
batch: [13570/21305] batch time: 1.336 trainign loss: 10.6627 avg training loss: 10.3825
batch: [13580/21305] batch time: 0.058 trainign loss: 10.6333 avg training loss: 10.3827
batch: [13590/21305] batch time: 1.129 trainign loss: 9.8945 avg training loss: 10.3828
batch: [13600/21305] batch time: 0.575 trainign loss: 10.5565 avg training loss: 10.3829
batch: [13610/21305] batch time: 0.969 trainign loss: 10.5893 avg training loss: 10.3831
batch: [13620/21305] batch time: 0.111 trainign loss: 10.5068 avg training loss: 10.3832
batch: [13630/21305] batch time: 0.880 trainign loss: 10.6089 avg training loss: 10.3833
batch: [13640/21305] batch time: 0.056 trainign loss: 10.6295 avg training loss: 10.3835
batch: [13650/21305] batch time: 1.736 trainign loss: 10.7703 avg training loss: 10.3837
batch: [13660/21305] batch time: 0.062 trainign loss: 10.3412 avg training loss: 10.3837
batch: [13670/21305] batch time: 2.071 trainign loss: 10.6233 avg training loss: 10.3839
batch: [13680/21305] batch time: 0.055 trainign loss: 10.4546 avg training loss: 10.3840
batch: [13690/21305] batch time: 1.685 trainign loss: 9.3358 avg training loss: 10.3840
batch: [13700/21305] batch time: 0.645 trainign loss: 10.7202 avg training loss: 10.3841
batch: [13710/21305] batch time: 1.366 trainign loss: 10.5548 avg training loss: 10.3843
batch: [13720/21305] batch time: 0.859 trainign loss: 10.5744 avg training loss: 10.3845
batch: [13730/21305] batch time: 1.408 trainign loss: 10.7182 avg training loss: 10.3847
batch: [13740/21305] batch time: 0.905 trainign loss: 10.2528 avg training loss: 10.3847
batch: [13750/21305] batch time: 0.403 trainign loss: 10.6976 avg training loss: 10.3848
batch: [13760/21305] batch time: 1.210 trainign loss: 10.7293 avg training loss: 10.3851
batch: [13770/21305] batch time: 0.359 trainign loss: 10.6610 avg training loss: 10.3852
batch: [13780/21305] batch time: 1.579 trainign loss: 10.1038 avg training loss: 10.3852
batch: [13790/21305] batch time: 0.063 trainign loss: 10.5166 avg training loss: 10.3854
batch: [13800/21305] batch time: 1.362 trainign loss: 10.5040 avg training loss: 10.3855
batch: [13810/21305] batch time: 0.056 trainign loss: 10.5167 avg training loss: 10.3856
batch: [13820/21305] batch time: 1.122 trainign loss: 10.6364 avg training loss: 10.3858
batch: [13830/21305] batch time: 0.061 trainign loss: 10.6962 avg training loss: 10.3859
batch: [13840/21305] batch time: 1.125 trainign loss: 10.6808 avg training loss: 10.3861
batch: [13850/21305] batch time: 0.056 trainign loss: 7.0076 avg training loss: 10.3855
batch: [13860/21305] batch time: 1.584 trainign loss: 10.7468 avg training loss: 10.3853
batch: [13870/21305] batch time: 0.059 trainign loss: 10.7346 avg training loss: 10.3854
batch: [13880/21305] batch time: 1.661 trainign loss: 10.4500 avg training loss: 10.3856
batch: [13890/21305] batch time: 0.062 trainign loss: 10.6105 avg training loss: 10.3856
batch: [13900/21305] batch time: 2.374 trainign loss: 10.5551 avg training loss: 10.3857
batch: [13910/21305] batch time: 0.056 trainign loss: 10.6492 avg training loss: 10.3855
batch: [13920/21305] batch time: 1.953 trainign loss: 10.1535 avg training loss: 10.3856
batch: [13930/21305] batch time: 0.059 trainign loss: 10.5941 avg training loss: 10.3854
batch: [13940/21305] batch time: 2.397 trainign loss: 10.6302 avg training loss: 10.3855
batch: [13950/21305] batch time: 0.056 trainign loss: 9.8069 avg training loss: 10.3856
batch: [13960/21305] batch time: 2.210 trainign loss: 10.2133 avg training loss: 10.3857
batch: [13970/21305] batch time: 0.056 trainign loss: 10.7100 avg training loss: 10.3859
batch: [13980/21305] batch time: 2.171 trainign loss: 10.7103 avg training loss: 10.3860
batch: [13990/21305] batch time: 0.062 trainign loss: 10.8174 avg training loss: 10.3861
batch: [14000/21305] batch time: 2.305 trainign loss: 10.2608 avg training loss: 10.3861
batch: [14010/21305] batch time: 0.055 trainign loss: 10.3950 avg training loss: 10.3861
batch: [14020/21305] batch time: 2.473 trainign loss: 10.7651 avg training loss: 10.3860
batch: [14030/21305] batch time: 0.061 trainign loss: 10.7025 avg training loss: 10.3862
batch: [14040/21305] batch time: 2.510 trainign loss: 10.6688 avg training loss: 10.3861
batch: [14050/21305] batch time: 0.056 trainign loss: 10.7304 avg training loss: 10.3863
batch: [14060/21305] batch time: 2.419 trainign loss: 10.5936 avg training loss: 10.3865
batch: [14070/21305] batch time: 0.057 trainign loss: 10.6611 avg training loss: 10.3866
batch: [14080/21305] batch time: 2.221 trainign loss: 10.5350 avg training loss: 10.3868
batch: [14090/21305] batch time: 0.533 trainign loss: 10.6814 avg training loss: 10.3869
batch: [14100/21305] batch time: 1.729 trainign loss: 10.7098 avg training loss: 10.3870
batch: [14110/21305] batch time: 0.056 trainign loss: 10.6965 avg training loss: 10.3872
batch: [14120/21305] batch time: 2.379 trainign loss: 10.6886 avg training loss: 10.3872
batch: [14130/21305] batch time: 0.056 trainign loss: 10.8482 avg training loss: 10.3870
batch: [14140/21305] batch time: 2.376 trainign loss: 10.7146 avg training loss: 10.3872
batch: [14150/21305] batch time: 0.244 trainign loss: 10.6545 avg training loss: 10.3874
batch: [14160/21305] batch time: 1.976 trainign loss: 10.5919 avg training loss: 10.3875
batch: [14170/21305] batch time: 0.712 trainign loss: 10.5165 avg training loss: 10.3876
batch: [14180/21305] batch time: 1.947 trainign loss: 10.5781 avg training loss: 10.3877
batch: [14190/21305] batch time: 0.520 trainign loss: 8.6558 avg training loss: 10.3875
batch: [14200/21305] batch time: 2.019 trainign loss: 10.7881 avg training loss: 10.3876
batch: [14210/21305] batch time: 0.056 trainign loss: 10.6643 avg training loss: 10.3877
batch: [14220/21305] batch time: 2.162 trainign loss: 10.7486 avg training loss: 10.3870
batch: [14230/21305] batch time: 0.062 trainign loss: 10.5959 avg training loss: 10.3873
batch: [14240/21305] batch time: 2.004 trainign loss: 10.6905 avg training loss: 10.3874
batch: [14250/21305] batch time: 0.174 trainign loss: 10.6217 avg training loss: 10.3876
batch: [14260/21305] batch time: 2.350 trainign loss: 10.3893 avg training loss: 10.3877
batch: [14270/21305] batch time: 0.051 trainign loss: 10.0552 avg training loss: 10.3877
batch: [14280/21305] batch time: 1.645 trainign loss: 10.0497 avg training loss: 10.3877
batch: [14290/21305] batch time: 0.590 trainign loss: 10.6739 avg training loss: 10.3879
batch: [14300/21305] batch time: 0.932 trainign loss: 10.6160 avg training loss: 10.3881
batch: [14310/21305] batch time: 1.797 trainign loss: 10.7091 avg training loss: 10.3883
batch: [14320/21305] batch time: 0.330 trainign loss: 8.6917 avg training loss: 10.3881
batch: [14330/21305] batch time: 1.336 trainign loss: 10.7161 avg training loss: 10.3878
batch: [14340/21305] batch time: 1.469 trainign loss: 10.2086 avg training loss: 10.3881
batch: [14350/21305] batch time: 1.316 trainign loss: 10.1641 avg training loss: 10.3882
batch: [14360/21305] batch time: 1.191 trainign loss: 10.3294 avg training loss: 10.3883
batch: [14370/21305] batch time: 1.339 trainign loss: 10.5000 avg training loss: 10.3884
batch: [14380/21305] batch time: 0.472 trainign loss: 10.6701 avg training loss: 10.3886
batch: [14390/21305] batch time: 1.575 trainign loss: 10.6168 avg training loss: 10.3887
batch: [14400/21305] batch time: 1.038 trainign loss: 10.7117 avg training loss: 10.3889
batch: [14410/21305] batch time: 1.092 trainign loss: 10.6015 avg training loss: 10.3890
batch: [14420/21305] batch time: 1.828 trainign loss: 10.4967 avg training loss: 10.3890
batch: [14430/21305] batch time: 0.077 trainign loss: 10.5064 avg training loss: 10.3891
batch: [14440/21305] batch time: 2.084 trainign loss: 10.6639 avg training loss: 10.3893
batch: [14450/21305] batch time: 0.271 trainign loss: 10.4286 avg training loss: 10.3895
batch: [14460/21305] batch time: 1.853 trainign loss: 10.6610 avg training loss: 10.3896
batch: [14470/21305] batch time: 1.329 trainign loss: 10.5727 avg training loss: 10.3897
batch: [14480/21305] batch time: 0.145 trainign loss: 9.7406 avg training loss: 10.3898
batch: [14490/21305] batch time: 2.365 trainign loss: 10.6726 avg training loss: 10.3898
batch: [14500/21305] batch time: 0.227 trainign loss: 9.4427 avg training loss: 10.3898
batch: [14510/21305] batch time: 1.841 trainign loss: 0.3581 avg training loss: 10.3858
batch: [14520/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 10.3787
batch: [14530/21305] batch time: 1.929 trainign loss: 0.0000 avg training loss: 10.3715
batch: [14540/21305] batch time: 0.050 trainign loss: 0.0000 avg training loss: 10.3644
batch: [14550/21305] batch time: 1.792 trainign loss: 0.0000 avg training loss: 10.3573
batch: [14560/21305] batch time: 0.063 trainign loss: 0.0000 avg training loss: 10.3501
batch: [14570/21305] batch time: 1.914 trainign loss: 0.0000 avg training loss: 10.3430
batch: [14580/21305] batch time: 0.057 trainign loss: 0.0000 avg training loss: 10.3359
batch: [14590/21305] batch time: 2.485 trainign loss: 0.0000 avg training loss: 10.3289
batch: [14600/21305] batch time: 0.056 trainign loss: 10.7438 avg training loss: 10.3277
batch: [14610/21305] batch time: 2.552 trainign loss: 10.7388 avg training loss: 10.3280
batch: [14620/21305] batch time: 0.056 trainign loss: 10.6146 avg training loss: 10.3281
batch: [14630/21305] batch time: 2.178 trainign loss: 10.3388 avg training loss: 10.3282
batch: [14640/21305] batch time: 0.061 trainign loss: 10.2899 avg training loss: 10.3283
batch: [14650/21305] batch time: 2.441 trainign loss: 10.7292 avg training loss: 10.3286
batch: [14660/21305] batch time: 0.057 trainign loss: 10.6857 avg training loss: 10.3287
batch: [14670/21305] batch time: 2.393 trainign loss: 10.4005 avg training loss: 10.3289
batch: [14680/21305] batch time: 0.056 trainign loss: 8.5915 avg training loss: 10.3287
batch: [14690/21305] batch time: 2.614 trainign loss: 10.6410 avg training loss: 10.3289
batch: [14700/21305] batch time: 0.056 trainign loss: 10.3950 avg training loss: 10.3292
batch: [14710/21305] batch time: 1.938 trainign loss: 2.0445 avg training loss: 10.3266
batch: [14720/21305] batch time: 0.060 trainign loss: 12.6741 avg training loss: 10.3257
batch: [14730/21305] batch time: 2.464 trainign loss: 10.3531 avg training loss: 10.3260
batch: [14740/21305] batch time: 0.062 trainign loss: 10.4586 avg training loss: 10.3261
batch: [14750/21305] batch time: 2.062 trainign loss: 10.7847 avg training loss: 10.3262
batch: [14760/21305] batch time: 0.063 trainign loss: 10.7232 avg training loss: 10.3264
batch: [14770/21305] batch time: 2.496 trainign loss: 10.6535 avg training loss: 10.3266
batch: [14780/21305] batch time: 0.056 trainign loss: 10.5782 avg training loss: 10.3268
batch: [14790/21305] batch time: 2.295 trainign loss: 10.5786 avg training loss: 10.3271
batch: [14800/21305] batch time: 0.057 trainign loss: 10.6681 avg training loss: 10.3273
batch: [14810/21305] batch time: 2.152 trainign loss: 10.6354 avg training loss: 10.3273
batch: [14820/21305] batch time: 0.057 trainign loss: 10.6632 avg training loss: 10.3275
batch: [14830/21305] batch time: 1.671 trainign loss: 10.6383 avg training loss: 10.3278
batch: [14840/21305] batch time: 0.625 trainign loss: 10.3902 avg training loss: 10.3279
batch: [14850/21305] batch time: 1.559 trainign loss: 10.6320 avg training loss: 10.3281
batch: [14860/21305] batch time: 0.835 trainign loss: 10.6717 avg training loss: 10.3283
batch: [14870/21305] batch time: 1.983 trainign loss: 10.6953 avg training loss: 10.3286
batch: [14880/21305] batch time: 1.542 trainign loss: 10.6589 avg training loss: 10.3288
batch: [14890/21305] batch time: 0.701 trainign loss: 10.6945 avg training loss: 10.3289
batch: [14900/21305] batch time: 1.665 trainign loss: 10.6073 avg training loss: 10.3291
batch: [14910/21305] batch time: 0.237 trainign loss: 9.2572 avg training loss: 10.3291
batch: [14920/21305] batch time: 2.022 trainign loss: 10.4938 avg training loss: 10.3293
batch: [14930/21305] batch time: 0.381 trainign loss: 10.5700 avg training loss: 10.3294
batch: [14940/21305] batch time: 1.982 trainign loss: 10.6763 avg training loss: 10.3296
batch: [14950/21305] batch time: 0.710 trainign loss: 10.5839 avg training loss: 10.3298
batch: [14960/21305] batch time: 1.685 trainign loss: 10.6624 avg training loss: 10.3297
batch: [14970/21305] batch time: 0.529 trainign loss: 10.7347 avg training loss: 10.3298
batch: [14980/21305] batch time: 1.379 trainign loss: 10.7550 avg training loss: 10.3301
batch: [14990/21305] batch time: 0.057 trainign loss: 10.5931 avg training loss: 10.3301
batch: [15000/21305] batch time: 2.066 trainign loss: 10.6531 avg training loss: 10.3303
batch: [15010/21305] batch time: 0.056 trainign loss: 10.6558 avg training loss: 10.3304
batch: [15020/21305] batch time: 1.732 trainign loss: 10.4552 avg training loss: 10.3306
batch: [15030/21305] batch time: 0.059 trainign loss: 10.6136 avg training loss: 10.3308
batch: [15040/21305] batch time: 1.367 trainign loss: 8.5549 avg training loss: 10.3307
batch: [15050/21305] batch time: 0.056 trainign loss: 10.6720 avg training loss: 10.3308
batch: [15060/21305] batch time: 1.273 trainign loss: 10.7640 avg training loss: 10.3310
batch: [15070/21305] batch time: 0.052 trainign loss: 10.6647 avg training loss: 10.3312
batch: [15080/21305] batch time: 1.485 trainign loss: 8.0629 avg training loss: 10.3302
batch: [15090/21305] batch time: 0.054 trainign loss: 10.7545 avg training loss: 10.3305
batch: [15100/21305] batch time: 0.701 trainign loss: 10.5259 avg training loss: 10.3307
batch: [15110/21305] batch time: 0.063 trainign loss: 9.3887 avg training loss: 10.3307
batch: [15120/21305] batch time: 0.250 trainign loss: 10.6450 avg training loss: 10.3307
batch: [15130/21305] batch time: 0.061 trainign loss: 10.7424 avg training loss: 10.3307
batch: [15140/21305] batch time: 0.258 trainign loss: 10.7243 avg training loss: 10.3309
batch: [15150/21305] batch time: 0.062 trainign loss: 10.6185 avg training loss: 10.3311
batch: [15160/21305] batch time: 0.056 trainign loss: 10.6569 avg training loss: 10.3313
batch: [15170/21305] batch time: 0.062 trainign loss: 10.5521 avg training loss: 10.3315
batch: [15180/21305] batch time: 0.060 trainign loss: 10.5948 avg training loss: 10.3316
batch: [15190/21305] batch time: 0.057 trainign loss: 10.5762 avg training loss: 10.3317
batch: [15200/21305] batch time: 0.061 trainign loss: 10.6964 avg training loss: 10.3319
batch: [15210/21305] batch time: 0.051 trainign loss: 10.4075 avg training loss: 10.3318
batch: [15220/21305] batch time: 0.056 trainign loss: 10.5488 avg training loss: 10.3320
batch: [15230/21305] batch time: 0.053 trainign loss: 10.6900 avg training loss: 10.3322
batch: [15240/21305] batch time: 0.058 trainign loss: 10.5851 avg training loss: 10.3324
batch: [15250/21305] batch time: 0.052 trainign loss: 10.6589 avg training loss: 10.3326
batch: [15260/21305] batch time: 0.062 trainign loss: 10.6253 avg training loss: 10.3327
batch: [15270/21305] batch time: 0.056 trainign loss: 10.6205 avg training loss: 10.3328
batch: [15280/21305] batch time: 0.056 trainign loss: 10.4749 avg training loss: 10.3329
batch: [15290/21305] batch time: 0.052 trainign loss: 10.7199 avg training loss: 10.3330
batch: [15300/21305] batch time: 0.056 trainign loss: 10.5760 avg training loss: 10.3332
batch: [15310/21305] batch time: 0.051 trainign loss: 10.7144 avg training loss: 10.3331
batch: [15320/21305] batch time: 0.056 trainign loss: 10.6210 avg training loss: 10.3332
batch: [15330/21305] batch time: 0.062 trainign loss: 10.5944 avg training loss: 10.3334
batch: [15340/21305] batch time: 0.057 trainign loss: 10.3985 avg training loss: 10.3336
batch: [15350/21305] batch time: 0.056 trainign loss: 10.7196 avg training loss: 10.3338
batch: [15360/21305] batch time: 0.056 trainign loss: 10.6870 avg training loss: 10.3339
batch: [15370/21305] batch time: 0.051 trainign loss: 9.5155 avg training loss: 10.3338
batch: [15380/21305] batch time: 0.062 trainign loss: 10.5146 avg training loss: 10.3340
batch: [15390/21305] batch time: 0.052 trainign loss: 10.7241 avg training loss: 10.3342
batch: [15400/21305] batch time: 0.062 trainign loss: 10.6488 avg training loss: 10.3344
batch: [15410/21305] batch time: 0.051 trainign loss: 10.7878 avg training loss: 10.3346
batch: [15420/21305] batch time: 0.063 trainign loss: 10.7084 avg training loss: 10.3349
batch: [15430/21305] batch time: 0.051 trainign loss: 10.5693 avg training loss: 10.3350
batch: [15440/21305] batch time: 0.056 trainign loss: 10.6561 avg training loss: 10.3352
batch: [15450/21305] batch time: 0.055 trainign loss: 10.6169 avg training loss: 10.3353
batch: [15460/21305] batch time: 0.058 trainign loss: 10.6796 avg training loss: 10.3354
batch: [15470/21305] batch time: 0.058 trainign loss: 10.5232 avg training loss: 10.3356
batch: [15480/21305] batch time: 0.058 trainign loss: 10.5703 avg training loss: 10.3358
batch: [15490/21305] batch time: 0.056 trainign loss: 10.5962 avg training loss: 10.3360
batch: [15500/21305] batch time: 0.056 trainign loss: 10.3067 avg training loss: 10.3362
batch: [15510/21305] batch time: 0.070 trainign loss: 10.0287 avg training loss: 10.3362
batch: [15520/21305] batch time: 0.062 trainign loss: 10.7960 avg training loss: 10.3356
batch: [15530/21305] batch time: 0.788 trainign loss: 10.7515 avg training loss: 10.3358
batch: [15540/21305] batch time: 0.062 trainign loss: 10.5634 avg training loss: 10.3361
batch: [15550/21305] batch time: 1.244 trainign loss: 10.5377 avg training loss: 10.3363
batch: [15560/21305] batch time: 0.056 trainign loss: 10.6710 avg training loss: 10.3363
batch: [15570/21305] batch time: 1.156 trainign loss: 10.6005 avg training loss: 10.3365
batch: [15580/21305] batch time: 0.057 trainign loss: 10.6238 avg training loss: 10.3367
batch: [15590/21305] batch time: 1.836 trainign loss: 10.6370 avg training loss: 10.3369
batch: [15600/21305] batch time: 0.056 trainign loss: 10.7211 avg training loss: 10.3370
batch: [15610/21305] batch time: 1.446 trainign loss: 10.4790 avg training loss: 10.3371
batch: [15620/21305] batch time: 0.056 trainign loss: 10.3922 avg training loss: 10.3373
batch: [15630/21305] batch time: 1.089 trainign loss: 10.5286 avg training loss: 10.3375
batch: [15640/21305] batch time: 0.062 trainign loss: 10.5734 avg training loss: 10.3377
batch: [15650/21305] batch time: 1.637 trainign loss: 10.0994 avg training loss: 10.3379
batch: [15660/21305] batch time: 0.062 trainign loss: 10.2598 avg training loss: 10.3380
batch: [15670/21305] batch time: 1.570 trainign loss: 10.6309 avg training loss: 10.3382
batch: [15680/21305] batch time: 0.062 trainign loss: 10.6262 avg training loss: 10.3383
batch: [15690/21305] batch time: 2.016 trainign loss: 10.6173 avg training loss: 10.3385
batch: [15700/21305] batch time: 0.062 trainign loss: 10.5798 avg training loss: 10.3387
batch: [15710/21305] batch time: 1.549 trainign loss: 10.6472 avg training loss: 10.3388
batch: [15720/21305] batch time: 0.056 trainign loss: 10.2534 avg training loss: 10.3389
batch: [15730/21305] batch time: 0.063 trainign loss: 6.5432 avg training loss: 10.3382
batch: [15740/21305] batch time: 0.056 trainign loss: 15.9571 avg training loss: 10.3347
batch: [15750/21305] batch time: 0.155 trainign loss: 10.7094 avg training loss: 10.3360
batch: [15760/21305] batch time: 0.056 trainign loss: 10.5936 avg training loss: 10.3362
batch: [15770/21305] batch time: 0.054 trainign loss: 10.4490 avg training loss: 10.3363
batch: [15780/21305] batch time: 0.056 trainign loss: 10.6275 avg training loss: 10.3365
batch: [15790/21305] batch time: 0.266 trainign loss: 10.6063 avg training loss: 10.3367
batch: [15800/21305] batch time: 0.056 trainign loss: 10.1284 avg training loss: 10.3368
batch: [15810/21305] batch time: 0.057 trainign loss: 10.7342 avg training loss: 10.3368
batch: [15820/21305] batch time: 0.056 trainign loss: 10.3620 avg training loss: 10.3370
batch: [15830/21305] batch time: 0.057 trainign loss: 10.4515 avg training loss: 10.3371
batch: [15840/21305] batch time: 0.056 trainign loss: 10.4456 avg training loss: 10.3374
batch: [15850/21305] batch time: 0.056 trainign loss: 10.4254 avg training loss: 10.3375
batch: [15860/21305] batch time: 0.060 trainign loss: 10.6836 avg training loss: 10.3376
batch: [15870/21305] batch time: 0.059 trainign loss: 10.3707 avg training loss: 10.3378
batch: [15880/21305] batch time: 0.062 trainign loss: 10.6280 avg training loss: 10.3380
batch: [15890/21305] batch time: 0.054 trainign loss: 10.6326 avg training loss: 10.3382
batch: [15900/21305] batch time: 0.063 trainign loss: 10.4955 avg training loss: 10.3383
batch: [15910/21305] batch time: 0.052 trainign loss: 10.6432 avg training loss: 10.3385
batch: [15920/21305] batch time: 0.053 trainign loss: 10.6290 avg training loss: 10.3387
batch: [15930/21305] batch time: 0.062 trainign loss: 10.6880 avg training loss: 10.3387
batch: [15940/21305] batch time: 0.056 trainign loss: 10.7953 avg training loss: 10.3385
batch: [15950/21305] batch time: 0.052 trainign loss: 10.7848 avg training loss: 10.3388
batch: [15960/21305] batch time: 0.061 trainign loss: 9.9153 avg training loss: 10.3388
batch: [15970/21305] batch time: 0.061 trainign loss: 9.9745 avg training loss: 10.3389
batch: [15980/21305] batch time: 0.059 trainign loss: 10.6619 avg training loss: 10.3391
batch: [15990/21305] batch time: 0.063 trainign loss: 10.6987 avg training loss: 10.3393
batch: [16000/21305] batch time: 0.056 trainign loss: 10.6136 avg training loss: 10.3395
batch: [16010/21305] batch time: 0.056 trainign loss: 7.6365 avg training loss: 10.3391
batch: [16020/21305] batch time: 0.056 trainign loss: 11.0308 avg training loss: 10.3389
batch: [16030/21305] batch time: 0.054 trainign loss: 10.6771 avg training loss: 10.3391
batch: [16040/21305] batch time: 0.062 trainign loss: 10.1912 avg training loss: 10.3392
batch: [16050/21305] batch time: 0.060 trainign loss: 10.6268 avg training loss: 10.3393
batch: [16060/21305] batch time: 0.060 trainign loss: 10.5708 avg training loss: 10.3395
batch: [16070/21305] batch time: 0.053 trainign loss: 10.6665 avg training loss: 10.3397
batch: [16080/21305] batch time: 0.057 trainign loss: 10.6409 avg training loss: 10.3398
batch: [16090/21305] batch time: 0.057 trainign loss: 10.6324 avg training loss: 10.3399
batch: [16100/21305] batch time: 0.056 trainign loss: 10.5526 avg training loss: 10.3401
batch: [16110/21305] batch time: 0.051 trainign loss: 10.3862 avg training loss: 10.3403
batch: [16120/21305] batch time: 0.057 trainign loss: 10.0401 avg training loss: 10.3404
batch: [16130/21305] batch time: 0.057 trainign loss: 10.7336 avg training loss: 10.3405
batch: [16140/21305] batch time: 0.059 trainign loss: 10.5187 avg training loss: 10.3407
batch: [16150/21305] batch time: 0.051 trainign loss: 10.6476 avg training loss: 10.3408
batch: [16160/21305] batch time: 0.056 trainign loss: 9.5910 avg training loss: 10.3408
batch: [16170/21305] batch time: 0.057 trainign loss: 10.7283 avg training loss: 10.3410
batch: [16180/21305] batch time: 0.056 trainign loss: 10.6580 avg training loss: 10.3412
batch: [16190/21305] batch time: 0.051 trainign loss: 10.7031 avg training loss: 10.3414
batch: [16200/21305] batch time: 0.056 trainign loss: 10.5144 avg training loss: 10.3416
batch: [16210/21305] batch time: 0.056 trainign loss: 10.7229 avg training loss: 10.3418
batch: [16220/21305] batch time: 0.056 trainign loss: 10.5831 avg training loss: 10.3419
batch: [16230/21305] batch time: 0.051 trainign loss: 10.5637 avg training loss: 10.3421
batch: [16240/21305] batch time: 0.057 trainign loss: 10.6781 avg training loss: 10.3423
batch: [16250/21305] batch time: 0.057 trainign loss: 10.5923 avg training loss: 10.3424
batch: [16260/21305] batch time: 0.058 trainign loss: 10.6743 avg training loss: 10.3426
batch: [16270/21305] batch time: 0.056 trainign loss: 10.6269 avg training loss: 10.3428
batch: [16280/21305] batch time: 0.056 trainign loss: 10.6949 avg training loss: 10.3430
batch: [16290/21305] batch time: 0.057 trainign loss: 10.5432 avg training loss: 10.3432
batch: [16300/21305] batch time: 0.062 trainign loss: 10.5298 avg training loss: 10.3433
batch: [16310/21305] batch time: 0.051 trainign loss: 10.6599 avg training loss: 10.3434
batch: [16320/21305] batch time: 0.060 trainign loss: 10.6925 avg training loss: 10.3436
batch: [16330/21305] batch time: 0.334 trainign loss: 10.7369 avg training loss: 10.3437
batch: [16340/21305] batch time: 0.056 trainign loss: 10.3221 avg training loss: 10.3437
batch: [16350/21305] batch time: 0.056 trainign loss: 10.6971 avg training loss: 10.3438
batch: [16360/21305] batch time: 0.062 trainign loss: 10.5926 avg training loss: 10.3437
batch: [16370/21305] batch time: 0.062 trainign loss: 10.6220 avg training loss: 10.3439
batch: [16380/21305] batch time: 0.057 trainign loss: 10.6147 avg training loss: 10.3439
batch: [16390/21305] batch time: 0.056 trainign loss: 10.2916 avg training loss: 10.3440
batch: [16400/21305] batch time: 0.058 trainign loss: 10.6976 avg training loss: 10.3442
batch: [16410/21305] batch time: 0.057 trainign loss: 10.6328 avg training loss: 10.3444
batch: [16420/21305] batch time: 0.056 trainign loss: 10.6220 avg training loss: 10.3445
batch: [16430/21305] batch time: 0.056 trainign loss: 10.6118 avg training loss: 10.3446
batch: [16440/21305] batch time: 0.058 trainign loss: 10.6270 avg training loss: 10.3448
batch: [16450/21305] batch time: 0.053 trainign loss: 10.7145 avg training loss: 10.3450
batch: [16460/21305] batch time: 0.438 trainign loss: 10.6779 avg training loss: 10.3452
batch: [16470/21305] batch time: 0.056 trainign loss: 10.6393 avg training loss: 10.3453
batch: [16480/21305] batch time: 0.058 trainign loss: 10.4645 avg training loss: 10.3455
batch: [16490/21305] batch time: 0.056 trainign loss: 10.5687 avg training loss: 10.3456
batch: [16500/21305] batch time: 0.056 trainign loss: 10.4464 avg training loss: 10.3458
batch: [16510/21305] batch time: 0.051 trainign loss: 10.7031 avg training loss: 10.3459
batch: [16520/21305] batch time: 0.056 trainign loss: 10.5456 avg training loss: 10.3460
batch: [16530/21305] batch time: 0.054 trainign loss: 10.7018 avg training loss: 10.3462
batch: [16540/21305] batch time: 0.060 trainign loss: 10.8066 avg training loss: 10.3462
batch: [16550/21305] batch time: 0.056 trainign loss: 10.6540 avg training loss: 10.3463
batch: [16560/21305] batch time: 0.062 trainign loss: 10.6968 avg training loss: 10.3465
batch: [16570/21305] batch time: 0.056 trainign loss: 10.6708 avg training loss: 10.3466
batch: [16580/21305] batch time: 0.058 trainign loss: 10.7238 avg training loss: 10.3468
batch: [16590/21305] batch time: 0.056 trainign loss: 10.7243 avg training loss: 10.3469
batch: [16600/21305] batch time: 0.062 trainign loss: 10.5465 avg training loss: 10.3471
batch: [16610/21305] batch time: 0.057 trainign loss: 10.7360 avg training loss: 10.3468
batch: [16620/21305] batch time: 0.063 trainign loss: 10.8590 avg training loss: 10.3467
batch: [16630/21305] batch time: 0.051 trainign loss: 10.1299 avg training loss: 10.3467
batch: [16640/21305] batch time: 0.322 trainign loss: 10.6814 avg training loss: 10.3469
batch: [16650/21305] batch time: 0.269 trainign loss: 10.6556 avg training loss: 10.3470
batch: [16660/21305] batch time: 0.630 trainign loss: 10.7504 avg training loss: 10.3472
batch: [16670/21305] batch time: 0.278 trainign loss: 10.6419 avg training loss: 10.3473
batch: [16680/21305] batch time: 1.349 trainign loss: 10.6905 avg training loss: 10.3474
batch: [16690/21305] batch time: 0.055 trainign loss: 10.6668 avg training loss: 10.3476
batch: [16700/21305] batch time: 1.986 trainign loss: 10.6336 avg training loss: 10.3477
batch: [16710/21305] batch time: 0.056 trainign loss: 10.7052 avg training loss: 10.3479
batch: [16720/21305] batch time: 1.358 trainign loss: 10.7195 avg training loss: 10.3481
batch: [16730/21305] batch time: 0.051 trainign loss: 10.4232 avg training loss: 10.3482
batch: [16740/21305] batch time: 2.812 trainign loss: 10.6254 avg training loss: 10.3484
batch: [16750/21305] batch time: 0.061 trainign loss: 10.5524 avg training loss: 10.3485
batch: [16760/21305] batch time: 2.489 trainign loss: 10.6934 avg training loss: 10.3487
batch: [16770/21305] batch time: 0.056 trainign loss: 10.6870 avg training loss: 10.3489
batch: [16780/21305] batch time: 2.181 trainign loss: 10.5042 avg training loss: 10.3490
batch: [16790/21305] batch time: 0.056 trainign loss: 10.6203 avg training loss: 10.3492
batch: [16800/21305] batch time: 1.705 trainign loss: 8.8405 avg training loss: 10.3489
batch: [16810/21305] batch time: 0.052 trainign loss: 10.8328 avg training loss: 10.3490
batch: [16820/21305] batch time: 2.317 trainign loss: 10.2548 avg training loss: 10.3492
batch: [16830/21305] batch time: 0.062 trainign loss: 10.6685 avg training loss: 10.3494
batch: [16840/21305] batch time: 1.782 trainign loss: 10.6398 avg training loss: 10.3495
batch: [16850/21305] batch time: 0.056 trainign loss: 10.5510 avg training loss: 10.3496
batch: [16860/21305] batch time: 1.380 trainign loss: 10.6549 avg training loss: 10.3497
batch: [16870/21305] batch time: 0.063 trainign loss: 10.6534 avg training loss: 10.3497
batch: [16880/21305] batch time: 1.440 trainign loss: 10.6155 avg training loss: 10.3499
batch: [16890/21305] batch time: 0.054 trainign loss: 10.6732 avg training loss: 10.3500
batch: [16900/21305] batch time: 1.094 trainign loss: 10.4320 avg training loss: 10.3502
batch: [16910/21305] batch time: 0.056 trainign loss: 1.9354 avg training loss: 10.3479
batch: [16920/21305] batch time: 1.090 trainign loss: 10.8119 avg training loss: 10.3475
batch: [16930/21305] batch time: 0.060 trainign loss: 10.0922 avg training loss: 10.3477
batch: [16940/21305] batch time: 0.789 trainign loss: 10.6785 avg training loss: 10.3477
batch: [16950/21305] batch time: 0.063 trainign loss: 10.7117 avg training loss: 10.3478
batch: [16960/21305] batch time: 1.020 trainign loss: 10.0111 avg training loss: 10.3480
batch: [16970/21305] batch time: 0.053 trainign loss: 10.4817 avg training loss: 10.3480
batch: [16980/21305] batch time: 1.418 trainign loss: 9.1171 avg training loss: 10.3480
batch: [16990/21305] batch time: 0.056 trainign loss: 9.6999 avg training loss: 10.3478
batch: [17000/21305] batch time: 1.333 trainign loss: 8.9173 avg training loss: 10.3477
batch: [17010/21305] batch time: 0.056 trainign loss: 10.6529 avg training loss: 10.3479
batch: [17020/21305] batch time: 1.667 trainign loss: 7.1062 avg training loss: 10.3475
batch: [17030/21305] batch time: 0.062 trainign loss: 11.8372 avg training loss: 10.3466
batch: [17040/21305] batch time: 1.851 trainign loss: 10.6806 avg training loss: 10.3469
batch: [17050/21305] batch time: 0.057 trainign loss: 10.7080 avg training loss: 10.3469
batch: [17060/21305] batch time: 1.789 trainign loss: 10.7311 avg training loss: 10.3468
batch: [17070/21305] batch time: 0.063 trainign loss: 10.7151 avg training loss: 10.3467
batch: [17080/21305] batch time: 1.532 trainign loss: 10.6209 avg training loss: 10.3469
batch: [17090/21305] batch time: 0.056 trainign loss: 10.5967 avg training loss: 10.3471
batch: [17100/21305] batch time: 1.189 trainign loss: 8.2642 avg training loss: 10.3469
batch: [17110/21305] batch time: 0.063 trainign loss: 10.7325 avg training loss: 10.3471
batch: [17120/21305] batch time: 0.839 trainign loss: 10.6955 avg training loss: 10.3473
batch: [17130/21305] batch time: 0.060 trainign loss: 10.6457 avg training loss: 10.3475
batch: [17140/21305] batch time: 1.317 trainign loss: 10.5111 avg training loss: 10.3476
batch: [17150/21305] batch time: 0.058 trainign loss: 10.5935 avg training loss: 10.3478
batch: [17160/21305] batch time: 1.216 trainign loss: 10.7672 avg training loss: 10.3478
batch: [17170/21305] batch time: 0.059 trainign loss: 10.1813 avg training loss: 10.3476
batch: [17180/21305] batch time: 1.653 trainign loss: 10.9063 avg training loss: 10.3478
batch: [17190/21305] batch time: 0.062 trainign loss: 10.4202 avg training loss: 10.3478
batch: [17200/21305] batch time: 1.437 trainign loss: 10.6331 avg training loss: 10.3479
batch: [17210/21305] batch time: 0.057 trainign loss: 10.5839 avg training loss: 10.3480
batch: [17220/21305] batch time: 0.743 trainign loss: 10.6737 avg training loss: 10.3481
batch: [17230/21305] batch time: 0.062 trainign loss: 10.4404 avg training loss: 10.3481
batch: [17240/21305] batch time: 0.053 trainign loss: 10.4982 avg training loss: 10.3481
batch: [17250/21305] batch time: 0.062 trainign loss: 10.7015 avg training loss: 10.3480
batch: [17260/21305] batch time: 0.056 trainign loss: 10.5830 avg training loss: 10.3481
batch: [17270/21305] batch time: 0.056 trainign loss: 10.5877 avg training loss: 10.3483
batch: [17280/21305] batch time: 0.390 trainign loss: 10.7663 avg training loss: 10.3484
batch: [17290/21305] batch time: 0.336 trainign loss: 9.9774 avg training loss: 10.3486
batch: [17300/21305] batch time: 0.061 trainign loss: 10.6064 avg training loss: 10.3487
batch: [17310/21305] batch time: 1.185 trainign loss: 10.0122 avg training loss: 10.3488
batch: [17320/21305] batch time: 0.056 trainign loss: 11.3417 avg training loss: 10.3466
batch: [17330/21305] batch time: 1.821 trainign loss: 10.7480 avg training loss: 10.3471
batch: [17340/21305] batch time: 0.063 trainign loss: 10.6250 avg training loss: 10.3473
batch: [17350/21305] batch time: 1.628 trainign loss: 10.6757 avg training loss: 10.3468
batch: [17360/21305] batch time: 0.060 trainign loss: 10.8765 avg training loss: 10.3470
batch: [17370/21305] batch time: 1.938 trainign loss: 10.5051 avg training loss: 10.3471
batch: [17380/21305] batch time: 0.396 trainign loss: 10.5701 avg training loss: 10.3473
batch: [17390/21305] batch time: 1.826 trainign loss: 10.6837 avg training loss: 10.3474
batch: [17400/21305] batch time: 1.134 trainign loss: 10.6319 avg training loss: 10.3476
batch: [17410/21305] batch time: 1.460 trainign loss: 10.6976 avg training loss: 10.3478
batch: [17420/21305] batch time: 1.344 trainign loss: 10.5221 avg training loss: 10.3480
batch: [17430/21305] batch time: 0.879 trainign loss: 10.4829 avg training loss: 10.3481
batch: [17440/21305] batch time: 1.321 trainign loss: 10.7207 avg training loss: 10.3482
batch: [17450/21305] batch time: 1.396 trainign loss: 10.7209 avg training loss: 10.3484
batch: [17460/21305] batch time: 1.011 trainign loss: 10.3901 avg training loss: 10.3485
batch: [17470/21305] batch time: 1.304 trainign loss: 10.6393 avg training loss: 10.3486
batch: [17480/21305] batch time: 1.824 trainign loss: 10.5865 avg training loss: 10.3486
batch: [17490/21305] batch time: 0.274 trainign loss: 10.6218 avg training loss: 10.3488
batch: [17500/21305] batch time: 1.935 trainign loss: 10.5527 avg training loss: 10.3489
batch: [17510/21305] batch time: 0.743 trainign loss: 10.7372 avg training loss: 10.3490
batch: [17520/21305] batch time: 1.037 trainign loss: 10.3871 avg training loss: 10.3491
batch: [17530/21305] batch time: 1.263 trainign loss: 10.3726 avg training loss: 10.3492
batch: [17540/21305] batch time: 1.041 trainign loss: 10.6346 avg training loss: 10.3493
batch: [17550/21305] batch time: 1.697 trainign loss: 10.5510 avg training loss: 10.3495
batch: [17560/21305] batch time: 0.888 trainign loss: 10.6796 avg training loss: 10.3497
batch: [17570/21305] batch time: 1.782 trainign loss: 10.6670 avg training loss: 10.3497
batch: [17580/21305] batch time: 0.058 trainign loss: 8.6118 avg training loss: 10.3496
batch: [17590/21305] batch time: 2.648 trainign loss: 13.5117 avg training loss: 10.3466
batch: [17600/21305] batch time: 0.056 trainign loss: 10.6342 avg training loss: 10.3470
batch: [17610/21305] batch time: 2.192 trainign loss: 10.7308 avg training loss: 10.3472
batch: [17620/21305] batch time: 0.441 trainign loss: 10.3231 avg training loss: 10.3473
batch: [17630/21305] batch time: 1.453 trainign loss: 10.6049 avg training loss: 10.3474
batch: [17640/21305] batch time: 0.833 trainign loss: 10.2730 avg training loss: 10.3476
batch: [17650/21305] batch time: 1.575 trainign loss: 10.4021 avg training loss: 10.3477
batch: [17660/21305] batch time: 1.128 trainign loss: 10.5855 avg training loss: 10.3479
batch: [17670/21305] batch time: 1.249 trainign loss: 10.3801 avg training loss: 10.3480
batch: [17680/21305] batch time: 1.552 trainign loss: 10.4409 avg training loss: 10.3481
batch: [17690/21305] batch time: 1.781 trainign loss: 10.1855 avg training loss: 10.3482
batch: [17700/21305] batch time: 1.258 trainign loss: 10.5939 avg training loss: 10.3481
batch: [17710/21305] batch time: 0.968 trainign loss: 10.5205 avg training loss: 10.3483
batch: [17720/21305] batch time: 0.454 trainign loss: 10.6088 avg training loss: 10.3484
batch: [17730/21305] batch time: 1.655 trainign loss: 10.6453 avg training loss: 10.3486
batch: [17740/21305] batch time: 1.038 trainign loss: 10.6552 avg training loss: 10.3487
batch: [17750/21305] batch time: 0.674 trainign loss: 10.7557 avg training loss: 10.3489
batch: [17760/21305] batch time: 1.183 trainign loss: 10.7077 avg training loss: 10.3491
batch: [17770/21305] batch time: 1.402 trainign loss: 10.5941 avg training loss: 10.3492
batch: [17780/21305] batch time: 1.099 trainign loss: 10.6222 avg training loss: 10.3494
batch: [17790/21305] batch time: 1.516 trainign loss: 10.6164 avg training loss: 10.3495
batch: [17800/21305] batch time: 1.022 trainign loss: 10.5335 avg training loss: 10.3497
batch: [17810/21305] batch time: 1.280 trainign loss: 10.6749 avg training loss: 10.3498
batch: [17820/21305] batch time: 2.011 trainign loss: 10.6520 avg training loss: 10.3500
batch: [17830/21305] batch time: 0.061 trainign loss: 10.3701 avg training loss: 10.3501
batch: [17840/21305] batch time: 2.495 trainign loss: 10.6632 avg training loss: 10.3503
batch: [17850/21305] batch time: 0.056 trainign loss: 10.7026 avg training loss: 10.3504
batch: [17860/21305] batch time: 2.398 trainign loss: 10.7019 avg training loss: 10.3505
batch: [17870/21305] batch time: 0.343 trainign loss: 10.3956 avg training loss: 10.3506
batch: [17880/21305] batch time: 2.124 trainign loss: 10.5997 avg training loss: 10.3508
batch: [17890/21305] batch time: 0.056 trainign loss: 10.6786 avg training loss: 10.3509
batch: [17900/21305] batch time: 2.142 trainign loss: 10.6957 avg training loss: 10.3511
batch: [17910/21305] batch time: 0.054 trainign loss: 9.0127 avg training loss: 10.3509
batch: [17920/21305] batch time: 2.375 trainign loss: 10.7498 avg training loss: 10.3511
batch: [17930/21305] batch time: 0.056 trainign loss: 10.4993 avg training loss: 10.3512
batch: [17940/21305] batch time: 2.421 trainign loss: 10.6290 avg training loss: 10.3514
batch: [17950/21305] batch time: 0.056 trainign loss: 10.6734 avg training loss: 10.3514
batch: [17960/21305] batch time: 2.416 trainign loss: 10.7406 avg training loss: 10.3515
batch: [17970/21305] batch time: 0.060 trainign loss: 10.5946 avg training loss: 10.3516
batch: [17980/21305] batch time: 2.117 trainign loss: 7.7335 avg training loss: 10.3514
batch: [17990/21305] batch time: 0.061 trainign loss: 10.0709 avg training loss: 10.3511
batch: [18000/21305] batch time: 2.286 trainign loss: 10.7125 avg training loss: 10.3512
batch: [18010/21305] batch time: 0.056 trainign loss: 10.6937 avg training loss: 10.3513
batch: [18020/21305] batch time: 1.766 trainign loss: 9.3684 avg training loss: 10.3513
batch: [18030/21305] batch time: 0.060 trainign loss: 10.8246 avg training loss: 10.3514
batch: [18040/21305] batch time: 2.548 trainign loss: 10.4880 avg training loss: 10.3515
batch: [18050/21305] batch time: 0.144 trainign loss: 10.7420 avg training loss: 10.3517
batch: [18060/21305] batch time: 2.087 trainign loss: 10.7154 avg training loss: 10.3518
batch: [18070/21305] batch time: 0.460 trainign loss: 10.6421 avg training loss: 10.3520
batch: [18080/21305] batch time: 1.874 trainign loss: 10.6937 avg training loss: 10.3521
batch: [18090/21305] batch time: 0.778 trainign loss: 10.3891 avg training loss: 10.3522
batch: [18100/21305] batch time: 1.349 trainign loss: 10.5747 avg training loss: 10.3523
batch: [18110/21305] batch time: 1.034 trainign loss: 10.2278 avg training loss: 10.3525
batch: [18120/21305] batch time: 1.062 trainign loss: 10.7051 avg training loss: 10.3521
batch: [18130/21305] batch time: 1.914 trainign loss: 10.9149 avg training loss: 10.3523
batch: [18140/21305] batch time: 0.770 trainign loss: 10.6694 avg training loss: 10.3524
batch: [18150/21305] batch time: 1.206 trainign loss: 10.6417 avg training loss: 10.3526
batch: [18160/21305] batch time: 0.062 trainign loss: 10.3585 avg training loss: 10.3526
batch: [18170/21305] batch time: 1.726 trainign loss: 10.6375 avg training loss: 10.3528
batch: [18180/21305] batch time: 0.056 trainign loss: 5.0156 avg training loss: 10.3518
batch: [18190/21305] batch time: 2.212 trainign loss: 10.6193 avg training loss: 10.3516
batch: [18200/21305] batch time: 0.057 trainign loss: 10.6402 avg training loss: 10.3517
batch: [18210/21305] batch time: 2.578 trainign loss: 10.6488 avg training loss: 10.3517
batch: [18220/21305] batch time: 0.062 trainign loss: 10.1716 avg training loss: 10.3517
batch: [18230/21305] batch time: 2.184 trainign loss: 10.6505 avg training loss: 10.3519
batch: [18240/21305] batch time: 0.062 trainign loss: 10.4957 avg training loss: 10.3520
batch: [18250/21305] batch time: 1.973 trainign loss: 10.5613 avg training loss: 10.3522
batch: [18260/21305] batch time: 0.057 trainign loss: 8.9806 avg training loss: 10.3521
batch: [18270/21305] batch time: 1.838 trainign loss: 10.4673 avg training loss: 10.3522
batch: [18280/21305] batch time: 0.061 trainign loss: 10.7634 avg training loss: 10.3521
batch: [18290/21305] batch time: 2.118 trainign loss: 10.6752 avg training loss: 10.3523
batch: [18300/21305] batch time: 0.058 trainign loss: 10.4656 avg training loss: 10.3525
batch: [18310/21305] batch time: 2.559 trainign loss: 10.6256 avg training loss: 10.3525
batch: [18320/21305] batch time: 0.056 trainign loss: 10.5299 avg training loss: 10.3527
batch: [18330/21305] batch time: 2.216 trainign loss: 10.4528 avg training loss: 10.3526
batch: [18340/21305] batch time: 0.056 trainign loss: 10.7418 avg training loss: 10.3528
batch: [18350/21305] batch time: 2.344 trainign loss: 10.6682 avg training loss: 10.3530
batch: [18360/21305] batch time: 0.056 trainign loss: 10.6162 avg training loss: 10.3531
batch: [18370/21305] batch time: 2.049 trainign loss: 10.6852 avg training loss: 10.3532
batch: [18380/21305] batch time: 0.059 trainign loss: 10.6920 avg training loss: 10.3533
batch: [18390/21305] batch time: 1.819 trainign loss: 10.4872 avg training loss: 10.3534
batch: [18400/21305] batch time: 0.062 trainign loss: 10.7341 avg training loss: 10.3536
batch: [18410/21305] batch time: 1.548 trainign loss: 10.1752 avg training loss: 10.3537
batch: [18420/21305] batch time: 0.056 trainign loss: 10.7212 avg training loss: 10.3538
batch: [18430/21305] batch time: 2.259 trainign loss: 10.6817 avg training loss: 10.3540
batch: [18440/21305] batch time: 0.055 trainign loss: 5.2508 avg training loss: 10.3530
batch: [18450/21305] batch time: 1.443 trainign loss: 10.7595 avg training loss: 10.3517
batch: [18460/21305] batch time: 0.056 trainign loss: 10.6735 avg training loss: 10.3519
batch: [18470/21305] batch time: 0.447 trainign loss: 10.6786 avg training loss: 10.3521
batch: [18480/21305] batch time: 0.056 trainign loss: 10.6928 avg training loss: 10.3522
batch: [18490/21305] batch time: 0.055 trainign loss: 10.4378 avg training loss: 10.3523
batch: [18500/21305] batch time: 0.056 trainign loss: 2.7668 avg training loss: 10.3504
batch: [18510/21305] batch time: 0.056 trainign loss: 10.8060 avg training loss: 10.3498
batch: [18520/21305] batch time: 0.057 trainign loss: 10.6527 avg training loss: 10.3500
batch: [18530/21305] batch time: 0.053 trainign loss: 10.5506 avg training loss: 10.3501
batch: [18540/21305] batch time: 0.058 trainign loss: 10.7060 avg training loss: 10.3503
batch: [18550/21305] batch time: 0.099 trainign loss: 10.0351 avg training loss: 10.3503
batch: [18560/21305] batch time: 0.056 trainign loss: 10.6267 avg training loss: 10.3503
batch: [18570/21305] batch time: 0.070 trainign loss: 10.6911 avg training loss: 10.3505
batch: [18580/21305] batch time: 0.057 trainign loss: 10.7857 avg training loss: 10.3506
batch: [18590/21305] batch time: 0.450 trainign loss: 10.7716 avg training loss: 10.3507
batch: [18600/21305] batch time: 0.057 trainign loss: 10.6854 avg training loss: 10.3509
batch: [18610/21305] batch time: 0.113 trainign loss: 10.6133 avg training loss: 10.3506
batch: [18620/21305] batch time: 0.060 trainign loss: 10.8244 avg training loss: 10.3508
batch: [18630/21305] batch time: 0.056 trainign loss: 10.7038 avg training loss: 10.3510
batch: [18640/21305] batch time: 0.056 trainign loss: 10.4610 avg training loss: 10.3511
batch: [18650/21305] batch time: 0.053 trainign loss: 9.9784 avg training loss: 10.3512
batch: [18660/21305] batch time: 0.056 trainign loss: 1.3821 avg training loss: 10.3488
batch: [18670/21305] batch time: 0.053 trainign loss: 11.3834 avg training loss: 10.3477
batch: [18680/21305] batch time: 0.057 trainign loss: 10.5893 avg training loss: 10.3479
batch: [18690/21305] batch time: 0.056 trainign loss: 10.7118 avg training loss: 10.3480
batch: [18700/21305] batch time: 0.063 trainign loss: 10.6504 avg training loss: 10.3482
batch: [18710/21305] batch time: 0.054 trainign loss: 10.7550 avg training loss: 10.3483
batch: [18720/21305] batch time: 0.058 trainign loss: 10.6268 avg training loss: 10.3485
batch: [18730/21305] batch time: 0.051 trainign loss: 10.6968 avg training loss: 10.3486
batch: [18740/21305] batch time: 0.056 trainign loss: 10.6789 avg training loss: 10.3488
batch: [18750/21305] batch time: 0.052 trainign loss: 10.5023 avg training loss: 10.3489
batch: [18760/21305] batch time: 0.056 trainign loss: 10.4960 avg training loss: 10.3490
batch: [18770/21305] batch time: 0.056 trainign loss: 10.5856 avg training loss: 10.3489
batch: [18780/21305] batch time: 0.058 trainign loss: 10.7650 avg training loss: 10.3490
batch: [18790/21305] batch time: 0.670 trainign loss: 10.3025 avg training loss: 10.3491
batch: [18800/21305] batch time: 1.077 trainign loss: 10.7509 avg training loss: 10.3493
batch: [18810/21305] batch time: 0.056 trainign loss: 10.6764 avg training loss: 10.3495
batch: [18820/21305] batch time: 1.463 trainign loss: 9.7600 avg training loss: 10.3496
batch: [18830/21305] batch time: 0.167 trainign loss: 10.6976 avg training loss: 10.3497
batch: [18840/21305] batch time: 0.927 trainign loss: 10.9066 avg training loss: 10.3499
batch: [18850/21305] batch time: 0.435 trainign loss: 10.6752 avg training loss: 10.3498
batch: [18860/21305] batch time: 0.615 trainign loss: 10.7920 avg training loss: 10.3499
batch: [18870/21305] batch time: 0.052 trainign loss: 10.6719 avg training loss: 10.3501
batch: [18880/21305] batch time: 0.813 trainign loss: 9.3800 avg training loss: 10.3499
batch: [18890/21305] batch time: 0.062 trainign loss: 10.8269 avg training loss: 10.3501
batch: [18900/21305] batch time: 1.436 trainign loss: 10.4636 avg training loss: 10.3503
batch: [18910/21305] batch time: 0.055 trainign loss: 10.6139 avg training loss: 10.3505
batch: [18920/21305] batch time: 1.533 trainign loss: 10.6507 avg training loss: 10.3506
batch: [18930/21305] batch time: 0.062 trainign loss: 10.6354 avg training loss: 10.3507
batch: [18940/21305] batch time: 1.509 trainign loss: 10.7404 avg training loss: 10.3509
batch: [18950/21305] batch time: 0.063 trainign loss: 10.6806 avg training loss: 10.3511
batch: [18960/21305] batch time: 2.361 trainign loss: 10.6493 avg training loss: 10.3510
batch: [18970/21305] batch time: 0.061 trainign loss: 10.8046 avg training loss: 10.3511
batch: [18980/21305] batch time: 1.929 trainign loss: 10.7364 avg training loss: 10.3508
batch: [18990/21305] batch time: 0.056 trainign loss: 10.7968 avg training loss: 10.3510
batch: [19000/21305] batch time: 2.211 trainign loss: 10.7142 avg training loss: 10.3511
batch: [19010/21305] batch time: 0.060 trainign loss: 10.6744 avg training loss: 10.3513
batch: [19020/21305] batch time: 2.346 trainign loss: 10.5476 avg training loss: 10.3514
batch: [19030/21305] batch time: 0.056 trainign loss: 10.7399 avg training loss: 10.3515
batch: [19040/21305] batch time: 2.304 trainign loss: 10.6753 avg training loss: 10.3516
batch: [19050/21305] batch time: 0.056 trainign loss: 10.7505 avg training loss: 10.3518
batch: [19060/21305] batch time: 2.247 trainign loss: 10.3255 avg training loss: 10.3517
batch: [19070/21305] batch time: 0.062 trainign loss: 10.6910 avg training loss: 10.3518
batch: [19080/21305] batch time: 2.063 trainign loss: 10.7213 avg training loss: 10.3520
batch: [19090/21305] batch time: 0.275 trainign loss: 10.6849 avg training loss: 10.3521
batch: [19100/21305] batch time: 1.974 trainign loss: 10.6492 avg training loss: 10.3523
batch: [19110/21305] batch time: 0.874 trainign loss: 9.4950 avg training loss: 10.3523
batch: [19120/21305] batch time: 1.592 trainign loss: 10.6468 avg training loss: 10.3525
batch: [19130/21305] batch time: 1.555 trainign loss: 10.7137 avg training loss: 10.3526
batch: [19140/21305] batch time: 1.378 trainign loss: 10.5933 avg training loss: 10.3527
batch: [19150/21305] batch time: 0.285 trainign loss: 10.5541 avg training loss: 10.3529
batch: [19160/21305] batch time: 1.299 trainign loss: 10.6693 avg training loss: 10.3530
batch: [19170/21305] batch time: 0.320 trainign loss: 10.6719 avg training loss: 10.3532
batch: [19180/21305] batch time: 0.687 trainign loss: 10.1257 avg training loss: 10.3533
batch: [19190/21305] batch time: 0.674 trainign loss: 10.6085 avg training loss: 10.3534
batch: [19200/21305] batch time: 0.062 trainign loss: 9.9353 avg training loss: 10.3534
batch: [19210/21305] batch time: 1.351 trainign loss: 10.5648 avg training loss: 10.3535
batch: [19220/21305] batch time: 0.177 trainign loss: 10.6394 avg training loss: 10.3535
batch: [19230/21305] batch time: 1.352 trainign loss: 10.3675 avg training loss: 10.3536
batch: [19240/21305] batch time: 1.010 trainign loss: 10.6381 avg training loss: 10.3538
batch: [19250/21305] batch time: 0.731 trainign loss: 9.3867 avg training loss: 10.3538
batch: [19260/21305] batch time: 1.510 trainign loss: 10.5399 avg training loss: 10.3537
batch: [19270/21305] batch time: 0.062 trainign loss: 10.5305 avg training loss: 10.3538
batch: [19280/21305] batch time: 1.381 trainign loss: 10.7304 avg training loss: 10.3540
batch: [19290/21305] batch time: 0.056 trainign loss: 10.6811 avg training loss: 10.3541
batch: [19300/21305] batch time: 0.753 trainign loss: 10.6637 avg training loss: 10.3542
batch: [19310/21305] batch time: 0.492 trainign loss: 10.7263 avg training loss: 10.3544
batch: [19320/21305] batch time: 1.088 trainign loss: 10.7078 avg training loss: 10.3546
batch: [19330/21305] batch time: 0.056 trainign loss: 10.6452 avg training loss: 10.3547
batch: [19340/21305] batch time: 0.539 trainign loss: 10.3668 avg training loss: 10.3549
batch: [19350/21305] batch time: 0.062 trainign loss: 10.6259 avg training loss: 10.3550
batch: [19360/21305] batch time: 0.366 trainign loss: 10.5966 avg training loss: 10.3552
batch: [19370/21305] batch time: 0.057 trainign loss: 10.6909 avg training loss: 10.3553
batch: [19380/21305] batch time: 0.786 trainign loss: 10.3601 avg training loss: 10.3553
batch: [19390/21305] batch time: 0.062 trainign loss: 10.7622 avg training loss: 10.3554
batch: [19400/21305] batch time: 0.869 trainign loss: 10.7200 avg training loss: 10.3556
batch: [19410/21305] batch time: 0.063 trainign loss: 10.3572 avg training loss: 10.3557
batch: [19420/21305] batch time: 0.539 trainign loss: 10.6723 avg training loss: 10.3558
batch: [19430/21305] batch time: 0.058 trainign loss: 10.6542 avg training loss: 10.3560
batch: [19440/21305] batch time: 0.712 trainign loss: 10.6669 avg training loss: 10.3561
batch: [19450/21305] batch time: 0.060 trainign loss: 10.6223 avg training loss: 10.3562
batch: [19460/21305] batch time: 0.781 trainign loss: 10.4720 avg training loss: 10.3564
batch: [19470/21305] batch time: 0.063 trainign loss: 10.6742 avg training loss: 10.3565
batch: [19480/21305] batch time: 1.850 trainign loss: 10.5274 avg training loss: 10.3566
batch: [19490/21305] batch time: 0.056 trainign loss: 10.4636 avg training loss: 10.3568
batch: [19500/21305] batch time: 1.409 trainign loss: 10.6075 avg training loss: 10.3569
batch: [19510/21305] batch time: 0.928 trainign loss: 10.6798 avg training loss: 10.3571
batch: [19520/21305] batch time: 1.459 trainign loss: 10.7811 avg training loss: 10.3572
batch: [19530/21305] batch time: 0.745 trainign loss: 10.6896 avg training loss: 10.3573
batch: [19540/21305] batch time: 1.006 trainign loss: 10.6214 avg training loss: 10.3574
batch: [19550/21305] batch time: 0.542 trainign loss: 10.7285 avg training loss: 10.3574
batch: [19560/21305] batch time: 1.273 trainign loss: 10.7466 avg training loss: 10.3576
batch: [19570/21305] batch time: 0.874 trainign loss: 10.6627 avg training loss: 10.3577
batch: [19580/21305] batch time: 2.049 trainign loss: 10.5952 avg training loss: 10.3579
batch: [19590/21305] batch time: 0.372 trainign loss: 10.6098 avg training loss: 10.3578
batch: [19600/21305] batch time: 1.696 trainign loss: 10.5571 avg training loss: 10.3579
batch: [19610/21305] batch time: 0.581 trainign loss: 10.5086 avg training loss: 10.3580
batch: [19620/21305] batch time: 1.270 trainign loss: 10.7740 avg training loss: 10.3581
batch: [19630/21305] batch time: 1.644 trainign loss: 10.6502 avg training loss: 10.3583
batch: [19640/21305] batch time: 0.585 trainign loss: 10.7160 avg training loss: 10.3584
batch: [19650/21305] batch time: 1.035 trainign loss: 10.6707 avg training loss: 10.3586
batch: [19660/21305] batch time: 1.511 trainign loss: 10.6962 avg training loss: 10.3587
batch: [19670/21305] batch time: 0.439 trainign loss: 10.6659 avg training loss: 10.3589
batch: [19680/21305] batch time: 1.687 trainign loss: 4.3104 avg training loss: 10.3577
batch: [19690/21305] batch time: 1.305 trainign loss: 10.8318 avg training loss: 10.3575
batch: [19700/21305] batch time: 1.299 trainign loss: 10.7208 avg training loss: 10.3577
batch: [19710/21305] batch time: 0.158 trainign loss: 10.6441 avg training loss: 10.3578
batch: [19720/21305] batch time: 2.483 trainign loss: 10.6439 avg training loss: 10.3579
batch: [19730/21305] batch time: 0.056 trainign loss: 10.4189 avg training loss: 10.3580
batch: [19740/21305] batch time: 2.236 trainign loss: 10.4941 avg training loss: 10.3582
batch: [19750/21305] batch time: 0.059 trainign loss: 10.7546 avg training loss: 10.3583
batch: [19760/21305] batch time: 2.194 trainign loss: 10.6440 avg training loss: 10.3584
batch: [19770/21305] batch time: 0.057 trainign loss: 10.7110 avg training loss: 10.3586
batch: [19780/21305] batch time: 2.150 trainign loss: 10.3999 avg training loss: 10.3588
batch: [19790/21305] batch time: 0.439 trainign loss: 10.6321 avg training loss: 10.3588
batch: [19800/21305] batch time: 1.629 trainign loss: 10.7578 avg training loss: 10.3589
batch: [19810/21305] batch time: 0.257 trainign loss: 10.5251 avg training loss: 10.3589
batch: [19820/21305] batch time: 1.616 trainign loss: 10.6971 avg training loss: 10.3590
batch: [19830/21305] batch time: 0.057 trainign loss: 10.6286 avg training loss: 10.3592
batch: [19840/21305] batch time: 1.315 trainign loss: 10.5355 avg training loss: 10.3593
batch: [19850/21305] batch time: 0.063 trainign loss: 10.8065 avg training loss: 10.3594
batch: [19860/21305] batch time: 0.864 trainign loss: 10.6774 avg training loss: 10.3595
batch: [19870/21305] batch time: 0.518 trainign loss: 10.7356 avg training loss: 10.3597
batch: [19880/21305] batch time: 0.823 trainign loss: 10.5395 avg training loss: 10.3598
batch: [19890/21305] batch time: 0.491 trainign loss: 10.3463 avg training loss: 10.3599
batch: [19900/21305] batch time: 1.729 trainign loss: 10.7818 avg training loss: 10.3592
batch: [19910/21305] batch time: 0.725 trainign loss: 5.6824 avg training loss: 10.3582
batch: [19920/21305] batch time: 1.718 trainign loss: 10.5283 avg training loss: 10.3584
batch: [19930/21305] batch time: 0.849 trainign loss: 10.6280 avg training loss: 10.3586
batch: [19940/21305] batch time: 1.672 trainign loss: 10.5567 avg training loss: 10.3587
batch: [19950/21305] batch time: 1.044 trainign loss: 10.7888 avg training loss: 10.3586
batch: [19960/21305] batch time: 0.774 trainign loss: 10.7492 avg training loss: 10.3587
batch: [19970/21305] batch time: 1.816 trainign loss: 10.6457 avg training loss: 10.3588
batch: [19980/21305] batch time: 0.386 trainign loss: 10.7227 avg training loss: 10.3589
batch: [19990/21305] batch time: 1.858 trainign loss: 10.7473 avg training loss: 10.3591
batch: [20000/21305] batch time: 0.788 trainign loss: 10.6824 avg training loss: 10.3592
batch: [20010/21305] batch time: 0.833 trainign loss: 10.6935 avg training loss: 10.3593
batch: [20020/21305] batch time: 1.102 trainign loss: 7.1818 avg training loss: 10.3589
batch: [20030/21305] batch time: 1.204 trainign loss: 11.3522 avg training loss: 10.3584
batch: [20040/21305] batch time: 0.689 trainign loss: 10.4559 avg training loss: 10.3584
batch: [20050/21305] batch time: 1.104 trainign loss: 8.8109 avg training loss: 10.3583
batch: [20060/21305] batch time: 1.560 trainign loss: 9.9567 avg training loss: 10.3580
batch: [20070/21305] batch time: 0.764 trainign loss: 10.6749 avg training loss: 10.3581
batch: [20080/21305] batch time: 1.741 trainign loss: 10.7188 avg training loss: 10.3583
batch: [20090/21305] batch time: 0.564 trainign loss: 10.4264 avg training loss: 10.3584
batch: [20100/21305] batch time: 1.674 trainign loss: 10.8515 avg training loss: 10.3573
batch: [20110/21305] batch time: 0.444 trainign loss: 10.4083 avg training loss: 10.3576
batch: [20120/21305] batch time: 1.889 trainign loss: 10.6476 avg training loss: 10.3578
batch: [20130/21305] batch time: 0.676 trainign loss: 10.7169 avg training loss: 10.3579
batch: [20140/21305] batch time: 1.597 trainign loss: 10.6508 avg training loss: 10.3581
batch: [20150/21305] batch time: 0.343 trainign loss: 10.4183 avg training loss: 10.3582
batch: [20160/21305] batch time: 2.037 trainign loss: 10.6836 avg training loss: 10.3583
batch: [20170/21305] batch time: 0.062 trainign loss: 10.6993 avg training loss: 10.3584
batch: [20180/21305] batch time: 2.162 trainign loss: 10.7418 avg training loss: 10.3586
batch: [20190/21305] batch time: 0.240 trainign loss: 9.1406 avg training loss: 10.3585
batch: [20200/21305] batch time: 2.259 trainign loss: 10.5341 avg training loss: 10.3586
batch: [20210/21305] batch time: 0.370 trainign loss: 10.6795 avg training loss: 10.3587
batch: [20220/21305] batch time: 2.405 trainign loss: 10.7859 avg training loss: 10.3589
batch: [20230/21305] batch time: 0.294 trainign loss: 10.6884 avg training loss: 10.3590
batch: [20240/21305] batch time: 1.789 trainign loss: 10.4497 avg training loss: 10.3591
batch: [20250/21305] batch time: 0.062 trainign loss: 10.7015 avg training loss: 10.3592
batch: [20260/21305] batch time: 1.777 trainign loss: 10.6737 avg training loss: 10.3593
batch: [20270/21305] batch time: 0.054 trainign loss: 8.1254 avg training loss: 10.3592
batch: [20280/21305] batch time: 1.969 trainign loss: 11.2718 avg training loss: 10.3587
batch: [20290/21305] batch time: 0.056 trainign loss: 10.7614 avg training loss: 10.3589
batch: [20300/21305] batch time: 2.124 trainign loss: 10.5621 avg training loss: 10.3590
batch: [20310/21305] batch time: 0.063 trainign loss: 10.6841 avg training loss: 10.3591
batch: [20320/21305] batch time: 1.885 trainign loss: 10.7132 avg training loss: 10.3593
batch: [20330/21305] batch time: 0.055 trainign loss: 10.4628 avg training loss: 10.3592
batch: [20340/21305] batch time: 1.829 trainign loss: 10.6940 avg training loss: 10.3590
batch: [20350/21305] batch time: 0.058 trainign loss: 10.7469 avg training loss: 10.3593
batch: [20360/21305] batch time: 1.620 trainign loss: 10.6705 avg training loss: 10.3594
batch: [20370/21305] batch time: 0.057 trainign loss: 10.7590 avg training loss: 10.3594
batch: [20380/21305] batch time: 0.258 trainign loss: 10.5111 avg training loss: 10.3595
batch: [20390/21305] batch time: 0.056 trainign loss: 10.7506 avg training loss: 10.3597
batch: [20400/21305] batch time: 0.062 trainign loss: 10.6722 avg training loss: 10.3598
batch: [20410/21305] batch time: 0.062 trainign loss: 10.5428 avg training loss: 10.3599
batch: [20420/21305] batch time: 1.134 trainign loss: 10.7102 avg training loss: 10.3600
batch: [20430/21305] batch time: 0.058 trainign loss: 10.5254 avg training loss: 10.3598
batch: [20440/21305] batch time: 0.807 trainign loss: 10.7030 avg training loss: 10.3598
batch: [20450/21305] batch time: 0.056 trainign loss: 10.0568 avg training loss: 10.3600
batch: [20460/21305] batch time: 1.711 trainign loss: 10.6591 avg training loss: 10.3601
batch: [20470/21305] batch time: 0.059 trainign loss: 10.6254 avg training loss: 10.3602
batch: [20480/21305] batch time: 1.345 trainign loss: 5.3502 avg training loss: 10.3594
batch: [20490/21305] batch time: 0.062 trainign loss: 15.8366 avg training loss: 10.3563
batch: [20500/21305] batch time: 1.916 trainign loss: 10.6719 avg training loss: 10.3567
batch: [20510/21305] batch time: 0.058 trainign loss: 10.5670 avg training loss: 10.3569
batch: [20520/21305] batch time: 1.450 trainign loss: 10.7527 avg training loss: 10.3570
batch: [20530/21305] batch time: 0.063 trainign loss: 10.6042 avg training loss: 10.3571
batch: [20540/21305] batch time: 1.452 trainign loss: 10.7020 avg training loss: 10.3568
batch: [20550/21305] batch time: 0.056 trainign loss: 10.7695 avg training loss: 10.3570
batch: [20560/21305] batch time: 1.653 trainign loss: 10.6382 avg training loss: 10.3570
batch: [20570/21305] batch time: 0.052 trainign loss: 10.7403 avg training loss: 10.3571
batch: [20580/21305] batch time: 2.224 trainign loss: 10.6881 avg training loss: 10.3573
batch: [20590/21305] batch time: 0.056 trainign loss: 10.6194 avg training loss: 10.3574
batch: [20600/21305] batch time: 2.140 trainign loss: 10.6118 avg training loss: 10.3575
batch: [20610/21305] batch time: 0.062 trainign loss: 10.6737 avg training loss: 10.3577
batch: [20620/21305] batch time: 2.470 trainign loss: 10.7552 avg training loss: 10.3578
batch: [20630/21305] batch time: 0.062 trainign loss: 10.6294 avg training loss: 10.3580
batch: [20640/21305] batch time: 1.849 trainign loss: 10.6902 avg training loss: 10.3581
batch: [20650/21305] batch time: 0.058 trainign loss: 10.6692 avg training loss: 10.3583
batch: [20660/21305] batch time: 2.104 trainign loss: 10.6372 avg training loss: 10.3584
batch: [20670/21305] batch time: 0.052 trainign loss: 10.7277 avg training loss: 10.3585
batch: [20680/21305] batch time: 1.866 trainign loss: 9.9983 avg training loss: 10.3586
batch: [20690/21305] batch time: 0.057 trainign loss: 10.6969 avg training loss: 10.3587
batch: [20700/21305] batch time: 1.069 trainign loss: 10.6700 avg training loss: 10.3588
batch: [20710/21305] batch time: 0.056 trainign loss: 10.7242 avg training loss: 10.3590
batch: [20720/21305] batch time: 0.215 trainign loss: 10.5970 avg training loss: 10.3591
batch: [20730/21305] batch time: 0.056 trainign loss: 10.6293 avg training loss: 10.3593
batch: [20740/21305] batch time: 0.056 trainign loss: 10.8200 avg training loss: 10.3595
batch: [20750/21305] batch time: 0.057 trainign loss: 10.2443 avg training loss: 10.3596
batch: [20760/21305] batch time: 0.328 trainign loss: 10.7003 avg training loss: 10.3597
batch: [20770/21305] batch time: 0.056 trainign loss: 10.6036 avg training loss: 10.3599
batch: [20780/21305] batch time: 0.421 trainign loss: 10.7164 avg training loss: 10.3600
batch: [20790/21305] batch time: 0.062 trainign loss: 10.4913 avg training loss: 10.3602
batch: [20800/21305] batch time: 0.770 trainign loss: 10.6510 avg training loss: 10.3603
batch: [20810/21305] batch time: 0.062 trainign loss: 10.6541 avg training loss: 10.3604
batch: [20820/21305] batch time: 0.364 trainign loss: 10.5285 avg training loss: 10.3606
batch: [20830/21305] batch time: 0.061 trainign loss: 10.6992 avg training loss: 10.3608
batch: [20840/21305] batch time: 0.050 trainign loss: 10.5608 avg training loss: 10.3609
batch: [20850/21305] batch time: 0.057 trainign loss: 10.7250 avg training loss: 10.3608
batch: [20860/21305] batch time: 0.058 trainign loss: 10.6993 avg training loss: 10.3609
batch: [20870/21305] batch time: 0.058 trainign loss: 10.7503 avg training loss: 10.3611
batch: [20880/21305] batch time: 0.062 trainign loss: 10.5565 avg training loss: 10.3611
batch: [20890/21305] batch time: 0.461 trainign loss: 10.6804 avg training loss: 10.3613
batch: [20900/21305] batch time: 0.052 trainign loss: 10.1576 avg training loss: 10.3613
batch: [20910/21305] batch time: 0.102 trainign loss: 10.5221 avg training loss: 10.3613
batch: [20920/21305] batch time: 0.062 trainign loss: 9.9797 avg training loss: 10.3615
batch: [20930/21305] batch time: 0.057 trainign loss: 10.8869 avg training loss: 10.3611
batch: [20940/21305] batch time: 0.056 trainign loss: 8.9787 avg training loss: 10.3607
batch: [20950/21305] batch time: 0.057 trainign loss: 10.5396 avg training loss: 10.3608
batch: [20960/21305] batch time: 0.063 trainign loss: 10.7442 avg training loss: 10.3610
batch: [20970/21305] batch time: 0.056 trainign loss: 10.3838 avg training loss: 10.3611
batch: [20980/21305] batch time: 0.056 trainign loss: 10.6799 avg training loss: 10.3612
batch: [20990/21305] batch time: 0.056 trainign loss: 10.7070 avg training loss: 10.3614
batch: [21000/21305] batch time: 0.061 trainign loss: 10.6769 avg training loss: 10.3615
batch: [21010/21305] batch time: 0.056 trainign loss: 10.6581 avg training loss: 10.3616
batch: [21020/21305] batch time: 0.054 trainign loss: 10.6245 avg training loss: 10.3617
batch: [21030/21305] batch time: 0.060 trainign loss: 10.6949 avg training loss: 10.3619
batch: [21040/21305] batch time: 0.057 trainign loss: 10.6195 avg training loss: 10.3620
batch: [21050/21305] batch time: 0.056 trainign loss: 10.7266 avg training loss: 10.3621
batch: [21060/21305] batch time: 0.054 trainign loss: 10.5477 avg training loss: 10.3622
batch: [21070/21305] batch time: 0.063 trainign loss: 10.7325 avg training loss: 10.3623
batch: [21080/21305] batch time: 0.052 trainign loss: 10.6707 avg training loss: 10.3623
batch: [21090/21305] batch time: 0.056 trainign loss: 10.6417 avg training loss: 10.3624
batch: [21100/21305] batch time: 0.056 trainign loss: 10.3661 avg training loss: 10.3624
batch: [21110/21305] batch time: 0.056 trainign loss: 10.6668 avg training loss: 10.3624
batch: [21120/21305] batch time: 0.051 trainign loss: 10.6994 avg training loss: 10.3625
batch: [21130/21305] batch time: 0.057 trainign loss: 10.6790 avg training loss: 10.3626
batch: [21140/21305] batch time: 0.056 trainign loss: 10.7287 avg training loss: 10.3627
batch: [21150/21305] batch time: 0.058 trainign loss: 10.6320 avg training loss: 10.3628
batch: [21160/21305] batch time: 0.057 trainign loss: 10.7217 avg training loss: 10.3629
batch: [21170/21305] batch time: 0.056 trainign loss: 8.8182 avg training loss: 10.3625
batch: [21180/21305] batch time: 0.051 trainign loss: 10.8478 avg training loss: 10.3627
batch: [21190/21305] batch time: 0.062 trainign loss: 10.6703 avg training loss: 10.3629
batch: [21200/21305] batch time: 0.063 trainign loss: 10.6641 avg training loss: 10.3629
batch: [21210/21305] batch time: 0.062 trainign loss: 10.6740 avg training loss: 10.3631
batch: [21220/21305] batch time: 0.052 trainign loss: 10.6794 avg training loss: 10.3632
batch: [21230/21305] batch time: 0.059 trainign loss: 10.7567 avg training loss: 10.3633
batch: [21240/21305] batch time: 0.056 trainign loss: 10.4804 avg training loss: 10.3635
batch: [21250/21305] batch time: 0.057 trainign loss: 10.7217 avg training loss: 10.3636
batch: [21260/21305] batch time: 0.056 trainign loss: 10.7306 avg training loss: 10.3637
batch: [21270/21305] batch time: 0.062 trainign loss: 9.3509 avg training loss: 10.3638
batch: [21280/21305] batch time: 0.057 trainign loss: 10.6069 avg training loss: 10.3638
batch: [21290/21305] batch time: 0.057 trainign loss: 10.3202 avg training loss: 10.3638
batch: [21300/21305] batch time: 0.056 trainign loss: 10.6817 avg training loss: 10.3640
Epoch: 2
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.491 trainign loss: 10.6604 avg training loss: 10.3640
batch: [10/21305] batch time: 0.061 trainign loss: 10.7274 avg training loss: 10.3634
batch: [20/21305] batch time: 1.471 trainign loss: 10.7759 avg training loss: 10.3636
batch: [30/21305] batch time: 0.056 trainign loss: 10.6082 avg training loss: 10.3637
batch: [40/21305] batch time: 0.811 trainign loss: 10.6201 avg training loss: 10.3638
batch: [50/21305] batch time: 0.057 trainign loss: 9.4511 avg training loss: 10.3637
batch: [60/21305] batch time: 1.335 trainign loss: 10.7635 avg training loss: 10.3638
batch: [70/21305] batch time: 0.060 trainign loss: 10.7282 avg training loss: 10.3638
batch: [80/21305] batch time: 1.236 trainign loss: 10.5328 avg training loss: 10.3639
batch: [90/21305] batch time: 0.057 trainign loss: 10.5467 avg training loss: 10.3640
batch: [100/21305] batch time: 1.015 trainign loss: 10.6131 avg training loss: 10.3641
batch: [110/21305] batch time: 0.056 trainign loss: 10.4816 avg training loss: 10.3641
batch: [120/21305] batch time: 1.017 trainign loss: 10.5392 avg training loss: 10.3640
batch: [130/21305] batch time: 0.061 trainign loss: 10.5629 avg training loss: 10.3640
batch: [140/21305] batch time: 1.108 trainign loss: 9.9334 avg training loss: 10.3640
batch: [150/21305] batch time: 0.055 trainign loss: 10.7323 avg training loss: 10.3640
batch: [160/21305] batch time: 0.795 trainign loss: 10.6069 avg training loss: 10.3640
batch: [170/21305] batch time: 0.062 trainign loss: 10.5881 avg training loss: 10.3641
batch: [180/21305] batch time: 0.739 trainign loss: 10.5267 avg training loss: 10.3641
batch: [190/21305] batch time: 0.058 trainign loss: 9.9975 avg training loss: 10.3641
batch: [200/21305] batch time: 0.054 trainign loss: 1.0425 avg training loss: 10.3619
batch: [210/21305] batch time: 0.062 trainign loss: 11.9689 avg training loss: 10.3611
batch: [220/21305] batch time: 0.056 trainign loss: 10.4612 avg training loss: 10.3612
batch: [230/21305] batch time: 0.063 trainign loss: 10.4249 avg training loss: 10.3612
batch: [240/21305] batch time: 0.056 trainign loss: 10.5667 avg training loss: 10.3612
batch: [250/21305] batch time: 0.056 trainign loss: 10.5852 avg training loss: 10.3612
batch: [260/21305] batch time: 0.061 trainign loss: 10.5476 avg training loss: 10.3613
batch: [270/21305] batch time: 0.056 trainign loss: 9.3211 avg training loss: 10.3612
batch: [280/21305] batch time: 0.051 trainign loss: 10.1999 avg training loss: 10.3612
batch: [290/21305] batch time: 0.056 trainign loss: 10.6427 avg training loss: 10.3608
batch: [300/21305] batch time: 0.056 trainign loss: 10.7226 avg training loss: 10.3608
batch: [310/21305] batch time: 0.056 trainign loss: 9.5821 avg training loss: 10.3607
batch: [320/21305] batch time: 0.056 trainign loss: 10.3413 avg training loss: 10.3607
batch: [330/21305] batch time: 0.056 trainign loss: 10.3195 avg training loss: 10.3605
batch: [340/21305] batch time: 0.052 trainign loss: 10.6325 avg training loss: 10.3605
batch: [350/21305] batch time: 0.063 trainign loss: 10.6211 avg training loss: 10.3605
batch: [360/21305] batch time: 0.056 trainign loss: 10.5575 avg training loss: 10.3604
batch: [370/21305] batch time: 0.056 trainign loss: 10.0300 avg training loss: 10.3603
batch: [380/21305] batch time: 0.062 trainign loss: 10.0543 avg training loss: 10.3603
batch: [390/21305] batch time: 0.058 trainign loss: 8.5943 avg training loss: 10.3600
batch: [400/21305] batch time: 0.058 trainign loss: 10.2382 avg training loss: 10.3597
batch: [410/21305] batch time: 0.057 trainign loss: 10.5060 avg training loss: 10.3596
batch: [420/21305] batch time: 0.051 trainign loss: 10.2278 avg training loss: 10.3595
batch: [430/21305] batch time: 0.056 trainign loss: 10.3299 avg training loss: 10.3595
batch: [440/21305] batch time: 0.056 trainign loss: 10.1813 avg training loss: 10.3592
batch: [450/21305] batch time: 0.057 trainign loss: 9.6497 avg training loss: 10.3591
batch: [460/21305] batch time: 0.056 trainign loss: 10.3923 avg training loss: 10.3588
batch: [470/21305] batch time: 0.063 trainign loss: 9.6122 avg training loss: 10.3588
batch: [480/21305] batch time: 0.050 trainign loss: 10.4220 avg training loss: 10.3587
batch: [490/21305] batch time: 0.056 trainign loss: 10.2878 avg training loss: 10.3586
batch: [500/21305] batch time: 0.053 trainign loss: 8.7894 avg training loss: 10.3585
batch: [510/21305] batch time: 0.056 trainign loss: 9.5521 avg training loss: 10.3584
batch: [520/21305] batch time: 0.053 trainign loss: 8.1514 avg training loss: 10.3582
batch: [530/21305] batch time: 0.062 trainign loss: 10.3390 avg training loss: 10.3580
batch: [540/21305] batch time: 0.056 trainign loss: 10.1029 avg training loss: 10.3578
batch: [550/21305] batch time: 0.058 trainign loss: 8.9908 avg training loss: 10.3572
batch: [560/21305] batch time: 0.057 trainign loss: 9.3301 avg training loss: 10.3572
batch: [570/21305] batch time: 0.093 trainign loss: 9.7357 avg training loss: 10.3568
batch: [580/21305] batch time: 0.056 trainign loss: 10.3505 avg training loss: 10.3564
batch: [590/21305] batch time: 0.773 trainign loss: 10.3290 avg training loss: 10.3562
batch: [600/21305] batch time: 0.053 trainign loss: 9.6878 avg training loss: 10.3561
batch: [610/21305] batch time: 0.060 trainign loss: 10.2940 avg training loss: 10.3560
batch: [620/21305] batch time: 0.057 trainign loss: 10.1516 avg training loss: 10.3560
batch: [630/21305] batch time: 0.057 trainign loss: 10.0266 avg training loss: 10.3558
batch: [640/21305] batch time: 0.056 trainign loss: 10.4172 avg training loss: 10.3557
batch: [650/21305] batch time: 0.056 trainign loss: 9.5603 avg training loss: 10.3556
batch: [660/21305] batch time: 0.062 trainign loss: 9.6983 avg training loss: 10.3550
batch: [670/21305] batch time: 0.057 trainign loss: 10.5795 avg training loss: 10.3544
batch: [680/21305] batch time: 0.052 trainign loss: 10.3276 avg training loss: 10.3543
batch: [690/21305] batch time: 0.057 trainign loss: 9.9875 avg training loss: 10.3543
batch: [700/21305] batch time: 0.056 trainign loss: 10.2819 avg training loss: 10.3540
batch: [710/21305] batch time: 0.057 trainign loss: 10.0216 avg training loss: 10.3538
batch: [720/21305] batch time: 0.057 trainign loss: 9.9467 avg training loss: 10.3536
batch: [730/21305] batch time: 0.057 trainign loss: 10.3434 avg training loss: 10.3534
batch: [740/21305] batch time: 0.056 trainign loss: 10.3525 avg training loss: 10.3532
batch: [750/21305] batch time: 0.056 trainign loss: 10.0701 avg training loss: 10.3530
batch: [760/21305] batch time: 0.054 trainign loss: 10.0135 avg training loss: 10.3528
batch: [770/21305] batch time: 0.056 trainign loss: 9.0905 avg training loss: 10.3525
batch: [780/21305] batch time: 0.056 trainign loss: 10.3141 avg training loss: 10.3523
batch: [790/21305] batch time: 0.057 trainign loss: 10.1175 avg training loss: 10.3522
batch: [800/21305] batch time: 0.056 trainign loss: 10.0698 avg training loss: 10.3521
batch: [810/21305] batch time: 0.056 trainign loss: 9.8940 avg training loss: 10.3519
batch: [820/21305] batch time: 0.056 trainign loss: 10.1483 avg training loss: 10.3517
batch: [830/21305] batch time: 0.060 trainign loss: 9.8089 avg training loss: 10.3515
batch: [840/21305] batch time: 0.385 trainign loss: 10.0554 avg training loss: 10.3513
batch: [850/21305] batch time: 0.056 trainign loss: 9.4266 avg training loss: 10.3509
batch: [860/21305] batch time: 0.665 trainign loss: 9.0733 avg training loss: 10.3505
batch: [870/21305] batch time: 0.058 trainign loss: 7.8036 avg training loss: 10.3501
batch: [880/21305] batch time: 0.488 trainign loss: 10.3815 avg training loss: 10.3489
batch: [890/21305] batch time: 0.057 trainign loss: 10.3739 avg training loss: 10.3489
batch: [900/21305] batch time: 0.731 trainign loss: 9.3293 avg training loss: 10.3486
batch: [910/21305] batch time: 0.484 trainign loss: 9.8459 avg training loss: 10.3483
batch: [920/21305] batch time: 0.056 trainign loss: 7.6158 avg training loss: 10.3477
batch: [930/21305] batch time: 0.833 trainign loss: 10.0265 avg training loss: 10.3473
batch: [940/21305] batch time: 0.056 trainign loss: 6.2849 avg training loss: 10.3467
batch: [950/21305] batch time: 0.381 trainign loss: 0.0078 avg training loss: 10.3429
batch: [960/21305] batch time: 0.056 trainign loss: 10.9271 avg training loss: 10.3434
batch: [970/21305] batch time: 0.493 trainign loss: 10.1237 avg training loss: 10.3432
batch: [980/21305] batch time: 0.063 trainign loss: 10.0676 avg training loss: 10.3432
batch: [990/21305] batch time: 0.380 trainign loss: 9.9878 avg training loss: 10.3430
batch: [1000/21305] batch time: 0.056 trainign loss: 7.2067 avg training loss: 10.3424
batch: [1010/21305] batch time: 1.165 trainign loss: 10.2688 avg training loss: 10.3416
batch: [1020/21305] batch time: 0.056 trainign loss: 9.9662 avg training loss: 10.3413
batch: [1030/21305] batch time: 1.401 trainign loss: 9.5986 avg training loss: 10.3407
batch: [1040/21305] batch time: 0.056 trainign loss: 9.4637 avg training loss: 10.3404
batch: [1050/21305] batch time: 0.739 trainign loss: 9.3270 avg training loss: 10.3397
batch: [1060/21305] batch time: 0.063 trainign loss: 8.9941 avg training loss: 10.3390
batch: [1070/21305] batch time: 1.543 trainign loss: 7.8553 avg training loss: 10.3383
batch: [1080/21305] batch time: 0.057 trainign loss: 0.2658 avg training loss: 10.3355
batch: [1090/21305] batch time: 2.218 trainign loss: 14.2125 avg training loss: 10.3351
batch: [1100/21305] batch time: 0.062 trainign loss: 8.9943 avg training loss: 10.3345
batch: [1110/21305] batch time: 2.109 trainign loss: 9.6954 avg training loss: 10.3342
batch: [1120/21305] batch time: 0.060 trainign loss: 9.9421 avg training loss: 10.3340
batch: [1130/21305] batch time: 2.138 trainign loss: 8.6116 avg training loss: 10.3336
batch: [1140/21305] batch time: 0.063 trainign loss: 9.6260 avg training loss: 10.3331
batch: [1150/21305] batch time: 1.861 trainign loss: 9.5859 avg training loss: 10.3326
batch: [1160/21305] batch time: 0.056 trainign loss: 9.4358 avg training loss: 10.3320
batch: [1170/21305] batch time: 1.492 trainign loss: 9.3023 avg training loss: 10.3314
batch: [1180/21305] batch time: 0.056 trainign loss: 9.1115 avg training loss: 10.3308
batch: [1190/21305] batch time: 1.828 trainign loss: 9.5661 avg training loss: 10.3303
batch: [1200/21305] batch time: 0.062 trainign loss: 6.8476 avg training loss: 10.3292
batch: [1210/21305] batch time: 0.697 trainign loss: 9.4893 avg training loss: 10.3285
batch: [1220/21305] batch time: 0.056 trainign loss: 9.5715 avg training loss: 10.3281
batch: [1230/21305] batch time: 0.659 trainign loss: 8.9019 avg training loss: 10.3274
batch: [1240/21305] batch time: 0.058 trainign loss: 9.0959 avg training loss: 10.3264
batch: [1250/21305] batch time: 1.344 trainign loss: 9.4232 avg training loss: 10.3259
batch: [1260/21305] batch time: 0.061 trainign loss: 8.0793 avg training loss: 10.3252
batch: [1270/21305] batch time: 1.371 trainign loss: 9.3329 avg training loss: 10.3246
batch: [1280/21305] batch time: 0.061 trainign loss: 9.5825 avg training loss: 10.3240
batch: [1290/21305] batch time: 1.289 trainign loss: 8.5175 avg training loss: 10.3235
batch: [1300/21305] batch time: 0.057 trainign loss: 8.8546 avg training loss: 10.3227
batch: [1310/21305] batch time: 1.157 trainign loss: 9.3751 avg training loss: 10.3222
batch: [1320/21305] batch time: 0.056 trainign loss: 7.9252 avg training loss: 10.3213
batch: [1330/21305] batch time: 1.050 trainign loss: 9.2597 avg training loss: 10.3208
batch: [1340/21305] batch time: 0.057 trainign loss: 9.3487 avg training loss: 10.3202
batch: [1350/21305] batch time: 0.553 trainign loss: 9.3330 avg training loss: 10.3197
batch: [1360/21305] batch time: 0.057 trainign loss: 5.4780 avg training loss: 10.3187
batch: [1370/21305] batch time: 0.901 trainign loss: 10.8059 avg training loss: 10.3176
batch: [1380/21305] batch time: 0.063 trainign loss: 7.3069 avg training loss: 10.3169
batch: [1390/21305] batch time: 0.337 trainign loss: 9.4967 avg training loss: 10.3160
batch: [1400/21305] batch time: 0.056 trainign loss: 8.9752 avg training loss: 10.3154
batch: [1410/21305] batch time: 0.339 trainign loss: 9.2893 avg training loss: 10.3141
batch: [1420/21305] batch time: 0.056 trainign loss: 8.5554 avg training loss: 10.3131
batch: [1430/21305] batch time: 0.353 trainign loss: 8.0227 avg training loss: 10.3124
batch: [1440/21305] batch time: 0.056 trainign loss: 7.1787 avg training loss: 10.3117
batch: [1450/21305] batch time: 0.056 trainign loss: 9.2193 avg training loss: 10.3108
batch: [1460/21305] batch time: 0.056 trainign loss: 8.9031 avg training loss: 10.3103
batch: [1470/21305] batch time: 0.292 trainign loss: 8.0817 avg training loss: 10.3092
batch: [1480/21305] batch time: 0.056 trainign loss: 9.3662 avg training loss: 10.3085
batch: [1490/21305] batch time: 0.103 trainign loss: 9.1465 avg training loss: 10.3077
batch: [1500/21305] batch time: 0.053 trainign loss: 8.9616 avg training loss: 10.3070
batch: [1510/21305] batch time: 0.062 trainign loss: 9.5615 avg training loss: 10.3064
batch: [1520/21305] batch time: 0.063 trainign loss: 3.6211 avg training loss: 10.3049
batch: [1530/21305] batch time: 0.056 trainign loss: 15.9556 avg training loss: 10.3018
batch: [1540/21305] batch time: 0.054 trainign loss: 9.9117 avg training loss: 10.3023
batch: [1550/21305] batch time: 0.057 trainign loss: 9.0102 avg training loss: 10.3022
batch: [1560/21305] batch time: 0.050 trainign loss: 9.3269 avg training loss: 10.3020
batch: [1570/21305] batch time: 0.057 trainign loss: 8.8656 avg training loss: 10.3016
batch: [1580/21305] batch time: 0.054 trainign loss: 9.2815 avg training loss: 10.3007
batch: [1590/21305] batch time: 0.056 trainign loss: 10.7897 avg training loss: 10.2987
batch: [1600/21305] batch time: 0.051 trainign loss: 9.4838 avg training loss: 10.2988
batch: [1610/21305] batch time: 0.055 trainign loss: 9.7152 avg training loss: 10.2985
batch: [1620/21305] batch time: 0.052 trainign loss: 9.2000 avg training loss: 10.2981
batch: [1630/21305] batch time: 0.059 trainign loss: 9.5418 avg training loss: 10.2974
batch: [1640/21305] batch time: 0.056 trainign loss: 8.9660 avg training loss: 10.2969
batch: [1650/21305] batch time: 0.063 trainign loss: 7.6871 avg training loss: 10.2962
batch: [1660/21305] batch time: 0.061 trainign loss: 8.0643 avg training loss: 10.2955
batch: [1670/21305] batch time: 0.062 trainign loss: 8.8716 avg training loss: 10.2947
batch: [1680/21305] batch time: 0.063 trainign loss: 7.9319 avg training loss: 10.2939
batch: [1690/21305] batch time: 0.056 trainign loss: 8.5189 avg training loss: 10.2932
batch: [1700/21305] batch time: 0.057 trainign loss: 8.5513 avg training loss: 10.2925
batch: [1710/21305] batch time: 0.056 trainign loss: 6.2868 avg training loss: 10.2915
batch: [1720/21305] batch time: 0.051 trainign loss: 9.6096 avg training loss: 10.2906
batch: [1730/21305] batch time: 0.063 trainign loss: 8.3870 avg training loss: 10.2898
batch: [1740/21305] batch time: 0.063 trainign loss: 8.5444 avg training loss: 10.2891
batch: [1750/21305] batch time: 0.061 trainign loss: 9.2749 avg training loss: 10.2886
batch: [1760/21305] batch time: 0.061 trainign loss: 9.3970 avg training loss: 10.2881
batch: [1770/21305] batch time: 0.061 trainign loss: 8.5472 avg training loss: 10.2875
batch: [1780/21305] batch time: 0.057 trainign loss: 8.6916 avg training loss: 10.2868
batch: [1790/21305] batch time: 0.056 trainign loss: 9.1266 avg training loss: 10.2860
batch: [1800/21305] batch time: 0.063 trainign loss: 7.4491 avg training loss: 10.2855
batch: [1810/21305] batch time: 0.052 trainign loss: 8.8526 avg training loss: 10.2847
batch: [1820/21305] batch time: 0.057 trainign loss: 6.2900 avg training loss: 10.2833
batch: [1830/21305] batch time: 0.062 trainign loss: 9.3537 avg training loss: 10.2829
batch: [1840/21305] batch time: 0.057 trainign loss: 9.0666 avg training loss: 10.2822
batch: [1850/21305] batch time: 0.059 trainign loss: 8.8058 avg training loss: 10.2816
batch: [1860/21305] batch time: 0.055 trainign loss: 8.7133 avg training loss: 10.2811
batch: [1870/21305] batch time: 0.056 trainign loss: 8.9162 avg training loss: 10.2805
batch: [1880/21305] batch time: 0.062 trainign loss: 6.1053 avg training loss: 10.2797
batch: [1890/21305] batch time: 0.052 trainign loss: 6.9857 avg training loss: 10.2776
batch: [1900/21305] batch time: 0.058 trainign loss: 9.1094 avg training loss: 10.2772
batch: [1910/21305] batch time: 0.062 trainign loss: 9.3492 avg training loss: 10.2767
batch: [1920/21305] batch time: 1.328 trainign loss: 8.8605 avg training loss: 10.2761
batch: [1930/21305] batch time: 0.056 trainign loss: 4.3103 avg training loss: 10.2749
batch: [1940/21305] batch time: 0.515 trainign loss: 8.7199 avg training loss: 10.2743
batch: [1950/21305] batch time: 0.059 trainign loss: 8.3280 avg training loss: 10.2736
batch: [1960/21305] batch time: 0.432 trainign loss: 6.2581 avg training loss: 10.2727
batch: [1970/21305] batch time: 0.061 trainign loss: 9.1885 avg training loss: 10.2718
batch: [1980/21305] batch time: 0.699 trainign loss: 6.4407 avg training loss: 10.2709
batch: [1990/21305] batch time: 0.059 trainign loss: 8.6492 avg training loss: 10.2703
batch: [2000/21305] batch time: 0.203 trainign loss: 7.2506 avg training loss: 10.2691
batch: [2010/21305] batch time: 0.061 trainign loss: 9.5460 avg training loss: 10.2682
batch: [2020/21305] batch time: 0.057 trainign loss: 3.7316 avg training loss: 10.2669
batch: [2030/21305] batch time: 0.060 trainign loss: 8.9945 avg training loss: 10.2655
batch: [2040/21305] batch time: 0.058 trainign loss: 9.0628 avg training loss: 10.2651
batch: [2050/21305] batch time: 0.054 trainign loss: 8.8477 avg training loss: 10.2648
batch: [2060/21305] batch time: 0.056 trainign loss: 4.0854 avg training loss: 10.2636
batch: [2070/21305] batch time: 0.055 trainign loss: 2.6685 avg training loss: 10.2619
batch: [2080/21305] batch time: 0.057 trainign loss: 0.5859 avg training loss: 10.2595
batch: [2090/21305] batch time: 0.062 trainign loss: 11.4340 avg training loss: 10.2577
batch: [2100/21305] batch time: 0.415 trainign loss: 10.2490 avg training loss: 10.2576
batch: [2110/21305] batch time: 0.062 trainign loss: 9.9559 avg training loss: 10.2576
batch: [2120/21305] batch time: 0.053 trainign loss: 2.8653 avg training loss: 10.2563
batch: [2130/21305] batch time: 0.063 trainign loss: 0.0085 avg training loss: 10.2520
batch: [2140/21305] batch time: 0.052 trainign loss: 0.0001 avg training loss: 10.2477
batch: [2150/21305] batch time: 0.060 trainign loss: 10.6130 avg training loss: 10.2489
batch: [2160/21305] batch time: 0.053 trainign loss: 10.1579 avg training loss: 10.2489
batch: [2170/21305] batch time: 0.062 trainign loss: 9.9395 avg training loss: 10.2487
batch: [2180/21305] batch time: 0.056 trainign loss: 9.5334 avg training loss: 10.2481
batch: [2190/21305] batch time: 0.063 trainign loss: 8.7275 avg training loss: 10.2476
batch: [2200/21305] batch time: 0.051 trainign loss: 9.1706 avg training loss: 10.2470
batch: [2210/21305] batch time: 0.052 trainign loss: 8.4777 avg training loss: 10.2464
batch: [2220/21305] batch time: 0.235 trainign loss: 8.3029 avg training loss: 10.2458
batch: [2230/21305] batch time: 0.063 trainign loss: 8.9456 avg training loss: 10.2449
batch: [2240/21305] batch time: 0.165 trainign loss: 9.3162 avg training loss: 10.2437
batch: [2250/21305] batch time: 0.052 trainign loss: 8.3207 avg training loss: 10.2431
batch: [2260/21305] batch time: 0.867 trainign loss: 8.3285 avg training loss: 10.2423
batch: [2270/21305] batch time: 0.058 trainign loss: 5.9672 avg training loss: 10.2410
batch: [2280/21305] batch time: 0.588 trainign loss: 9.0461 avg training loss: 10.2396
batch: [2290/21305] batch time: 0.059 trainign loss: 7.0200 avg training loss: 10.2380
batch: [2300/21305] batch time: 0.774 trainign loss: 7.1262 avg training loss: 10.2375
batch: [2310/21305] batch time: 0.057 trainign loss: 9.3468 avg training loss: 10.2365
batch: [2320/21305] batch time: 2.177 trainign loss: 9.7444 avg training loss: 10.2360
batch: [2330/21305] batch time: 0.056 trainign loss: 10.0244 avg training loss: 10.2357
batch: [2340/21305] batch time: 1.161 trainign loss: 8.3392 avg training loss: 10.2353
batch: [2350/21305] batch time: 0.056 trainign loss: 7.9583 avg training loss: 10.2348
batch: [2360/21305] batch time: 1.404 trainign loss: 9.5585 avg training loss: 10.2343
batch: [2370/21305] batch time: 0.056 trainign loss: 7.1649 avg training loss: 10.2332
batch: [2380/21305] batch time: 1.032 trainign loss: 8.2093 avg training loss: 10.2325
batch: [2390/21305] batch time: 0.056 trainign loss: 8.7406 avg training loss: 10.2320
batch: [2400/21305] batch time: 0.659 trainign loss: 8.0209 avg training loss: 10.2311
batch: [2410/21305] batch time: 0.061 trainign loss: 9.2305 avg training loss: 10.2305
batch: [2420/21305] batch time: 1.184 trainign loss: 8.7957 avg training loss: 10.2298
batch: [2430/21305] batch time: 0.058 trainign loss: 6.7829 avg training loss: 10.2290
batch: [2440/21305] batch time: 1.127 trainign loss: 8.7863 avg training loss: 10.2283
batch: [2450/21305] batch time: 0.056 trainign loss: 9.0933 avg training loss: 10.2275
batch: [2460/21305] batch time: 0.491 trainign loss: 8.0245 avg training loss: 10.2267
batch: [2470/21305] batch time: 0.063 trainign loss: 9.1143 avg training loss: 10.2256
batch: [2480/21305] batch time: 0.062 trainign loss: 9.1341 avg training loss: 10.2248
batch: [2490/21305] batch time: 0.061 trainign loss: 9.7709 avg training loss: 10.2235
batch: [2500/21305] batch time: 0.481 trainign loss: 9.1846 avg training loss: 10.2230
batch: [2510/21305] batch time: 0.056 trainign loss: 6.2615 avg training loss: 10.2222
batch: [2520/21305] batch time: 0.057 trainign loss: 7.0118 avg training loss: 10.2215
batch: [2530/21305] batch time: 0.054 trainign loss: 7.3362 avg training loss: 10.2205
batch: [2540/21305] batch time: 0.059 trainign loss: 8.6090 avg training loss: 10.2197
batch: [2550/21305] batch time: 0.050 trainign loss: 7.9200 avg training loss: 10.2188
batch: [2560/21305] batch time: 0.660 trainign loss: 7.8226 avg training loss: 10.2182
batch: [2570/21305] batch time: 0.060 trainign loss: 9.4023 avg training loss: 10.2177
batch: [2580/21305] batch time: 0.782 trainign loss: 7.3606 avg training loss: 10.2168
batch: [2590/21305] batch time: 0.061 trainign loss: 8.2307 avg training loss: 10.2161
batch: [2600/21305] batch time: 0.325 trainign loss: 0.0510 avg training loss: 10.2132
batch: [2610/21305] batch time: 0.056 trainign loss: 10.1195 avg training loss: 10.2127
batch: [2620/21305] batch time: 0.161 trainign loss: 10.3052 avg training loss: 10.2128
batch: [2630/21305] batch time: 0.056 trainign loss: 9.6461 avg training loss: 10.2126
batch: [2640/21305] batch time: 0.274 trainign loss: 4.9840 avg training loss: 10.2118
batch: [2650/21305] batch time: 0.058 trainign loss: 10.3370 avg training loss: 10.2109
batch: [2660/21305] batch time: 0.054 trainign loss: 9.3101 avg training loss: 10.2103
batch: [2670/21305] batch time: 0.059 trainign loss: 8.6800 avg training loss: 10.2096
batch: [2680/21305] batch time: 0.061 trainign loss: 6.9919 avg training loss: 10.2085
batch: [2690/21305] batch time: 0.052 trainign loss: 9.2865 avg training loss: 10.2079
batch: [2700/21305] batch time: 0.056 trainign loss: 9.1853 avg training loss: 10.2070
batch: [2710/21305] batch time: 0.056 trainign loss: 6.8767 avg training loss: 10.2056
batch: [2720/21305] batch time: 0.374 trainign loss: 3.6418 avg training loss: 10.2040
batch: [2730/21305] batch time: 0.062 trainign loss: 8.0531 avg training loss: 10.2033
batch: [2740/21305] batch time: 0.062 trainign loss: 9.4820 avg training loss: 10.2025
batch: [2750/21305] batch time: 0.054 trainign loss: 8.8783 avg training loss: 10.2019
batch: [2760/21305] batch time: 0.503 trainign loss: 8.6784 avg training loss: 10.2011
batch: [2770/21305] batch time: 0.062 trainign loss: 8.6929 avg training loss: 10.2003
batch: [2780/21305] batch time: 1.234 trainign loss: 8.8630 avg training loss: 10.1995
batch: [2790/21305] batch time: 0.062 trainign loss: 8.5412 avg training loss: 10.1986
batch: [2800/21305] batch time: 0.769 trainign loss: 9.0145 avg training loss: 10.1979
batch: [2810/21305] batch time: 0.060 trainign loss: 8.4565 avg training loss: 10.1969
batch: [2820/21305] batch time: 1.066 trainign loss: 9.7579 avg training loss: 10.1954
batch: [2830/21305] batch time: 0.062 trainign loss: 9.2101 avg training loss: 10.1949
batch: [2840/21305] batch time: 0.229 trainign loss: 1.6008 avg training loss: 10.1933
batch: [2850/21305] batch time: 0.056 trainign loss: 7.9362 avg training loss: 10.1925
batch: [2860/21305] batch time: 0.390 trainign loss: 8.9816 avg training loss: 10.1920
batch: [2870/21305] batch time: 0.063 trainign loss: 9.3830 avg training loss: 10.1908
batch: [2880/21305] batch time: 0.825 trainign loss: 7.7694 avg training loss: 10.1901
batch: [2890/21305] batch time: 0.056 trainign loss: 9.0100 avg training loss: 10.1892
batch: [2900/21305] batch time: 0.062 trainign loss: 9.0935 avg training loss: 10.1885
batch: [2910/21305] batch time: 0.062 trainign loss: 9.2298 avg training loss: 10.1880
batch: [2920/21305] batch time: 0.062 trainign loss: 8.2173 avg training loss: 10.1873
batch: [2930/21305] batch time: 0.062 trainign loss: 7.8558 avg training loss: 10.1866
batch: [2940/21305] batch time: 0.051 trainign loss: 7.4623 avg training loss: 10.1857
batch: [2950/21305] batch time: 0.057 trainign loss: 1.1626 avg training loss: 10.1835
batch: [2960/21305] batch time: 0.056 trainign loss: 6.9045 avg training loss: 10.1832
batch: [2970/21305] batch time: 0.056 trainign loss: 8.7711 avg training loss: 10.1825
batch: [2980/21305] batch time: 0.678 trainign loss: 9.1005 avg training loss: 10.1821
batch: [2990/21305] batch time: 0.058 trainign loss: 8.8119 avg training loss: 10.1816
batch: [3000/21305] batch time: 1.471 trainign loss: 5.5273 avg training loss: 10.1809
batch: [3010/21305] batch time: 0.062 trainign loss: 10.6996 avg training loss: 10.1797
batch: [3020/21305] batch time: 0.971 trainign loss: 6.5992 avg training loss: 10.1789
batch: [3030/21305] batch time: 0.056 trainign loss: 8.1613 avg training loss: 10.1783
batch: [3040/21305] batch time: 1.632 trainign loss: 8.4233 avg training loss: 10.1777
batch: [3050/21305] batch time: 0.056 trainign loss: 8.6736 avg training loss: 10.1770
batch: [3060/21305] batch time: 1.505 trainign loss: 8.0283 avg training loss: 10.1761
batch: [3070/21305] batch time: 0.062 trainign loss: 8.2424 avg training loss: 10.1750
batch: [3080/21305] batch time: 1.029 trainign loss: 8.8829 avg training loss: 10.1742
batch: [3090/21305] batch time: 0.056 trainign loss: 8.3666 avg training loss: 10.1732
batch: [3100/21305] batch time: 1.274 trainign loss: 9.1247 avg training loss: 10.1725
batch: [3110/21305] batch time: 1.431 trainign loss: 9.6268 avg training loss: 10.1719
batch: [3120/21305] batch time: 0.831 trainign loss: 8.5239 avg training loss: 10.1710
batch: [3130/21305] batch time: 1.867 trainign loss: 0.9745 avg training loss: 10.1690
batch: [3140/21305] batch time: 0.458 trainign loss: 9.1750 avg training loss: 10.1686
batch: [3150/21305] batch time: 1.768 trainign loss: 7.8914 avg training loss: 10.1680
batch: [3160/21305] batch time: 0.230 trainign loss: 9.5710 avg training loss: 10.1671
batch: [3170/21305] batch time: 2.348 trainign loss: 8.0841 avg training loss: 10.1665
batch: [3180/21305] batch time: 0.058 trainign loss: 7.0487 avg training loss: 10.1659
batch: [3190/21305] batch time: 1.917 trainign loss: 8.4346 avg training loss: 10.1650
batch: [3200/21305] batch time: 0.798 trainign loss: 9.2036 avg training loss: 10.1644
batch: [3210/21305] batch time: 1.308 trainign loss: 9.0171 avg training loss: 10.1638
batch: [3220/21305] batch time: 0.404 trainign loss: 9.0315 avg training loss: 10.1628
batch: [3230/21305] batch time: 2.259 trainign loss: 8.7128 avg training loss: 10.1621
batch: [3240/21305] batch time: 0.559 trainign loss: 7.2165 avg training loss: 10.1615
batch: [3250/21305] batch time: 1.227 trainign loss: 8.5054 avg training loss: 10.1600
batch: [3260/21305] batch time: 0.554 trainign loss: 9.3794 avg training loss: 10.1597
batch: [3270/21305] batch time: 2.083 trainign loss: 7.1371 avg training loss: 10.1588
batch: [3280/21305] batch time: 0.737 trainign loss: 4.0812 avg training loss: 10.1575
batch: [3290/21305] batch time: 0.691 trainign loss: 8.5945 avg training loss: 10.1561
batch: [3300/21305] batch time: 1.675 trainign loss: 8.1231 avg training loss: 10.1551
batch: [3310/21305] batch time: 0.817 trainign loss: 2.0908 avg training loss: 10.1534
batch: [3320/21305] batch time: 1.642 trainign loss: 9.4226 avg training loss: 10.1531
batch: [3330/21305] batch time: 0.721 trainign loss: 8.2518 avg training loss: 10.1525
batch: [3340/21305] batch time: 1.843 trainign loss: 5.3063 avg training loss: 10.1515
batch: [3350/21305] batch time: 0.777 trainign loss: 7.2250 avg training loss: 10.1507
batch: [3360/21305] batch time: 1.642 trainign loss: 9.7538 avg training loss: 10.1493
batch: [3370/21305] batch time: 1.081 trainign loss: 4.9255 avg training loss: 10.1475
batch: [3380/21305] batch time: 2.146 trainign loss: 9.4944 avg training loss: 10.1470
batch: [3390/21305] batch time: 0.063 trainign loss: 8.9351 avg training loss: 10.1461
batch: [3400/21305] batch time: 2.250 trainign loss: 8.2726 avg training loss: 10.1456
batch: [3410/21305] batch time: 0.056 trainign loss: 7.7254 avg training loss: 10.1450
batch: [3420/21305] batch time: 2.152 trainign loss: 9.1600 avg training loss: 10.1437
batch: [3430/21305] batch time: 0.092 trainign loss: 8.0965 avg training loss: 10.1433
batch: [3440/21305] batch time: 1.604 trainign loss: 9.2534 avg training loss: 10.1425
batch: [3450/21305] batch time: 0.778 trainign loss: 7.6686 avg training loss: 10.1415
batch: [3460/21305] batch time: 1.488 trainign loss: 8.0210 avg training loss: 10.1405
batch: [3470/21305] batch time: 0.373 trainign loss: 9.1061 avg training loss: 10.1400
batch: [3480/21305] batch time: 1.971 trainign loss: 9.6947 avg training loss: 10.1396
batch: [3490/21305] batch time: 0.065 trainign loss: 9.1062 avg training loss: 10.1389
batch: [3500/21305] batch time: 2.172 trainign loss: 8.5662 avg training loss: 10.1385
batch: [3510/21305] batch time: 0.058 trainign loss: 8.7281 avg training loss: 10.1379
batch: [3520/21305] batch time: 2.208 trainign loss: 8.0212 avg training loss: 10.1372
batch: [3530/21305] batch time: 0.053 trainign loss: 9.2166 avg training loss: 10.1366
batch: [3540/21305] batch time: 2.021 trainign loss: 7.4623 avg training loss: 10.1356
batch: [3550/21305] batch time: 0.053 trainign loss: 9.3275 avg training loss: 10.1342
batch: [3560/21305] batch time: 2.757 trainign loss: 8.5389 avg training loss: 10.1330
batch: [3570/21305] batch time: 0.062 trainign loss: 9.0359 avg training loss: 10.1325
batch: [3580/21305] batch time: 2.397 trainign loss: 8.3353 avg training loss: 10.1320
batch: [3590/21305] batch time: 0.056 trainign loss: 5.2488 avg training loss: 10.1312
batch: [3600/21305] batch time: 2.293 trainign loss: 10.3780 avg training loss: 10.1301
batch: [3610/21305] batch time: 0.062 trainign loss: 9.6813 avg training loss: 10.1295
batch: [3620/21305] batch time: 2.561 trainign loss: 9.0463 avg training loss: 10.1291
batch: [3630/21305] batch time: 0.056 trainign loss: 9.1901 avg training loss: 10.1285
batch: [3640/21305] batch time: 2.704 trainign loss: 9.1670 avg training loss: 10.1278
batch: [3650/21305] batch time: 0.056 trainign loss: 7.9885 avg training loss: 10.1269
batch: [3660/21305] batch time: 1.972 trainign loss: 7.6740 avg training loss: 10.1262
batch: [3670/21305] batch time: 0.062 trainign loss: 6.6274 avg training loss: 10.1252
batch: [3680/21305] batch time: 2.083 trainign loss: 9.1547 avg training loss: 10.1246
batch: [3690/21305] batch time: 0.056 trainign loss: 8.1968 avg training loss: 10.1238
batch: [3700/21305] batch time: 2.152 trainign loss: 6.6996 avg training loss: 10.1231
batch: [3710/21305] batch time: 0.063 trainign loss: 6.4859 avg training loss: 10.1223
batch: [3720/21305] batch time: 2.279 trainign loss: 8.4434 avg training loss: 10.1214
batch: [3730/21305] batch time: 0.177 trainign loss: 7.3233 avg training loss: 10.1207
batch: [3740/21305] batch time: 1.979 trainign loss: 9.2947 avg training loss: 10.1202
batch: [3750/21305] batch time: 0.062 trainign loss: 9.1583 avg training loss: 10.1196
batch: [3760/21305] batch time: 2.150 trainign loss: 8.8513 avg training loss: 10.1183
batch: [3770/21305] batch time: 1.421 trainign loss: 5.9774 avg training loss: 10.1173
batch: [3780/21305] batch time: 0.758 trainign loss: 8.3027 avg training loss: 10.1167
batch: [3790/21305] batch time: 1.219 trainign loss: 5.9618 avg training loss: 10.1156
batch: [3800/21305] batch time: 1.053 trainign loss: 8.1031 avg training loss: 10.1150
batch: [3810/21305] batch time: 0.743 trainign loss: 8.9784 avg training loss: 10.1145
batch: [3820/21305] batch time: 1.787 trainign loss: 5.9843 avg training loss: 10.1135
batch: [3830/21305] batch time: 0.056 trainign loss: 7.8265 avg training loss: 10.1128
batch: [3840/21305] batch time: 1.971 trainign loss: 7.6029 avg training loss: 10.1120
batch: [3850/21305] batch time: 0.680 trainign loss: 8.1475 avg training loss: 10.1110
batch: [3860/21305] batch time: 2.069 trainign loss: 8.2401 avg training loss: 10.1104
batch: [3870/21305] batch time: 0.557 trainign loss: 7.5693 avg training loss: 10.1097
batch: [3880/21305] batch time: 2.148 trainign loss: 0.2473 avg training loss: 10.1074
batch: [3890/21305] batch time: 0.056 trainign loss: 12.7592 avg training loss: 10.1062
batch: [3900/21305] batch time: 1.864 trainign loss: 10.0716 avg training loss: 10.1061
batch: [3910/21305] batch time: 0.056 trainign loss: 9.4560 avg training loss: 10.1059
batch: [3920/21305] batch time: 1.932 trainign loss: 7.0150 avg training loss: 10.1054
batch: [3930/21305] batch time: 0.084 trainign loss: 9.0581 avg training loss: 10.1048
batch: [3940/21305] batch time: 1.457 trainign loss: 8.2883 avg training loss: 10.1042
batch: [3950/21305] batch time: 0.129 trainign loss: 8.9278 avg training loss: 10.1038
batch: [3960/21305] batch time: 2.300 trainign loss: 9.7075 avg training loss: 10.1034
batch: [3970/21305] batch time: 0.056 trainign loss: 9.2325 avg training loss: 10.1030
batch: [3980/21305] batch time: 2.180 trainign loss: 7.7463 avg training loss: 10.1024
batch: [3990/21305] batch time: 0.056 trainign loss: 6.7532 avg training loss: 10.1010
batch: [4000/21305] batch time: 1.823 trainign loss: 9.1433 avg training loss: 10.1001
batch: [4010/21305] batch time: 0.056 trainign loss: 8.5295 avg training loss: 10.0994
batch: [4020/21305] batch time: 1.290 trainign loss: 5.9051 avg training loss: 10.0983
batch: [4030/21305] batch time: 0.063 trainign loss: 8.8789 avg training loss: 10.0978
batch: [4040/21305] batch time: 2.616 trainign loss: 9.0621 avg training loss: 10.0972
batch: [4050/21305] batch time: 0.056 trainign loss: 5.1293 avg training loss: 10.0963
batch: [4060/21305] batch time: 2.405 trainign loss: 13.9017 avg training loss: 10.0934
batch: [4070/21305] batch time: 0.056 trainign loss: 9.9010 avg training loss: 10.0925
batch: [4080/21305] batch time: 2.229 trainign loss: 9.8348 avg training loss: 10.0923
batch: [4090/21305] batch time: 0.063 trainign loss: 9.4205 avg training loss: 10.0916
batch: [4100/21305] batch time: 2.480 trainign loss: 7.2313 avg training loss: 10.0912
batch: [4110/21305] batch time: 0.056 trainign loss: 6.5499 avg training loss: 10.0904
batch: [4120/21305] batch time: 2.295 trainign loss: 7.2892 avg training loss: 10.0896
batch: [4130/21305] batch time: 0.060 trainign loss: 8.5024 avg training loss: 10.0887
batch: [4140/21305] batch time: 2.046 trainign loss: 8.2229 avg training loss: 10.0871
batch: [4150/21305] batch time: 0.057 trainign loss: 9.3590 avg training loss: 10.0866
batch: [4160/21305] batch time: 2.263 trainign loss: 9.4512 avg training loss: 10.0859
batch: [4170/21305] batch time: 0.060 trainign loss: 7.9265 avg training loss: 10.0852
batch: [4180/21305] batch time: 2.308 trainign loss: 8.9549 avg training loss: 10.0846
batch: [4190/21305] batch time: 0.056 trainign loss: 9.1030 avg training loss: 10.0836
batch: [4200/21305] batch time: 2.314 trainign loss: 7.9942 avg training loss: 10.0829
batch: [4210/21305] batch time: 0.057 trainign loss: 1.5834 avg training loss: 10.0810
batch: [4220/21305] batch time: 2.048 trainign loss: 8.6298 avg training loss: 10.0805
batch: [4230/21305] batch time: 0.061 trainign loss: 9.3996 avg training loss: 10.0793
batch: [4240/21305] batch time: 2.364 trainign loss: 7.2451 avg training loss: 10.0787
batch: [4250/21305] batch time: 0.056 trainign loss: 0.6224 avg training loss: 10.0758
batch: [4260/21305] batch time: 2.560 trainign loss: 9.6941 avg training loss: 10.0760
batch: [4270/21305] batch time: 0.058 trainign loss: 10.1299 avg training loss: 10.0759
batch: [4280/21305] batch time: 2.354 trainign loss: 9.4955 avg training loss: 10.0758
batch: [4290/21305] batch time: 0.060 trainign loss: 9.0717 avg training loss: 10.0754
batch: [4300/21305] batch time: 2.146 trainign loss: 7.9866 avg training loss: 10.0750
batch: [4310/21305] batch time: 0.052 trainign loss: 9.0800 avg training loss: 10.0742
batch: [4320/21305] batch time: 2.149 trainign loss: 7.1852 avg training loss: 10.0735
batch: [4330/21305] batch time: 0.062 trainign loss: 5.3301 avg training loss: 10.0725
batch: [4340/21305] batch time: 2.136 trainign loss: 6.7576 avg training loss: 10.0716
batch: [4350/21305] batch time: 0.059 trainign loss: 7.7762 avg training loss: 10.0709
batch: [4360/21305] batch time: 2.147 trainign loss: 6.0318 avg training loss: 10.0699
batch: [4370/21305] batch time: 0.058 trainign loss: 9.5497 avg training loss: 10.0687
batch: [4380/21305] batch time: 2.145 trainign loss: 7.7654 avg training loss: 10.0681
batch: [4390/21305] batch time: 0.058 trainign loss: 9.1399 avg training loss: 10.0671
batch: [4400/21305] batch time: 2.153 trainign loss: 9.0250 avg training loss: 10.0667
batch: [4410/21305] batch time: 0.056 trainign loss: 9.2745 avg training loss: 10.0663
batch: [4420/21305] batch time: 1.405 trainign loss: 9.0176 avg training loss: 10.0658
batch: [4430/21305] batch time: 0.062 trainign loss: 1.7873 avg training loss: 10.0644
batch: [4440/21305] batch time: 0.056 trainign loss: 12.8876 avg training loss: 10.0626
batch: [4450/21305] batch time: 0.056 trainign loss: 8.8467 avg training loss: 10.0626
batch: [4460/21305] batch time: 0.050 trainign loss: 6.1275 avg training loss: 10.0618
batch: [4470/21305] batch time: 0.056 trainign loss: 11.6632 avg training loss: 10.0603
batch: [4480/21305] batch time: 0.056 trainign loss: 8.7359 avg training loss: 10.0601
batch: [4490/21305] batch time: 0.371 trainign loss: 6.0454 avg training loss: 10.0594
batch: [4500/21305] batch time: 0.051 trainign loss: 9.9700 avg training loss: 10.0588
batch: [4510/21305] batch time: 0.193 trainign loss: 9.7559 avg training loss: 10.0576
batch: [4520/21305] batch time: 0.056 trainign loss: 9.6999 avg training loss: 10.0572
batch: [4530/21305] batch time: 1.669 trainign loss: 9.9221 avg training loss: 10.0570
batch: [4540/21305] batch time: 0.056 trainign loss: 9.2730 avg training loss: 10.0568
batch: [4550/21305] batch time: 2.214 trainign loss: 8.8994 avg training loss: 10.0561
batch: [4560/21305] batch time: 0.058 trainign loss: 9.6244 avg training loss: 10.0556
batch: [4570/21305] batch time: 1.551 trainign loss: 8.9977 avg training loss: 10.0551
batch: [4580/21305] batch time: 0.053 trainign loss: 8.8265 avg training loss: 10.0546
batch: [4590/21305] batch time: 1.617 trainign loss: 6.8296 avg training loss: 10.0535
batch: [4600/21305] batch time: 0.057 trainign loss: 8.1482 avg training loss: 10.0530
batch: [4610/21305] batch time: 2.259 trainign loss: 7.9057 avg training loss: 10.0522
batch: [4620/21305] batch time: 0.057 trainign loss: 8.4579 avg training loss: 10.0510
batch: [4630/21305] batch time: 2.284 trainign loss: 7.1199 avg training loss: 10.0503
batch: [4640/21305] batch time: 0.059 trainign loss: 6.8660 avg training loss: 10.0497
batch: [4650/21305] batch time: 1.685 trainign loss: 8.2728 avg training loss: 10.0493
batch: [4660/21305] batch time: 0.221 trainign loss: 9.4513 avg training loss: 10.0488
batch: [4670/21305] batch time: 1.035 trainign loss: 9.3256 avg training loss: 10.0475
batch: [4680/21305] batch time: 0.190 trainign loss: 9.5350 avg training loss: 10.0472
batch: [4690/21305] batch time: 0.448 trainign loss: 6.0890 avg training loss: 10.0465
batch: [4700/21305] batch time: 0.126 trainign loss: 9.2163 avg training loss: 10.0456
batch: [4710/21305] batch time: 0.276 trainign loss: 9.3877 avg training loss: 10.0452
batch: [4720/21305] batch time: 0.059 trainign loss: 8.4086 avg training loss: 10.0447
batch: [4730/21305] batch time: 0.062 trainign loss: 8.7843 avg training loss: 10.0442
batch: [4740/21305] batch time: 0.057 trainign loss: 9.7808 avg training loss: 10.0434
batch: [4750/21305] batch time: 0.830 trainign loss: 8.6630 avg training loss: 10.0429
batch: [4760/21305] batch time: 0.056 trainign loss: 5.1062 avg training loss: 10.0418
batch: [4770/21305] batch time: 1.495 trainign loss: 8.9720 avg training loss: 10.0411
batch: [4780/21305] batch time: 0.056 trainign loss: 3.8676 avg training loss: 10.0400
batch: [4790/21305] batch time: 1.490 trainign loss: 8.9765 avg training loss: 10.0391
batch: [4800/21305] batch time: 0.063 trainign loss: 9.6200 avg training loss: 10.0386
batch: [4810/21305] batch time: 1.656 trainign loss: 7.5742 avg training loss: 10.0383
batch: [4820/21305] batch time: 0.056 trainign loss: 9.0450 avg training loss: 10.0379
batch: [4830/21305] batch time: 0.555 trainign loss: 8.3777 avg training loss: 10.0372
batch: [4840/21305] batch time: 0.057 trainign loss: 1.8888 avg training loss: 10.0358
batch: [4850/21305] batch time: 0.532 trainign loss: 14.4385 avg training loss: 10.0340
batch: [4860/21305] batch time: 0.056 trainign loss: 9.6005 avg training loss: 10.0344
batch: [4870/21305] batch time: 0.411 trainign loss: 10.1999 avg training loss: 10.0344
batch: [4880/21305] batch time: 0.051 trainign loss: 9.8553 avg training loss: 10.0343
batch: [4890/21305] batch time: 0.056 trainign loss: 9.5379 avg training loss: 10.0340
batch: [4900/21305] batch time: 0.051 trainign loss: 9.0446 avg training loss: 10.0334
batch: [4910/21305] batch time: 0.057 trainign loss: 7.5143 avg training loss: 10.0325
batch: [4920/21305] batch time: 0.057 trainign loss: 9.2696 avg training loss: 10.0322
batch: [4930/21305] batch time: 0.058 trainign loss: 7.6845 avg training loss: 10.0309
batch: [4940/21305] batch time: 0.053 trainign loss: 9.4812 avg training loss: 10.0302
batch: [4950/21305] batch time: 0.056 trainign loss: 7.3409 avg training loss: 10.0296
batch: [4960/21305] batch time: 0.055 trainign loss: 9.1641 avg training loss: 10.0287
batch: [4970/21305] batch time: 0.056 trainign loss: 8.8793 avg training loss: 10.0283
batch: [4980/21305] batch time: 0.050 trainign loss: 4.8504 avg training loss: 10.0277
batch: [4990/21305] batch time: 0.058 trainign loss: 0.0120 avg training loss: 10.0244
batch: [5000/21305] batch time: 0.051 trainign loss: 10.3798 avg training loss: 10.0241
batch: [5010/21305] batch time: 0.056 trainign loss: 9.9722 avg training loss: 10.0240
batch: [5020/21305] batch time: 0.056 trainign loss: 9.2406 avg training loss: 10.0236
batch: [5030/21305] batch time: 0.063 trainign loss: 8.6889 avg training loss: 10.0228
batch: [5040/21305] batch time: 0.060 trainign loss: 4.2064 avg training loss: 10.0220
batch: [5050/21305] batch time: 1.084 trainign loss: 8.9905 avg training loss: 10.0209
batch: [5060/21305] batch time: 0.056 trainign loss: 9.8581 avg training loss: 10.0206
batch: [5070/21305] batch time: 0.871 trainign loss: 6.8953 avg training loss: 10.0203
batch: [5080/21305] batch time: 0.061 trainign loss: 1.0410 avg training loss: 10.0174
batch: [5090/21305] batch time: 1.808 trainign loss: 9.6118 avg training loss: 10.0184
batch: [5100/21305] batch time: 0.057 trainign loss: 9.6071 avg training loss: 10.0182
batch: [5110/21305] batch time: 2.180 trainign loss: 8.7256 avg training loss: 10.0179
batch: [5120/21305] batch time: 0.054 trainign loss: 8.8147 avg training loss: 10.0175
batch: [5130/21305] batch time: 2.299 trainign loss: 9.2210 avg training loss: 10.0169
batch: [5140/21305] batch time: 0.058 trainign loss: 9.0840 avg training loss: 10.0157
batch: [5150/21305] batch time: 2.609 trainign loss: 9.1546 avg training loss: 10.0155
batch: [5160/21305] batch time: 0.056 trainign loss: 9.0831 avg training loss: 10.0150
batch: [5170/21305] batch time: 2.139 trainign loss: 9.2129 avg training loss: 10.0144
batch: [5180/21305] batch time: 0.057 trainign loss: 8.8701 avg training loss: 10.0140
batch: [5190/21305] batch time: 2.213 trainign loss: 9.0363 avg training loss: 10.0135
batch: [5200/21305] batch time: 0.285 trainign loss: 7.8587 avg training loss: 10.0129
batch: [5210/21305] batch time: 1.790 trainign loss: 7.5741 avg training loss: 10.0123
batch: [5220/21305] batch time: 1.167 trainign loss: 8.3672 avg training loss: 10.0116
batch: [5230/21305] batch time: 1.258 trainign loss: 8.1206 avg training loss: 10.0111
batch: [5240/21305] batch time: 0.695 trainign loss: 5.6706 avg training loss: 10.0103
batch: [5250/21305] batch time: 1.182 trainign loss: 7.5342 avg training loss: 10.0092
batch: [5260/21305] batch time: 1.356 trainign loss: 8.3163 avg training loss: 10.0087
batch: [5270/21305] batch time: 1.313 trainign loss: 9.3075 avg training loss: 10.0083
batch: [5280/21305] batch time: 0.732 trainign loss: 9.4988 avg training loss: 10.0080
batch: [5290/21305] batch time: 1.291 trainign loss: 8.2438 avg training loss: 10.0073
batch: [5300/21305] batch time: 1.436 trainign loss: 8.9687 avg training loss: 10.0068
batch: [5310/21305] batch time: 1.538 trainign loss: 8.8617 avg training loss: 10.0060
batch: [5320/21305] batch time: 0.399 trainign loss: 8.8823 avg training loss: 10.0056
batch: [5330/21305] batch time: 1.921 trainign loss: 8.8015 avg training loss: 10.0051
batch: [5340/21305] batch time: 0.550 trainign loss: 8.5801 avg training loss: 10.0040
batch: [5350/21305] batch time: 2.193 trainign loss: 7.6311 avg training loss: 10.0028
batch: [5360/21305] batch time: 0.097 trainign loss: 9.6273 avg training loss: 10.0022
batch: [5370/21305] batch time: 2.609 trainign loss: 9.4746 avg training loss: 10.0016
batch: [5380/21305] batch time: 0.093 trainign loss: 9.3902 avg training loss: 10.0012
batch: [5390/21305] batch time: 1.904 trainign loss: 6.9330 avg training loss: 10.0006
batch: [5400/21305] batch time: 0.211 trainign loss: 8.9870 avg training loss: 10.0002
batch: [5410/21305] batch time: 2.404 trainign loss: 8.7378 avg training loss: 9.9996
batch: [5420/21305] batch time: 0.431 trainign loss: 9.5616 avg training loss: 9.9987
batch: [5430/21305] batch time: 1.668 trainign loss: 7.8923 avg training loss: 9.9982
batch: [5440/21305] batch time: 0.056 trainign loss: 8.6861 avg training loss: 9.9972
batch: [5450/21305] batch time: 2.274 trainign loss: 7.7850 avg training loss: 9.9962
batch: [5460/21305] batch time: 0.059 trainign loss: 9.2621 avg training loss: 9.9958
batch: [5470/21305] batch time: 2.523 trainign loss: 7.6621 avg training loss: 9.9949
batch: [5480/21305] batch time: 0.056 trainign loss: 9.7229 avg training loss: 9.9946
batch: [5490/21305] batch time: 2.082 trainign loss: 7.4372 avg training loss: 9.9942
batch: [5500/21305] batch time: 0.135 trainign loss: 7.6512 avg training loss: 9.9936
batch: [5510/21305] batch time: 1.265 trainign loss: 2.4351 avg training loss: 9.9921
batch: [5520/21305] batch time: 0.056 trainign loss: 9.7700 avg training loss: 9.9914
batch: [5530/21305] batch time: 0.470 trainign loss: 9.1172 avg training loss: 9.9907
batch: [5540/21305] batch time: 0.063 trainign loss: 9.0723 avg training loss: 9.9902
batch: [5550/21305] batch time: 0.846 trainign loss: 9.7981 avg training loss: 9.9899
batch: [5560/21305] batch time: 0.056 trainign loss: 6.9641 avg training loss: 9.9889
batch: [5570/21305] batch time: 1.659 trainign loss: 2.8007 avg training loss: 9.9879
batch: [5580/21305] batch time: 0.062 trainign loss: 11.9858 avg training loss: 9.9867
batch: [5590/21305] batch time: 2.183 trainign loss: 8.9529 avg training loss: 9.9866
batch: [5600/21305] batch time: 0.053 trainign loss: 9.1562 avg training loss: 9.9861
batch: [5610/21305] batch time: 2.665 trainign loss: 9.2760 avg training loss: 9.9856
batch: [5620/21305] batch time: 0.063 trainign loss: 8.9716 avg training loss: 9.9852
batch: [5630/21305] batch time: 2.420 trainign loss: 8.7749 avg training loss: 9.9843
batch: [5640/21305] batch time: 0.061 trainign loss: 7.7598 avg training loss: 9.9837
batch: [5650/21305] batch time: 2.013 trainign loss: 9.7300 avg training loss: 9.9831
batch: [5660/21305] batch time: 0.056 trainign loss: 5.8463 avg training loss: 9.9822
batch: [5670/21305] batch time: 2.152 trainign loss: 9.1954 avg training loss: 9.9818
batch: [5680/21305] batch time: 0.052 trainign loss: 8.1900 avg training loss: 9.9811
batch: [5690/21305] batch time: 2.504 trainign loss: 7.8198 avg training loss: 9.9807
batch: [5700/21305] batch time: 0.052 trainign loss: 8.4532 avg training loss: 9.9798
batch: [5710/21305] batch time: 2.259 trainign loss: 5.7462 avg training loss: 9.9791
batch: [5720/21305] batch time: 0.056 trainign loss: 9.6843 avg training loss: 9.9777
batch: [5730/21305] batch time: 2.403 trainign loss: 8.1067 avg training loss: 9.9774
batch: [5740/21305] batch time: 0.062 trainign loss: 8.8928 avg training loss: 9.9770
batch: [5750/21305] batch time: 2.595 trainign loss: 8.5964 avg training loss: 9.9766
batch: [5760/21305] batch time: 0.062 trainign loss: 7.7904 avg training loss: 9.9761
batch: [5770/21305] batch time: 2.373 trainign loss: 8.7480 avg training loss: 9.9757
batch: [5780/21305] batch time: 0.057 trainign loss: 8.5070 avg training loss: 9.9751
batch: [5790/21305] batch time: 2.024 trainign loss: 9.5727 avg training loss: 9.9746
batch: [5800/21305] batch time: 0.057 trainign loss: 6.7916 avg training loss: 9.9739
batch: [5810/21305] batch time: 2.236 trainign loss: 9.5110 avg training loss: 9.9735
batch: [5820/21305] batch time: 0.057 trainign loss: 9.2135 avg training loss: 9.9729
batch: [5830/21305] batch time: 1.221 trainign loss: 7.7273 avg training loss: 9.9721
batch: [5840/21305] batch time: 0.236 trainign loss: 8.7842 avg training loss: 9.9715
batch: [5850/21305] batch time: 1.274 trainign loss: 8.0646 avg training loss: 9.9710
batch: [5860/21305] batch time: 0.534 trainign loss: 9.2276 avg training loss: 9.9705
batch: [5870/21305] batch time: 0.356 trainign loss: 8.7633 avg training loss: 9.9701
batch: [5880/21305] batch time: 0.463 trainign loss: 7.9606 avg training loss: 9.9694
batch: [5890/21305] batch time: 0.502 trainign loss: 8.6024 avg training loss: 9.9689
batch: [5900/21305] batch time: 1.301 trainign loss: 6.7804 avg training loss: 9.9683
batch: [5910/21305] batch time: 0.062 trainign loss: 7.3583 avg training loss: 9.9669
batch: [5920/21305] batch time: 1.016 trainign loss: 6.9658 avg training loss: 9.9659
batch: [5930/21305] batch time: 0.062 trainign loss: 7.5108 avg training loss: 9.9654
batch: [5940/21305] batch time: 0.597 trainign loss: 6.3813 avg training loss: 9.9648
batch: [5950/21305] batch time: 0.062 trainign loss: 7.6926 avg training loss: 9.9639
batch: [5960/21305] batch time: 1.608 trainign loss: 6.0413 avg training loss: 9.9627
batch: [5970/21305] batch time: 0.057 trainign loss: 9.8317 avg training loss: 9.9625
batch: [5980/21305] batch time: 0.608 trainign loss: 8.0711 avg training loss: 9.9621
batch: [5990/21305] batch time: 0.063 trainign loss: 9.4933 avg training loss: 9.9619
batch: [6000/21305] batch time: 0.950 trainign loss: 9.4069 avg training loss: 9.9616
batch: [6010/21305] batch time: 0.062 trainign loss: 7.5609 avg training loss: 9.9611
batch: [6020/21305] batch time: 0.768 trainign loss: 9.2738 avg training loss: 9.9607
batch: [6030/21305] batch time: 0.693 trainign loss: 8.2845 avg training loss: 9.9597
batch: [6040/21305] batch time: 0.761 trainign loss: 3.5430 avg training loss: 9.9582
batch: [6050/21305] batch time: 0.663 trainign loss: 8.0832 avg training loss: 9.9577
batch: [6060/21305] batch time: 0.126 trainign loss: 9.4155 avg training loss: 9.9567
batch: [6070/21305] batch time: 0.305 trainign loss: 3.9114 avg training loss: 9.9557
batch: [6080/21305] batch time: 0.324 trainign loss: 5.3635 avg training loss: 9.9548
batch: [6090/21305] batch time: 1.196 trainign loss: 8.8392 avg training loss: 9.9541
batch: [6100/21305] batch time: 0.060 trainign loss: 9.8962 avg training loss: 9.9538
batch: [6110/21305] batch time: 1.173 trainign loss: 7.4531 avg training loss: 9.9531
batch: [6120/21305] batch time: 0.056 trainign loss: 8.4277 avg training loss: 9.9526
batch: [6130/21305] batch time: 1.606 trainign loss: 9.0924 avg training loss: 9.9513
batch: [6140/21305] batch time: 0.056 trainign loss: 7.3241 avg training loss: 9.9508
batch: [6150/21305] batch time: 2.179 trainign loss: 7.4201 avg training loss: 9.9503
batch: [6160/21305] batch time: 0.056 trainign loss: 7.5314 avg training loss: 9.9498
batch: [6170/21305] batch time: 2.382 trainign loss: 5.1371 avg training loss: 9.9490
batch: [6180/21305] batch time: 0.056 trainign loss: 9.6598 avg training loss: 9.9481
batch: [6190/21305] batch time: 1.425 trainign loss: 9.0817 avg training loss: 9.9477
batch: [6200/21305] batch time: 0.063 trainign loss: 8.3436 avg training loss: 9.9472
batch: [6210/21305] batch time: 0.535 trainign loss: 8.3788 avg training loss: 9.9465
batch: [6220/21305] batch time: 0.057 trainign loss: 9.3092 avg training loss: 9.9460
batch: [6230/21305] batch time: 2.025 trainign loss: 8.5803 avg training loss: 9.9456
batch: [6240/21305] batch time: 0.051 trainign loss: 9.5577 avg training loss: 9.9450
batch: [6250/21305] batch time: 2.065 trainign loss: 8.8831 avg training loss: 9.9445
batch: [6260/21305] batch time: 0.056 trainign loss: 9.6839 avg training loss: 9.9438
batch: [6270/21305] batch time: 2.519 trainign loss: 7.5530 avg training loss: 9.9434
batch: [6280/21305] batch time: 0.060 trainign loss: 8.9863 avg training loss: 9.9427
batch: [6290/21305] batch time: 2.296 trainign loss: 4.9295 avg training loss: 9.9417
batch: [6300/21305] batch time: 0.057 trainign loss: 9.5511 avg training loss: 9.9414
batch: [6310/21305] batch time: 2.420 trainign loss: 8.3592 avg training loss: 9.9407
batch: [6320/21305] batch time: 0.061 trainign loss: 8.8295 avg training loss: 9.9401
batch: [6330/21305] batch time: 2.098 trainign loss: 8.6463 avg training loss: 9.9393
batch: [6340/21305] batch time: 0.057 trainign loss: 5.4433 avg training loss: 9.9384
batch: [6350/21305] batch time: 2.306 trainign loss: 8.9143 avg training loss: 9.9377
batch: [6360/21305] batch time: 0.056 trainign loss: 8.9722 avg training loss: 9.9373
batch: [6370/21305] batch time: 2.490 trainign loss: 6.8565 avg training loss: 9.9368
batch: [6380/21305] batch time: 0.057 trainign loss: 8.6430 avg training loss: 9.9361
batch: [6390/21305] batch time: 1.924 trainign loss: 5.1668 avg training loss: 9.9352
batch: [6400/21305] batch time: 0.999 trainign loss: 9.2613 avg training loss: 9.9344
batch: [6410/21305] batch time: 0.580 trainign loss: 9.4242 avg training loss: 9.9339
batch: [6420/21305] batch time: 1.796 trainign loss: 8.3526 avg training loss: 9.9328
batch: [6430/21305] batch time: 0.197 trainign loss: 6.2535 avg training loss: 9.9320
batch: [6440/21305] batch time: 1.298 trainign loss: 8.8935 avg training loss: 9.9308
batch: [6450/21305] batch time: 0.830 trainign loss: 8.9125 avg training loss: 9.9297
batch: [6460/21305] batch time: 1.373 trainign loss: 8.3270 avg training loss: 9.9293
batch: [6470/21305] batch time: 1.393 trainign loss: 9.5715 avg training loss: 9.9288
batch: [6480/21305] batch time: 0.956 trainign loss: 8.8036 avg training loss: 9.9284
batch: [6490/21305] batch time: 1.774 trainign loss: 8.3782 avg training loss: 9.9280
batch: [6500/21305] batch time: 0.911 trainign loss: 6.8420 avg training loss: 9.9275
batch: [6510/21305] batch time: 0.860 trainign loss: 8.8671 avg training loss: 9.9270
batch: [6520/21305] batch time: 1.579 trainign loss: 7.9581 avg training loss: 9.9264
batch: [6530/21305] batch time: 1.705 trainign loss: 6.3577 avg training loss: 9.9255
batch: [6540/21305] batch time: 0.513 trainign loss: 8.9889 avg training loss: 9.9247
batch: [6550/21305] batch time: 1.976 trainign loss: 6.9193 avg training loss: 9.9240
batch: [6560/21305] batch time: 0.056 trainign loss: 8.6395 avg training loss: 9.9232
batch: [6570/21305] batch time: 1.162 trainign loss: 7.5436 avg training loss: 9.9225
batch: [6580/21305] batch time: 0.061 trainign loss: 5.1566 avg training loss: 9.9218
batch: [6590/21305] batch time: 1.071 trainign loss: 8.0417 avg training loss: 9.9213
batch: [6600/21305] batch time: 0.266 trainign loss: 9.1400 avg training loss: 9.9209
batch: [6610/21305] batch time: 1.827 trainign loss: 9.6972 avg training loss: 9.9202
batch: [6620/21305] batch time: 0.057 trainign loss: 8.7975 avg training loss: 9.9199
batch: [6630/21305] batch time: 1.616 trainign loss: 8.4404 avg training loss: 9.9189
batch: [6640/21305] batch time: 0.056 trainign loss: 6.0358 avg training loss: 9.9181
batch: [6650/21305] batch time: 1.978 trainign loss: 7.4663 avg training loss: 9.9175
batch: [6660/21305] batch time: 0.062 trainign loss: 7.7589 avg training loss: 9.9167
batch: [6670/21305] batch time: 1.893 trainign loss: 8.4430 avg training loss: 9.9158
batch: [6680/21305] batch time: 0.393 trainign loss: 8.1628 avg training loss: 9.9154
batch: [6690/21305] batch time: 1.973 trainign loss: 9.1837 avg training loss: 9.9149
batch: [6700/21305] batch time: 0.604 trainign loss: 6.6005 avg training loss: 9.9139
batch: [6710/21305] batch time: 2.139 trainign loss: 6.9991 avg training loss: 9.9134
batch: [6720/21305] batch time: 0.470 trainign loss: 8.8637 avg training loss: 9.9126
batch: [6730/21305] batch time: 1.977 trainign loss: 7.9976 avg training loss: 9.9119
batch: [6740/21305] batch time: 0.927 trainign loss: 9.0545 avg training loss: 9.9115
batch: [6750/21305] batch time: 1.511 trainign loss: 9.4038 avg training loss: 9.9112
batch: [6760/21305] batch time: 0.744 trainign loss: 9.0646 avg training loss: 9.9109
batch: [6770/21305] batch time: 1.342 trainign loss: 7.2598 avg training loss: 9.9102
batch: [6780/21305] batch time: 1.601 trainign loss: 7.2568 avg training loss: 9.9094
batch: [6790/21305] batch time: 1.194 trainign loss: 8.8580 avg training loss: 9.9088
batch: [6800/21305] batch time: 0.855 trainign loss: 7.6381 avg training loss: 9.9081
batch: [6810/21305] batch time: 1.936 trainign loss: 8.4681 avg training loss: 9.9074
batch: [6820/21305] batch time: 0.499 trainign loss: 6.4358 avg training loss: 9.9068
batch: [6830/21305] batch time: 1.502 trainign loss: 9.3931 avg training loss: 9.9065
batch: [6840/21305] batch time: 0.597 trainign loss: 11.8022 avg training loss: 9.9043
batch: [6850/21305] batch time: 0.627 trainign loss: 9.3276 avg training loss: 9.9047
batch: [6860/21305] batch time: 0.358 trainign loss: 8.6485 avg training loss: 9.9047
batch: [6870/21305] batch time: 0.056 trainign loss: 8.7916 avg training loss: 9.9040
batch: [6880/21305] batch time: 1.155 trainign loss: 8.6300 avg training loss: 9.9034
batch: [6890/21305] batch time: 0.816 trainign loss: 9.4438 avg training loss: 9.9030
batch: [6900/21305] batch time: 1.279 trainign loss: 8.7711 avg training loss: 9.9024
batch: [6910/21305] batch time: 0.640 trainign loss: 8.4489 avg training loss: 9.9021
batch: [6920/21305] batch time: 1.011 trainign loss: 9.2357 avg training loss: 9.9017
batch: [6930/21305] batch time: 0.058 trainign loss: 7.8626 avg training loss: 9.9012
batch: [6940/21305] batch time: 0.690 trainign loss: 7.4213 avg training loss: 9.8999
batch: [6950/21305] batch time: 0.056 trainign loss: 8.3252 avg training loss: 9.8994
batch: [6960/21305] batch time: 0.199 trainign loss: 7.0210 avg training loss: 9.8987
batch: [6970/21305] batch time: 0.876 trainign loss: 4.4715 avg training loss: 9.8980
batch: [6980/21305] batch time: 0.185 trainign loss: 8.8983 avg training loss: 9.8975
batch: [6990/21305] batch time: 0.062 trainign loss: 9.0575 avg training loss: 9.8972
batch: [7000/21305] batch time: 1.106 trainign loss: 7.2332 avg training loss: 9.8967
batch: [7010/21305] batch time: 0.056 trainign loss: 8.1038 avg training loss: 9.8958
batch: [7020/21305] batch time: 0.062 trainign loss: 9.1903 avg training loss: 9.8954
batch: [7030/21305] batch time: 0.056 trainign loss: 8.2257 avg training loss: 9.8948
batch: [7040/21305] batch time: 0.051 trainign loss: 9.0775 avg training loss: 9.8944
batch: [7050/21305] batch time: 0.057 trainign loss: 7.8858 avg training loss: 9.8933
batch: [7060/21305] batch time: 0.051 trainign loss: 7.1312 avg training loss: 9.8927
batch: [7070/21305] batch time: 0.057 trainign loss: 8.5178 avg training loss: 9.8921
batch: [7080/21305] batch time: 0.057 trainign loss: 8.5182 avg training loss: 9.8915
batch: [7090/21305] batch time: 0.246 trainign loss: 7.8427 avg training loss: 9.8908
batch: [7100/21305] batch time: 0.056 trainign loss: 7.7458 avg training loss: 9.8903
batch: [7110/21305] batch time: 1.165 trainign loss: 8.7133 avg training loss: 9.8892
batch: [7120/21305] batch time: 0.056 trainign loss: 5.6792 avg training loss: 9.8876
batch: [7130/21305] batch time: 2.074 trainign loss: 8.4843 avg training loss: 9.8871
batch: [7140/21305] batch time: 0.061 trainign loss: 9.7285 avg training loss: 9.8864
batch: [7150/21305] batch time: 2.083 trainign loss: 5.9896 avg training loss: 9.8859
batch: [7160/21305] batch time: 0.056 trainign loss: 9.4386 avg training loss: 9.8850
batch: [7170/21305] batch time: 2.132 trainign loss: 7.5561 avg training loss: 9.8842
batch: [7180/21305] batch time: 0.061 trainign loss: 8.3540 avg training loss: 9.8837
batch: [7190/21305] batch time: 2.177 trainign loss: 5.0619 avg training loss: 9.8830
batch: [7200/21305] batch time: 0.056 trainign loss: 7.6549 avg training loss: 9.8821
batch: [7210/21305] batch time: 1.221 trainign loss: 7.8170 avg training loss: 9.8815
batch: [7220/21305] batch time: 0.056 trainign loss: 7.4131 avg training loss: 9.8806
batch: [7230/21305] batch time: 0.404 trainign loss: 5.5688 avg training loss: 9.8799
batch: [7240/21305] batch time: 0.056 trainign loss: 8.8826 avg training loss: 9.8791
batch: [7250/21305] batch time: 1.096 trainign loss: 6.6983 avg training loss: 9.8786
batch: [7260/21305] batch time: 0.060 trainign loss: 6.8206 avg training loss: 9.8777
batch: [7270/21305] batch time: 0.408 trainign loss: 8.8691 avg training loss: 9.8773
batch: [7280/21305] batch time: 0.056 trainign loss: 7.6239 avg training loss: 9.8769
batch: [7290/21305] batch time: 0.062 trainign loss: 5.4844 avg training loss: 9.8762
batch: [7300/21305] batch time: 0.056 trainign loss: 8.9305 avg training loss: 9.8755
batch: [7310/21305] batch time: 0.302 trainign loss: 8.7204 avg training loss: 9.8748
batch: [7320/21305] batch time: 0.053 trainign loss: 8.8135 avg training loss: 9.8743
batch: [7330/21305] batch time: 0.557 trainign loss: 7.0596 avg training loss: 9.8738
batch: [7340/21305] batch time: 0.056 trainign loss: 9.4567 avg training loss: 9.8729
batch: [7350/21305] batch time: 0.347 trainign loss: 8.6540 avg training loss: 9.8722
batch: [7360/21305] batch time: 0.056 trainign loss: 9.0236 avg training loss: 9.8713
batch: [7370/21305] batch time: 0.058 trainign loss: 2.8307 avg training loss: 9.8702
batch: [7380/21305] batch time: 0.056 trainign loss: 7.9819 avg training loss: 9.8695
batch: [7390/21305] batch time: 0.083 trainign loss: 9.3816 avg training loss: 9.8688
batch: [7400/21305] batch time: 0.062 trainign loss: 5.4260 avg training loss: 9.8681
batch: [7410/21305] batch time: 0.052 trainign loss: 9.8661 avg training loss: 9.8669
batch: [7420/21305] batch time: 0.056 trainign loss: 8.3817 avg training loss: 9.8664
batch: [7430/21305] batch time: 0.051 trainign loss: 8.6437 avg training loss: 9.8660
batch: [7440/21305] batch time: 0.056 trainign loss: 8.0232 avg training loss: 9.8655
batch: [7450/21305] batch time: 0.054 trainign loss: 8.8298 avg training loss: 9.8650
batch: [7460/21305] batch time: 0.057 trainign loss: 3.9076 avg training loss: 9.8634
batch: [7470/21305] batch time: 0.057 trainign loss: 4.0277 avg training loss: 9.8628
batch: [7480/21305] batch time: 0.057 trainign loss: 7.8919 avg training loss: 9.8621
batch: [7490/21305] batch time: 0.056 trainign loss: 8.0088 avg training loss: 9.8615
batch: [7500/21305] batch time: 0.062 trainign loss: 9.2096 avg training loss: 9.8611
batch: [7510/21305] batch time: 0.051 trainign loss: 7.4449 avg training loss: 9.8603
batch: [7520/21305] batch time: 0.063 trainign loss: 8.9338 avg training loss: 9.8597
batch: [7530/21305] batch time: 0.060 trainign loss: 5.6409 avg training loss: 9.8590
batch: [7540/21305] batch time: 0.057 trainign loss: 8.9206 avg training loss: 9.8582
batch: [7550/21305] batch time: 0.056 trainign loss: 9.0608 avg training loss: 9.8577
batch: [7560/21305] batch time: 0.056 trainign loss: 8.3573 avg training loss: 9.8573
batch: [7570/21305] batch time: 0.052 trainign loss: 8.9562 avg training loss: 9.8565
batch: [7580/21305] batch time: 0.057 trainign loss: 8.1831 avg training loss: 9.8561
batch: [7590/21305] batch time: 0.051 trainign loss: 9.5260 avg training loss: 9.8553
batch: [7600/21305] batch time: 0.056 trainign loss: 7.6647 avg training loss: 9.8548
batch: [7610/21305] batch time: 0.056 trainign loss: 8.1717 avg training loss: 9.8542
batch: [7620/21305] batch time: 0.056 trainign loss: 6.2141 avg training loss: 9.8532
batch: [7630/21305] batch time: 0.052 trainign loss: 9.3667 avg training loss: 9.8526
batch: [7640/21305] batch time: 0.056 trainign loss: 8.3799 avg training loss: 9.8520
batch: [7650/21305] batch time: 0.054 trainign loss: 5.2024 avg training loss: 9.8512
batch: [7660/21305] batch time: 0.056 trainign loss: 9.0575 avg training loss: 9.8503
batch: [7670/21305] batch time: 0.053 trainign loss: 8.7890 avg training loss: 9.8497
batch: [7680/21305] batch time: 0.056 trainign loss: 7.0236 avg training loss: 9.8491
batch: [7690/21305] batch time: 0.056 trainign loss: 6.6942 avg training loss: 9.8483
batch: [7700/21305] batch time: 0.055 trainign loss: 9.0683 avg training loss: 9.8476
batch: [7710/21305] batch time: 0.055 trainign loss: 7.5856 avg training loss: 9.8462
batch: [7720/21305] batch time: 0.056 trainign loss: 7.8168 avg training loss: 9.8458
batch: [7730/21305] batch time: 0.056 trainign loss: 5.3048 avg training loss: 9.8440
batch: [7740/21305] batch time: 0.056 trainign loss: 8.6170 avg training loss: 9.8436
batch: [7750/21305] batch time: 0.056 trainign loss: 9.5183 avg training loss: 9.8434
batch: [7760/21305] batch time: 0.058 trainign loss: 9.2294 avg training loss: 9.8431
batch: [7770/21305] batch time: 0.062 trainign loss: 5.4364 avg training loss: 9.8425
batch: [7780/21305] batch time: 0.059 trainign loss: 8.8579 avg training loss: 9.8419
batch: [7790/21305] batch time: 0.052 trainign loss: 8.4294 avg training loss: 9.8415
batch: [7800/21305] batch time: 0.056 trainign loss: 8.9006 avg training loss: 9.8407
batch: [7810/21305] batch time: 0.051 trainign loss: 7.8927 avg training loss: 9.8398
batch: [7820/21305] batch time: 0.056 trainign loss: 8.2163 avg training loss: 9.8394
batch: [7830/21305] batch time: 0.056 trainign loss: 7.3236 avg training loss: 9.8389
batch: [7840/21305] batch time: 0.057 trainign loss: 8.8388 avg training loss: 9.8385
batch: [7850/21305] batch time: 0.057 trainign loss: 6.0425 avg training loss: 9.8378
batch: [7860/21305] batch time: 0.056 trainign loss: 7.0833 avg training loss: 9.8370
batch: [7870/21305] batch time: 0.059 trainign loss: 7.5261 avg training loss: 9.8364
batch: [7880/21305] batch time: 0.056 trainign loss: 4.9334 avg training loss: 9.8356
batch: [7890/21305] batch time: 0.052 trainign loss: 7.1262 avg training loss: 9.8350
batch: [7900/21305] batch time: 0.056 trainign loss: 8.1321 avg training loss: 9.8336
batch: [7910/21305] batch time: 0.051 trainign loss: 9.3283 avg training loss: 9.8333
batch: [7920/21305] batch time: 0.056 trainign loss: 8.2343 avg training loss: 9.8331
batch: [7930/21305] batch time: 0.057 trainign loss: 8.8770 avg training loss: 9.8328
batch: [7940/21305] batch time: 0.062 trainign loss: 8.8703 avg training loss: 9.8326
batch: [7950/21305] batch time: 0.056 trainign loss: 8.5382 avg training loss: 9.8321
batch: [7960/21305] batch time: 0.056 trainign loss: 7.4006 avg training loss: 9.8314
batch: [7970/21305] batch time: 0.056 trainign loss: 8.3072 avg training loss: 9.8309
batch: [7980/21305] batch time: 0.058 trainign loss: 9.2471 avg training loss: 9.8300
batch: [7990/21305] batch time: 0.053 trainign loss: 8.9196 avg training loss: 9.8297
batch: [8000/21305] batch time: 0.051 trainign loss: 8.7346 avg training loss: 9.8289
batch: [8010/21305] batch time: 0.056 trainign loss: 7.6628 avg training loss: 9.8281
batch: [8020/21305] batch time: 0.055 trainign loss: 3.1170 avg training loss: 9.8271
batch: [8030/21305] batch time: 0.057 trainign loss: 7.9349 avg training loss: 9.8256
batch: [8040/21305] batch time: 0.054 trainign loss: 9.2713 avg training loss: 9.8252
batch: [8050/21305] batch time: 0.056 trainign loss: 9.8158 avg training loss: 9.8252
batch: [8060/21305] batch time: 0.058 trainign loss: 9.1080 avg training loss: 9.8249
batch: [8070/21305] batch time: 0.056 trainign loss: 7.4006 avg training loss: 9.8246
batch: [8080/21305] batch time: 0.056 trainign loss: 8.6473 avg training loss: 9.8242
batch: [8090/21305] batch time: 0.053 trainign loss: 8.7629 avg training loss: 9.8235
batch: [8100/21305] batch time: 0.052 trainign loss: 7.5616 avg training loss: 9.8229
batch: [8110/21305] batch time: 0.057 trainign loss: 9.3046 avg training loss: 9.8222
batch: [8120/21305] batch time: 0.060 trainign loss: 8.1389 avg training loss: 9.8216
batch: [8130/21305] batch time: 0.058 trainign loss: 7.6962 avg training loss: 9.8208
batch: [8140/21305] batch time: 0.063 trainign loss: 7.6877 avg training loss: 9.8202
batch: [8150/21305] batch time: 0.059 trainign loss: 7.4756 avg training loss: 9.8196
batch: [8160/21305] batch time: 0.052 trainign loss: 8.5745 avg training loss: 9.8190
batch: [8170/21305] batch time: 0.056 trainign loss: 8.5742 avg training loss: 9.8185
batch: [8180/21305] batch time: 0.063 trainign loss: 7.9745 avg training loss: 9.8176
batch: [8190/21305] batch time: 0.062 trainign loss: 7.8753 avg training loss: 9.8172
batch: [8200/21305] batch time: 0.546 trainign loss: 8.1868 avg training loss: 9.8168
batch: [8210/21305] batch time: 0.056 trainign loss: 7.8317 avg training loss: 9.8161
batch: [8220/21305] batch time: 0.552 trainign loss: 8.0924 avg training loss: 9.8156
batch: [8230/21305] batch time: 0.059 trainign loss: 9.2020 avg training loss: 9.8148
batch: [8240/21305] batch time: 0.952 trainign loss: 8.7663 avg training loss: 9.8144
batch: [8250/21305] batch time: 0.056 trainign loss: 8.2068 avg training loss: 9.8140
batch: [8260/21305] batch time: 2.351 trainign loss: 9.1035 avg training loss: 9.8131
batch: [8270/21305] batch time: 0.063 trainign loss: 9.3249 avg training loss: 9.8125
batch: [8280/21305] batch time: 2.341 trainign loss: 8.4614 avg training loss: 9.8122
batch: [8290/21305] batch time: 0.062 trainign loss: 8.1809 avg training loss: 9.8116
batch: [8300/21305] batch time: 2.261 trainign loss: 7.3807 avg training loss: 9.8110
batch: [8310/21305] batch time: 0.056 trainign loss: 9.1269 avg training loss: 9.8105
batch: [8320/21305] batch time: 2.359 trainign loss: 8.5014 avg training loss: 9.8100
batch: [8330/21305] batch time: 0.059 trainign loss: 8.9869 avg training loss: 9.8096
batch: [8340/21305] batch time: 2.314 trainign loss: 7.9551 avg training loss: 9.8088
batch: [8350/21305] batch time: 0.063 trainign loss: 5.8737 avg training loss: 9.8079
batch: [8360/21305] batch time: 2.080 trainign loss: 5.9455 avg training loss: 9.8072
batch: [8370/21305] batch time: 0.063 trainign loss: 9.4923 avg training loss: 9.8065
batch: [8380/21305] batch time: 2.313 trainign loss: 6.4780 avg training loss: 9.8060
batch: [8390/21305] batch time: 0.052 trainign loss: 4.4468 avg training loss: 9.8046
batch: [8400/21305] batch time: 2.598 trainign loss: 10.1380 avg training loss: 9.8030
batch: [8410/21305] batch time: 0.060 trainign loss: 8.8270 avg training loss: 9.8027
batch: [8420/21305] batch time: 0.929 trainign loss: 8.4940 avg training loss: 9.8014
batch: [8430/21305] batch time: 0.063 trainign loss: 9.4371 avg training loss: 9.8013
batch: [8440/21305] batch time: 0.823 trainign loss: 9.7946 avg training loss: 9.8011
batch: [8450/21305] batch time: 0.061 trainign loss: 9.2489 avg training loss: 9.8008
batch: [8460/21305] batch time: 0.373 trainign loss: 9.0566 avg training loss: 9.8005
batch: [8470/21305] batch time: 0.053 trainign loss: 9.4517 avg training loss: 9.7999
batch: [8480/21305] batch time: 0.060 trainign loss: 8.4631 avg training loss: 9.7994
batch: [8490/21305] batch time: 0.063 trainign loss: 8.7356 avg training loss: 9.7987
batch: [8500/21305] batch time: 0.063 trainign loss: 6.9870 avg training loss: 9.7979
batch: [8510/21305] batch time: 0.057 trainign loss: 7.4097 avg training loss: 9.7970
batch: [8520/21305] batch time: 0.057 trainign loss: 9.2149 avg training loss: 9.7964
batch: [8530/21305] batch time: 0.054 trainign loss: 8.0898 avg training loss: 9.7958
batch: [8540/21305] batch time: 0.056 trainign loss: 8.8524 avg training loss: 9.7953
batch: [8550/21305] batch time: 0.052 trainign loss: 9.1899 avg training loss: 9.7948
batch: [8560/21305] batch time: 0.058 trainign loss: 8.0215 avg training loss: 9.7942
batch: [8570/21305] batch time: 0.057 trainign loss: 7.1070 avg training loss: 9.7934
batch: [8580/21305] batch time: 0.059 trainign loss: 7.7737 avg training loss: 9.7929
batch: [8590/21305] batch time: 0.052 trainign loss: 9.0815 avg training loss: 9.7923
batch: [8600/21305] batch time: 0.056 trainign loss: 8.1576 avg training loss: 9.7916
batch: [8610/21305] batch time: 0.055 trainign loss: 7.5824 avg training loss: 9.7907
batch: [8620/21305] batch time: 0.063 trainign loss: 8.0546 avg training loss: 9.7902
batch: [8630/21305] batch time: 0.052 trainign loss: 6.3126 avg training loss: 9.7894
batch: [8640/21305] batch time: 0.056 trainign loss: 8.4162 avg training loss: 9.7890
batch: [8650/21305] batch time: 0.056 trainign loss: 4.3286 avg training loss: 9.7881
batch: [8660/21305] batch time: 0.056 trainign loss: 5.5321 avg training loss: 9.7873
batch: [8670/21305] batch time: 0.053 trainign loss: 6.2154 avg training loss: 9.7866
batch: [8680/21305] batch time: 0.056 trainign loss: 9.4105 avg training loss: 9.7862
batch: [8690/21305] batch time: 0.056 trainign loss: 7.5925 avg training loss: 9.7856
batch: [8700/21305] batch time: 0.061 trainign loss: 7.1186 avg training loss: 9.7850
batch: [8710/21305] batch time: 0.062 trainign loss: 1.3461 avg training loss: 9.7839
batch: [8720/21305] batch time: 0.057 trainign loss: 0.0002 avg training loss: 9.7806
batch: [8730/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 9.7774
batch: [8740/21305] batch time: 0.058 trainign loss: 10.4923 avg training loss: 9.7778
batch: [8750/21305] batch time: 0.056 trainign loss: 9.8756 avg training loss: 9.7779
batch: [8760/21305] batch time: 0.059 trainign loss: 8.8902 avg training loss: 9.7774
batch: [8770/21305] batch time: 0.052 trainign loss: 7.8631 avg training loss: 9.7768
batch: [8780/21305] batch time: 0.227 trainign loss: 7.7694 avg training loss: 9.7761
batch: [8790/21305] batch time: 0.056 trainign loss: 5.9317 avg training loss: 9.7757
batch: [8800/21305] batch time: 0.618 trainign loss: 7.4654 avg training loss: 9.7748
batch: [8810/21305] batch time: 0.056 trainign loss: 9.2450 avg training loss: 9.7742
batch: [8820/21305] batch time: 0.701 trainign loss: 8.2542 avg training loss: 9.7740
batch: [8830/21305] batch time: 0.056 trainign loss: 8.6943 avg training loss: 9.7736
batch: [8840/21305] batch time: 1.675 trainign loss: 8.4298 avg training loss: 9.7729
batch: [8850/21305] batch time: 0.057 trainign loss: 8.1618 avg training loss: 9.7723
batch: [8860/21305] batch time: 0.931 trainign loss: 8.7685 avg training loss: 9.7718
batch: [8870/21305] batch time: 0.368 trainign loss: 8.6045 avg training loss: 9.7715
batch: [8880/21305] batch time: 0.328 trainign loss: 6.2429 avg training loss: 9.7708
batch: [8890/21305] batch time: 0.606 trainign loss: 9.2372 avg training loss: 9.7705
batch: [8900/21305] batch time: 0.497 trainign loss: 7.7568 avg training loss: 9.7697
batch: [8910/21305] batch time: 1.694 trainign loss: 6.5629 avg training loss: 9.7689
batch: [8920/21305] batch time: 0.763 trainign loss: 2.3431 avg training loss: 9.7677
batch: [8930/21305] batch time: 1.838 trainign loss: 8.8891 avg training loss: 9.7673
batch: [8940/21305] batch time: 0.791 trainign loss: 8.5482 avg training loss: 9.7667
batch: [8950/21305] batch time: 1.696 trainign loss: 8.0912 avg training loss: 9.7664
batch: [8960/21305] batch time: 0.859 trainign loss: 8.3463 avg training loss: 9.7659
batch: [8970/21305] batch time: 0.662 trainign loss: 8.2510 avg training loss: 9.7656
batch: [8980/21305] batch time: 2.236 trainign loss: 6.8015 avg training loss: 9.7646
batch: [8990/21305] batch time: 0.058 trainign loss: 8.9205 avg training loss: 9.7640
batch: [9000/21305] batch time: 2.128 trainign loss: 0.6600 avg training loss: 9.7624
batch: [9010/21305] batch time: 0.063 trainign loss: 9.8937 avg training loss: 9.7620
batch: [9020/21305] batch time: 1.965 trainign loss: 9.4637 avg training loss: 9.7620
batch: [9030/21305] batch time: 0.151 trainign loss: 8.5380 avg training loss: 9.7617
batch: [9040/21305] batch time: 2.351 trainign loss: 8.2135 avg training loss: 9.7614
batch: [9050/21305] batch time: 0.067 trainign loss: 8.4128 avg training loss: 9.7607
batch: [9060/21305] batch time: 2.182 trainign loss: 6.8588 avg training loss: 9.7600
batch: [9070/21305] batch time: 0.056 trainign loss: 4.2215 avg training loss: 9.7591
batch: [9080/21305] batch time: 2.388 trainign loss: 9.0697 avg training loss: 9.7588
batch: [9090/21305] batch time: 0.230 trainign loss: 8.2181 avg training loss: 9.7580
batch: [9100/21305] batch time: 2.266 trainign loss: 9.0198 avg training loss: 9.7576
batch: [9110/21305] batch time: 0.057 trainign loss: 7.7927 avg training loss: 9.7572
batch: [9120/21305] batch time: 2.463 trainign loss: 9.4970 avg training loss: 9.7565
batch: [9130/21305] batch time: 0.060 trainign loss: 6.6547 avg training loss: 9.7552
batch: [9140/21305] batch time: 2.343 trainign loss: 8.6057 avg training loss: 9.7544
batch: [9150/21305] batch time: 0.057 trainign loss: 8.0042 avg training loss: 9.7539
batch: [9160/21305] batch time: 2.074 trainign loss: 9.1792 avg training loss: 9.7535
batch: [9170/21305] batch time: 0.063 trainign loss: 8.1497 avg training loss: 9.7525
batch: [9180/21305] batch time: 2.246 trainign loss: 8.1999 avg training loss: 9.7523
batch: [9190/21305] batch time: 0.062 trainign loss: 8.8538 avg training loss: 9.7519
batch: [9200/21305] batch time: 2.170 trainign loss: 9.2935 avg training loss: 9.7514
batch: [9210/21305] batch time: 0.056 trainign loss: 7.6754 avg training loss: 9.7509
batch: [9220/21305] batch time: 2.346 trainign loss: 6.5217 avg training loss: 9.7502
batch: [9230/21305] batch time: 0.056 trainign loss: 7.6651 avg training loss: 9.7499
batch: [9240/21305] batch time: 2.504 trainign loss: 5.8226 avg training loss: 9.7492
batch: [9250/21305] batch time: 0.174 trainign loss: 9.0665 avg training loss: 9.7485
batch: [9260/21305] batch time: 1.480 trainign loss: 8.0989 avg training loss: 9.7479
batch: [9270/21305] batch time: 0.326 trainign loss: 7.9612 avg training loss: 9.7474
batch: [9280/21305] batch time: 1.890 trainign loss: 8.3936 avg training loss: 9.7467
batch: [9290/21305] batch time: 0.595 trainign loss: 5.9406 avg training loss: 9.7458
batch: [9300/21305] batch time: 1.818 trainign loss: 10.3999 avg training loss: 9.7449
batch: [9310/21305] batch time: 0.845 trainign loss: 8.8700 avg training loss: 9.7445
batch: [9320/21305] batch time: 1.128 trainign loss: 6.3426 avg training loss: 9.7439
batch: [9330/21305] batch time: 1.131 trainign loss: 7.3227 avg training loss: 9.7433
batch: [9340/21305] batch time: 1.193 trainign loss: 8.3661 avg training loss: 9.7430
batch: [9350/21305] batch time: 0.719 trainign loss: 8.9740 avg training loss: 9.7427
batch: [9360/21305] batch time: 1.416 trainign loss: 5.8263 avg training loss: 9.7418
batch: [9370/21305] batch time: 1.010 trainign loss: 6.2408 avg training loss: 9.7413
batch: [9380/21305] batch time: 1.706 trainign loss: 7.6733 avg training loss: 9.7407
batch: [9390/21305] batch time: 0.447 trainign loss: 8.8153 avg training loss: 9.7403
batch: [9400/21305] batch time: 0.799 trainign loss: 8.6730 avg training loss: 9.7399
batch: [9410/21305] batch time: 0.739 trainign loss: 7.3242 avg training loss: 9.7393
batch: [9420/21305] batch time: 0.443 trainign loss: 3.5217 avg training loss: 9.7384
batch: [9430/21305] batch time: 0.342 trainign loss: 8.7865 avg training loss: 9.7378
batch: [9440/21305] batch time: 0.624 trainign loss: 9.4744 avg training loss: 9.7374
batch: [9450/21305] batch time: 0.054 trainign loss: 6.7341 avg training loss: 9.7360
batch: [9460/21305] batch time: 1.070 trainign loss: 9.1536 avg training loss: 9.7358
batch: [9470/21305] batch time: 0.063 trainign loss: 9.6745 avg training loss: 9.7353
batch: [9480/21305] batch time: 0.256 trainign loss: 7.2023 avg training loss: 9.7350
batch: [9490/21305] batch time: 0.229 trainign loss: 10.0404 avg training loss: 9.7342
batch: [9500/21305] batch time: 0.551 trainign loss: 9.2076 avg training loss: 9.7340
batch: [9510/21305] batch time: 0.056 trainign loss: 8.0230 avg training loss: 9.7331
batch: [9520/21305] batch time: 0.736 trainign loss: 9.1094 avg training loss: 9.7329
batch: [9530/21305] batch time: 0.056 trainign loss: 8.8136 avg training loss: 9.7326
batch: [9540/21305] batch time: 0.311 trainign loss: 8.2022 avg training loss: 9.7315
batch: [9550/21305] batch time: 0.056 trainign loss: 4.8283 avg training loss: 9.7310
batch: [9560/21305] batch time: 1.123 trainign loss: 6.9288 avg training loss: 9.7304
batch: [9570/21305] batch time: 0.053 trainign loss: 8.9257 avg training loss: 9.7300
batch: [9580/21305] batch time: 0.334 trainign loss: 10.4362 avg training loss: 9.7291
batch: [9590/21305] batch time: 0.063 trainign loss: 8.5741 avg training loss: 9.7290
batch: [9600/21305] batch time: 0.332 trainign loss: 9.1262 avg training loss: 9.7287
batch: [9610/21305] batch time: 0.056 trainign loss: 8.6018 avg training loss: 9.7279
batch: [9620/21305] batch time: 0.056 trainign loss: 8.5664 avg training loss: 9.7274
batch: [9630/21305] batch time: 0.052 trainign loss: 9.7954 avg training loss: 9.7268
batch: [9640/21305] batch time: 0.306 trainign loss: 9.4818 avg training loss: 9.7267
batch: [9650/21305] batch time: 0.055 trainign loss: 8.7328 avg training loss: 9.7264
batch: [9660/21305] batch time: 0.210 trainign loss: 5.1423 avg training loss: 9.7256
batch: [9670/21305] batch time: 0.051 trainign loss: 8.6772 avg training loss: 9.7252
batch: [9680/21305] batch time: 0.062 trainign loss: 8.6388 avg training loss: 9.7247
batch: [9690/21305] batch time: 0.057 trainign loss: 8.5172 avg training loss: 9.7243
batch: [9700/21305] batch time: 0.057 trainign loss: 8.3455 avg training loss: 9.7240
batch: [9710/21305] batch time: 0.056 trainign loss: 8.4414 avg training loss: 9.7236
batch: [9720/21305] batch time: 0.056 trainign loss: 9.1267 avg training loss: 9.7232
batch: [9730/21305] batch time: 0.056 trainign loss: 7.9374 avg training loss: 9.7229
batch: [9740/21305] batch time: 0.056 trainign loss: 8.5843 avg training loss: 9.7221
batch: [9750/21305] batch time: 0.056 trainign loss: 9.1911 avg training loss: 9.7216
batch: [9760/21305] batch time: 0.628 trainign loss: 8.2573 avg training loss: 9.7209
batch: [9770/21305] batch time: 0.059 trainign loss: 9.0670 avg training loss: 9.7205
batch: [9780/21305] batch time: 1.271 trainign loss: 9.4920 avg training loss: 9.7202
batch: [9790/21305] batch time: 0.061 trainign loss: 8.8253 avg training loss: 9.7196
batch: [9800/21305] batch time: 0.910 trainign loss: 8.4882 avg training loss: 9.7192
batch: [9810/21305] batch time: 0.056 trainign loss: 9.8689 avg training loss: 9.7185
batch: [9820/21305] batch time: 1.210 trainign loss: 6.5957 avg training loss: 9.7178
batch: [9830/21305] batch time: 0.057 trainign loss: 6.7634 avg training loss: 9.7171
batch: [9840/21305] batch time: 1.981 trainign loss: 5.7011 avg training loss: 9.7162
batch: [9850/21305] batch time: 0.056 trainign loss: 8.1433 avg training loss: 9.7158
batch: [9860/21305] batch time: 1.840 trainign loss: 2.7571 avg training loss: 9.7147
batch: [9870/21305] batch time: 0.061 trainign loss: 6.0373 avg training loss: 9.7142
batch: [9880/21305] batch time: 1.420 trainign loss: 9.6379 avg training loss: 9.7134
batch: [9890/21305] batch time: 0.062 trainign loss: 8.5951 avg training loss: 9.7128
batch: [9900/21305] batch time: 0.596 trainign loss: 7.7432 avg training loss: 9.7125
batch: [9910/21305] batch time: 0.053 trainign loss: 8.8447 avg training loss: 9.7121
batch: [9920/21305] batch time: 0.058 trainign loss: 8.4284 avg training loss: 9.7116
batch: [9930/21305] batch time: 0.062 trainign loss: 6.9479 avg training loss: 9.7111
batch: [9940/21305] batch time: 0.056 trainign loss: 9.7476 avg training loss: 9.7106
batch: [9950/21305] batch time: 0.061 trainign loss: 8.4151 avg training loss: 9.7100
batch: [9960/21305] batch time: 0.059 trainign loss: 9.0118 avg training loss: 9.7096
batch: [9970/21305] batch time: 0.054 trainign loss: 9.0275 avg training loss: 9.7094
batch: [9980/21305] batch time: 0.057 trainign loss: 7.9401 avg training loss: 9.7090
batch: [9990/21305] batch time: 0.062 trainign loss: 8.7504 avg training loss: 9.7086
batch: [10000/21305] batch time: 0.063 trainign loss: 6.5023 avg training loss: 9.7082
batch: [10010/21305] batch time: 0.055 trainign loss: 8.6006 avg training loss: 9.7075
batch: [10020/21305] batch time: 0.056 trainign loss: 8.7702 avg training loss: 9.7071
batch: [10030/21305] batch time: 0.056 trainign loss: 8.3477 avg training loss: 9.7067
batch: [10040/21305] batch time: 0.056 trainign loss: 7.6217 avg training loss: 9.7063
batch: [10050/21305] batch time: 0.056 trainign loss: 7.2757 avg training loss: 9.7057
batch: [10060/21305] batch time: 0.056 trainign loss: 5.7916 avg training loss: 9.7047
batch: [10070/21305] batch time: 0.061 trainign loss: 7.9732 avg training loss: 9.7040
batch: [10080/21305] batch time: 0.735 trainign loss: 7.0665 avg training loss: 9.7036
batch: [10090/21305] batch time: 0.062 trainign loss: 11.1615 avg training loss: 9.7025
batch: [10100/21305] batch time: 1.467 trainign loss: 9.4265 avg training loss: 9.7021
batch: [10110/21305] batch time: 0.056 trainign loss: 8.7353 avg training loss: 9.7018
batch: [10120/21305] batch time: 2.565 trainign loss: 9.2666 avg training loss: 9.7015
batch: [10130/21305] batch time: 0.061 trainign loss: 7.6346 avg training loss: 9.7012
batch: [10140/21305] batch time: 2.401 trainign loss: 8.8680 avg training loss: 9.7007
batch: [10150/21305] batch time: 0.057 trainign loss: 8.1356 avg training loss: 9.7002
batch: [10160/21305] batch time: 2.283 trainign loss: 7.2392 avg training loss: 9.6997
batch: [10170/21305] batch time: 0.056 trainign loss: 8.7314 avg training loss: 9.6993
batch: [10180/21305] batch time: 2.084 trainign loss: 8.8118 avg training loss: 9.6989
batch: [10190/21305] batch time: 0.063 trainign loss: 9.0364 avg training loss: 9.6982
batch: [10200/21305] batch time: 2.111 trainign loss: 6.7001 avg training loss: 9.6975
batch: [10210/21305] batch time: 0.056 trainign loss: 8.6533 avg training loss: 9.6972
batch: [10220/21305] batch time: 2.071 trainign loss: 8.6419 avg training loss: 9.6962
batch: [10230/21305] batch time: 0.057 trainign loss: 6.4606 avg training loss: 9.6957
batch: [10240/21305] batch time: 2.501 trainign loss: 9.6405 avg training loss: 9.6952
batch: [10250/21305] batch time: 0.053 trainign loss: 9.3054 avg training loss: 9.6946
batch: [10260/21305] batch time: 2.285 trainign loss: 9.2249 avg training loss: 9.6943
batch: [10270/21305] batch time: 0.055 trainign loss: 9.1562 avg training loss: 9.6936
batch: [10280/21305] batch time: 2.457 trainign loss: 8.1867 avg training loss: 9.6933
batch: [10290/21305] batch time: 0.057 trainign loss: 8.7930 avg training loss: 9.6926
batch: [10300/21305] batch time: 2.774 trainign loss: 8.5734 avg training loss: 9.6922
batch: [10310/21305] batch time: 0.061 trainign loss: 8.1825 avg training loss: 9.6915
batch: [10320/21305] batch time: 2.289 trainign loss: 1.6407 avg training loss: 9.6904
batch: [10330/21305] batch time: 0.056 trainign loss: 8.9492 avg training loss: 9.6901
batch: [10340/21305] batch time: 2.371 trainign loss: 9.5022 avg training loss: 9.6899
batch: [10350/21305] batch time: 0.062 trainign loss: 9.2439 avg training loss: 9.6896
batch: [10360/21305] batch time: 2.077 trainign loss: 8.1731 avg training loss: 9.6893
batch: [10370/21305] batch time: 0.056 trainign loss: 8.3964 avg training loss: 9.6889
batch: [10380/21305] batch time: 2.468 trainign loss: 9.1360 avg training loss: 9.6886
batch: [10390/21305] batch time: 0.051 trainign loss: 8.8604 avg training loss: 9.6877
batch: [10400/21305] batch time: 2.245 trainign loss: 7.6214 avg training loss: 9.6870
batch: [10410/21305] batch time: 0.063 trainign loss: 8.7438 avg training loss: 9.6863
batch: [10420/21305] batch time: 2.370 trainign loss: 9.4119 avg training loss: 9.6860
batch: [10430/21305] batch time: 0.056 trainign loss: 7.3345 avg training loss: 9.6857
batch: [10440/21305] batch time: 2.212 trainign loss: 8.9622 avg training loss: 9.6847
batch: [10450/21305] batch time: 0.062 trainign loss: 8.3022 avg training loss: 9.6844
batch: [10460/21305] batch time: 2.382 trainign loss: 9.2583 avg training loss: 9.6840
batch: [10470/21305] batch time: 0.056 trainign loss: 7.2585 avg training loss: 9.6838
batch: [10480/21305] batch time: 2.397 trainign loss: 10.3262 avg training loss: 9.6828
batch: [10490/21305] batch time: 0.062 trainign loss: 8.4161 avg training loss: 9.6825
batch: [10500/21305] batch time: 1.890 trainign loss: 6.0134 avg training loss: 9.6819
batch: [10510/21305] batch time: 0.060 trainign loss: 6.9932 avg training loss: 9.6812
batch: [10520/21305] batch time: 2.181 trainign loss: 8.6002 avg training loss: 9.6808
batch: [10530/21305] batch time: 0.055 trainign loss: 8.5673 avg training loss: 9.6805
batch: [10540/21305] batch time: 2.528 trainign loss: 7.0147 avg training loss: 9.6799
batch: [10550/21305] batch time: 0.056 trainign loss: 4.7447 avg training loss: 9.6793
batch: [10560/21305] batch time: 2.759 trainign loss: 12.5609 avg training loss: 9.6777
batch: [10570/21305] batch time: 0.056 trainign loss: 9.5450 avg training loss: 9.6777
batch: [10580/21305] batch time: 2.501 trainign loss: 9.9197 avg training loss: 9.6778
batch: [10590/21305] batch time: 0.055 trainign loss: 9.0132 avg training loss: 9.6774
batch: [10600/21305] batch time: 2.187 trainign loss: 8.1891 avg training loss: 9.6771
batch: [10610/21305] batch time: 0.062 trainign loss: 4.9326 avg training loss: 9.6760
batch: [10620/21305] batch time: 2.170 trainign loss: 8.6177 avg training loss: 9.6758
batch: [10630/21305] batch time: 0.062 trainign loss: 7.9631 avg training loss: 9.6752
batch: [10640/21305] batch time: 2.849 trainign loss: 7.6789 avg training loss: 9.6743
batch: [10650/21305] batch time: 0.056 trainign loss: 9.3940 avg training loss: 9.6739
batch: [10660/21305] batch time: 2.130 trainign loss: 8.9384 avg training loss: 9.6735
batch: [10670/21305] batch time: 0.056 trainign loss: 5.2988 avg training loss: 9.6729
batch: [10680/21305] batch time: 2.108 trainign loss: 9.2985 avg training loss: 9.6724
batch: [10690/21305] batch time: 0.056 trainign loss: 8.2369 avg training loss: 9.6714
batch: [10700/21305] batch time: 2.410 trainign loss: 9.0556 avg training loss: 9.6709
batch: [10710/21305] batch time: 0.056 trainign loss: 8.3074 avg training loss: 9.6699
batch: [10720/21305] batch time: 2.548 trainign loss: 9.6915 avg training loss: 9.6690
batch: [10730/21305] batch time: 0.062 trainign loss: 9.2667 avg training loss: 9.6686
batch: [10740/21305] batch time: 2.152 trainign loss: 9.2677 avg training loss: 9.6684
batch: [10750/21305] batch time: 0.059 trainign loss: 8.4969 avg training loss: 9.6679
batch: [10760/21305] batch time: 2.315 trainign loss: 5.1794 avg training loss: 9.6672
batch: [10770/21305] batch time: 0.061 trainign loss: 8.2005 avg training loss: 9.6665
batch: [10780/21305] batch time: 2.547 trainign loss: 8.5486 avg training loss: 9.6662
batch: [10790/21305] batch time: 0.058 trainign loss: 8.7337 avg training loss: 9.6656
batch: [10800/21305] batch time: 2.276 trainign loss: 7.1682 avg training loss: 9.6653
batch: [10810/21305] batch time: 0.176 trainign loss: 8.7047 avg training loss: 9.6649
batch: [10820/21305] batch time: 1.909 trainign loss: 4.8643 avg training loss: 9.6638
batch: [10830/21305] batch time: 0.974 trainign loss: 2.4268 avg training loss: 9.6625
batch: [10840/21305] batch time: 2.126 trainign loss: 7.9047 avg training loss: 9.6623
batch: [10850/21305] batch time: 0.328 trainign loss: 9.4789 avg training loss: 9.6619
batch: [10860/21305] batch time: 2.054 trainign loss: 8.6273 avg training loss: 9.6618
batch: [10870/21305] batch time: 0.566 trainign loss: 7.4685 avg training loss: 9.6614
batch: [10880/21305] batch time: 1.452 trainign loss: 5.6174 avg training loss: 9.6608
batch: [10890/21305] batch time: 1.447 trainign loss: 8.8230 avg training loss: 9.6601
batch: [10900/21305] batch time: 0.648 trainign loss: 8.9456 avg training loss: 9.6597
batch: [10910/21305] batch time: 1.509 trainign loss: 9.2136 avg training loss: 9.6590
batch: [10920/21305] batch time: 1.114 trainign loss: 8.8607 avg training loss: 9.6586
batch: [10930/21305] batch time: 1.513 trainign loss: 6.9746 avg training loss: 9.6579
batch: [10940/21305] batch time: 0.364 trainign loss: 9.0694 avg training loss: 9.6573
batch: [10950/21305] batch time: 1.160 trainign loss: 8.1027 avg training loss: 9.6565
batch: [10960/21305] batch time: 1.113 trainign loss: 8.2226 avg training loss: 9.6563
batch: [10970/21305] batch time: 0.955 trainign loss: 8.8647 avg training loss: 9.6560
batch: [10980/21305] batch time: 1.461 trainign loss: 7.0360 avg training loss: 9.6551
batch: [10990/21305] batch time: 0.972 trainign loss: 8.8353 avg training loss: 9.6546
batch: [11000/21305] batch time: 1.524 trainign loss: 8.4071 avg training loss: 9.6540
batch: [11010/21305] batch time: 0.813 trainign loss: 8.0702 avg training loss: 9.6531
batch: [11020/21305] batch time: 1.286 trainign loss: 8.4370 avg training loss: 9.6527
batch: [11030/21305] batch time: 1.108 trainign loss: 7.1441 avg training loss: 9.6522
batch: [11040/21305] batch time: 0.807 trainign loss: 9.0476 avg training loss: 9.6517
batch: [11050/21305] batch time: 2.131 trainign loss: 8.1370 avg training loss: 9.6512
batch: [11060/21305] batch time: 0.451 trainign loss: 8.9822 avg training loss: 9.6507
batch: [11070/21305] batch time: 2.048 trainign loss: 6.1623 avg training loss: 9.6501
batch: [11080/21305] batch time: 0.407 trainign loss: 9.1030 avg training loss: 9.6495
batch: [11090/21305] batch time: 2.429 trainign loss: 9.3853 avg training loss: 9.6491
batch: [11100/21305] batch time: 0.061 trainign loss: 8.4452 avg training loss: 9.6487
batch: [11110/21305] batch time: 1.853 trainign loss: 8.7990 avg training loss: 9.6485
batch: [11120/21305] batch time: 0.053 trainign loss: 8.2507 avg training loss: 9.6482
batch: [11130/21305] batch time: 2.320 trainign loss: 5.2508 avg training loss: 9.6475
batch: [11140/21305] batch time: 0.053 trainign loss: 4.4400 avg training loss: 9.6468
batch: [11150/21305] batch time: 2.306 trainign loss: 8.4049 avg training loss: 9.6461
batch: [11160/21305] batch time: 0.060 trainign loss: 8.6362 avg training loss: 9.6457
batch: [11170/21305] batch time: 2.361 trainign loss: 3.9524 avg training loss: 9.6450
batch: [11180/21305] batch time: 0.056 trainign loss: 11.9279 avg training loss: 9.6439
batch: [11190/21305] batch time: 2.616 trainign loss: 9.2389 avg training loss: 9.6433
batch: [11200/21305] batch time: 0.057 trainign loss: 9.1384 avg training loss: 9.6431
batch: [11210/21305] batch time: 2.018 trainign loss: 6.5940 avg training loss: 9.6425
batch: [11220/21305] batch time: 0.056 trainign loss: 8.5398 avg training loss: 9.6422
batch: [11230/21305] batch time: 2.471 trainign loss: 7.2975 avg training loss: 9.6417
batch: [11240/21305] batch time: 0.063 trainign loss: 7.8704 avg training loss: 9.6413
batch: [11250/21305] batch time: 2.045 trainign loss: 0.5405 avg training loss: 9.6397
batch: [11260/21305] batch time: 0.056 trainign loss: 9.1972 avg training loss: 9.6397
batch: [11270/21305] batch time: 2.106 trainign loss: 8.5928 avg training loss: 9.6393
batch: [11280/21305] batch time: 0.170 trainign loss: 7.7819 avg training loss: 9.6390
batch: [11290/21305] batch time: 2.236 trainign loss: 8.9353 avg training loss: 9.6388
batch: [11300/21305] batch time: 0.060 trainign loss: 1.8044 avg training loss: 9.6376
batch: [11310/21305] batch time: 2.409 trainign loss: 5.5239 avg training loss: 9.6374
batch: [11320/21305] batch time: 0.056 trainign loss: 9.6510 avg training loss: 9.6365
batch: [11330/21305] batch time: 2.397 trainign loss: 9.5111 avg training loss: 9.6361
batch: [11340/21305] batch time: 0.063 trainign loss: 9.2651 avg training loss: 9.6360
batch: [11350/21305] batch time: 1.933 trainign loss: 9.0056 avg training loss: 9.6356
batch: [11360/21305] batch time: 0.056 trainign loss: 7.7538 avg training loss: 9.6351
batch: [11370/21305] batch time: 2.314 trainign loss: 7.4609 avg training loss: 9.6344
batch: [11380/21305] batch time: 0.058 trainign loss: 5.8502 avg training loss: 9.6337
batch: [11390/21305] batch time: 2.115 trainign loss: 9.8878 avg training loss: 9.6317
batch: [11400/21305] batch time: 0.062 trainign loss: 10.0744 avg training loss: 9.6321
batch: [11410/21305] batch time: 2.017 trainign loss: 9.6428 avg training loss: 9.6322
batch: [11420/21305] batch time: 0.056 trainign loss: 8.7818 avg training loss: 9.6321
batch: [11430/21305] batch time: 2.129 trainign loss: 8.5279 avg training loss: 9.6319
batch: [11440/21305] batch time: 0.536 trainign loss: 7.7453 avg training loss: 9.6315
batch: [11450/21305] batch time: 2.010 trainign loss: 8.0025 avg training loss: 9.6308
batch: [11460/21305] batch time: 0.060 trainign loss: 8.0655 avg training loss: 9.6305
batch: [11470/21305] batch time: 2.261 trainign loss: 9.1215 avg training loss: 9.6297
batch: [11480/21305] batch time: 0.062 trainign loss: 8.1659 avg training loss: 9.6293
batch: [11490/21305] batch time: 2.034 trainign loss: 9.5676 avg training loss: 9.6282
batch: [11500/21305] batch time: 0.056 trainign loss: 7.9695 avg training loss: 9.6279
batch: [11510/21305] batch time: 2.357 trainign loss: 8.6527 avg training loss: 9.6273
batch: [11520/21305] batch time: 0.056 trainign loss: 9.1658 avg training loss: 9.6270
batch: [11530/21305] batch time: 1.788 trainign loss: 8.1512 avg training loss: 9.6264
batch: [11540/21305] batch time: 0.063 trainign loss: 9.7499 avg training loss: 9.6261
batch: [11550/21305] batch time: 2.426 trainign loss: 5.0144 avg training loss: 9.6255
batch: [11560/21305] batch time: 0.055 trainign loss: 9.1745 avg training loss: 9.6252
batch: [11570/21305] batch time: 2.010 trainign loss: 5.9182 avg training loss: 9.6244
batch: [11580/21305] batch time: 0.057 trainign loss: 9.7667 avg training loss: 9.6240
batch: [11590/21305] batch time: 2.361 trainign loss: 8.6128 avg training loss: 9.6236
batch: [11600/21305] batch time: 0.061 trainign loss: 5.2266 avg training loss: 9.6230
batch: [11610/21305] batch time: 1.607 trainign loss: 7.7077 avg training loss: 9.6225
batch: [11620/21305] batch time: 0.591 trainign loss: 8.8465 avg training loss: 9.6218
batch: [11630/21305] batch time: 1.221 trainign loss: 8.6523 avg training loss: 9.6213
batch: [11640/21305] batch time: 0.428 trainign loss: 7.9153 avg training loss: 9.6208
batch: [11650/21305] batch time: 0.852 trainign loss: 9.3511 avg training loss: 9.6203
batch: [11660/21305] batch time: 0.823 trainign loss: 7.3918 avg training loss: 9.6199
batch: [11670/21305] batch time: 0.195 trainign loss: 8.8133 avg training loss: 9.6194
batch: [11680/21305] batch time: 0.245 trainign loss: 6.5581 avg training loss: 9.6190
batch: [11690/21305] batch time: 0.309 trainign loss: 8.2451 avg training loss: 9.6183
batch: [11700/21305] batch time: 0.777 trainign loss: 8.6713 avg training loss: 9.6175
batch: [11710/21305] batch time: 0.062 trainign loss: 5.0706 avg training loss: 9.6169
batch: [11720/21305] batch time: 0.478 trainign loss: 9.3261 avg training loss: 9.6165
batch: [11730/21305] batch time: 0.062 trainign loss: 6.6235 avg training loss: 9.6157
batch: [11740/21305] batch time: 0.961 trainign loss: 8.0212 avg training loss: 9.6153
batch: [11750/21305] batch time: 0.138 trainign loss: 8.6979 avg training loss: 9.6150
batch: [11760/21305] batch time: 0.335 trainign loss: 8.6405 avg training loss: 9.6147
batch: [11770/21305] batch time: 0.718 trainign loss: 8.9834 avg training loss: 9.6145
batch: [11780/21305] batch time: 0.374 trainign loss: 6.8982 avg training loss: 9.6140
batch: [11790/21305] batch time: 0.058 trainign loss: 0.7398 avg training loss: 9.6127
batch: [11800/21305] batch time: 1.382 trainign loss: 0.0003 avg training loss: 9.6098
batch: [11810/21305] batch time: 0.056 trainign loss: 9.9850 avg training loss: 9.6098
batch: [11820/21305] batch time: 1.915 trainign loss: 9.5230 avg training loss: 9.6099
batch: [11830/21305] batch time: 0.063 trainign loss: 9.6019 avg training loss: 9.6094
batch: [11840/21305] batch time: 1.804 trainign loss: 9.3115 avg training loss: 9.6091
batch: [11850/21305] batch time: 0.063 trainign loss: 8.3710 avg training loss: 9.6082
batch: [11860/21305] batch time: 1.861 trainign loss: 4.0994 avg training loss: 9.6077
batch: [11870/21305] batch time: 0.062 trainign loss: 7.9494 avg training loss: 9.6070
batch: [11880/21305] batch time: 1.783 trainign loss: 10.3945 avg training loss: 9.6057
batch: [11890/21305] batch time: 0.053 trainign loss: 8.9739 avg training loss: 9.6057
batch: [11900/21305] batch time: 1.012 trainign loss: 7.6875 avg training loss: 9.6054
batch: [11910/21305] batch time: 0.369 trainign loss: 9.2375 avg training loss: 9.6050
batch: [11920/21305] batch time: 0.924 trainign loss: 7.8899 avg training loss: 9.6047
batch: [11930/21305] batch time: 0.056 trainign loss: 6.7057 avg training loss: 9.6036
batch: [11940/21305] batch time: 1.225 trainign loss: 9.0896 avg training loss: 9.6035
batch: [11950/21305] batch time: 0.052 trainign loss: 8.6240 avg training loss: 9.6032
batch: [11960/21305] batch time: 0.149 trainign loss: 9.5538 avg training loss: 9.6026
batch: [11970/21305] batch time: 0.511 trainign loss: 8.3163 avg training loss: 9.6023
batch: [11980/21305] batch time: 0.063 trainign loss: 8.5119 avg training loss: 9.6018
batch: [11990/21305] batch time: 0.312 trainign loss: 6.8347 avg training loss: 9.6015
batch: [12000/21305] batch time: 1.062 trainign loss: 8.4609 avg training loss: 9.6010
batch: [12010/21305] batch time: 0.061 trainign loss: 9.1170 avg training loss: 9.6006
batch: [12020/21305] batch time: 0.620 trainign loss: 8.7900 avg training loss: 9.6003
batch: [12030/21305] batch time: 0.056 trainign loss: 7.4727 avg training loss: 9.6000
batch: [12040/21305] batch time: 0.533 trainign loss: 7.4868 avg training loss: 9.5995
batch: [12050/21305] batch time: 0.062 trainign loss: 8.1549 avg training loss: 9.5991
batch: [12060/21305] batch time: 0.265 trainign loss: 9.2934 avg training loss: 9.5986
batch: [12070/21305] batch time: 0.062 trainign loss: 4.4002 avg training loss: 9.5981
batch: [12080/21305] batch time: 0.056 trainign loss: 7.8719 avg training loss: 9.5964
batch: [12090/21305] batch time: 0.062 trainign loss: 7.2555 avg training loss: 9.5962
batch: [12100/21305] batch time: 0.056 trainign loss: 9.2241 avg training loss: 9.5960
batch: [12110/21305] batch time: 0.062 trainign loss: 9.1780 avg training loss: 9.5959
batch: [12120/21305] batch time: 0.060 trainign loss: 4.9230 avg training loss: 9.5950
batch: [12130/21305] batch time: 0.740 trainign loss: 10.2538 avg training loss: 9.5945
batch: [12140/21305] batch time: 0.056 trainign loss: 7.2690 avg training loss: 9.5933
batch: [12150/21305] batch time: 0.661 trainign loss: 9.1053 avg training loss: 9.5932
batch: [12160/21305] batch time: 0.061 trainign loss: 9.1776 avg training loss: 9.5930
batch: [12170/21305] batch time: 0.343 trainign loss: 8.7374 avg training loss: 9.5928
batch: [12180/21305] batch time: 0.057 trainign loss: 8.4680 avg training loss: 9.5926
batch: [12190/21305] batch time: 1.810 trainign loss: 8.8263 avg training loss: 9.5924
batch: [12200/21305] batch time: 0.054 trainign loss: 7.9042 avg training loss: 9.5919
batch: [12210/21305] batch time: 1.612 trainign loss: 9.2231 avg training loss: 9.5916
batch: [12220/21305] batch time: 0.062 trainign loss: 8.6306 avg training loss: 9.5913
batch: [12230/21305] batch time: 2.085 trainign loss: 9.1635 avg training loss: 9.5910
batch: [12240/21305] batch time: 0.057 trainign loss: 8.0658 avg training loss: 9.5906
batch: [12250/21305] batch time: 2.164 trainign loss: 7.8189 avg training loss: 9.5901
batch: [12260/21305] batch time: 0.058 trainign loss: 8.8271 avg training loss: 9.5898
batch: [12270/21305] batch time: 2.072 trainign loss: 7.0999 avg training loss: 9.5894
batch: [12280/21305] batch time: 0.052 trainign loss: 6.0346 avg training loss: 9.5884
batch: [12290/21305] batch time: 2.163 trainign loss: 7.9672 avg training loss: 9.5881
batch: [12300/21305] batch time: 0.059 trainign loss: 9.2324 avg training loss: 9.5878
batch: [12310/21305] batch time: 1.883 trainign loss: 8.9474 avg training loss: 9.5876
batch: [12320/21305] batch time: 0.058 trainign loss: 8.4879 avg training loss: 9.5872
batch: [12330/21305] batch time: 0.973 trainign loss: 8.7813 avg training loss: 9.5869
batch: [12340/21305] batch time: 0.056 trainign loss: 8.4030 avg training loss: 9.5865
batch: [12350/21305] batch time: 0.053 trainign loss: 8.9150 avg training loss: 9.5861
batch: [12360/21305] batch time: 0.055 trainign loss: 5.5599 avg training loss: 9.5856
batch: [12370/21305] batch time: 0.056 trainign loss: 7.2448 avg training loss: 9.5849
batch: [12380/21305] batch time: 0.051 trainign loss: 8.2491 avg training loss: 9.5844
batch: [12390/21305] batch time: 0.057 trainign loss: 9.3789 avg training loss: 9.5842
batch: [12400/21305] batch time: 0.051 trainign loss: 7.8784 avg training loss: 9.5837
batch: [12410/21305] batch time: 0.533 trainign loss: 9.2752 avg training loss: 9.5833
batch: [12420/21305] batch time: 0.057 trainign loss: 6.2019 avg training loss: 9.5828
batch: [12430/21305] batch time: 0.340 trainign loss: 5.8833 avg training loss: 9.5822
batch: [12440/21305] batch time: 0.056 trainign loss: 9.0040 avg training loss: 9.5814
batch: [12450/21305] batch time: 0.707 trainign loss: 8.8672 avg training loss: 9.5810
batch: [12460/21305] batch time: 0.056 trainign loss: 8.9823 avg training loss: 9.5807
batch: [12470/21305] batch time: 0.651 trainign loss: 7.4120 avg training loss: 9.5801
batch: [12480/21305] batch time: 0.056 trainign loss: 7.0556 avg training loss: 9.5798
batch: [12490/21305] batch time: 0.401 trainign loss: 8.2203 avg training loss: 9.5793
batch: [12500/21305] batch time: 0.059 trainign loss: 9.3366 avg training loss: 9.5783
batch: [12510/21305] batch time: 0.865 trainign loss: 8.9832 avg training loss: 9.5780
batch: [12520/21305] batch time: 0.054 trainign loss: 9.1195 avg training loss: 9.5777
batch: [12530/21305] batch time: 1.606 trainign loss: 9.1462 avg training loss: 9.5773
batch: [12540/21305] batch time: 0.056 trainign loss: 6.2768 avg training loss: 9.5767
batch: [12550/21305] batch time: 2.233 trainign loss: 8.5795 avg training loss: 9.5761
batch: [12560/21305] batch time: 0.056 trainign loss: 4.5245 avg training loss: 9.5755
batch: [12570/21305] batch time: 2.037 trainign loss: 0.0038 avg training loss: 9.5729
batch: [12580/21305] batch time: 0.056 trainign loss: 9.9823 avg training loss: 9.5733
batch: [12590/21305] batch time: 2.110 trainign loss: 6.8371 avg training loss: 9.5731
batch: [12600/21305] batch time: 0.056 trainign loss: 9.9058 avg training loss: 9.5729
batch: [12610/21305] batch time: 2.306 trainign loss: 4.5650 avg training loss: 9.5721
batch: [12620/21305] batch time: 0.056 trainign loss: 8.5497 avg training loss: 9.5720
batch: [12630/21305] batch time: 2.475 trainign loss: 8.0189 avg training loss: 9.5718
batch: [12640/21305] batch time: 0.061 trainign loss: 9.2302 avg training loss: 9.5715
batch: [12650/21305] batch time: 2.238 trainign loss: 7.4810 avg training loss: 9.5712
batch: [12660/21305] batch time: 0.062 trainign loss: 8.4134 avg training loss: 9.5709
batch: [12670/21305] batch time: 2.245 trainign loss: 9.3820 avg training loss: 9.5705
batch: [12680/21305] batch time: 0.052 trainign loss: 5.1200 avg training loss: 9.5700
batch: [12690/21305] batch time: 1.860 trainign loss: 10.3093 avg training loss: 9.5692
batch: [12700/21305] batch time: 0.744 trainign loss: 8.4390 avg training loss: 9.5687
batch: [12710/21305] batch time: 1.535 trainign loss: 8.1417 avg training loss: 9.5683
batch: [12720/21305] batch time: 1.224 trainign loss: 7.0285 avg training loss: 9.5679
batch: [12730/21305] batch time: 1.793 trainign loss: 7.4787 avg training loss: 9.5672
batch: [12740/21305] batch time: 0.337 trainign loss: 5.1291 avg training loss: 9.5668
batch: [12750/21305] batch time: 2.510 trainign loss: 4.9965 avg training loss: 9.5655
batch: [12760/21305] batch time: 1.340 trainign loss: 6.4754 avg training loss: 9.5654
batch: [12770/21305] batch time: 0.056 trainign loss: 8.3300 avg training loss: 9.5649
batch: [12780/21305] batch time: 2.195 trainign loss: 9.4127 avg training loss: 9.5647
batch: [12790/21305] batch time: 0.052 trainign loss: 6.8846 avg training loss: 9.5643
batch: [12800/21305] batch time: 2.153 trainign loss: 8.8631 avg training loss: 9.5640
batch: [12810/21305] batch time: 0.056 trainign loss: 8.7739 avg training loss: 9.5635
batch: [12820/21305] batch time: 1.914 trainign loss: 8.3892 avg training loss: 9.5630
batch: [12830/21305] batch time: 0.701 trainign loss: 5.0076 avg training loss: 9.5625
batch: [12840/21305] batch time: 1.643 trainign loss: 8.1845 avg training loss: 9.5620
batch: [12850/21305] batch time: 1.135 trainign loss: 8.5146 avg training loss: 9.5616
batch: [12860/21305] batch time: 1.370 trainign loss: 9.0965 avg training loss: 9.5612
batch: [12870/21305] batch time: 1.155 trainign loss: 8.5213 avg training loss: 9.5609
batch: [12880/21305] batch time: 1.447 trainign loss: 7.6270 avg training loss: 9.5602
batch: [12890/21305] batch time: 0.823 trainign loss: 7.7979 avg training loss: 9.5600
batch: [12900/21305] batch time: 1.581 trainign loss: 9.0740 avg training loss: 9.5597
batch: [12910/21305] batch time: 0.537 trainign loss: 8.0578 avg training loss: 9.5593
batch: [12920/21305] batch time: 1.201 trainign loss: 8.6071 avg training loss: 9.5589
batch: [12930/21305] batch time: 1.061 trainign loss: 8.0656 avg training loss: 9.5586
batch: [12940/21305] batch time: 1.673 trainign loss: 8.6896 avg training loss: 9.5583
batch: [12950/21305] batch time: 0.199 trainign loss: 6.6594 avg training loss: 9.5572
batch: [12960/21305] batch time: 1.773 trainign loss: 8.0519 avg training loss: 9.5571
batch: [12970/21305] batch time: 0.452 trainign loss: 6.8895 avg training loss: 9.5567
batch: [12980/21305] batch time: 1.323 trainign loss: 9.3732 avg training loss: 9.5561
batch: [12990/21305] batch time: 0.140 trainign loss: 7.8070 avg training loss: 9.5558
batch: [13000/21305] batch time: 1.507 trainign loss: 9.2567 avg training loss: 9.5554
batch: [13010/21305] batch time: 0.492 trainign loss: 9.1071 avg training loss: 9.5548
batch: [13020/21305] batch time: 2.270 trainign loss: 9.4277 avg training loss: 9.5547
batch: [13030/21305] batch time: 0.056 trainign loss: 8.9671 avg training loss: 9.5544
batch: [13040/21305] batch time: 2.225 trainign loss: 9.5407 avg training loss: 9.5539
batch: [13050/21305] batch time: 0.062 trainign loss: 1.9873 avg training loss: 9.5530
batch: [13060/21305] batch time: 2.327 trainign loss: 8.6886 avg training loss: 9.5527
batch: [13070/21305] batch time: 0.059 trainign loss: 9.2134 avg training loss: 9.5524
batch: [13080/21305] batch time: 1.964 trainign loss: 8.7208 avg training loss: 9.5521
batch: [13090/21305] batch time: 0.057 trainign loss: 5.3589 avg training loss: 9.5516
batch: [13100/21305] batch time: 2.387 trainign loss: 10.4442 avg training loss: 9.5507
batch: [13110/21305] batch time: 0.060 trainign loss: 9.1172 avg training loss: 9.5499
batch: [13120/21305] batch time: 2.385 trainign loss: 8.7201 avg training loss: 9.5498
batch: [13130/21305] batch time: 0.053 trainign loss: 8.0121 avg training loss: 9.5496
batch: [13140/21305] batch time: 2.505 trainign loss: 3.1412 avg training loss: 9.5489
batch: [13150/21305] batch time: 0.055 trainign loss: 8.7898 avg training loss: 9.5483
batch: [13160/21305] batch time: 2.097 trainign loss: 6.1999 avg training loss: 9.5480
batch: [13170/21305] batch time: 0.052 trainign loss: 8.9636 avg training loss: 9.5475
batch: [13180/21305] batch time: 1.920 trainign loss: 6.3614 avg training loss: 9.5470
batch: [13190/21305] batch time: 0.060 trainign loss: 8.9038 avg training loss: 9.5467
batch: [13200/21305] batch time: 2.338 trainign loss: 4.2612 avg training loss: 9.5457
batch: [13210/21305] batch time: 0.056 trainign loss: 0.0034 avg training loss: 9.5432
batch: [13220/21305] batch time: 1.804 trainign loss: 0.0000 avg training loss: 9.5405
batch: [13230/21305] batch time: 0.053 trainign loss: 0.0000 avg training loss: 9.5377
batch: [13240/21305] batch time: 2.052 trainign loss: 10.3670 avg training loss: 9.5382
batch: [13250/21305] batch time: 0.056 trainign loss: 10.0417 avg training loss: 9.5384
batch: [13260/21305] batch time: 2.375 trainign loss: 9.9395 avg training loss: 9.5385
batch: [13270/21305] batch time: 0.063 trainign loss: 9.0942 avg training loss: 9.5384
batch: [13280/21305] batch time: 2.309 trainign loss: 9.4526 avg training loss: 9.5381
batch: [13290/21305] batch time: 0.058 trainign loss: 8.7988 avg training loss: 9.5377
batch: [13300/21305] batch time: 2.270 trainign loss: 9.2246 avg training loss: 9.5373
batch: [13310/21305] batch time: 0.061 trainign loss: 6.1278 avg training loss: 9.5368
batch: [13320/21305] batch time: 2.735 trainign loss: 7.2830 avg training loss: 9.5354
batch: [13330/21305] batch time: 0.056 trainign loss: 8.0106 avg training loss: 9.5353
batch: [13340/21305] batch time: 2.272 trainign loss: 9.4332 avg training loss: 9.5351
batch: [13350/21305] batch time: 0.056 trainign loss: 7.7059 avg training loss: 9.5349
batch: [13360/21305] batch time: 2.081 trainign loss: 9.6942 avg training loss: 9.5346
batch: [13370/21305] batch time: 0.056 trainign loss: 8.0128 avg training loss: 9.5343
batch: [13380/21305] batch time: 2.139 trainign loss: 5.5228 avg training loss: 9.5337
batch: [13390/21305] batch time: 0.056 trainign loss: 8.4915 avg training loss: 9.5332
batch: [13400/21305] batch time: 2.201 trainign loss: 5.0884 avg training loss: 9.5326
batch: [13410/21305] batch time: 0.063 trainign loss: 8.6792 avg training loss: 9.5325
batch: [13420/21305] batch time: 2.102 trainign loss: 8.7331 avg training loss: 9.5323
batch: [13430/21305] batch time: 0.062 trainign loss: 8.7306 avg training loss: 9.5319
batch: [13440/21305] batch time: 2.764 trainign loss: 7.3903 avg training loss: 9.5316
batch: [13450/21305] batch time: 0.062 trainign loss: 9.0898 avg training loss: 9.5313
batch: [13460/21305] batch time: 2.595 trainign loss: 8.5676 avg training loss: 9.5311
batch: [13470/21305] batch time: 0.056 trainign loss: 3.3415 avg training loss: 9.5302
batch: [13480/21305] batch time: 2.246 trainign loss: 5.3527 avg training loss: 9.5297
batch: [13490/21305] batch time: 0.061 trainign loss: 8.7655 avg training loss: 9.5293
batch: [13500/21305] batch time: 1.865 trainign loss: 8.8239 avg training loss: 9.5287
batch: [13510/21305] batch time: 0.508 trainign loss: 6.7896 avg training loss: 9.5282
batch: [13520/21305] batch time: 1.997 trainign loss: 7.5900 avg training loss: 9.5278
batch: [13530/21305] batch time: 0.594 trainign loss: 8.1601 avg training loss: 9.5273
batch: [13540/21305] batch time: 1.494 trainign loss: 3.1249 avg training loss: 9.5266
batch: [13550/21305] batch time: 0.289 trainign loss: 8.4778 avg training loss: 9.5263
batch: [13560/21305] batch time: 2.343 trainign loss: 8.3082 avg training loss: 9.5260
batch: [13570/21305] batch time: 0.061 trainign loss: 9.4231 avg training loss: 9.5256
batch: [13580/21305] batch time: 2.268 trainign loss: 9.2177 avg training loss: 9.5254
batch: [13590/21305] batch time: 0.058 trainign loss: 5.1055 avg training loss: 9.5249
batch: [13600/21305] batch time: 1.932 trainign loss: 8.5534 avg training loss: 9.5244
batch: [13610/21305] batch time: 0.224 trainign loss: 9.4448 avg training loss: 9.5240
batch: [13620/21305] batch time: 0.915 trainign loss: 8.1808 avg training loss: 9.5236
batch: [13630/21305] batch time: 1.307 trainign loss: 8.4395 avg training loss: 9.5231
batch: [13640/21305] batch time: 0.239 trainign loss: 8.4038 avg training loss: 9.5229
batch: [13650/21305] batch time: 0.062 trainign loss: 8.1761 avg training loss: 9.5225
batch: [13660/21305] batch time: 0.642 trainign loss: 7.5843 avg training loss: 9.5218
batch: [13670/21305] batch time: 0.056 trainign loss: 8.2124 avg training loss: 9.5216
batch: [13680/21305] batch time: 1.010 trainign loss: 7.1717 avg training loss: 9.5210
batch: [13690/21305] batch time: 0.608 trainign loss: 4.3712 avg training loss: 9.5203
batch: [13700/21305] batch time: 0.064 trainign loss: 9.1035 avg training loss: 9.5200
batch: [13710/21305] batch time: 0.812 trainign loss: 8.0222 avg training loss: 9.5197
batch: [13720/21305] batch time: 0.056 trainign loss: 8.4403 avg training loss: 9.5193
batch: [13730/21305] batch time: 1.705 trainign loss: 9.2497 avg training loss: 9.5192
batch: [13740/21305] batch time: 0.056 trainign loss: 6.9647 avg training loss: 9.5185
batch: [13750/21305] batch time: 1.748 trainign loss: 9.3716 avg training loss: 9.5180
batch: [13760/21305] batch time: 0.056 trainign loss: 8.8972 avg training loss: 9.5178
batch: [13770/21305] batch time: 1.983 trainign loss: 9.0235 avg training loss: 9.5175
batch: [13780/21305] batch time: 0.056 trainign loss: 7.1842 avg training loss: 9.5167
batch: [13790/21305] batch time: 2.376 trainign loss: 8.2379 avg training loss: 9.5166
batch: [13800/21305] batch time: 0.056 trainign loss: 7.2723 avg training loss: 9.5161
batch: [13810/21305] batch time: 2.111 trainign loss: 8.8973 avg training loss: 9.5156
batch: [13820/21305] batch time: 0.052 trainign loss: 9.4664 avg training loss: 9.5155
batch: [13830/21305] batch time: 1.901 trainign loss: 7.9224 avg training loss: 9.5150
batch: [13840/21305] batch time: 0.063 trainign loss: 8.3943 avg training loss: 9.5146
batch: [13850/21305] batch time: 1.478 trainign loss: 0.4449 avg training loss: 9.5132
batch: [13860/21305] batch time: 0.270 trainign loss: 8.1873 avg training loss: 9.5129
batch: [13870/21305] batch time: 1.679 trainign loss: 9.6236 avg training loss: 9.5126
batch: [13880/21305] batch time: 0.059 trainign loss: 8.3094 avg training loss: 9.5124
batch: [13890/21305] batch time: 1.318 trainign loss: 8.9688 avg training loss: 9.5120
batch: [13900/21305] batch time: 0.248 trainign loss: 4.8635 avg training loss: 9.5116
batch: [13910/21305] batch time: 1.340 trainign loss: 9.7433 avg training loss: 9.5108
batch: [13920/21305] batch time: 0.062 trainign loss: 5.4490 avg training loss: 9.5101
batch: [13930/21305] batch time: 1.392 trainign loss: 8.6514 avg training loss: 9.5092
batch: [13940/21305] batch time: 0.056 trainign loss: 8.9998 avg training loss: 9.5090
batch: [13950/21305] batch time: 0.225 trainign loss: 5.6250 avg training loss: 9.5085
batch: [13960/21305] batch time: 0.050 trainign loss: 7.6370 avg training loss: 9.5082
batch: [13970/21305] batch time: 0.267 trainign loss: 8.8519 avg training loss: 9.5080
batch: [13980/21305] batch time: 0.061 trainign loss: 8.8776 avg training loss: 9.5076
batch: [13990/21305] batch time: 0.065 trainign loss: 8.8977 avg training loss: 9.5072
batch: [14000/21305] batch time: 0.373 trainign loss: 8.1831 avg training loss: 9.5065
batch: [14010/21305] batch time: 0.871 trainign loss: 6.2176 avg training loss: 9.5056
batch: [14020/21305] batch time: 1.206 trainign loss: 9.0088 avg training loss: 9.5048
batch: [14030/21305] batch time: 0.927 trainign loss: 8.0404 avg training loss: 9.5045
batch: [14040/21305] batch time: 0.733 trainign loss: 9.2986 avg training loss: 9.5041
batch: [14050/21305] batch time: 0.556 trainign loss: 8.8962 avg training loss: 9.5038
batch: [14060/21305] batch time: 0.254 trainign loss: 8.5251 avg training loss: 9.5036
batch: [14070/21305] batch time: 1.102 trainign loss: 8.9443 avg training loss: 9.5032
batch: [14080/21305] batch time: 0.060 trainign loss: 7.9351 avg training loss: 9.5030
batch: [14090/21305] batch time: 1.284 trainign loss: 8.5422 avg training loss: 9.5026
batch: [14100/21305] batch time: 0.057 trainign loss: 8.4823 avg training loss: 9.5022
batch: [14110/21305] batch time: 0.819 trainign loss: 8.1306 avg training loss: 9.5019
batch: [14120/21305] batch time: 0.058 trainign loss: 9.8481 avg training loss: 9.5013
batch: [14130/21305] batch time: 1.052 trainign loss: 8.1741 avg training loss: 9.5004
batch: [14140/21305] batch time: 0.053 trainign loss: 7.9117 avg training loss: 9.4999
batch: [14150/21305] batch time: 1.706 trainign loss: 8.9718 avg training loss: 9.4995
batch: [14160/21305] batch time: 0.056 trainign loss: 8.9363 avg training loss: 9.4992
batch: [14170/21305] batch time: 0.616 trainign loss: 7.8709 avg training loss: 9.4990
batch: [14180/21305] batch time: 0.062 trainign loss: 8.2573 avg training loss: 9.4984
batch: [14190/21305] batch time: 0.479 trainign loss: 2.4836 avg training loss: 9.4974
batch: [14200/21305] batch time: 0.056 trainign loss: 8.9908 avg training loss: 9.4972
batch: [14210/21305] batch time: 0.056 trainign loss: 8.4040 avg training loss: 9.4969
batch: [14220/21305] batch time: 0.712 trainign loss: 11.0167 avg training loss: 9.4958
batch: [14230/21305] batch time: 0.056 trainign loss: 7.6150 avg training loss: 9.4957
batch: [14240/21305] batch time: 1.405 trainign loss: 9.4674 avg training loss: 9.4955
batch: [14250/21305] batch time: 0.056 trainign loss: 9.1501 avg training loss: 9.4955
batch: [14260/21305] batch time: 1.911 trainign loss: 6.1728 avg training loss: 9.4950
batch: [14270/21305] batch time: 0.057 trainign loss: 6.1179 avg training loss: 9.4945
batch: [14280/21305] batch time: 1.305 trainign loss: 5.3174 avg training loss: 9.4939
batch: [14290/21305] batch time: 0.056 trainign loss: 8.1705 avg training loss: 9.4934
batch: [14300/21305] batch time: 1.141 trainign loss: 8.7155 avg training loss: 9.4930
batch: [14310/21305] batch time: 0.056 trainign loss: 8.7747 avg training loss: 9.4926
batch: [14320/21305] batch time: 0.767 trainign loss: 2.8780 avg training loss: 9.4918
batch: [14330/21305] batch time: 0.056 trainign loss: 9.0496 avg training loss: 9.4914
batch: [14340/21305] batch time: 2.052 trainign loss: 7.2801 avg training loss: 9.4910
batch: [14350/21305] batch time: 0.062 trainign loss: 6.9659 avg training loss: 9.4908
batch: [14360/21305] batch time: 1.235 trainign loss: 8.0604 avg training loss: 9.4904
batch: [14370/21305] batch time: 0.062 trainign loss: 7.8509 avg training loss: 9.4902
batch: [14380/21305] batch time: 2.025 trainign loss: 8.2440 avg training loss: 9.4900
batch: [14390/21305] batch time: 0.515 trainign loss: 8.5850 avg training loss: 9.4896
batch: [14400/21305] batch time: 1.862 trainign loss: 7.5919 avg training loss: 9.4893
batch: [14410/21305] batch time: 0.053 trainign loss: 8.6908 avg training loss: 9.4887
batch: [14420/21305] batch time: 2.376 trainign loss: 8.3255 avg training loss: 9.4881
batch: [14430/21305] batch time: 0.056 trainign loss: 7.9628 avg training loss: 9.4877
batch: [14440/21305] batch time: 2.063 trainign loss: 8.6998 avg training loss: 9.4875
batch: [14450/21305] batch time: 0.055 trainign loss: 7.2954 avg training loss: 9.4871
batch: [14460/21305] batch time: 2.517 trainign loss: 9.1210 avg training loss: 9.4868
batch: [14470/21305] batch time: 0.056 trainign loss: 7.9962 avg training loss: 9.4863
batch: [14480/21305] batch time: 1.822 trainign loss: 4.6606 avg training loss: 9.4858
batch: [14490/21305] batch time: 0.602 trainign loss: 7.7419 avg training loss: 9.4853
batch: [14500/21305] batch time: 1.081 trainign loss: 2.5300 avg training loss: 9.4846
batch: [14510/21305] batch time: 0.405 trainign loss: 0.0007 avg training loss: 9.4821
batch: [14520/21305] batch time: 1.868 trainign loss: 0.0000 avg training loss: 9.4794
batch: [14530/21305] batch time: 0.062 trainign loss: 0.0000 avg training loss: 9.4768
batch: [14540/21305] batch time: 1.877 trainign loss: 0.0001 avg training loss: 9.4741
batch: [14550/21305] batch time: 0.062 trainign loss: 0.0000 avg training loss: 9.4715
batch: [14560/21305] batch time: 1.671 trainign loss: 0.0000 avg training loss: 9.4688
batch: [14570/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 9.4662
batch: [14580/21305] batch time: 1.688 trainign loss: 0.0001 avg training loss: 9.4636
batch: [14590/21305] batch time: 0.057 trainign loss: 0.0000 avg training loss: 9.4609
batch: [14600/21305] batch time: 2.681 trainign loss: 13.1492 avg training loss: 9.4608
batch: [14610/21305] batch time: 0.060 trainign loss: 10.2995 avg training loss: 9.4611
batch: [14620/21305] batch time: 2.506 trainign loss: 9.4330 avg training loss: 9.4612
batch: [14630/21305] batch time: 0.057 trainign loss: 8.0879 avg training loss: 9.4608
batch: [14640/21305] batch time: 2.540 trainign loss: 7.0875 avg training loss: 9.4605
batch: [14650/21305] batch time: 0.062 trainign loss: 8.8620 avg training loss: 9.4603
batch: [14660/21305] batch time: 2.018 trainign loss: 8.0483 avg training loss: 9.4599
batch: [14670/21305] batch time: 0.052 trainign loss: 6.8562 avg training loss: 9.4596
batch: [14680/21305] batch time: 2.568 trainign loss: 2.7591 avg training loss: 9.4587
batch: [14690/21305] batch time: 0.051 trainign loss: 8.3888 avg training loss: 9.4585
batch: [14700/21305] batch time: 2.423 trainign loss: 5.2968 avg training loss: 9.4581
batch: [14710/21305] batch time: 0.056 trainign loss: 0.0047 avg training loss: 9.4558
batch: [14720/21305] batch time: 2.283 trainign loss: 12.7912 avg training loss: 9.4558
batch: [14730/21305] batch time: 0.056 trainign loss: 8.1868 avg training loss: 9.4557
batch: [14740/21305] batch time: 2.282 trainign loss: 7.2264 avg training loss: 9.4555
batch: [14750/21305] batch time: 0.063 trainign loss: 9.4766 avg training loss: 9.4553
batch: [14760/21305] batch time: 2.238 trainign loss: 8.6363 avg training loss: 9.4550
batch: [14770/21305] batch time: 0.061 trainign loss: 8.5770 avg training loss: 9.4548
batch: [14780/21305] batch time: 1.840 trainign loss: 8.3124 avg training loss: 9.4547
batch: [14790/21305] batch time: 0.387 trainign loss: 9.1029 avg training loss: 9.4544
batch: [14800/21305] batch time: 2.515 trainign loss: 8.7711 avg training loss: 9.4542
batch: [14810/21305] batch time: 0.057 trainign loss: 7.8472 avg training loss: 9.4536
batch: [14820/21305] batch time: 1.960 trainign loss: 7.6919 avg training loss: 9.4534
batch: [14830/21305] batch time: 0.052 trainign loss: 7.9973 avg training loss: 9.4531
batch: [14840/21305] batch time: 1.619 trainign loss: 6.6370 avg training loss: 9.4527
batch: [14850/21305] batch time: 0.063 trainign loss: 9.1868 avg training loss: 9.4524
batch: [14860/21305] batch time: 2.325 trainign loss: 9.2154 avg training loss: 9.4521
batch: [14870/21305] batch time: 0.063 trainign loss: 8.0611 avg training loss: 9.4519
batch: [14880/21305] batch time: 2.251 trainign loss: 8.7748 avg training loss: 9.4518
batch: [14890/21305] batch time: 0.060 trainign loss: 8.0503 avg training loss: 9.4513
batch: [14900/21305] batch time: 2.361 trainign loss: 8.3077 avg training loss: 9.4511
batch: [14910/21305] batch time: 0.056 trainign loss: 3.5623 avg training loss: 9.4504
batch: [14920/21305] batch time: 2.346 trainign loss: 7.1250 avg training loss: 9.4502
batch: [14930/21305] batch time: 0.056 trainign loss: 8.0359 avg training loss: 9.4496
batch: [14940/21305] batch time: 2.029 trainign loss: 9.1031 avg training loss: 9.4492
batch: [14950/21305] batch time: 0.061 trainign loss: 9.2567 avg training loss: 9.4490
batch: [14960/21305] batch time: 2.374 trainign loss: 9.6718 avg training loss: 9.4483
batch: [14970/21305] batch time: 0.057 trainign loss: 9.6672 avg training loss: 9.4479
batch: [14980/21305] batch time: 2.658 trainign loss: 9.5403 avg training loss: 9.4477
batch: [14990/21305] batch time: 0.056 trainign loss: 7.2335 avg training loss: 9.4472
batch: [15000/21305] batch time: 1.949 trainign loss: 8.2289 avg training loss: 9.4469
batch: [15010/21305] batch time: 0.053 trainign loss: 9.2220 avg training loss: 9.4468
batch: [15020/21305] batch time: 2.073 trainign loss: 7.3073 avg training loss: 9.4463
batch: [15030/21305] batch time: 0.062 trainign loss: 9.1632 avg training loss: 9.4460
batch: [15040/21305] batch time: 2.082 trainign loss: 2.7302 avg training loss: 9.4452
batch: [15050/21305] batch time: 0.058 trainign loss: 7.3817 avg training loss: 9.4452
batch: [15060/21305] batch time: 1.984 trainign loss: 8.7082 avg training loss: 9.4447
batch: [15070/21305] batch time: 0.056 trainign loss: 8.7475 avg training loss: 9.4444
batch: [15080/21305] batch time: 2.309 trainign loss: 5.3852 avg training loss: 9.4429
batch: [15090/21305] batch time: 0.057 trainign loss: 8.1148 avg training loss: 9.4431
batch: [15100/21305] batch time: 2.226 trainign loss: 7.4170 avg training loss: 9.4429
batch: [15110/21305] batch time: 0.061 trainign loss: 3.8478 avg training loss: 9.4423
batch: [15120/21305] batch time: 1.944 trainign loss: 9.6652 avg training loss: 9.4421
batch: [15130/21305] batch time: 0.055 trainign loss: 8.3097 avg training loss: 9.4413
batch: [15140/21305] batch time: 2.130 trainign loss: 9.7454 avg training loss: 9.4413
batch: [15150/21305] batch time: 0.063 trainign loss: 8.8821 avg training loss: 9.4411
batch: [15160/21305] batch time: 1.820 trainign loss: 8.8835 avg training loss: 9.4409
batch: [15170/21305] batch time: 0.056 trainign loss: 8.6744 avg training loss: 9.4405
batch: [15180/21305] batch time: 1.854 trainign loss: 7.8670 avg training loss: 9.4401
batch: [15190/21305] batch time: 0.056 trainign loss: 8.9266 avg training loss: 9.4398
batch: [15200/21305] batch time: 2.024 trainign loss: 8.9942 avg training loss: 9.4397
batch: [15210/21305] batch time: 0.062 trainign loss: 7.8925 avg training loss: 9.4389
batch: [15220/21305] batch time: 2.786 trainign loss: 8.0851 avg training loss: 9.4387
batch: [15230/21305] batch time: 0.053 trainign loss: 9.0557 avg training loss: 9.4384
batch: [15240/21305] batch time: 2.057 trainign loss: 8.8049 avg training loss: 9.4383
batch: [15250/21305] batch time: 0.063 trainign loss: 9.4265 avg training loss: 9.4381
batch: [15260/21305] batch time: 2.601 trainign loss: 6.9606 avg training loss: 9.4376
batch: [15270/21305] batch time: 0.062 trainign loss: 9.6666 avg training loss: 9.4371
batch: [15280/21305] batch time: 2.578 trainign loss: 6.3840 avg training loss: 9.4368
batch: [15290/21305] batch time: 0.056 trainign loss: 9.6344 avg training loss: 9.4363
batch: [15300/21305] batch time: 2.250 trainign loss: 8.5184 avg training loss: 9.4360
batch: [15310/21305] batch time: 0.057 trainign loss: 8.0179 avg training loss: 9.4351
batch: [15320/21305] batch time: 2.335 trainign loss: 9.2295 avg training loss: 9.4349
batch: [15330/21305] batch time: 0.057 trainign loss: 7.8254 avg training loss: 9.4347
batch: [15340/21305] batch time: 2.444 trainign loss: 6.4131 avg training loss: 9.4344
batch: [15350/21305] batch time: 0.056 trainign loss: 8.1952 avg training loss: 9.4341
batch: [15360/21305] batch time: 2.500 trainign loss: 9.4217 avg training loss: 9.4337
batch: [15370/21305] batch time: 0.055 trainign loss: 6.5292 avg training loss: 9.4329
batch: [15380/21305] batch time: 2.323 trainign loss: 7.5339 avg training loss: 9.4327
batch: [15390/21305] batch time: 0.057 trainign loss: 8.3065 avg training loss: 9.4324
batch: [15400/21305] batch time: 2.439 trainign loss: 9.1375 avg training loss: 9.4322
batch: [15410/21305] batch time: 0.062 trainign loss: 8.9490 avg training loss: 9.4321
batch: [15420/21305] batch time: 2.180 trainign loss: 7.7778 avg training loss: 9.4319
batch: [15430/21305] batch time: 0.062 trainign loss: 7.5714 avg training loss: 9.4314
batch: [15440/21305] batch time: 1.948 trainign loss: 9.1962 avg training loss: 9.4312
batch: [15450/21305] batch time: 0.056 trainign loss: 8.4466 avg training loss: 9.4307
batch: [15460/21305] batch time: 2.117 trainign loss: 8.6375 avg training loss: 9.4304
batch: [15470/21305] batch time: 0.053 trainign loss: 8.1262 avg training loss: 9.4300
batch: [15480/21305] batch time: 1.123 trainign loss: 8.7820 avg training loss: 9.4298
batch: [15490/21305] batch time: 0.057 trainign loss: 8.1567 avg training loss: 9.4295
batch: [15500/21305] batch time: 1.175 trainign loss: 5.8229 avg training loss: 9.4291
batch: [15510/21305] batch time: 0.063 trainign loss: 4.7327 avg training loss: 9.4284
batch: [15520/21305] batch time: 0.578 trainign loss: 10.7180 avg training loss: 9.4276
batch: [15530/21305] batch time: 0.057 trainign loss: 9.3048 avg training loss: 9.4273
batch: [15540/21305] batch time: 0.062 trainign loss: 9.5078 avg training loss: 9.4273
batch: [15550/21305] batch time: 0.056 trainign loss: 7.2848 avg training loss: 9.4271
batch: [15560/21305] batch time: 0.524 trainign loss: 9.8733 avg training loss: 9.4267
batch: [15570/21305] batch time: 0.056 trainign loss: 7.1762 avg training loss: 9.4266
batch: [15580/21305] batch time: 0.058 trainign loss: 8.2097 avg training loss: 9.4263
batch: [15590/21305] batch time: 0.057 trainign loss: 7.4438 avg training loss: 9.4260
batch: [15600/21305] batch time: 0.058 trainign loss: 8.0643 avg training loss: 9.4254
batch: [15610/21305] batch time: 0.051 trainign loss: 8.3503 avg training loss: 9.4250
batch: [15620/21305] batch time: 0.052 trainign loss: 7.2478 avg training loss: 9.4248
batch: [15630/21305] batch time: 0.051 trainign loss: 7.5684 avg training loss: 9.4245
batch: [15640/21305] batch time: 0.063 trainign loss: 9.1095 avg training loss: 9.4242
batch: [15650/21305] batch time: 0.053 trainign loss: 6.4776 avg training loss: 9.4239
batch: [15660/21305] batch time: 0.514 trainign loss: 6.6665 avg training loss: 9.4235
batch: [15670/21305] batch time: 0.062 trainign loss: 8.4387 avg training loss: 9.4230
batch: [15680/21305] batch time: 1.118 trainign loss: 8.8018 avg training loss: 9.4227
batch: [15690/21305] batch time: 0.056 trainign loss: 8.5955 avg training loss: 9.4224
batch: [15700/21305] batch time: 1.829 trainign loss: 8.6144 avg training loss: 9.4221
batch: [15710/21305] batch time: 0.057 trainign loss: 8.7961 avg training loss: 9.4215
batch: [15720/21305] batch time: 1.885 trainign loss: 6.3180 avg training loss: 9.4212
batch: [15730/21305] batch time: 0.062 trainign loss: 0.3535 avg training loss: 9.4197
batch: [15740/21305] batch time: 1.780 trainign loss: 14.8943 avg training loss: 9.4181
batch: [15750/21305] batch time: 0.058 trainign loss: 9.9814 avg training loss: 9.4187
batch: [15760/21305] batch time: 2.145 trainign loss: 9.8226 avg training loss: 9.4188
batch: [15770/21305] batch time: 0.057 trainign loss: 8.6718 avg training loss: 9.4188
batch: [15780/21305] batch time: 1.944 trainign loss: 7.8113 avg training loss: 9.4187
batch: [15790/21305] batch time: 0.056 trainign loss: 7.6884 avg training loss: 9.4185
batch: [15800/21305] batch time: 1.703 trainign loss: 4.4648 avg training loss: 9.4179
batch: [15810/21305] batch time: 0.057 trainign loss: 9.6421 avg training loss: 9.4173
batch: [15820/21305] batch time: 2.183 trainign loss: 7.0355 avg training loss: 9.4170
batch: [15830/21305] batch time: 0.056 trainign loss: 7.7612 avg training loss: 9.4166
batch: [15840/21305] batch time: 1.496 trainign loss: 7.9312 avg training loss: 9.4165
batch: [15850/21305] batch time: 0.056 trainign loss: 5.8706 avg training loss: 9.4161
batch: [15860/21305] batch time: 0.963 trainign loss: 9.9079 avg training loss: 9.4159
batch: [15870/21305] batch time: 0.056 trainign loss: 6.9190 avg training loss: 9.4156
batch: [15880/21305] batch time: 1.381 trainign loss: 8.9052 avg training loss: 9.4155
batch: [15890/21305] batch time: 0.056 trainign loss: 8.9703 avg training loss: 9.4153
batch: [15900/21305] batch time: 0.726 trainign loss: 6.7700 avg training loss: 9.4149
batch: [15910/21305] batch time: 0.056 trainign loss: 8.2480 avg training loss: 9.4146
batch: [15920/21305] batch time: 1.333 trainign loss: 8.7314 avg training loss: 9.4144
batch: [15930/21305] batch time: 0.058 trainign loss: 9.3564 avg training loss: 9.4137
batch: [15940/21305] batch time: 1.039 trainign loss: 8.3372 avg training loss: 9.4130
batch: [15950/21305] batch time: 0.061 trainign loss: 8.7271 avg training loss: 9.4128
batch: [15960/21305] batch time: 0.715 trainign loss: 6.6555 avg training loss: 9.4124
batch: [15970/21305] batch time: 0.056 trainign loss: 6.5658 avg training loss: 9.4121
batch: [15980/21305] batch time: 0.737 trainign loss: 7.4210 avg training loss: 9.4118
batch: [15990/21305] batch time: 0.056 trainign loss: 8.6392 avg training loss: 9.4116
batch: [16000/21305] batch time: 0.801 trainign loss: 8.9211 avg training loss: 9.4113
batch: [16010/21305] batch time: 0.057 trainign loss: 0.4750 avg training loss: 9.4101
batch: [16020/21305] batch time: 0.440 trainign loss: 7.4307 avg training loss: 9.4100
batch: [16030/21305] batch time: 0.061 trainign loss: 9.4176 avg training loss: 9.4096
batch: [16040/21305] batch time: 0.654 trainign loss: 6.7074 avg training loss: 9.4093
batch: [16050/21305] batch time: 0.058 trainign loss: 8.5568 avg training loss: 9.4090
batch: [16060/21305] batch time: 1.097 trainign loss: 7.4891 avg training loss: 9.4087
batch: [16070/21305] batch time: 0.063 trainign loss: 9.1956 avg training loss: 9.4085
batch: [16080/21305] batch time: 0.989 trainign loss: 8.7658 avg training loss: 9.4080
batch: [16090/21305] batch time: 0.056 trainign loss: 7.1928 avg training loss: 9.4077
batch: [16100/21305] batch time: 0.443 trainign loss: 6.9406 avg training loss: 9.4073
batch: [16110/21305] batch time: 0.057 trainign loss: 7.0966 avg training loss: 9.4070
batch: [16120/21305] batch time: 0.162 trainign loss: 5.0130 avg training loss: 9.4066
batch: [16130/21305] batch time: 0.056 trainign loss: 8.5127 avg training loss: 9.4061
batch: [16140/21305] batch time: 0.056 trainign loss: 8.5428 avg training loss: 9.4058
batch: [16150/21305] batch time: 0.057 trainign loss: 8.4505 avg training loss: 9.4055
batch: [16160/21305] batch time: 0.365 trainign loss: 5.1099 avg training loss: 9.4049
batch: [16170/21305] batch time: 0.056 trainign loss: 9.2387 avg training loss: 9.4048
batch: [16180/21305] batch time: 0.055 trainign loss: 8.9787 avg training loss: 9.4045
batch: [16190/21305] batch time: 0.057 trainign loss: 9.2509 avg training loss: 9.4044
batch: [16200/21305] batch time: 0.050 trainign loss: 6.6797 avg training loss: 9.4041
batch: [16210/21305] batch time: 0.062 trainign loss: 8.8725 avg training loss: 9.4039
batch: [16220/21305] batch time: 0.056 trainign loss: 8.3671 avg training loss: 9.4037
batch: [16230/21305] batch time: 0.057 trainign loss: 8.6944 avg training loss: 9.4034
batch: [16240/21305] batch time: 0.373 trainign loss: 9.2134 avg training loss: 9.4031
batch: [16250/21305] batch time: 0.056 trainign loss: 8.3438 avg training loss: 9.4027
batch: [16260/21305] batch time: 1.635 trainign loss: 8.8237 avg training loss: 9.4024
batch: [16270/21305] batch time: 0.056 trainign loss: 9.0137 avg training loss: 9.4022
batch: [16280/21305] batch time: 2.052 trainign loss: 9.2470 avg training loss: 9.4019
batch: [16290/21305] batch time: 0.062 trainign loss: 8.3981 avg training loss: 9.4016
batch: [16300/21305] batch time: 2.310 trainign loss: 8.8193 avg training loss: 9.4011
batch: [16310/21305] batch time: 0.056 trainign loss: 8.6799 avg training loss: 9.4007
batch: [16320/21305] batch time: 1.901 trainign loss: 8.9945 avg training loss: 9.4004
batch: [16330/21305] batch time: 0.058 trainign loss: 7.5713 avg training loss: 9.4001
batch: [16340/21305] batch time: 1.849 trainign loss: 7.0482 avg training loss: 9.3994
batch: [16350/21305] batch time: 0.063 trainign loss: 9.0870 avg training loss: 9.3990
batch: [16360/21305] batch time: 1.896 trainign loss: 8.7045 avg training loss: 9.3982
batch: [16370/21305] batch time: 0.053 trainign loss: 8.1362 avg training loss: 9.3980
batch: [16380/21305] batch time: 2.401 trainign loss: 9.4354 avg training loss: 9.3975
batch: [16390/21305] batch time: 0.056 trainign loss: 7.8437 avg training loss: 9.3973
batch: [16400/21305] batch time: 2.094 trainign loss: 9.1714 avg training loss: 9.3971
batch: [16410/21305] batch time: 0.062 trainign loss: 9.0880 avg training loss: 9.3968
batch: [16420/21305] batch time: 1.438 trainign loss: 8.6074 avg training loss: 9.3966
batch: [16430/21305] batch time: 0.062 trainign loss: 7.8220 avg training loss: 9.3961
batch: [16440/21305] batch time: 1.091 trainign loss: 8.9464 avg training loss: 9.3958
batch: [16450/21305] batch time: 0.056 trainign loss: 9.2365 avg training loss: 9.3955
batch: [16460/21305] batch time: 1.772 trainign loss: 8.8351 avg training loss: 9.3952
batch: [16470/21305] batch time: 0.062 trainign loss: 8.4139 avg training loss: 9.3950
batch: [16480/21305] batch time: 1.955 trainign loss: 7.6839 avg training loss: 9.3946
batch: [16490/21305] batch time: 0.056 trainign loss: 6.8384 avg training loss: 9.3942
batch: [16500/21305] batch time: 1.848 trainign loss: 6.5421 avg training loss: 9.3938
batch: [16510/21305] batch time: 0.056 trainign loss: 8.7296 avg training loss: 9.3935
batch: [16520/21305] batch time: 1.534 trainign loss: 8.1480 avg training loss: 9.3931
batch: [16530/21305] batch time: 0.056 trainign loss: 9.0164 avg training loss: 9.3927
batch: [16540/21305] batch time: 1.442 trainign loss: 8.4797 avg training loss: 9.3918
batch: [16550/21305] batch time: 0.058 trainign loss: 8.2073 avg training loss: 9.3916
batch: [16560/21305] batch time: 1.300 trainign loss: 8.9717 avg training loss: 9.3912
batch: [16570/21305] batch time: 0.056 trainign loss: 7.7420 avg training loss: 9.3909
batch: [16580/21305] batch time: 1.226 trainign loss: 9.2103 avg training loss: 9.3906
batch: [16590/21305] batch time: 0.060 trainign loss: 9.1116 avg training loss: 9.3901
batch: [16600/21305] batch time: 1.936 trainign loss: 8.6570 avg training loss: 9.3898
batch: [16610/21305] batch time: 0.061 trainign loss: 6.4206 avg training loss: 9.3888
batch: [16620/21305] batch time: 2.153 trainign loss: 6.1148 avg training loss: 9.3882
batch: [16630/21305] batch time: 0.056 trainign loss: 7.5582 avg training loss: 9.3875
batch: [16640/21305] batch time: 2.161 trainign loss: 9.4668 avg training loss: 9.3874
batch: [16650/21305] batch time: 0.051 trainign loss: 8.7452 avg training loss: 9.3873
batch: [16660/21305] batch time: 2.104 trainign loss: 9.0042 avg training loss: 9.3871
batch: [16670/21305] batch time: 0.056 trainign loss: 8.8289 avg training loss: 9.3865
batch: [16680/21305] batch time: 2.350 trainign loss: 8.7467 avg training loss: 9.3863
batch: [16690/21305] batch time: 0.056 trainign loss: 8.8681 avg training loss: 9.3860
batch: [16700/21305] batch time: 0.833 trainign loss: 8.6337 avg training loss: 9.3856
batch: [16710/21305] batch time: 0.056 trainign loss: 8.3487 avg training loss: 9.3854
batch: [16720/21305] batch time: 0.331 trainign loss: 8.9821 avg training loss: 9.3851
batch: [16730/21305] batch time: 0.061 trainign loss: 5.9133 avg training loss: 9.3848
batch: [16740/21305] batch time: 0.318 trainign loss: 8.4461 avg training loss: 9.3844
batch: [16750/21305] batch time: 0.057 trainign loss: 8.0028 avg training loss: 9.3842
batch: [16760/21305] batch time: 0.743 trainign loss: 7.1498 avg training loss: 9.3838
batch: [16770/21305] batch time: 0.056 trainign loss: 8.9932 avg training loss: 9.3834
batch: [16780/21305] batch time: 0.777 trainign loss: 7.8297 avg training loss: 9.3832
batch: [16790/21305] batch time: 0.056 trainign loss: 8.6473 avg training loss: 9.3829
batch: [16800/21305] batch time: 0.304 trainign loss: 3.6087 avg training loss: 9.3817
batch: [16810/21305] batch time: 0.056 trainign loss: 8.5354 avg training loss: 9.3816
batch: [16820/21305] batch time: 0.062 trainign loss: 7.4727 avg training loss: 9.3813
batch: [16830/21305] batch time: 0.057 trainign loss: 8.9897 avg training loss: 9.3811
batch: [16840/21305] batch time: 0.056 trainign loss: 9.1136 avg training loss: 9.3807
batch: [16850/21305] batch time: 0.058 trainign loss: 8.4744 avg training loss: 9.3803
batch: [16860/21305] batch time: 0.057 trainign loss: 8.5838 avg training loss: 9.3800
batch: [16870/21305] batch time: 0.055 trainign loss: 8.7008 avg training loss: 9.3793
batch: [16880/21305] batch time: 0.057 trainign loss: 7.8461 avg training loss: 9.3790
batch: [16890/21305] batch time: 0.052 trainign loss: 8.9910 avg training loss: 9.3786
batch: [16900/21305] batch time: 0.057 trainign loss: 4.7358 avg training loss: 9.3783
batch: [16910/21305] batch time: 0.056 trainign loss: 0.0021 avg training loss: 9.3761
batch: [16920/21305] batch time: 0.057 trainign loss: 10.7127 avg training loss: 9.3764
batch: [16930/21305] batch time: 0.057 trainign loss: 7.1095 avg training loss: 9.3763
batch: [16940/21305] batch time: 0.056 trainign loss: 8.2880 avg training loss: 9.3761
batch: [16950/21305] batch time: 0.056 trainign loss: 9.1085 avg training loss: 9.3759
batch: [16960/21305] batch time: 0.063 trainign loss: 4.9816 avg training loss: 9.3755
batch: [16970/21305] batch time: 0.050 trainign loss: 8.1074 avg training loss: 9.3750
batch: [16980/21305] batch time: 0.057 trainign loss: 3.6709 avg training loss: 9.3744
batch: [16990/21305] batch time: 0.056 trainign loss: 4.6949 avg training loss: 9.3736
batch: [17000/21305] batch time: 0.056 trainign loss: 4.0799 avg training loss: 9.3728
batch: [17010/21305] batch time: 0.056 trainign loss: 9.3295 avg training loss: 9.3727
batch: [17020/21305] batch time: 0.056 trainign loss: 1.4399 avg training loss: 9.3718
batch: [17030/21305] batch time: 0.057 trainign loss: 11.0709 avg training loss: 9.3714
batch: [17040/21305] batch time: 0.056 trainign loss: 9.6684 avg training loss: 9.3714
batch: [17050/21305] batch time: 0.056 trainign loss: 9.3470 avg training loss: 9.3713
batch: [17060/21305] batch time: 0.063 trainign loss: 9.4625 avg training loss: 9.3708
batch: [17070/21305] batch time: 0.057 trainign loss: 9.5751 avg training loss: 9.3701
batch: [17080/21305] batch time: 0.056 trainign loss: 8.8027 avg training loss: 9.3700
batch: [17090/21305] batch time: 0.051 trainign loss: 9.0050 avg training loss: 9.3700
batch: [17100/21305] batch time: 0.058 trainign loss: 3.0216 avg training loss: 9.3693
batch: [17110/21305] batch time: 0.053 trainign loss: 9.8861 avg training loss: 9.3694
batch: [17120/21305] batch time: 0.057 trainign loss: 9.0162 avg training loss: 9.3693
batch: [17130/21305] batch time: 0.051 trainign loss: 7.4321 avg training loss: 9.3690
batch: [17140/21305] batch time: 0.056 trainign loss: 7.8846 avg training loss: 9.3688
batch: [17150/21305] batch time: 0.056 trainign loss: 8.8126 avg training loss: 9.3686
batch: [17160/21305] batch time: 0.062 trainign loss: 8.9835 avg training loss: 9.3681
batch: [17170/21305] batch time: 0.052 trainign loss: 6.9567 avg training loss: 9.3672
batch: [17180/21305] batch time: 0.056 trainign loss: 8.8786 avg training loss: 9.3671
batch: [17190/21305] batch time: 0.052 trainign loss: 6.1738 avg training loss: 9.3666
batch: [17200/21305] batch time: 0.056 trainign loss: 8.7744 avg training loss: 9.3663
batch: [17210/21305] batch time: 0.062 trainign loss: 8.6674 avg training loss: 9.3660
batch: [17220/21305] batch time: 0.221 trainign loss: 8.7656 avg training loss: 9.3658
batch: [17230/21305] batch time: 0.057 trainign loss: 5.4018 avg training loss: 9.3651
batch: [17240/21305] batch time: 0.511 trainign loss: 8.9139 avg training loss: 9.3645
batch: [17250/21305] batch time: 0.060 trainign loss: 10.0325 avg training loss: 9.3638
batch: [17260/21305] batch time: 0.062 trainign loss: 9.2870 avg training loss: 9.3638
batch: [17270/21305] batch time: 0.056 trainign loss: 8.6230 avg training loss: 9.3638
batch: [17280/21305] batch time: 0.063 trainign loss: 9.3276 avg training loss: 9.3636
batch: [17290/21305] batch time: 0.053 trainign loss: 5.9163 avg training loss: 9.3634
batch: [17300/21305] batch time: 0.056 trainign loss: 8.8570 avg training loss: 9.3631
batch: [17310/21305] batch time: 0.051 trainign loss: 4.1486 avg training loss: 9.3624
batch: [17320/21305] batch time: 0.062 trainign loss: 15.1122 avg training loss: 9.3606
batch: [17330/21305] batch time: 0.054 trainign loss: 10.1834 avg training loss: 9.3614
batch: [17340/21305] batch time: 0.058 trainign loss: 10.0968 avg training loss: 9.3616
batch: [17350/21305] batch time: 0.061 trainign loss: 9.6395 avg training loss: 9.3611
batch: [17360/21305] batch time: 0.056 trainign loss: 8.4102 avg training loss: 9.3609
batch: [17370/21305] batch time: 0.057 trainign loss: 8.4739 avg training loss: 9.3606
batch: [17380/21305] batch time: 0.056 trainign loss: 8.7170 avg training loss: 9.3605
batch: [17390/21305] batch time: 0.207 trainign loss: 9.1081 avg training loss: 9.3604
batch: [17400/21305] batch time: 0.057 trainign loss: 8.8247 avg training loss: 9.3602
batch: [17410/21305] batch time: 0.062 trainign loss: 8.7016 avg training loss: 9.3600
batch: [17420/21305] batch time: 0.057 trainign loss: 7.1411 avg training loss: 9.3598
batch: [17430/21305] batch time: 0.056 trainign loss: 6.4979 avg training loss: 9.3593
batch: [17440/21305] batch time: 0.056 trainign loss: 8.5362 avg training loss: 9.3590
batch: [17450/21305] batch time: 0.058 trainign loss: 7.9707 avg training loss: 9.3587
batch: [17460/21305] batch time: 0.056 trainign loss: 5.9334 avg training loss: 9.3581
batch: [17470/21305] batch time: 0.063 trainign loss: 8.9864 avg training loss: 9.3578
batch: [17480/21305] batch time: 0.056 trainign loss: 7.3976 avg training loss: 9.3572
batch: [17490/21305] batch time: 0.054 trainign loss: 8.5842 avg training loss: 9.3570
batch: [17500/21305] batch time: 0.059 trainign loss: 8.1604 avg training loss: 9.3569
batch: [17510/21305] batch time: 0.054 trainign loss: 8.8054 avg training loss: 9.3566
batch: [17520/21305] batch time: 0.057 trainign loss: 7.2365 avg training loss: 9.3561
batch: [17530/21305] batch time: 0.052 trainign loss: 6.1714 avg training loss: 9.3558
batch: [17540/21305] batch time: 0.056 trainign loss: 8.3124 avg training loss: 9.3554
batch: [17550/21305] batch time: 0.052 trainign loss: 8.3536 avg training loss: 9.3553
batch: [17560/21305] batch time: 0.058 trainign loss: 8.2023 avg training loss: 9.3550
batch: [17570/21305] batch time: 0.063 trainign loss: 9.5587 avg training loss: 9.3546
batch: [17580/21305] batch time: 0.051 trainign loss: 2.0400 avg training loss: 9.3537
batch: [17590/21305] batch time: 0.062 trainign loss: 12.9542 avg training loss: 9.3517
batch: [17600/21305] batch time: 0.053 trainign loss: 11.3155 avg training loss: 9.3518
batch: [17610/21305] batch time: 0.056 trainign loss: 10.0607 avg training loss: 9.3519
batch: [17620/21305] batch time: 0.062 trainign loss: 7.9463 avg training loss: 9.3518
batch: [17630/21305] batch time: 0.056 trainign loss: 8.5681 avg training loss: 9.3517
batch: [17640/21305] batch time: 0.059 trainign loss: 6.3631 avg training loss: 9.3515
batch: [17650/21305] batch time: 0.056 trainign loss: 6.8163 avg training loss: 9.3512
batch: [17660/21305] batch time: 0.063 trainign loss: 7.6617 avg training loss: 9.3509
batch: [17670/21305] batch time: 0.054 trainign loss: 7.6281 avg training loss: 9.3505
batch: [17680/21305] batch time: 0.063 trainign loss: 8.7272 avg training loss: 9.3500
batch: [17690/21305] batch time: 0.062 trainign loss: 6.6648 avg training loss: 9.3498
batch: [17700/21305] batch time: 0.051 trainign loss: 7.9912 avg training loss: 9.3491
batch: [17710/21305] batch time: 0.056 trainign loss: 7.7135 avg training loss: 9.3489
batch: [17720/21305] batch time: 0.056 trainign loss: 8.2361 avg training loss: 9.3485
batch: [17730/21305] batch time: 0.056 trainign loss: 8.8969 avg training loss: 9.3482
batch: [17740/21305] batch time: 0.056 trainign loss: 7.8201 avg training loss: 9.3480
batch: [17750/21305] batch time: 0.061 trainign loss: 8.0985 avg training loss: 9.3478
batch: [17760/21305] batch time: 0.062 trainign loss: 7.3550 avg training loss: 9.3476
batch: [17770/21305] batch time: 0.052 trainign loss: 8.7404 avg training loss: 9.3473
batch: [17780/21305] batch time: 0.062 trainign loss: 7.9041 avg training loss: 9.3470
batch: [17790/21305] batch time: 0.056 trainign loss: 9.0789 avg training loss: 9.3467
batch: [17800/21305] batch time: 0.057 trainign loss: 7.2303 avg training loss: 9.3463
batch: [17810/21305] batch time: 0.055 trainign loss: 8.8869 avg training loss: 9.3461
batch: [17820/21305] batch time: 0.056 trainign loss: 8.7657 avg training loss: 9.3459
batch: [17830/21305] batch time: 0.056 trainign loss: 7.5288 avg training loss: 9.3456
batch: [17840/21305] batch time: 0.057 trainign loss: 9.2762 avg training loss: 9.3455
batch: [17850/21305] batch time: 0.051 trainign loss: 8.8078 avg training loss: 9.3453
batch: [17860/21305] batch time: 0.051 trainign loss: 8.2760 avg training loss: 9.3447
batch: [17870/21305] batch time: 0.062 trainign loss: 7.0544 avg training loss: 9.3444
batch: [17880/21305] batch time: 0.059 trainign loss: 7.8471 avg training loss: 9.3441
batch: [17890/21305] batch time: 0.721 trainign loss: 8.5249 avg training loss: 9.3438
batch: [17900/21305] batch time: 0.483 trainign loss: 8.6303 avg training loss: 9.3433
batch: [17910/21305] batch time: 1.208 trainign loss: 4.2930 avg training loss: 9.3425
batch: [17920/21305] batch time: 0.940 trainign loss: 8.9280 avg training loss: 9.3425
batch: [17930/21305] batch time: 1.141 trainign loss: 7.6154 avg training loss: 9.3422
batch: [17940/21305] batch time: 0.837 trainign loss: 9.0585 avg training loss: 9.3421
batch: [17950/21305] batch time: 1.270 trainign loss: 9.3466 avg training loss: 9.3416
batch: [17960/21305] batch time: 0.967 trainign loss: 8.4486 avg training loss: 9.3412
batch: [17970/21305] batch time: 1.216 trainign loss: 8.1517 avg training loss: 9.3410
batch: [17980/21305] batch time: 0.769 trainign loss: 1.3090 avg training loss: 9.3400
batch: [17990/21305] batch time: 1.006 trainign loss: 5.9139 avg training loss: 9.3399
batch: [18000/21305] batch time: 1.323 trainign loss: 9.4307 avg training loss: 9.3396
batch: [18010/21305] batch time: 1.281 trainign loss: 9.7145 avg training loss: 9.3394
batch: [18020/21305] batch time: 0.975 trainign loss: 3.6360 avg training loss: 9.3389
batch: [18030/21305] batch time: 1.334 trainign loss: 9.0436 avg training loss: 9.3386
batch: [18040/21305] batch time: 0.348 trainign loss: 7.8298 avg training loss: 9.3384
batch: [18050/21305] batch time: 2.068 trainign loss: 8.5856 avg training loss: 9.3381
batch: [18060/21305] batch time: 0.387 trainign loss: 9.2590 avg training loss: 9.3378
batch: [18070/21305] batch time: 2.070 trainign loss: 9.1035 avg training loss: 9.3377
batch: [18080/21305] batch time: 1.001 trainign loss: 7.4343 avg training loss: 9.3374
batch: [18090/21305] batch time: 1.486 trainign loss: 6.1775 avg training loss: 9.3369
batch: [18100/21305] batch time: 0.654 trainign loss: 7.6612 avg training loss: 9.3364
batch: [18110/21305] batch time: 2.358 trainign loss: 6.5568 avg training loss: 9.3362
batch: [18120/21305] batch time: 0.056 trainign loss: 9.5425 avg training loss: 9.3352
batch: [18130/21305] batch time: 2.432 trainign loss: 9.0892 avg training loss: 9.3353
batch: [18140/21305] batch time: 0.063 trainign loss: 9.3587 avg training loss: 9.3349
batch: [18150/21305] batch time: 2.375 trainign loss: 9.3438 avg training loss: 9.3349
batch: [18160/21305] batch time: 0.054 trainign loss: 7.1707 avg training loss: 9.3345
batch: [18170/21305] batch time: 2.407 trainign loss: 8.5941 avg training loss: 9.3344
batch: [18180/21305] batch time: 0.061 trainign loss: 0.0318 avg training loss: 9.3328
batch: [18190/21305] batch time: 2.418 trainign loss: 9.9918 avg training loss: 9.3329
batch: [18200/21305] batch time: 0.055 trainign loss: 9.7768 avg training loss: 9.3331
batch: [18210/21305] batch time: 2.147 trainign loss: 8.9185 avg training loss: 9.3329
batch: [18220/21305] batch time: 0.061 trainign loss: 5.4078 avg training loss: 9.3323
batch: [18230/21305] batch time: 2.230 trainign loss: 9.2213 avg training loss: 9.3321
batch: [18240/21305] batch time: 0.062 trainign loss: 7.5156 avg training loss: 9.3318
batch: [18250/21305] batch time: 2.096 trainign loss: 7.5795 avg training loss: 9.3316
batch: [18260/21305] batch time: 0.062 trainign loss: 2.3950 avg training loss: 9.3308
batch: [18270/21305] batch time: 1.049 trainign loss: 6.3228 avg training loss: 9.3306
batch: [18280/21305] batch time: 0.062 trainign loss: 8.6992 avg training loss: 9.3299
batch: [18290/21305] batch time: 1.162 trainign loss: 9.2244 avg training loss: 9.3299
batch: [18300/21305] batch time: 0.335 trainign loss: 8.3109 avg training loss: 9.3299
batch: [18310/21305] batch time: 0.695 trainign loss: 8.9604 avg training loss: 9.3295
batch: [18320/21305] batch time: 0.056 trainign loss: 4.9177 avg training loss: 9.3292
batch: [18330/21305] batch time: 0.147 trainign loss: 8.1061 avg training loss: 9.3287
batch: [18340/21305] batch time: 0.160 trainign loss: 8.6970 avg training loss: 9.3285
batch: [18350/21305] batch time: 0.057 trainign loss: 9.3590 avg training loss: 9.3283
batch: [18360/21305] batch time: 0.255 trainign loss: 7.0251 avg training loss: 9.3281
batch: [18370/21305] batch time: 0.058 trainign loss: 9.0486 avg training loss: 9.3277
batch: [18380/21305] batch time: 0.933 trainign loss: 7.3379 avg training loss: 9.3273
batch: [18390/21305] batch time: 0.057 trainign loss: 8.1622 avg training loss: 9.3267
batch: [18400/21305] batch time: 0.880 trainign loss: 8.5264 avg training loss: 9.3266
batch: [18410/21305] batch time: 0.062 trainign loss: 5.9329 avg training loss: 9.3261
batch: [18420/21305] batch time: 0.511 trainign loss: 8.7602 avg training loss: 9.3260
batch: [18430/21305] batch time: 0.057 trainign loss: 9.0392 avg training loss: 9.3257
batch: [18440/21305] batch time: 0.669 trainign loss: 0.0447 avg training loss: 9.3242
batch: [18450/21305] batch time: 0.057 trainign loss: 12.6523 avg training loss: 9.3238
batch: [18460/21305] batch time: 0.845 trainign loss: 9.8647 avg training loss: 9.3241
batch: [18470/21305] batch time: 0.056 trainign loss: 9.4717 avg training loss: 9.3242
batch: [18480/21305] batch time: 0.389 trainign loss: 8.7123 avg training loss: 9.3240
batch: [18490/21305] batch time: 0.062 trainign loss: 3.8841 avg training loss: 9.3235
batch: [18500/21305] batch time: 0.058 trainign loss: 0.0052 avg training loss: 9.3214
batch: [18510/21305] batch time: 0.056 trainign loss: 13.1923 avg training loss: 9.3217
batch: [18520/21305] batch time: 0.766 trainign loss: 10.2571 avg training loss: 9.3219
batch: [18530/21305] batch time: 0.058 trainign loss: 8.9733 avg training loss: 9.3220
batch: [18540/21305] batch time: 0.253 trainign loss: 8.0510 avg training loss: 9.3219
batch: [18550/21305] batch time: 0.062 trainign loss: 5.1495 avg training loss: 9.3214
batch: [18560/21305] batch time: 0.056 trainign loss: 9.3861 avg training loss: 9.3210
batch: [18570/21305] batch time: 0.058 trainign loss: 8.6743 avg training loss: 9.3208
batch: [18580/21305] batch time: 0.056 trainign loss: 8.7621 avg training loss: 9.3204
batch: [18590/21305] batch time: 0.056 trainign loss: 9.1093 avg training loss: 9.3201
batch: [18600/21305] batch time: 0.050 trainign loss: 6.6816 avg training loss: 9.3197
batch: [18610/21305] batch time: 0.056 trainign loss: 8.1313 avg training loss: 9.3189
batch: [18620/21305] batch time: 0.057 trainign loss: 9.2899 avg training loss: 9.3188
batch: [18630/21305] batch time: 0.057 trainign loss: 8.5992 avg training loss: 9.3186
batch: [18640/21305] batch time: 0.057 trainign loss: 7.1741 avg training loss: 9.3185
batch: [18650/21305] batch time: 0.056 trainign loss: 3.3827 avg training loss: 9.3180
batch: [18660/21305] batch time: 0.056 trainign loss: 0.0014 avg training loss: 9.3158
batch: [18670/21305] batch time: 0.063 trainign loss: 13.9315 avg training loss: 9.3157
batch: [18680/21305] batch time: 0.052 trainign loss: 9.9326 avg training loss: 9.3160
batch: [18690/21305] batch time: 0.063 trainign loss: 9.8474 avg training loss: 9.3161
batch: [18700/21305] batch time: 0.057 trainign loss: 9.2356 avg training loss: 9.3161
batch: [18710/21305] batch time: 0.057 trainign loss: 8.9481 avg training loss: 9.3160
batch: [18720/21305] batch time: 0.056 trainign loss: 7.7980 avg training loss: 9.3158
batch: [18730/21305] batch time: 0.058 trainign loss: 6.4089 avg training loss: 9.3156
batch: [18740/21305] batch time: 0.056 trainign loss: 9.7089 avg training loss: 9.3153
batch: [18750/21305] batch time: 0.055 trainign loss: 6.4956 avg training loss: 9.3150
batch: [18760/21305] batch time: 0.058 trainign loss: 5.6423 avg training loss: 9.3146
batch: [18770/21305] batch time: 0.060 trainign loss: 9.7088 avg training loss: 9.3141
batch: [18780/21305] batch time: 0.051 trainign loss: 8.8356 avg training loss: 9.3136
batch: [18790/21305] batch time: 0.056 trainign loss: 7.2728 avg training loss: 9.3135
batch: [18800/21305] batch time: 0.054 trainign loss: 9.4004 avg training loss: 9.3134
batch: [18810/21305] batch time: 0.063 trainign loss: 9.0129 avg training loss: 9.3133
batch: [18820/21305] batch time: 0.052 trainign loss: 5.3717 avg training loss: 9.3129
batch: [18830/21305] batch time: 0.062 trainign loss: 9.7731 avg training loss: 9.3127
batch: [18840/21305] batch time: 0.056 trainign loss: 8.4870 avg training loss: 9.3124
batch: [18850/21305] batch time: 0.056 trainign loss: 9.6300 avg training loss: 9.3118
batch: [18860/21305] batch time: 0.395 trainign loss: 8.8399 avg training loss: 9.3115
batch: [18870/21305] batch time: 0.061 trainign loss: 9.0110 avg training loss: 9.3112
batch: [18880/21305] batch time: 0.290 trainign loss: 5.7180 avg training loss: 9.3104
batch: [18890/21305] batch time: 0.056 trainign loss: 8.4072 avg training loss: 9.3105
batch: [18900/21305] batch time: 0.061 trainign loss: 7.5269 avg training loss: 9.3103
batch: [18910/21305] batch time: 0.056 trainign loss: 9.0559 avg training loss: 9.3102
batch: [18920/21305] batch time: 0.050 trainign loss: 9.3972 avg training loss: 9.3100
batch: [18930/21305] batch time: 0.062 trainign loss: 9.4693 avg training loss: 9.3099
batch: [18940/21305] batch time: 0.056 trainign loss: 7.4960 avg training loss: 9.3098
batch: [18950/21305] batch time: 0.758 trainign loss: 9.2067 avg training loss: 9.3095
batch: [18960/21305] batch time: 0.062 trainign loss: 9.3294 avg training loss: 9.3088
batch: [18970/21305] batch time: 0.316 trainign loss: 8.5069 avg training loss: 9.3086
batch: [18980/21305] batch time: 0.055 trainign loss: 10.8159 avg training loss: 9.3077
batch: [18990/21305] batch time: 0.063 trainign loss: 9.1230 avg training loss: 9.3076
batch: [19000/21305] batch time: 0.057 trainign loss: 8.6594 avg training loss: 9.3073
batch: [19010/21305] batch time: 0.405 trainign loss: 9.5665 avg training loss: 9.3072
batch: [19020/21305] batch time: 0.062 trainign loss: 8.3026 avg training loss: 9.3071
batch: [19030/21305] batch time: 0.782 trainign loss: 9.4870 avg training loss: 9.3065
batch: [19040/21305] batch time: 0.057 trainign loss: 9.0103 avg training loss: 9.3063
batch: [19050/21305] batch time: 0.643 trainign loss: 9.4950 avg training loss: 9.3061
batch: [19060/21305] batch time: 0.056 trainign loss: 7.7471 avg training loss: 9.3053
batch: [19070/21305] batch time: 0.614 trainign loss: 8.2957 avg training loss: 9.3050
batch: [19080/21305] batch time: 0.056 trainign loss: 8.5611 avg training loss: 9.3048
batch: [19090/21305] batch time: 1.451 trainign loss: 9.6168 avg training loss: 9.3046
batch: [19100/21305] batch time: 0.062 trainign loss: 9.4374 avg training loss: 9.3045
batch: [19110/21305] batch time: 2.023 trainign loss: 4.3883 avg training loss: 9.3040
batch: [19120/21305] batch time: 0.061 trainign loss: 8.6198 avg training loss: 9.3039
batch: [19130/21305] batch time: 1.615 trainign loss: 9.0418 avg training loss: 9.3036
batch: [19140/21305] batch time: 0.062 trainign loss: 7.7937 avg training loss: 9.3032
batch: [19150/21305] batch time: 2.234 trainign loss: 7.6118 avg training loss: 9.3030
batch: [19160/21305] batch time: 0.057 trainign loss: 8.8753 avg training loss: 9.3029
batch: [19170/21305] batch time: 2.112 trainign loss: 8.5994 avg training loss: 9.3026
batch: [19180/21305] batch time: 0.054 trainign loss: 5.1245 avg training loss: 9.3023
batch: [19190/21305] batch time: 2.304 trainign loss: 9.2006 avg training loss: 9.3019
batch: [19200/21305] batch time: 0.055 trainign loss: 5.4876 avg training loss: 9.3013
batch: [19210/21305] batch time: 2.312 trainign loss: 8.1780 avg training loss: 9.3010
batch: [19220/21305] batch time: 0.062 trainign loss: 9.0614 avg training loss: 9.3004
batch: [19230/21305] batch time: 2.070 trainign loss: 7.1299 avg training loss: 9.3003
batch: [19240/21305] batch time: 0.056 trainign loss: 7.8154 avg training loss: 9.3001
batch: [19250/21305] batch time: 2.108 trainign loss: 3.9307 avg training loss: 9.2996
batch: [19260/21305] batch time: 0.056 trainign loss: 5.8685 avg training loss: 9.2991
batch: [19270/21305] batch time: 1.626 trainign loss: 8.4729 avg training loss: 9.2987
batch: [19280/21305] batch time: 0.056 trainign loss: 9.6643 avg training loss: 9.2985
batch: [19290/21305] batch time: 1.883 trainign loss: 9.2231 avg training loss: 9.2983
batch: [19300/21305] batch time: 0.057 trainign loss: 9.2524 avg training loss: 9.2982
batch: [19310/21305] batch time: 1.505 trainign loss: 9.3131 avg training loss: 9.2980
batch: [19320/21305] batch time: 0.056 trainign loss: 9.1230 avg training loss: 9.2978
batch: [19330/21305] batch time: 0.637 trainign loss: 8.4097 avg training loss: 9.2976
batch: [19340/21305] batch time: 0.056 trainign loss: 6.3327 avg training loss: 9.2972
batch: [19350/21305] batch time: 1.360 trainign loss: 8.7999 avg training loss: 9.2971
batch: [19360/21305] batch time: 0.056 trainign loss: 7.6732 avg training loss: 9.2968
batch: [19370/21305] batch time: 1.091 trainign loss: 8.5943 avg training loss: 9.2966
batch: [19380/21305] batch time: 0.058 trainign loss: 8.0726 avg training loss: 9.2959
batch: [19390/21305] batch time: 1.230 trainign loss: 9.3470 avg training loss: 9.2958
batch: [19400/21305] batch time: 0.062 trainign loss: 8.8138 avg training loss: 9.2956
batch: [19410/21305] batch time: 0.453 trainign loss: 6.3258 avg training loss: 9.2952
batch: [19420/21305] batch time: 0.053 trainign loss: 8.9519 avg training loss: 9.2949
batch: [19430/21305] batch time: 0.056 trainign loss: 8.7251 avg training loss: 9.2948
batch: [19440/21305] batch time: 0.051 trainign loss: 9.0302 avg training loss: 9.2945
batch: [19450/21305] batch time: 0.057 trainign loss: 8.6024 avg training loss: 9.2942
batch: [19460/21305] batch time: 0.057 trainign loss: 7.5718 avg training loss: 9.2941
batch: [19470/21305] batch time: 0.062 trainign loss: 8.9328 avg training loss: 9.2939
batch: [19480/21305] batch time: 0.056 trainign loss: 7.4157 avg training loss: 9.2936
batch: [19490/21305] batch time: 0.062 trainign loss: 6.2979 avg training loss: 9.2932
batch: [19500/21305] batch time: 0.056 trainign loss: 8.4058 avg training loss: 9.2930
batch: [19510/21305] batch time: 0.056 trainign loss: 7.5089 avg training loss: 9.2927
batch: [19520/21305] batch time: 0.062 trainign loss: 8.3201 avg training loss: 9.2926
batch: [19530/21305] batch time: 0.057 trainign loss: 9.0778 avg training loss: 9.2922
batch: [19540/21305] batch time: 0.052 trainign loss: 8.5134 avg training loss: 9.2917
batch: [19550/21305] batch time: 0.058 trainign loss: 8.1937 avg training loss: 9.2912
batch: [19560/21305] batch time: 0.062 trainign loss: 8.7045 avg training loss: 9.2909
batch: [19570/21305] batch time: 0.058 trainign loss: 8.7063 avg training loss: 9.2906
batch: [19580/21305] batch time: 0.058 trainign loss: 6.4536 avg training loss: 9.2903
batch: [19590/21305] batch time: 0.062 trainign loss: 8.5240 avg training loss: 9.2896
batch: [19600/21305] batch time: 0.056 trainign loss: 7.7826 avg training loss: 9.2894
batch: [19610/21305] batch time: 0.056 trainign loss: 7.9003 avg training loss: 9.2892
batch: [19620/21305] batch time: 0.056 trainign loss: 8.8946 avg training loss: 9.2888
batch: [19630/21305] batch time: 0.058 trainign loss: 8.7408 avg training loss: 9.2886
batch: [19640/21305] batch time: 0.057 trainign loss: 9.1937 avg training loss: 9.2884
batch: [19650/21305] batch time: 0.053 trainign loss: 8.8612 avg training loss: 9.2881
batch: [19660/21305] batch time: 0.056 trainign loss: 8.9359 avg training loss: 9.2879
batch: [19670/21305] batch time: 0.063 trainign loss: 9.1431 avg training loss: 9.2878
batch: [19680/21305] batch time: 0.056 trainign loss: 0.0185 avg training loss: 9.2859
batch: [19690/21305] batch time: 0.052 trainign loss: 9.0952 avg training loss: 9.2861
batch: [19700/21305] batch time: 0.053 trainign loss: 9.8210 avg training loss: 9.2861
batch: [19710/21305] batch time: 0.051 trainign loss: 8.8175 avg training loss: 9.2860
batch: [19720/21305] batch time: 0.056 trainign loss: 8.0713 avg training loss: 9.2858
batch: [19730/21305] batch time: 0.054 trainign loss: 6.7628 avg training loss: 9.2855
batch: [19740/21305] batch time: 0.056 trainign loss: 6.8300 avg training loss: 9.2852
batch: [19750/21305] batch time: 0.063 trainign loss: 9.2849 avg training loss: 9.2848
batch: [19760/21305] batch time: 0.056 trainign loss: 7.7900 avg training loss: 9.2846
batch: [19770/21305] batch time: 0.054 trainign loss: 7.4489 avg training loss: 9.2844
batch: [19780/21305] batch time: 0.062 trainign loss: 5.5740 avg training loss: 9.2840
batch: [19790/21305] batch time: 0.056 trainign loss: 8.3281 avg training loss: 9.2836
batch: [19800/21305] batch time: 0.057 trainign loss: 7.4250 avg training loss: 9.2833
batch: [19810/21305] batch time: 0.056 trainign loss: 6.9938 avg training loss: 9.2826
batch: [19820/21305] batch time: 0.058 trainign loss: 8.4753 avg training loss: 9.2823
batch: [19830/21305] batch time: 0.057 trainign loss: 8.9506 avg training loss: 9.2822
batch: [19840/21305] batch time: 0.052 trainign loss: 8.5080 avg training loss: 9.2819
batch: [19850/21305] batch time: 0.057 trainign loss: 8.7840 avg training loss: 9.2816
batch: [19860/21305] batch time: 0.058 trainign loss: 9.2438 avg training loss: 9.2815
batch: [19870/21305] batch time: 0.056 trainign loss: 9.1832 avg training loss: 9.2813
batch: [19880/21305] batch time: 0.056 trainign loss: 6.1485 avg training loss: 9.2809
batch: [19890/21305] batch time: 0.057 trainign loss: 5.5589 avg training loss: 9.2805
batch: [19900/21305] batch time: 0.063 trainign loss: 12.0926 avg training loss: 9.2796
batch: [19910/21305] batch time: 0.061 trainign loss: 2.0206 avg training loss: 9.2783
batch: [19920/21305] batch time: 0.062 trainign loss: 8.9298 avg training loss: 9.2785
batch: [19930/21305] batch time: 0.052 trainign loss: 9.8660 avg training loss: 9.2786
batch: [19940/21305] batch time: 0.057 trainign loss: 8.5283 avg training loss: 9.2785
batch: [19950/21305] batch time: 0.052 trainign loss: 8.9046 avg training loss: 9.2779
batch: [19960/21305] batch time: 0.051 trainign loss: 8.7435 avg training loss: 9.2775
batch: [19970/21305] batch time: 0.052 trainign loss: 8.4041 avg training loss: 9.2772
batch: [19980/21305] batch time: 0.056 trainign loss: 9.3866 avg training loss: 9.2771
batch: [19990/21305] batch time: 0.056 trainign loss: 7.7591 avg training loss: 9.2770
batch: [20000/21305] batch time: 0.063 trainign loss: 9.0691 avg training loss: 9.2766
batch: [20010/21305] batch time: 0.056 trainign loss: 8.1319 avg training loss: 9.2764
batch: [20020/21305] batch time: 0.061 trainign loss: 0.7253 avg training loss: 9.2754
batch: [20030/21305] batch time: 0.051 trainign loss: 9.2062 avg training loss: 9.2750
batch: [20040/21305] batch time: 0.056 trainign loss: 7.9043 avg training loss: 9.2745
batch: [20050/21305] batch time: 0.056 trainign loss: 3.8421 avg training loss: 9.2740
batch: [20060/21305] batch time: 0.056 trainign loss: 5.3755 avg training loss: 9.2733
batch: [20070/21305] batch time: 0.051 trainign loss: 8.8112 avg training loss: 9.2731
batch: [20080/21305] batch time: 0.055 trainign loss: 9.2590 avg training loss: 9.2731
batch: [20090/21305] batch time: 0.052 trainign loss: 4.8952 avg training loss: 9.2728
batch: [20100/21305] batch time: 0.057 trainign loss: 13.5162 avg training loss: 9.2716
batch: [20110/21305] batch time: 0.056 trainign loss: 7.7893 avg training loss: 9.2717
batch: [20120/21305] batch time: 0.056 trainign loss: 9.3809 avg training loss: 9.2717
batch: [20130/21305] batch time: 0.052 trainign loss: 8.9080 avg training loss: 9.2716
batch: [20140/21305] batch time: 0.057 trainign loss: 8.7318 avg training loss: 9.2714
batch: [20150/21305] batch time: 0.061 trainign loss: 6.0927 avg training loss: 9.2713
batch: [20160/21305] batch time: 0.057 trainign loss: 9.1736 avg training loss: 9.2708
batch: [20170/21305] batch time: 0.063 trainign loss: 9.0254 avg training loss: 9.2706
batch: [20180/21305] batch time: 0.052 trainign loss: 7.6882 avg training loss: 9.2704
batch: [20190/21305] batch time: 0.061 trainign loss: 4.4136 avg training loss: 9.2697
batch: [20200/21305] batch time: 0.051 trainign loss: 6.1747 avg training loss: 9.2694
batch: [20210/21305] batch time: 0.052 trainign loss: 9.2044 avg training loss: 9.2690
batch: [20220/21305] batch time: 0.056 trainign loss: 9.1151 avg training loss: 9.2688
batch: [20230/21305] batch time: 0.054 trainign loss: 8.6651 avg training loss: 9.2686
batch: [20240/21305] batch time: 0.056 trainign loss: 6.6324 avg training loss: 9.2683
batch: [20250/21305] batch time: 0.052 trainign loss: 9.4157 avg training loss: 9.2678
batch: [20260/21305] batch time: 0.056 trainign loss: 8.9266 avg training loss: 9.2677
batch: [20270/21305] batch time: 0.054 trainign loss: 1.8163 avg training loss: 9.2668
batch: [20280/21305] batch time: 0.062 trainign loss: 10.4062 avg training loss: 9.2665
batch: [20290/21305] batch time: 0.052 trainign loss: 9.1393 avg training loss: 9.2664
batch: [20300/21305] batch time: 0.062 trainign loss: 9.1779 avg training loss: 9.2662
batch: [20310/21305] batch time: 0.056 trainign loss: 8.8696 avg training loss: 9.2662
batch: [20320/21305] batch time: 0.056 trainign loss: 8.7693 avg training loss: 9.2660
batch: [20330/21305] batch time: 0.056 trainign loss: 5.1171 avg training loss: 9.2654
batch: [20340/21305] batch time: 0.057 trainign loss: 9.1140 avg training loss: 9.2648
batch: [20350/21305] batch time: 0.056 trainign loss: 8.5395 avg training loss: 9.2648
batch: [20360/21305] batch time: 0.056 trainign loss: 9.3194 avg training loss: 9.2645
batch: [20370/21305] batch time: 0.054 trainign loss: 9.4034 avg training loss: 9.2641
batch: [20380/21305] batch time: 0.060 trainign loss: 7.8769 avg training loss: 9.2639
batch: [20390/21305] batch time: 0.055 trainign loss: 9.0237 avg training loss: 9.2636
batch: [20400/21305] batch time: 0.063 trainign loss: 8.6677 avg training loss: 9.2632
batch: [20410/21305] batch time: 0.056 trainign loss: 8.7265 avg training loss: 9.2630
batch: [20420/21305] batch time: 0.056 trainign loss: 6.5238 avg training loss: 9.2628
batch: [20430/21305] batch time: 0.051 trainign loss: 9.5826 avg training loss: 9.2621
batch: [20440/21305] batch time: 0.056 trainign loss: 8.1113 avg training loss: 9.2616
batch: [20450/21305] batch time: 0.051 trainign loss: 6.6589 avg training loss: 9.2616
batch: [20460/21305] batch time: 0.062 trainign loss: 9.8325 avg training loss: 9.2615
batch: [20470/21305] batch time: 0.055 trainign loss: 8.1883 avg training loss: 9.2615
batch: [20480/21305] batch time: 0.062 trainign loss: 0.0175 avg training loss: 9.2598
batch: [20490/21305] batch time: 0.055 trainign loss: 17.8318 avg training loss: 9.2584
batch: [20500/21305] batch time: 0.056 trainign loss: 10.0299 avg training loss: 9.2593
batch: [20510/21305] batch time: 0.055 trainign loss: 9.7607 avg training loss: 9.2595
batch: [20520/21305] batch time: 0.056 trainign loss: 9.5109 avg training loss: 9.2595
batch: [20530/21305] batch time: 0.056 trainign loss: 8.9580 avg training loss: 9.2592
batch: [20540/21305] batch time: 0.056 trainign loss: 10.8068 avg training loss: 9.2585
batch: [20550/21305] batch time: 0.051 trainign loss: 8.1901 avg training loss: 9.2584
batch: [20560/21305] batch time: 0.056 trainign loss: 8.8208 avg training loss: 9.2578
batch: [20570/21305] batch time: 0.050 trainign loss: 9.4515 avg training loss: 9.2578
batch: [20580/21305] batch time: 0.440 trainign loss: 8.8242 avg training loss: 9.2577
batch: [20590/21305] batch time: 0.051 trainign loss: 8.4852 avg training loss: 9.2573
batch: [20600/21305] batch time: 0.056 trainign loss: 8.4418 avg training loss: 9.2572
batch: [20610/21305] batch time: 0.055 trainign loss: 8.9360 avg training loss: 9.2571
batch: [20620/21305] batch time: 0.058 trainign loss: 9.0146 avg training loss: 9.2570
batch: [20630/21305] batch time: 0.053 trainign loss: 8.4871 avg training loss: 9.2567
batch: [20640/21305] batch time: 0.061 trainign loss: 8.8561 avg training loss: 9.2565
batch: [20650/21305] batch time: 0.052 trainign loss: 9.0001 avg training loss: 9.2564
batch: [20660/21305] batch time: 0.057 trainign loss: 7.1756 avg training loss: 9.2560
batch: [20670/21305] batch time: 0.061 trainign loss: 9.5107 avg training loss: 9.2558
batch: [20680/21305] batch time: 0.056 trainign loss: 6.5725 avg training loss: 9.2554
batch: [20690/21305] batch time: 0.057 trainign loss: 8.8124 avg training loss: 9.2551
batch: [20700/21305] batch time: 0.284 trainign loss: 8.6153 avg training loss: 9.2549
batch: [20710/21305] batch time: 0.062 trainign loss: 8.5745 avg training loss: 9.2547
batch: [20720/21305] batch time: 0.563 trainign loss: 8.8473 avg training loss: 9.2545
batch: [20730/21305] batch time: 0.057 trainign loss: 8.2697 avg training loss: 9.2544
batch: [20740/21305] batch time: 0.317 trainign loss: 8.5400 avg training loss: 9.2542
batch: [20750/21305] batch time: 0.055 trainign loss: 6.7155 avg training loss: 9.2539
batch: [20760/21305] batch time: 0.060 trainign loss: 9.2280 avg training loss: 9.2538
batch: [20770/21305] batch time: 0.056 trainign loss: 8.1960 avg training loss: 9.2536
batch: [20780/21305] batch time: 0.410 trainign loss: 8.6022 avg training loss: 9.2534
batch: [20790/21305] batch time: 0.054 trainign loss: 7.5041 avg training loss: 9.2531
batch: [20800/21305] batch time: 0.485 trainign loss: 8.1251 avg training loss: 9.2527
batch: [20810/21305] batch time: 0.060 trainign loss: 8.4842 avg training loss: 9.2526
batch: [20820/21305] batch time: 0.057 trainign loss: 7.8032 avg training loss: 9.2525
batch: [20830/21305] batch time: 0.056 trainign loss: 8.1691 avg training loss: 9.2523
batch: [20840/21305] batch time: 0.056 trainign loss: 7.1507 avg training loss: 9.2520
batch: [20850/21305] batch time: 0.056 trainign loss: 7.9967 avg training loss: 9.2514
batch: [20860/21305] batch time: 0.056 trainign loss: 7.0343 avg training loss: 9.2511
batch: [20870/21305] batch time: 0.062 trainign loss: 8.1789 avg training loss: 9.2508
batch: [20880/21305] batch time: 0.062 trainign loss: 7.8034 avg training loss: 9.2504
batch: [20890/21305] batch time: 0.056 trainign loss: 9.3865 avg training loss: 9.2503
batch: [20900/21305] batch time: 0.058 trainign loss: 5.8622 avg training loss: 9.2499
batch: [20910/21305] batch time: 0.058 trainign loss: 8.3154 avg training loss: 9.2495
batch: [20920/21305] batch time: 0.062 trainign loss: 4.5913 avg training loss: 9.2492
batch: [20930/21305] batch time: 0.057 trainign loss: 10.3108 avg training loss: 9.2486
batch: [20940/21305] batch time: 0.062 trainign loss: 3.9988 avg training loss: 9.2474
batch: [20950/21305] batch time: 0.056 trainign loss: 8.7066 avg training loss: 9.2474
batch: [20960/21305] batch time: 0.208 trainign loss: 8.0709 avg training loss: 9.2475
batch: [20970/21305] batch time: 0.056 trainign loss: 6.3755 avg training loss: 9.2472
batch: [20980/21305] batch time: 0.059 trainign loss: 9.1104 avg training loss: 9.2471
batch: [20990/21305] batch time: 0.062 trainign loss: 9.1582 avg training loss: 9.2469
batch: [21000/21305] batch time: 0.399 trainign loss: 7.3984 avg training loss: 9.2467
batch: [21010/21305] batch time: 0.057 trainign loss: 8.7796 avg training loss: 9.2464
batch: [21020/21305] batch time: 0.063 trainign loss: 8.0484 avg training loss: 9.2462
batch: [21030/21305] batch time: 0.054 trainign loss: 8.7124 avg training loss: 9.2460
batch: [21040/21305] batch time: 0.871 trainign loss: 9.2357 avg training loss: 9.2459
batch: [21050/21305] batch time: 0.058 trainign loss: 8.5747 avg training loss: 9.2455
batch: [21060/21305] batch time: 0.247 trainign loss: 8.1453 avg training loss: 9.2453
batch: [21070/21305] batch time: 0.061 trainign loss: 7.6116 avg training loss: 9.2449
batch: [21080/21305] batch time: 0.546 trainign loss: 8.8243 avg training loss: 9.2443
batch: [21090/21305] batch time: 0.060 trainign loss: 9.0123 avg training loss: 9.2441
batch: [21100/21305] batch time: 0.404 trainign loss: 5.3788 avg training loss: 9.2434
batch: [21110/21305] batch time: 0.056 trainign loss: 9.3012 avg training loss: 9.2431
batch: [21120/21305] batch time: 0.050 trainign loss: 9.3409 avg training loss: 9.2430
batch: [21130/21305] batch time: 0.057 trainign loss: 9.3569 avg training loss: 9.2427
batch: [21140/21305] batch time: 1.202 trainign loss: 9.0207 avg training loss: 9.2425
batch: [21150/21305] batch time: 0.058 trainign loss: 8.8464 avg training loss: 9.2422
batch: [21160/21305] batch time: 2.159 trainign loss: 8.8418 avg training loss: 9.2419
batch: [21170/21305] batch time: 0.063 trainign loss: 6.1080 avg training loss: 9.2409
batch: [21180/21305] batch time: 2.273 trainign loss: 8.8144 avg training loss: 9.2410
batch: [21190/21305] batch time: 0.058 trainign loss: 9.2015 avg training loss: 9.2410
batch: [21200/21305] batch time: 2.357 trainign loss: 8.6266 avg training loss: 9.2407
batch: [21210/21305] batch time: 0.057 trainign loss: 8.0888 avg training loss: 9.2407
batch: [21220/21305] batch time: 2.092 trainign loss: 6.5927 avg training loss: 9.2403
batch: [21230/21305] batch time: 0.052 trainign loss: 8.4371 avg training loss: 9.2400
batch: [21240/21305] batch time: 2.031 trainign loss: 6.5089 avg training loss: 9.2397
batch: [21250/21305] batch time: 0.056 trainign loss: 8.6591 avg training loss: 9.2395
batch: [21260/21305] batch time: 2.301 trainign loss: 9.3649 avg training loss: 9.2393
batch: [21270/21305] batch time: 0.052 trainign loss: 3.6103 avg training loss: 9.2388
batch: [21280/21305] batch time: 2.267 trainign loss: 7.8635 avg training loss: 9.2385
batch: [21290/21305] batch time: 0.062 trainign loss: 6.4636 avg training loss: 9.2379
batch: [21300/21305] batch time: 1.253 trainign loss: 9.4927 avg training loss: 9.2378
Epoch: 3
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.720 trainign loss: 9.3391 avg training loss: 9.2377
batch: [10/21305] batch time: 0.056 trainign loss: 11.7297 avg training loss: 9.2366
batch: [20/21305] batch time: 2.160 trainign loss: 9.0198 avg training loss: 9.2368
batch: [30/21305] batch time: 0.062 trainign loss: 8.9926 avg training loss: 9.2366
batch: [40/21305] batch time: 1.072 trainign loss: 9.4096 avg training loss: 9.2365
batch: [50/21305] batch time: 0.056 trainign loss: 4.4321 avg training loss: 9.2361
batch: [60/21305] batch time: 1.785 trainign loss: 9.4625 avg training loss: 9.2359
batch: [70/21305] batch time: 0.062 trainign loss: 9.1190 avg training loss: 9.2355
batch: [80/21305] batch time: 1.770 trainign loss: 8.3560 avg training loss: 9.2353
batch: [90/21305] batch time: 0.056 trainign loss: 8.7701 avg training loss: 9.2350
batch: [100/21305] batch time: 2.172 trainign loss: 8.9161 avg training loss: 9.2348
batch: [110/21305] batch time: 0.056 trainign loss: 8.1623 avg training loss: 9.2345
batch: [120/21305] batch time: 2.345 trainign loss: 8.4589 avg training loss: 9.2341
batch: [130/21305] batch time: 0.056 trainign loss: 9.0265 avg training loss: 9.2338
batch: [140/21305] batch time: 1.863 trainign loss: 5.5941 avg training loss: 9.2334
batch: [150/21305] batch time: 0.087 trainign loss: 10.0108 avg training loss: 9.2331
batch: [160/21305] batch time: 2.162 trainign loss: 8.8470 avg training loss: 9.2330
batch: [170/21305] batch time: 0.060 trainign loss: 8.7427 avg training loss: 9.2328
batch: [180/21305] batch time: 2.203 trainign loss: 8.3272 avg training loss: 9.2326
batch: [190/21305] batch time: 0.059 trainign loss: 6.1315 avg training loss: 9.2324
batch: [200/21305] batch time: 0.724 trainign loss: 0.0042 avg training loss: 9.2305
batch: [210/21305] batch time: 0.056 trainign loss: 14.0354 avg training loss: 9.2305
batch: [220/21305] batch time: 0.823 trainign loss: 9.4823 avg training loss: 9.2307
batch: [230/21305] batch time: 0.056 trainign loss: 8.2040 avg training loss: 9.2307
batch: [240/21305] batch time: 0.421 trainign loss: 9.0657 avg training loss: 9.2305
batch: [250/21305] batch time: 0.062 trainign loss: 9.2678 avg training loss: 9.2304
batch: [260/21305] batch time: 0.373 trainign loss: 8.6939 avg training loss: 9.2302
batch: [270/21305] batch time: 0.063 trainign loss: 5.5814 avg training loss: 9.2299
batch: [280/21305] batch time: 0.373 trainign loss: 7.4907 avg training loss: 9.2296
batch: [290/21305] batch time: 0.057 trainign loss: 9.1037 avg training loss: 9.2291
batch: [300/21305] batch time: 0.548 trainign loss: 9.0108 avg training loss: 9.2288
batch: [310/21305] batch time: 0.056 trainign loss: 7.1596 avg training loss: 9.2287
batch: [320/21305] batch time: 0.371 trainign loss: 8.6803 avg training loss: 9.2285
batch: [330/21305] batch time: 0.060 trainign loss: 8.4626 avg training loss: 9.2282
batch: [340/21305] batch time: 0.050 trainign loss: 9.2894 avg training loss: 9.2280
batch: [350/21305] batch time: 0.056 trainign loss: 9.3511 avg training loss: 9.2278
batch: [360/21305] batch time: 0.052 trainign loss: 8.9612 avg training loss: 9.2275
batch: [370/21305] batch time: 0.056 trainign loss: 7.5090 avg training loss: 9.2272
batch: [380/21305] batch time: 0.052 trainign loss: 7.7758 avg training loss: 9.2271
batch: [390/21305] batch time: 0.063 trainign loss: 4.8692 avg training loss: 9.2264
batch: [400/21305] batch time: 0.053 trainign loss: 8.7441 avg training loss: 9.2260
batch: [410/21305] batch time: 0.056 trainign loss: 9.5131 avg training loss: 9.2259
batch: [420/21305] batch time: 0.052 trainign loss: 9.0504 avg training loss: 9.2258
batch: [430/21305] batch time: 0.058 trainign loss: 8.5344 avg training loss: 9.2258
batch: [440/21305] batch time: 0.056 trainign loss: 7.7975 avg training loss: 9.2253
batch: [450/21305] batch time: 0.062 trainign loss: 5.6930 avg training loss: 9.2249
batch: [460/21305] batch time: 0.056 trainign loss: 9.0109 avg training loss: 9.2245
batch: [470/21305] batch time: 0.056 trainign loss: 7.9005 avg training loss: 9.2244
batch: [480/21305] batch time: 0.056 trainign loss: 9.3226 avg training loss: 9.2243
batch: [490/21305] batch time: 0.056 trainign loss: 8.4310 avg training loss: 9.2241
batch: [500/21305] batch time: 0.055 trainign loss: 5.1002 avg training loss: 9.2237
batch: [510/21305] batch time: 0.056 trainign loss: 6.6185 avg training loss: 9.2235
batch: [520/21305] batch time: 0.056 trainign loss: 4.5177 avg training loss: 9.2231
batch: [530/21305] batch time: 0.056 trainign loss: 8.5169 avg training loss: 9.2229
batch: [540/21305] batch time: 0.056 trainign loss: 8.6222 avg training loss: 9.2226
batch: [550/21305] batch time: 0.056 trainign loss: 6.6165 avg training loss: 9.2220
batch: [560/21305] batch time: 0.058 trainign loss: 6.6361 avg training loss: 9.2219
batch: [570/21305] batch time: 0.056 trainign loss: 7.9330 avg training loss: 9.2215
batch: [580/21305] batch time: 0.053 trainign loss: 8.5179 avg training loss: 9.2209
batch: [590/21305] batch time: 0.062 trainign loss: 8.4397 avg training loss: 9.2205
batch: [600/21305] batch time: 0.051 trainign loss: 8.0576 avg training loss: 9.2204
batch: [610/21305] batch time: 0.062 trainign loss: 9.1581 avg training loss: 9.2203
batch: [620/21305] batch time: 0.053 trainign loss: 8.3549 avg training loss: 9.2202
batch: [630/21305] batch time: 0.062 trainign loss: 7.6581 avg training loss: 9.2199
batch: [640/21305] batch time: 0.055 trainign loss: 8.8642 avg training loss: 9.2196
batch: [650/21305] batch time: 0.063 trainign loss: 7.0506 avg training loss: 9.2194
batch: [660/21305] batch time: 0.052 trainign loss: 7.6078 avg training loss: 9.2188
batch: [670/21305] batch time: 0.060 trainign loss: 9.9062 avg training loss: 9.2182
batch: [680/21305] batch time: 0.054 trainign loss: 9.4971 avg training loss: 9.2182
batch: [690/21305] batch time: 0.056 trainign loss: 8.2526 avg training loss: 9.2181
batch: [700/21305] batch time: 0.051 trainign loss: 9.1858 avg training loss: 9.2178
batch: [710/21305] batch time: 0.056 trainign loss: 7.6669 avg training loss: 9.2175
batch: [720/21305] batch time: 0.056 trainign loss: 7.8129 avg training loss: 9.2172
batch: [730/21305] batch time: 0.056 trainign loss: 9.0635 avg training loss: 9.2169
batch: [740/21305] batch time: 0.051 trainign loss: 9.0758 avg training loss: 9.2166
batch: [750/21305] batch time: 0.057 trainign loss: 8.3883 avg training loss: 9.2164
batch: [760/21305] batch time: 0.052 trainign loss: 8.0182 avg training loss: 9.2161
batch: [770/21305] batch time: 0.057 trainign loss: 6.4997 avg training loss: 9.2156
batch: [780/21305] batch time: 0.051 trainign loss: 8.5739 avg training loss: 9.2154
batch: [790/21305] batch time: 0.062 trainign loss: 8.5577 avg training loss: 9.2152
batch: [800/21305] batch time: 0.056 trainign loss: 8.4649 avg training loss: 9.2151
batch: [810/21305] batch time: 0.058 trainign loss: 8.0203 avg training loss: 9.2147
batch: [820/21305] batch time: 0.050 trainign loss: 8.5534 avg training loss: 9.2144
batch: [830/21305] batch time: 0.056 trainign loss: 7.4637 avg training loss: 9.2141
batch: [840/21305] batch time: 0.050 trainign loss: 8.3601 avg training loss: 9.2139
batch: [850/21305] batch time: 0.059 trainign loss: 7.2119 avg training loss: 9.2134
batch: [860/21305] batch time: 0.051 trainign loss: 6.5672 avg training loss: 9.2131
batch: [870/21305] batch time: 0.452 trainign loss: 6.4167 avg training loss: 9.2127
batch: [880/21305] batch time: 0.055 trainign loss: 8.7527 avg training loss: 9.2119
batch: [890/21305] batch time: 0.059 trainign loss: 9.1046 avg training loss: 9.2118
batch: [900/21305] batch time: 0.063 trainign loss: 7.5233 avg training loss: 9.2116
batch: [910/21305] batch time: 0.063 trainign loss: 8.2167 avg training loss: 9.2113
batch: [920/21305] batch time: 0.052 trainign loss: 5.9200 avg training loss: 9.2108
batch: [930/21305] batch time: 0.057 trainign loss: 8.2788 avg training loss: 9.2105
batch: [940/21305] batch time: 0.062 trainign loss: 5.8431 avg training loss: 9.2100
batch: [950/21305] batch time: 0.062 trainign loss: 0.0142 avg training loss: 9.2083
batch: [960/21305] batch time: 0.056 trainign loss: 9.8845 avg training loss: 9.2084
batch: [970/21305] batch time: 0.063 trainign loss: 9.4575 avg training loss: 9.2084
batch: [980/21305] batch time: 0.062 trainign loss: 8.7125 avg training loss: 9.2084
batch: [990/21305] batch time: 0.056 trainign loss: 8.5290 avg training loss: 9.2082
batch: [1000/21305] batch time: 0.056 trainign loss: 5.9568 avg training loss: 9.2079
batch: [1010/21305] batch time: 0.056 trainign loss: 9.0707 avg training loss: 9.2074
batch: [1020/21305] batch time: 0.222 trainign loss: 8.9681 avg training loss: 9.2071
batch: [1030/21305] batch time: 0.055 trainign loss: 8.2823 avg training loss: 9.2068
batch: [1040/21305] batch time: 0.509 trainign loss: 8.1268 avg training loss: 9.2066
batch: [1050/21305] batch time: 0.063 trainign loss: 7.6942 avg training loss: 9.2062
batch: [1060/21305] batch time: 0.062 trainign loss: 6.8183 avg training loss: 9.2057
batch: [1070/21305] batch time: 0.777 trainign loss: 5.8581 avg training loss: 9.2052
batch: [1080/21305] batch time: 0.061 trainign loss: 0.9816 avg training loss: 9.2042
batch: [1090/21305] batch time: 1.075 trainign loss: 11.4266 avg training loss: 9.2038
batch: [1100/21305] batch time: 0.058 trainign loss: 7.8063 avg training loss: 9.2034
batch: [1110/21305] batch time: 0.962 trainign loss: 8.4355 avg training loss: 9.2032
batch: [1120/21305] batch time: 0.056 trainign loss: 8.8358 avg training loss: 9.2031
batch: [1130/21305] batch time: 0.440 trainign loss: 7.2757 avg training loss: 9.2028
batch: [1140/21305] batch time: 0.056 trainign loss: 8.5548 avg training loss: 9.2026
batch: [1150/21305] batch time: 0.051 trainign loss: 8.3131 avg training loss: 9.2022
batch: [1160/21305] batch time: 0.157 trainign loss: 8.2751 avg training loss: 9.2019
batch: [1170/21305] batch time: 0.970 trainign loss: 7.8241 avg training loss: 9.2015
batch: [1180/21305] batch time: 0.213 trainign loss: 7.5641 avg training loss: 9.2011
batch: [1190/21305] batch time: 1.752 trainign loss: 8.2519 avg training loss: 9.2008
batch: [1200/21305] batch time: 0.060 trainign loss: 5.6974 avg training loss: 9.2002
batch: [1210/21305] batch time: 2.006 trainign loss: 8.4969 avg training loss: 9.1999
batch: [1220/21305] batch time: 0.185 trainign loss: 8.7832 avg training loss: 9.1997
batch: [1230/21305] batch time: 2.068 trainign loss: 7.4483 avg training loss: 9.1993
batch: [1240/21305] batch time: 0.063 trainign loss: 7.7197 avg training loss: 9.1989
batch: [1250/21305] batch time: 2.246 trainign loss: 8.0335 avg training loss: 9.1985
batch: [1260/21305] batch time: 0.056 trainign loss: 6.6460 avg training loss: 9.1981
batch: [1270/21305] batch time: 2.679 trainign loss: 7.8254 avg training loss: 9.1977
batch: [1280/21305] batch time: 0.062 trainign loss: 8.2628 avg training loss: 9.1974
batch: [1290/21305] batch time: 2.102 trainign loss: 7.2123 avg training loss: 9.1970
batch: [1300/21305] batch time: 0.055 trainign loss: 7.3195 avg training loss: 9.1966
batch: [1310/21305] batch time: 2.254 trainign loss: 8.2060 avg training loss: 9.1963
batch: [1320/21305] batch time: 0.055 trainign loss: 6.4132 avg training loss: 9.1958
batch: [1330/21305] batch time: 2.351 trainign loss: 8.0685 avg training loss: 9.1955
batch: [1340/21305] batch time: 0.053 trainign loss: 8.2745 avg training loss: 9.1952
batch: [1350/21305] batch time: 2.680 trainign loss: 8.1737 avg training loss: 9.1949
batch: [1360/21305] batch time: 0.063 trainign loss: 6.4828 avg training loss: 9.1945
batch: [1370/21305] batch time: 2.194 trainign loss: 9.0069 avg training loss: 9.1939
batch: [1380/21305] batch time: 0.056 trainign loss: 6.5802 avg training loss: 9.1936
batch: [1390/21305] batch time: 2.086 trainign loss: 7.9216 avg training loss: 9.1931
batch: [1400/21305] batch time: 0.056 trainign loss: 7.3162 avg training loss: 9.1927
batch: [1410/21305] batch time: 1.701 trainign loss: 7.8872 avg training loss: 9.1920
batch: [1420/21305] batch time: 0.056 trainign loss: 7.4996 avg training loss: 9.1914
batch: [1430/21305] batch time: 1.076 trainign loss: 6.5954 avg training loss: 9.1910
batch: [1440/21305] batch time: 0.056 trainign loss: 6.3998 avg training loss: 9.1906
batch: [1450/21305] batch time: 1.032 trainign loss: 7.4866 avg training loss: 9.1901
batch: [1460/21305] batch time: 0.057 trainign loss: 7.4063 avg training loss: 9.1898
batch: [1470/21305] batch time: 0.955 trainign loss: 6.5609 avg training loss: 9.1893
batch: [1480/21305] batch time: 0.063 trainign loss: 8.1332 avg training loss: 9.1888
batch: [1490/21305] batch time: 0.890 trainign loss: 7.8763 avg training loss: 9.1884
batch: [1500/21305] batch time: 0.056 trainign loss: 7.5114 avg training loss: 9.1880
batch: [1510/21305] batch time: 0.295 trainign loss: 8.1844 avg training loss: 9.1876
batch: [1520/21305] batch time: 0.058 trainign loss: 4.0154 avg training loss: 9.1869
batch: [1530/21305] batch time: 0.056 trainign loss: 13.0126 avg training loss: 9.1855
batch: [1540/21305] batch time: 0.056 trainign loss: 9.0159 avg training loss: 9.1856
batch: [1550/21305] batch time: 0.052 trainign loss: 7.2535 avg training loss: 9.1856
batch: [1560/21305] batch time: 0.058 trainign loss: 7.8752 avg training loss: 9.1854
batch: [1570/21305] batch time: 0.050 trainign loss: 6.9919 avg training loss: 9.1852
batch: [1580/21305] batch time: 0.058 trainign loss: 7.6560 avg training loss: 9.1846
batch: [1590/21305] batch time: 0.056 trainign loss: 7.7527 avg training loss: 9.1839
batch: [1600/21305] batch time: 0.058 trainign loss: 7.9582 avg training loss: 9.1836
batch: [1610/21305] batch time: 0.063 trainign loss: 8.3756 avg training loss: 9.1834
batch: [1620/21305] batch time: 0.056 trainign loss: 7.7427 avg training loss: 9.1831
batch: [1630/21305] batch time: 0.058 trainign loss: 8.8122 avg training loss: 9.1828
batch: [1640/21305] batch time: 0.056 trainign loss: 7.9927 avg training loss: 9.1825
batch: [1650/21305] batch time: 0.051 trainign loss: 6.9873 avg training loss: 9.1821
batch: [1660/21305] batch time: 0.056 trainign loss: 6.8928 avg training loss: 9.1818
batch: [1670/21305] batch time: 0.279 trainign loss: 7.4639 avg training loss: 9.1814
batch: [1680/21305] batch time: 0.063 trainign loss: 6.9907 avg training loss: 9.1810
batch: [1690/21305] batch time: 0.056 trainign loss: 7.1289 avg training loss: 9.1806
batch: [1700/21305] batch time: 0.056 trainign loss: 7.3982 avg training loss: 9.1802
batch: [1710/21305] batch time: 0.058 trainign loss: 6.0031 avg training loss: 9.1797
batch: [1720/21305] batch time: 0.057 trainign loss: 8.1416 avg training loss: 9.1792
batch: [1730/21305] batch time: 0.057 trainign loss: 7.1919 avg training loss: 9.1788
batch: [1740/21305] batch time: 0.058 trainign loss: 6.9692 avg training loss: 9.1784
batch: [1750/21305] batch time: 0.063 trainign loss: 7.5919 avg training loss: 9.1780
batch: [1760/21305] batch time: 0.123 trainign loss: 7.9391 avg training loss: 9.1777
batch: [1770/21305] batch time: 0.060 trainign loss: 6.9932 avg training loss: 9.1773
batch: [1780/21305] batch time: 0.312 trainign loss: 7.4037 avg training loss: 9.1769
batch: [1790/21305] batch time: 0.056 trainign loss: 7.5192 avg training loss: 9.1764
batch: [1800/21305] batch time: 0.659 trainign loss: 6.7122 avg training loss: 9.1761
batch: [1810/21305] batch time: 0.056 trainign loss: 7.3478 avg training loss: 9.1756
batch: [1820/21305] batch time: 0.847 trainign loss: 5.9871 avg training loss: 9.1750
batch: [1830/21305] batch time: 0.062 trainign loss: 7.9256 avg training loss: 9.1746
batch: [1840/21305] batch time: 0.666 trainign loss: 7.8684 avg training loss: 9.1743
batch: [1850/21305] batch time: 0.062 trainign loss: 7.4781 avg training loss: 9.1739
batch: [1860/21305] batch time: 1.487 trainign loss: 7.1110 avg training loss: 9.1736
batch: [1870/21305] batch time: 0.063 trainign loss: 7.5105 avg training loss: 9.1732
batch: [1880/21305] batch time: 1.538 trainign loss: 5.3177 avg training loss: 9.1727
batch: [1890/21305] batch time: 0.056 trainign loss: 4.9669 avg training loss: 9.1718
batch: [1900/21305] batch time: 1.503 trainign loss: 7.7338 avg training loss: 9.1713
batch: [1910/21305] batch time: 0.063 trainign loss: 8.0856 avg training loss: 9.1711
batch: [1920/21305] batch time: 1.577 trainign loss: 7.0670 avg training loss: 9.1706
batch: [1930/21305] batch time: 0.056 trainign loss: 3.7793 avg training loss: 9.1700
batch: [1940/21305] batch time: 1.393 trainign loss: 7.1625 avg training loss: 9.1696
batch: [1950/21305] batch time: 0.056 trainign loss: 7.1997 avg training loss: 9.1692
batch: [1960/21305] batch time: 1.368 trainign loss: 5.5419 avg training loss: 9.1687
batch: [1970/21305] batch time: 0.057 trainign loss: 7.4454 avg training loss: 9.1682
batch: [1980/21305] batch time: 0.981 trainign loss: 5.5587 avg training loss: 9.1677
batch: [1990/21305] batch time: 0.056 trainign loss: 6.9674 avg training loss: 9.1673
batch: [2000/21305] batch time: 0.343 trainign loss: 6.4583 avg training loss: 9.1667
batch: [2010/21305] batch time: 0.056 trainign loss: 7.7803 avg training loss: 9.1662
batch: [2020/21305] batch time: 0.544 trainign loss: 4.2159 avg training loss: 9.1656
batch: [2030/21305] batch time: 0.056 trainign loss: 6.9750 avg training loss: 9.1648
batch: [2040/21305] batch time: 0.569 trainign loss: 7.3194 avg training loss: 9.1646
batch: [2050/21305] batch time: 0.062 trainign loss: 6.4037 avg training loss: 9.1643
batch: [2060/21305] batch time: 1.545 trainign loss: 3.4887 avg training loss: 9.1636
batch: [2070/21305] batch time: 0.058 trainign loss: 2.4740 avg training loss: 9.1628
batch: [2080/21305] batch time: 0.663 trainign loss: 0.0588 avg training loss: 9.1615
batch: [2090/21305] batch time: 0.056 trainign loss: 9.4926 avg training loss: 9.1606
batch: [2100/21305] batch time: 0.914 trainign loss: 9.5265 avg training loss: 9.1606
batch: [2110/21305] batch time: 0.056 trainign loss: 8.4885 avg training loss: 9.1606
batch: [2120/21305] batch time: 0.394 trainign loss: 0.1610 avg training loss: 9.1595
batch: [2130/21305] batch time: 0.062 trainign loss: 0.0001 avg training loss: 9.1575
batch: [2140/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 9.1555
batch: [2150/21305] batch time: 0.056 trainign loss: 9.6197 avg training loss: 9.1562
batch: [2160/21305] batch time: 0.051 trainign loss: 9.6344 avg training loss: 9.1564
batch: [2170/21305] batch time: 0.055 trainign loss: 9.0448 avg training loss: 9.1563
batch: [2180/21305] batch time: 0.054 trainign loss: 8.2373 avg training loss: 9.1560
batch: [2190/21305] batch time: 0.057 trainign loss: 6.7684 avg training loss: 9.1556
batch: [2200/21305] batch time: 0.051 trainign loss: 7.8837 avg training loss: 9.1553
batch: [2210/21305] batch time: 0.056 trainign loss: 6.6024 avg training loss: 9.1548
batch: [2220/21305] batch time: 0.056 trainign loss: 8.0758 avg training loss: 9.1546
batch: [2230/21305] batch time: 0.057 trainign loss: 7.5698 avg training loss: 9.1541
batch: [2240/21305] batch time: 0.057 trainign loss: 8.2759 avg training loss: 9.1535
batch: [2250/21305] batch time: 0.059 trainign loss: 8.1647 avg training loss: 9.1533
batch: [2260/21305] batch time: 0.053 trainign loss: 7.0892 avg training loss: 9.1529
batch: [2270/21305] batch time: 0.056 trainign loss: 4.7403 avg training loss: 9.1521
batch: [2280/21305] batch time: 0.054 trainign loss: 7.1045 avg training loss: 9.1513
batch: [2290/21305] batch time: 0.057 trainign loss: 5.5329 avg training loss: 9.1505
batch: [2300/21305] batch time: 0.057 trainign loss: 6.4322 avg training loss: 9.1503
batch: [2310/21305] batch time: 0.056 trainign loss: 7.7291 avg training loss: 9.1498
batch: [2320/21305] batch time: 0.057 trainign loss: 8.9029 avg training loss: 9.1494
batch: [2330/21305] batch time: 0.063 trainign loss: 8.6776 avg training loss: 9.1492
batch: [2340/21305] batch time: 0.054 trainign loss: 6.1874 avg training loss: 9.1489
batch: [2350/21305] batch time: 0.057 trainign loss: 6.5946 avg training loss: 9.1485
batch: [2360/21305] batch time: 0.056 trainign loss: 8.4383 avg training loss: 9.1482
batch: [2370/21305] batch time: 0.057 trainign loss: 5.6225 avg training loss: 9.1476
batch: [2380/21305] batch time: 0.056 trainign loss: 7.2128 avg training loss: 9.1472
batch: [2390/21305] batch time: 0.266 trainign loss: 7.2830 avg training loss: 9.1470
batch: [2400/21305] batch time: 0.062 trainign loss: 6.6203 avg training loss: 9.1465
batch: [2410/21305] batch time: 0.187 trainign loss: 8.2621 avg training loss: 9.1461
batch: [2420/21305] batch time: 0.062 trainign loss: 7.3657 avg training loss: 9.1457
batch: [2430/21305] batch time: 0.679 trainign loss: 6.0435 avg training loss: 9.1453
batch: [2440/21305] batch time: 0.057 trainign loss: 7.7597 avg training loss: 9.1449
batch: [2450/21305] batch time: 0.474 trainign loss: 7.8124 avg training loss: 9.1445
batch: [2460/21305] batch time: 0.056 trainign loss: 7.1103 avg training loss: 9.1440
batch: [2470/21305] batch time: 1.015 trainign loss: 7.6555 avg training loss: 9.1434
batch: [2480/21305] batch time: 0.056 trainign loss: 7.9837 avg training loss: 9.1430
batch: [2490/21305] batch time: 0.383 trainign loss: 7.6522 avg training loss: 9.1424
batch: [2500/21305] batch time: 0.062 trainign loss: 8.4181 avg training loss: 9.1421
batch: [2510/21305] batch time: 0.816 trainign loss: 6.7899 avg training loss: 9.1418
batch: [2520/21305] batch time: 0.054 trainign loss: 5.9017 avg training loss: 9.1414
batch: [2530/21305] batch time: 0.950 trainign loss: 6.0336 avg training loss: 9.1409
batch: [2540/21305] batch time: 0.063 trainign loss: 7.6257 avg training loss: 9.1405
batch: [2550/21305] batch time: 0.491 trainign loss: 6.3443 avg training loss: 9.1400
batch: [2560/21305] batch time: 0.056 trainign loss: 6.7889 avg training loss: 9.1397
batch: [2570/21305] batch time: 0.550 trainign loss: 8.0571 avg training loss: 9.1394
batch: [2580/21305] batch time: 0.056 trainign loss: 6.0765 avg training loss: 9.1388
batch: [2590/21305] batch time: 0.632 trainign loss: 6.7144 avg training loss: 9.1384
batch: [2600/21305] batch time: 0.056 trainign loss: 0.2712 avg training loss: 9.1371
batch: [2610/21305] batch time: 0.659 trainign loss: 9.2546 avg training loss: 9.1367
batch: [2620/21305] batch time: 0.058 trainign loss: 9.7070 avg training loss: 9.1368
batch: [2630/21305] batch time: 0.774 trainign loss: 8.2520 avg training loss: 9.1367
batch: [2640/21305] batch time: 0.057 trainign loss: 5.2906 avg training loss: 9.1363
batch: [2650/21305] batch time: 0.056 trainign loss: 9.2963 avg training loss: 9.1358
batch: [2660/21305] batch time: 0.056 trainign loss: 8.4044 avg training loss: 9.1355
batch: [2670/21305] batch time: 0.427 trainign loss: 7.3542 avg training loss: 9.1351
batch: [2680/21305] batch time: 0.062 trainign loss: 5.7810 avg training loss: 9.1345
batch: [2690/21305] batch time: 0.891 trainign loss: 8.2599 avg training loss: 9.1342
batch: [2700/21305] batch time: 1.240 trainign loss: 7.9016 avg training loss: 9.1338
batch: [2710/21305] batch time: 1.361 trainign loss: 5.7238 avg training loss: 9.1331
batch: [2720/21305] batch time: 0.066 trainign loss: 3.2087 avg training loss: 9.1323
batch: [2730/21305] batch time: 1.455 trainign loss: 8.0733 avg training loss: 9.1320
batch: [2740/21305] batch time: 0.632 trainign loss: 8.4123 avg training loss: 9.1316
batch: [2750/21305] batch time: 1.585 trainign loss: 8.1212 avg training loss: 9.1313
batch: [2760/21305] batch time: 1.212 trainign loss: 7.8567 avg training loss: 9.1310
batch: [2770/21305] batch time: 0.825 trainign loss: 7.9369 avg training loss: 9.1307
batch: [2780/21305] batch time: 0.967 trainign loss: 8.1652 avg training loss: 9.1304
batch: [2790/21305] batch time: 1.041 trainign loss: 7.2217 avg training loss: 9.1300
batch: [2800/21305] batch time: 0.858 trainign loss: 7.5667 avg training loss: 9.1296
batch: [2810/21305] batch time: 1.631 trainign loss: 6.9866 avg training loss: 9.1290
batch: [2820/21305] batch time: 0.056 trainign loss: 7.9531 avg training loss: 9.1283
batch: [2830/21305] batch time: 2.318 trainign loss: 8.2716 avg training loss: 9.1280
batch: [2840/21305] batch time: 0.054 trainign loss: 1.4108 avg training loss: 9.1272
batch: [2850/21305] batch time: 2.084 trainign loss: 6.7401 avg training loss: 9.1268
batch: [2860/21305] batch time: 0.061 trainign loss: 8.1603 avg training loss: 9.1265
batch: [2870/21305] batch time: 2.344 trainign loss: 8.5157 avg training loss: 9.1260
batch: [2880/21305] batch time: 0.056 trainign loss: 6.6739 avg training loss: 9.1257
batch: [2890/21305] batch time: 2.456 trainign loss: 7.9536 avg training loss: 9.1252
batch: [2900/21305] batch time: 0.063 trainign loss: 7.9244 avg training loss: 9.1248
batch: [2910/21305] batch time: 2.156 trainign loss: 8.2035 avg training loss: 9.1246
batch: [2920/21305] batch time: 0.504 trainign loss: 7.2307 avg training loss: 9.1243
batch: [2930/21305] batch time: 1.649 trainign loss: 6.1244 avg training loss: 9.1239
batch: [2940/21305] batch time: 1.140 trainign loss: 6.3350 avg training loss: 9.1234
batch: [2950/21305] batch time: 0.703 trainign loss: 1.8218 avg training loss: 9.1224
batch: [2960/21305] batch time: 1.405 trainign loss: 6.1482 avg training loss: 9.1220
batch: [2970/21305] batch time: 0.624 trainign loss: 7.7356 avg training loss: 9.1217
batch: [2980/21305] batch time: 1.476 trainign loss: 7.8178 avg training loss: 9.1215
batch: [2990/21305] batch time: 0.061 trainign loss: 7.5717 avg training loss: 9.1212
batch: [3000/21305] batch time: 0.746 trainign loss: 6.2393 avg training loss: 9.1208
batch: [3010/21305] batch time: 0.063 trainign loss: 8.8919 avg training loss: 9.1202
batch: [3020/21305] batch time: 0.899 trainign loss: 5.5740 avg training loss: 9.1198
batch: [3030/21305] batch time: 0.056 trainign loss: 6.7180 avg training loss: 9.1194
batch: [3040/21305] batch time: 2.184 trainign loss: 7.3065 avg training loss: 9.1190
batch: [3050/21305] batch time: 0.062 trainign loss: 7.9297 avg training loss: 9.1187
batch: [3060/21305] batch time: 2.305 trainign loss: 7.3088 avg training loss: 9.1183
batch: [3070/21305] batch time: 0.056 trainign loss: 7.0920 avg training loss: 9.1179
batch: [3080/21305] batch time: 1.269 trainign loss: 7.9039 avg training loss: 9.1174
batch: [3090/21305] batch time: 0.056 trainign loss: 7.0062 avg training loss: 9.1170
batch: [3100/21305] batch time: 1.541 trainign loss: 7.8210 avg training loss: 9.1166
batch: [3110/21305] batch time: 0.055 trainign loss: 8.5733 avg training loss: 9.1163
batch: [3120/21305] batch time: 1.827 trainign loss: 6.8825 avg training loss: 9.1157
batch: [3130/21305] batch time: 0.056 trainign loss: 1.7462 avg training loss: 9.1149
batch: [3140/21305] batch time: 1.464 trainign loss: 8.2831 avg training loss: 9.1146
batch: [3150/21305] batch time: 0.056 trainign loss: 7.7665 avg training loss: 9.1143
batch: [3160/21305] batch time: 1.511 trainign loss: 7.8128 avg training loss: 9.1139
batch: [3170/21305] batch time: 0.056 trainign loss: 7.1776 avg training loss: 9.1136
batch: [3180/21305] batch time: 1.408 trainign loss: 6.8308 avg training loss: 9.1133
batch: [3190/21305] batch time: 0.056 trainign loss: 7.2403 avg training loss: 9.1128
batch: [3200/21305] batch time: 1.285 trainign loss: 7.9545 avg training loss: 9.1124
batch: [3210/21305] batch time: 0.056 trainign loss: 7.8457 avg training loss: 9.1121
batch: [3220/21305] batch time: 0.685 trainign loss: 7.6748 avg training loss: 9.1116
batch: [3230/21305] batch time: 0.056 trainign loss: 7.8071 avg training loss: 9.1113
batch: [3240/21305] batch time: 0.058 trainign loss: 6.5014 avg training loss: 9.1110
batch: [3250/21305] batch time: 0.056 trainign loss: 7.0142 avg training loss: 9.1105
batch: [3260/21305] batch time: 0.321 trainign loss: 7.9125 avg training loss: 9.1101
batch: [3270/21305] batch time: 0.056 trainign loss: 6.4863 avg training loss: 9.1097
batch: [3280/21305] batch time: 0.563 trainign loss: 4.6882 avg training loss: 9.1090
batch: [3290/21305] batch time: 0.056 trainign loss: 6.7498 avg training loss: 9.1083
batch: [3300/21305] batch time: 1.261 trainign loss: 6.7245 avg training loss: 9.1078
batch: [3310/21305] batch time: 0.057 trainign loss: 1.6796 avg training loss: 9.1069
batch: [3320/21305] batch time: 1.656 trainign loss: 7.6700 avg training loss: 9.1066
batch: [3330/21305] batch time: 0.061 trainign loss: 7.2204 avg training loss: 9.1064
batch: [3340/21305] batch time: 1.143 trainign loss: 5.3594 avg training loss: 9.1059
batch: [3350/21305] batch time: 0.059 trainign loss: 5.8933 avg training loss: 9.1054
batch: [3360/21305] batch time: 0.743 trainign loss: 8.2552 avg training loss: 9.1047
batch: [3370/21305] batch time: 0.062 trainign loss: 4.4815 avg training loss: 9.1039
batch: [3380/21305] batch time: 1.399 trainign loss: 8.5761 avg training loss: 9.1036
batch: [3390/21305] batch time: 0.063 trainign loss: 8.0259 avg training loss: 9.1032
batch: [3400/21305] batch time: 1.837 trainign loss: 7.0094 avg training loss: 9.1029
batch: [3410/21305] batch time: 0.058 trainign loss: 6.3390 avg training loss: 9.1025
batch: [3420/21305] batch time: 1.203 trainign loss: 7.7095 avg training loss: 9.1019
batch: [3430/21305] batch time: 0.056 trainign loss: 7.1210 avg training loss: 9.1016
batch: [3440/21305] batch time: 0.596 trainign loss: 8.1492 avg training loss: 9.1012
batch: [3450/21305] batch time: 0.445 trainign loss: 6.2918 avg training loss: 9.1008
batch: [3460/21305] batch time: 0.056 trainign loss: 6.8301 avg training loss: 9.1002
batch: [3470/21305] batch time: 0.363 trainign loss: 7.9214 avg training loss: 9.0999
batch: [3480/21305] batch time: 0.062 trainign loss: 8.5000 avg training loss: 9.0997
batch: [3490/21305] batch time: 0.161 trainign loss: 8.0515 avg training loss: 9.0993
batch: [3500/21305] batch time: 0.055 trainign loss: 7.3908 avg training loss: 9.0991
batch: [3510/21305] batch time: 0.056 trainign loss: 7.6149 avg training loss: 9.0987
batch: [3520/21305] batch time: 0.056 trainign loss: 6.9765 avg training loss: 9.0984
batch: [3530/21305] batch time: 0.051 trainign loss: 7.6993 avg training loss: 9.0980
batch: [3540/21305] batch time: 0.062 trainign loss: 6.1092 avg training loss: 9.0974
batch: [3550/21305] batch time: 0.056 trainign loss: 7.9139 avg training loss: 9.0968
batch: [3560/21305] batch time: 0.057 trainign loss: 7.4051 avg training loss: 9.0961
batch: [3570/21305] batch time: 0.053 trainign loss: 8.2344 avg training loss: 9.0959
batch: [3580/21305] batch time: 0.058 trainign loss: 6.9198 avg training loss: 9.0957
batch: [3590/21305] batch time: 0.054 trainign loss: 6.4747 avg training loss: 9.0953
batch: [3600/21305] batch time: 0.056 trainign loss: 8.6667 avg training loss: 9.0948
batch: [3610/21305] batch time: 0.053 trainign loss: 8.6813 avg training loss: 9.0944
batch: [3620/21305] batch time: 0.056 trainign loss: 7.8342 avg training loss: 9.0942
batch: [3630/21305] batch time: 0.062 trainign loss: 7.7780 avg training loss: 9.0939
batch: [3640/21305] batch time: 0.056 trainign loss: 7.7309 avg training loss: 9.0935
batch: [3650/21305] batch time: 0.061 trainign loss: 6.8130 avg training loss: 9.0931
batch: [3660/21305] batch time: 0.056 trainign loss: 6.5442 avg training loss: 9.0927
batch: [3670/21305] batch time: 0.052 trainign loss: 6.3116 avg training loss: 9.0922
batch: [3680/21305] batch time: 0.050 trainign loss: 7.9742 avg training loss: 9.0919
batch: [3690/21305] batch time: 0.058 trainign loss: 7.2577 avg training loss: 9.0915
batch: [3700/21305] batch time: 0.062 trainign loss: 6.6239 avg training loss: 9.0912
batch: [3710/21305] batch time: 0.062 trainign loss: 6.2413 avg training loss: 9.0907
batch: [3720/21305] batch time: 0.059 trainign loss: 7.2946 avg training loss: 9.0902
batch: [3730/21305] batch time: 0.053 trainign loss: 6.6511 avg training loss: 9.0899
batch: [3740/21305] batch time: 0.056 trainign loss: 8.1821 avg training loss: 9.0896
batch: [3750/21305] batch time: 0.056 trainign loss: 7.9134 avg training loss: 9.0892
batch: [3760/21305] batch time: 0.050 trainign loss: 7.2310 avg training loss: 9.0887
batch: [3770/21305] batch time: 0.062 trainign loss: 5.4567 avg training loss: 9.0881
batch: [3780/21305] batch time: 0.058 trainign loss: 7.0815 avg training loss: 9.0878
batch: [3790/21305] batch time: 0.057 trainign loss: 5.5947 avg training loss: 9.0873
batch: [3800/21305] batch time: 0.054 trainign loss: 6.7404 avg training loss: 9.0869
batch: [3810/21305] batch time: 0.058 trainign loss: 8.0522 avg training loss: 9.0866
batch: [3820/21305] batch time: 0.056 trainign loss: 5.8850 avg training loss: 9.0861
batch: [3830/21305] batch time: 0.063 trainign loss: 6.8868 avg training loss: 9.0857
batch: [3840/21305] batch time: 0.062 trainign loss: 7.2092 avg training loss: 9.0854
batch: [3850/21305] batch time: 0.053 trainign loss: 6.9279 avg training loss: 9.0849
batch: [3860/21305] batch time: 0.063 trainign loss: 7.0307 avg training loss: 9.0845
batch: [3870/21305] batch time: 0.055 trainign loss: 5.9932 avg training loss: 9.0841
batch: [3880/21305] batch time: 0.059 trainign loss: 0.4177 avg training loss: 9.0830
batch: [3890/21305] batch time: 0.053 trainign loss: 10.8255 avg training loss: 9.0824
batch: [3900/21305] batch time: 0.057 trainign loss: 9.3804 avg training loss: 9.0823
batch: [3910/21305] batch time: 0.053 trainign loss: 8.6692 avg training loss: 9.0823
batch: [3920/21305] batch time: 0.062 trainign loss: 6.6154 avg training loss: 9.0820
batch: [3930/21305] batch time: 0.061 trainign loss: 7.6400 avg training loss: 9.0817
batch: [3940/21305] batch time: 0.058 trainign loss: 6.4888 avg training loss: 9.0813
batch: [3950/21305] batch time: 0.052 trainign loss: 7.4125 avg training loss: 9.0810
batch: [3960/21305] batch time: 0.056 trainign loss: 8.7898 avg training loss: 9.0807
batch: [3970/21305] batch time: 0.059 trainign loss: 8.0728 avg training loss: 9.0804
batch: [3980/21305] batch time: 0.057 trainign loss: 6.4107 avg training loss: 9.0800
batch: [3990/21305] batch time: 0.051 trainign loss: 6.6536 avg training loss: 9.0795
batch: [4000/21305] batch time: 0.056 trainign loss: 8.0849 avg training loss: 9.0790
batch: [4010/21305] batch time: 0.056 trainign loss: 7.3354 avg training loss: 9.0786
batch: [4020/21305] batch time: 0.056 trainign loss: 5.1913 avg training loss: 9.0780
batch: [4030/21305] batch time: 0.055 trainign loss: 7.7755 avg training loss: 9.0777
batch: [4040/21305] batch time: 0.056 trainign loss: 7.8181 avg training loss: 9.0774
batch: [4050/21305] batch time: 0.050 trainign loss: 5.0622 avg training loss: 9.0769
batch: [4060/21305] batch time: 0.058 trainign loss: 11.9978 avg training loss: 9.0755
batch: [4070/21305] batch time: 0.056 trainign loss: 9.5708 avg training loss: 9.0751
batch: [4080/21305] batch time: 0.058 trainign loss: 8.8572 avg training loss: 9.0750
batch: [4090/21305] batch time: 0.056 trainign loss: 8.6556 avg training loss: 9.0747
batch: [4100/21305] batch time: 0.057 trainign loss: 7.1427 avg training loss: 9.0745
batch: [4110/21305] batch time: 0.054 trainign loss: 6.4314 avg training loss: 9.0741
batch: [4120/21305] batch time: 0.056 trainign loss: 6.0845 avg training loss: 9.0737
batch: [4130/21305] batch time: 0.053 trainign loss: 7.0626 avg training loss: 9.0732
batch: [4140/21305] batch time: 0.057 trainign loss: 6.0388 avg training loss: 9.0724
batch: [4150/21305] batch time: 0.051 trainign loss: 8.6159 avg training loss: 9.0722
batch: [4160/21305] batch time: 0.059 trainign loss: 8.0613 avg training loss: 9.0717
batch: [4170/21305] batch time: 0.058 trainign loss: 6.7990 avg training loss: 9.0712
batch: [4180/21305] batch time: 0.056 trainign loss: 7.9055 avg training loss: 9.0709
batch: [4190/21305] batch time: 0.051 trainign loss: 7.9726 avg training loss: 9.0705
batch: [4200/21305] batch time: 0.057 trainign loss: 6.9576 avg training loss: 9.0701
batch: [4210/21305] batch time: 0.056 trainign loss: 3.5474 avg training loss: 9.0695
batch: [4220/21305] batch time: 0.056 trainign loss: 7.1864 avg training loss: 9.0689
batch: [4230/21305] batch time: 0.051 trainign loss: 7.7750 avg training loss: 9.0683
batch: [4240/21305] batch time: 0.056 trainign loss: 6.4548 avg training loss: 9.0679
batch: [4250/21305] batch time: 0.056 trainign loss: 0.5415 avg training loss: 9.0665
batch: [4260/21305] batch time: 0.056 trainign loss: 8.6604 avg training loss: 9.0665
batch: [4270/21305] batch time: 0.054 trainign loss: 9.5551 avg training loss: 9.0665
batch: [4280/21305] batch time: 0.056 trainign loss: 8.3712 avg training loss: 9.0664
batch: [4290/21305] batch time: 0.050 trainign loss: 8.0484 avg training loss: 9.0662
batch: [4300/21305] batch time: 0.056 trainign loss: 7.8747 avg training loss: 9.0660
batch: [4310/21305] batch time: 0.051 trainign loss: 7.6432 avg training loss: 9.0656
batch: [4320/21305] batch time: 0.056 trainign loss: 5.9209 avg training loss: 9.0652
batch: [4330/21305] batch time: 0.063 trainign loss: 5.3497 avg training loss: 9.0646
batch: [4340/21305] batch time: 0.944 trainign loss: 5.7413 avg training loss: 9.0641
batch: [4350/21305] batch time: 0.056 trainign loss: 6.3122 avg training loss: 9.0637
batch: [4360/21305] batch time: 1.315 trainign loss: 5.5258 avg training loss: 9.0631
batch: [4370/21305] batch time: 0.062 trainign loss: 8.1291 avg training loss: 9.0625
batch: [4380/21305] batch time: 1.618 trainign loss: 6.6696 avg training loss: 9.0622
batch: [4390/21305] batch time: 0.062 trainign loss: 7.8482 avg training loss: 9.0616
batch: [4400/21305] batch time: 1.147 trainign loss: 8.2974 avg training loss: 9.0614
batch: [4410/21305] batch time: 0.054 trainign loss: 7.7747 avg training loss: 9.0612
batch: [4420/21305] batch time: 0.840 trainign loss: 6.6970 avg training loss: 9.0607
batch: [4430/21305] batch time: 0.052 trainign loss: 2.8390 avg training loss: 9.0600
batch: [4440/21305] batch time: 0.483 trainign loss: 10.5905 avg training loss: 9.0591
batch: [4450/21305] batch time: 0.056 trainign loss: 8.0220 avg training loss: 9.0589
batch: [4460/21305] batch time: 0.057 trainign loss: 4.9056 avg training loss: 9.0585
batch: [4470/21305] batch time: 0.056 trainign loss: 10.5907 avg training loss: 9.0577
batch: [4480/21305] batch time: 0.063 trainign loss: 8.3904 avg training loss: 9.0576
batch: [4490/21305] batch time: 0.056 trainign loss: 5.1204 avg training loss: 9.0572
batch: [4500/21305] batch time: 0.057 trainign loss: 8.8047 avg training loss: 9.0569
batch: [4510/21305] batch time: 0.054 trainign loss: 7.7279 avg training loss: 9.0563
batch: [4520/21305] batch time: 0.080 trainign loss: 8.5009 avg training loss: 9.0560
batch: [4530/21305] batch time: 0.054 trainign loss: 8.6556 avg training loss: 9.0558
batch: [4540/21305] batch time: 0.422 trainign loss: 7.4578 avg training loss: 9.0555
batch: [4550/21305] batch time: 0.059 trainign loss: 7.2919 avg training loss: 9.0551
batch: [4560/21305] batch time: 0.375 trainign loss: 8.4705 avg training loss: 9.0547
batch: [4570/21305] batch time: 0.063 trainign loss: 7.9043 avg training loss: 9.0544
batch: [4580/21305] batch time: 1.297 trainign loss: 7.8614 avg training loss: 9.0542
batch: [4590/21305] batch time: 0.056 trainign loss: 6.1918 avg training loss: 9.0537
batch: [4600/21305] batch time: 0.821 trainign loss: 6.9944 avg training loss: 9.0534
batch: [4610/21305] batch time: 0.057 trainign loss: 6.6403 avg training loss: 9.0529
batch: [4620/21305] batch time: 0.368 trainign loss: 6.9049 avg training loss: 9.0524
batch: [4630/21305] batch time: 0.053 trainign loss: 6.5569 avg training loss: 9.0520
batch: [4640/21305] batch time: 0.358 trainign loss: 6.3854 avg training loss: 9.0516
batch: [4650/21305] batch time: 0.062 trainign loss: 6.8578 avg training loss: 9.0514
batch: [4660/21305] batch time: 0.759 trainign loss: 8.1077 avg training loss: 9.0510
batch: [4670/21305] batch time: 0.054 trainign loss: 7.5554 avg training loss: 9.0504
batch: [4680/21305] batch time: 1.309 trainign loss: 8.3466 avg training loss: 9.0501
batch: [4690/21305] batch time: 0.062 trainign loss: 6.7377 avg training loss: 9.0497
batch: [4700/21305] batch time: 1.182 trainign loss: 7.2427 avg training loss: 9.0492
batch: [4710/21305] batch time: 0.056 trainign loss: 8.0221 avg training loss: 9.0489
batch: [4720/21305] batch time: 0.563 trainign loss: 7.5755 avg training loss: 9.0486
batch: [4730/21305] batch time: 0.056 trainign loss: 7.1908 avg training loss: 9.0482
batch: [4740/21305] batch time: 0.621 trainign loss: 8.5735 avg training loss: 9.0478
batch: [4750/21305] batch time: 0.056 trainign loss: 7.7090 avg training loss: 9.0475
batch: [4760/21305] batch time: 0.488 trainign loss: 5.7723 avg training loss: 9.0470
batch: [4770/21305] batch time: 0.056 trainign loss: 7.2374 avg training loss: 9.0465
batch: [4780/21305] batch time: 1.143 trainign loss: 4.0508 avg training loss: 9.0460
batch: [4790/21305] batch time: 0.054 trainign loss: 6.6716 avg training loss: 9.0454
batch: [4800/21305] batch time: 0.815 trainign loss: 8.5182 avg training loss: 9.0450
batch: [4810/21305] batch time: 0.061 trainign loss: 6.1861 avg training loss: 9.0447
batch: [4820/21305] batch time: 0.735 trainign loss: 7.7297 avg training loss: 9.0444
batch: [4830/21305] batch time: 0.055 trainign loss: 6.6366 avg training loss: 9.0439
batch: [4840/21305] batch time: 1.055 trainign loss: 2.1016 avg training loss: 9.0432
batch: [4850/21305] batch time: 0.056 trainign loss: 12.0140 avg training loss: 9.0422
batch: [4860/21305] batch time: 1.966 trainign loss: 7.6994 avg training loss: 9.0422
batch: [4870/21305] batch time: 0.057 trainign loss: 9.3792 avg training loss: 9.0422
batch: [4880/21305] batch time: 2.434 trainign loss: 8.3709 avg training loss: 9.0420
batch: [4890/21305] batch time: 0.062 trainign loss: 7.7053 avg training loss: 9.0417
batch: [4900/21305] batch time: 2.204 trainign loss: 6.9536 avg training loss: 9.0411
batch: [4910/21305] batch time: 0.063 trainign loss: 5.8190 avg training loss: 9.0405
batch: [4920/21305] batch time: 2.058 trainign loss: 8.3585 avg training loss: 9.0403
batch: [4930/21305] batch time: 0.060 trainign loss: 5.6378 avg training loss: 9.0398
batch: [4940/21305] batch time: 2.579 trainign loss: 8.3295 avg training loss: 9.0392
batch: [4950/21305] batch time: 0.062 trainign loss: 5.8160 avg training loss: 9.0388
batch: [4960/21305] batch time: 2.166 trainign loss: 7.9788 avg training loss: 9.0383
batch: [4970/21305] batch time: 0.056 trainign loss: 7.2850 avg training loss: 9.0379
batch: [4980/21305] batch time: 2.253 trainign loss: 4.9122 avg training loss: 9.0375
batch: [4990/21305] batch time: 0.051 trainign loss: 0.0065 avg training loss: 9.0359
batch: [5000/21305] batch time: 2.312 trainign loss: 8.3432 avg training loss: 9.0356
batch: [5010/21305] batch time: 0.054 trainign loss: 9.1882 avg training loss: 9.0356
batch: [5020/21305] batch time: 2.346 trainign loss: 7.6452 avg training loss: 9.0354
batch: [5030/21305] batch time: 0.062 trainign loss: 6.8030 avg training loss: 9.0349
batch: [5040/21305] batch time: 2.370 trainign loss: 3.5038 avg training loss: 9.0344
batch: [5050/21305] batch time: 0.056 trainign loss: 7.7032 avg training loss: 9.0337
batch: [5060/21305] batch time: 2.186 trainign loss: 8.7651 avg training loss: 9.0334
batch: [5070/21305] batch time: 0.057 trainign loss: 6.4781 avg training loss: 9.0332
batch: [5080/21305] batch time: 2.098 trainign loss: 0.8695 avg training loss: 9.0318
batch: [5090/21305] batch time: 0.058 trainign loss: 7.1726 avg training loss: 9.0320
batch: [5100/21305] batch time: 2.378 trainign loss: 8.6701 avg training loss: 9.0318
batch: [5110/21305] batch time: 0.058 trainign loss: 7.4600 avg training loss: 9.0316
batch: [5120/21305] batch time: 2.286 trainign loss: 7.4360 avg training loss: 9.0313
batch: [5130/21305] batch time: 0.061 trainign loss: 7.7600 avg training loss: 9.0309
batch: [5140/21305] batch time: 1.855 trainign loss: 7.6770 avg training loss: 9.0303
batch: [5150/21305] batch time: 0.519 trainign loss: 8.0725 avg training loss: 9.0301
batch: [5160/21305] batch time: 2.027 trainign loss: 8.2973 avg training loss: 9.0298
batch: [5170/21305] batch time: 0.056 trainign loss: 7.9113 avg training loss: 9.0296
batch: [5180/21305] batch time: 2.365 trainign loss: 7.2175 avg training loss: 9.0293
batch: [5190/21305] batch time: 0.056 trainign loss: 7.4510 avg training loss: 9.0289
batch: [5200/21305] batch time: 2.348 trainign loss: 6.7030 avg training loss: 9.0286
batch: [5210/21305] batch time: 0.060 trainign loss: 6.5160 avg training loss: 9.0282
batch: [5220/21305] batch time: 2.225 trainign loss: 7.4267 avg training loss: 9.0278
batch: [5230/21305] batch time: 0.057 trainign loss: 6.9923 avg training loss: 9.0275
batch: [5240/21305] batch time: 2.607 trainign loss: 6.7970 avg training loss: 9.0271
batch: [5250/21305] batch time: 0.056 trainign loss: 6.4496 avg training loss: 9.0266
batch: [5260/21305] batch time: 1.872 trainign loss: 7.3117 avg training loss: 9.0263
batch: [5270/21305] batch time: 0.558 trainign loss: 8.0757 avg training loss: 9.0260
batch: [5280/21305] batch time: 1.696 trainign loss: 7.9665 avg training loss: 9.0258
batch: [5290/21305] batch time: 0.637 trainign loss: 6.5833 avg training loss: 9.0254
batch: [5300/21305] batch time: 2.220 trainign loss: 7.8899 avg training loss: 9.0250
batch: [5310/21305] batch time: 0.452 trainign loss: 7.8570 avg training loss: 9.0246
batch: [5320/21305] batch time: 1.928 trainign loss: 7.7193 avg training loss: 9.0243
batch: [5330/21305] batch time: 0.564 trainign loss: 7.3552 avg training loss: 9.0240
batch: [5340/21305] batch time: 1.915 trainign loss: 6.6391 avg training loss: 9.0234
batch: [5350/21305] batch time: 0.408 trainign loss: 6.4755 avg training loss: 9.0228
batch: [5360/21305] batch time: 2.254 trainign loss: 8.3789 avg training loss: 9.0224
batch: [5370/21305] batch time: 0.055 trainign loss: 8.3570 avg training loss: 9.0221
batch: [5380/21305] batch time: 2.429 trainign loss: 8.1824 avg training loss: 9.0218
batch: [5390/21305] batch time: 0.061 trainign loss: 6.2354 avg training loss: 9.0215
batch: [5400/21305] batch time: 2.736 trainign loss: 7.6364 avg training loss: 9.0212
batch: [5410/21305] batch time: 0.061 trainign loss: 7.3346 avg training loss: 9.0208
batch: [5420/21305] batch time: 2.648 trainign loss: 8.3703 avg training loss: 9.0204
batch: [5430/21305] batch time: 0.062 trainign loss: 6.7747 avg training loss: 9.0200
batch: [5440/21305] batch time: 2.581 trainign loss: 7.5453 avg training loss: 9.0195
batch: [5450/21305] batch time: 0.061 trainign loss: 6.4356 avg training loss: 9.0190
batch: [5460/21305] batch time: 2.271 trainign loss: 8.1728 avg training loss: 9.0186
batch: [5470/21305] batch time: 0.053 trainign loss: 5.9105 avg training loss: 9.0182
batch: [5480/21305] batch time: 2.378 trainign loss: 8.5911 avg training loss: 9.0178
batch: [5490/21305] batch time: 0.061 trainign loss: 6.4560 avg training loss: 9.0176
batch: [5500/21305] batch time: 2.447 trainign loss: 6.1863 avg training loss: 9.0171
batch: [5510/21305] batch time: 0.056 trainign loss: 3.9083 avg training loss: 9.0165
batch: [5520/21305] batch time: 2.377 trainign loss: 7.7765 avg training loss: 9.0160
batch: [5530/21305] batch time: 0.057 trainign loss: 8.0022 avg training loss: 9.0156
batch: [5540/21305] batch time: 2.327 trainign loss: 7.7833 avg training loss: 9.0152
batch: [5550/21305] batch time: 0.075 trainign loss: 8.7719 avg training loss: 9.0150
batch: [5560/21305] batch time: 2.188 trainign loss: 5.2772 avg training loss: 9.0144
batch: [5570/21305] batch time: 0.054 trainign loss: 3.3880 avg training loss: 9.0139
batch: [5580/21305] batch time: 2.111 trainign loss: 9.8673 avg training loss: 9.0133
batch: [5590/21305] batch time: 1.141 trainign loss: 7.9505 avg training loss: 9.0131
batch: [5600/21305] batch time: 1.843 trainign loss: 8.0607 avg training loss: 9.0128
batch: [5610/21305] batch time: 0.649 trainign loss: 8.2584 avg training loss: 9.0126
batch: [5620/21305] batch time: 1.582 trainign loss: 7.7062 avg training loss: 9.0123
batch: [5630/21305] batch time: 0.445 trainign loss: 7.4095 avg training loss: 9.0119
batch: [5640/21305] batch time: 1.508 trainign loss: 6.5458 avg training loss: 9.0116
batch: [5650/21305] batch time: 1.128 trainign loss: 8.5644 avg training loss: 9.0112
batch: [5660/21305] batch time: 0.647 trainign loss: 5.3837 avg training loss: 9.0107
batch: [5670/21305] batch time: 1.319 trainign loss: 7.6957 avg training loss: 9.0104
batch: [5680/21305] batch time: 0.062 trainign loss: 6.7290 avg training loss: 9.0099
batch: [5690/21305] batch time: 1.697 trainign loss: 6.5286 avg training loss: 9.0096
batch: [5700/21305] batch time: 0.056 trainign loss: 7.2173 avg training loss: 9.0091
batch: [5710/21305] batch time: 2.644 trainign loss: 6.0350 avg training loss: 9.0087
batch: [5720/21305] batch time: 0.063 trainign loss: 7.9277 avg training loss: 9.0080
batch: [5730/21305] batch time: 2.342 trainign loss: 7.7525 avg training loss: 9.0078
batch: [5740/21305] batch time: 0.062 trainign loss: 7.8783 avg training loss: 9.0076
batch: [5750/21305] batch time: 2.044 trainign loss: 7.0040 avg training loss: 9.0073
batch: [5760/21305] batch time: 0.058 trainign loss: 6.3276 avg training loss: 9.0070
batch: [5770/21305] batch time: 2.060 trainign loss: 7.5899 avg training loss: 9.0067
batch: [5780/21305] batch time: 0.055 trainign loss: 7.3209 avg training loss: 9.0063
batch: [5790/21305] batch time: 1.480 trainign loss: 8.3719 avg training loss: 9.0060
batch: [5800/21305] batch time: 0.056 trainign loss: 6.5139 avg training loss: 9.0057
batch: [5810/21305] batch time: 0.712 trainign loss: 8.5961 avg training loss: 9.0054
batch: [5820/21305] batch time: 0.412 trainign loss: 8.1947 avg training loss: 9.0051
batch: [5830/21305] batch time: 0.057 trainign loss: 6.9415 avg training loss: 9.0048
batch: [5840/21305] batch time: 0.063 trainign loss: 7.4027 avg training loss: 9.0044
batch: [5850/21305] batch time: 0.460 trainign loss: 7.1884 avg training loss: 9.0041
batch: [5860/21305] batch time: 0.060 trainign loss: 8.1199 avg training loss: 9.0038
batch: [5870/21305] batch time: 0.113 trainign loss: 7.5332 avg training loss: 9.0035
batch: [5880/21305] batch time: 0.728 trainign loss: 7.0655 avg training loss: 9.0032
batch: [5890/21305] batch time: 0.864 trainign loss: 7.3351 avg training loss: 9.0029
batch: [5900/21305] batch time: 1.096 trainign loss: 5.9956 avg training loss: 9.0025
batch: [5910/21305] batch time: 0.825 trainign loss: 6.3158 avg training loss: 9.0019
batch: [5920/21305] batch time: 0.855 trainign loss: 6.0624 avg training loss: 9.0013
batch: [5930/21305] batch time: 0.058 trainign loss: 7.4804 avg training loss: 9.0010
batch: [5940/21305] batch time: 0.845 trainign loss: 6.7412 avg training loss: 9.0008
batch: [5950/21305] batch time: 0.056 trainign loss: 6.1628 avg training loss: 9.0003
batch: [5960/21305] batch time: 0.419 trainign loss: 5.6339 avg training loss: 8.9997
batch: [5970/21305] batch time: 0.056 trainign loss: 8.5044 avg training loss: 8.9995
batch: [5980/21305] batch time: 0.239 trainign loss: 6.9199 avg training loss: 8.9992
batch: [5990/21305] batch time: 0.056 trainign loss: 8.3568 avg training loss: 8.9991
batch: [6000/21305] batch time: 0.056 trainign loss: 7.8984 avg training loss: 8.9989
batch: [6010/21305] batch time: 0.062 trainign loss: 6.4405 avg training loss: 8.9985
batch: [6020/21305] batch time: 0.055 trainign loss: 7.9394 avg training loss: 8.9982
batch: [6030/21305] batch time: 0.056 trainign loss: 6.7592 avg training loss: 8.9976
batch: [6040/21305] batch time: 0.053 trainign loss: 3.2812 avg training loss: 8.9969
batch: [6050/21305] batch time: 0.056 trainign loss: 7.2833 avg training loss: 8.9965
batch: [6060/21305] batch time: 0.058 trainign loss: 8.1966 avg training loss: 8.9960
batch: [6070/21305] batch time: 0.052 trainign loss: 4.2222 avg training loss: 8.9955
batch: [6080/21305] batch time: 0.056 trainign loss: 4.7432 avg training loss: 8.9949
batch: [6090/21305] batch time: 0.056 trainign loss: 7.4843 avg training loss: 8.9945
batch: [6100/21305] batch time: 0.056 trainign loss: 9.1736 avg training loss: 8.9943
batch: [6110/21305] batch time: 0.057 trainign loss: 6.4949 avg training loss: 8.9939
batch: [6120/21305] batch time: 0.056 trainign loss: 7.1014 avg training loss: 8.9935
batch: [6130/21305] batch time: 0.051 trainign loss: 7.9376 avg training loss: 8.9930
batch: [6140/21305] batch time: 0.057 trainign loss: 6.4936 avg training loss: 8.9926
batch: [6150/21305] batch time: 0.051 trainign loss: 6.5598 avg training loss: 8.9923
batch: [6160/21305] batch time: 0.051 trainign loss: 6.3875 avg training loss: 8.9920
batch: [6170/21305] batch time: 0.061 trainign loss: 6.3848 avg training loss: 8.9916
batch: [6180/21305] batch time: 0.056 trainign loss: 8.1205 avg training loss: 8.9912
batch: [6190/21305] batch time: 0.053 trainign loss: 8.1287 avg training loss: 8.9909
batch: [6200/21305] batch time: 0.062 trainign loss: 7.1434 avg training loss: 8.9907
batch: [6210/21305] batch time: 0.051 trainign loss: 7.0797 avg training loss: 8.9903
batch: [6220/21305] batch time: 0.056 trainign loss: 8.0708 avg training loss: 8.9899
batch: [6230/21305] batch time: 0.056 trainign loss: 7.3952 avg training loss: 8.9896
batch: [6240/21305] batch time: 0.056 trainign loss: 8.5160 avg training loss: 8.9893
batch: [6250/21305] batch time: 0.055 trainign loss: 7.7604 avg training loss: 8.9890
batch: [6260/21305] batch time: 0.059 trainign loss: 8.5649 avg training loss: 8.9887
batch: [6270/21305] batch time: 0.056 trainign loss: 7.3377 avg training loss: 8.9884
batch: [6280/21305] batch time: 0.055 trainign loss: 7.8039 avg training loss: 8.9881
batch: [6290/21305] batch time: 0.057 trainign loss: 5.2311 avg training loss: 8.9876
batch: [6300/21305] batch time: 0.061 trainign loss: 8.3075 avg training loss: 8.9873
batch: [6310/21305] batch time: 0.056 trainign loss: 7.4194 avg training loss: 8.9869
batch: [6320/21305] batch time: 0.051 trainign loss: 7.8580 avg training loss: 8.9867
batch: [6330/21305] batch time: 0.056 trainign loss: 7.5146 avg training loss: 8.9863
batch: [6340/21305] batch time: 0.055 trainign loss: 5.2314 avg training loss: 8.9858
batch: [6350/21305] batch time: 0.056 trainign loss: 7.6115 avg training loss: 8.9854
batch: [6360/21305] batch time: 0.052 trainign loss: 8.1185 avg training loss: 8.9852
batch: [6370/21305] batch time: 0.062 trainign loss: 6.9160 avg training loss: 8.9849
batch: [6380/21305] batch time: 0.057 trainign loss: 7.2392 avg training loss: 8.9845
batch: [6390/21305] batch time: 0.056 trainign loss: 5.4925 avg training loss: 8.9840
batch: [6400/21305] batch time: 0.058 trainign loss: 7.5363 avg training loss: 8.9836
batch: [6410/21305] batch time: 0.051 trainign loss: 8.3357 avg training loss: 8.9832
batch: [6420/21305] batch time: 0.056 trainign loss: 7.2774 avg training loss: 8.9827
batch: [6430/21305] batch time: 0.051 trainign loss: 5.4197 avg training loss: 8.9822
batch: [6440/21305] batch time: 0.060 trainign loss: 7.3335 avg training loss: 8.9816
batch: [6450/21305] batch time: 0.056 trainign loss: 7.5279 avg training loss: 8.9809
batch: [6460/21305] batch time: 0.057 trainign loss: 7.2978 avg training loss: 8.9807
batch: [6470/21305] batch time: 0.061 trainign loss: 8.4447 avg training loss: 8.9804
batch: [6480/21305] batch time: 0.056 trainign loss: 7.7053 avg training loss: 8.9801
batch: [6490/21305] batch time: 0.056 trainign loss: 6.9592 avg training loss: 8.9798
batch: [6500/21305] batch time: 0.063 trainign loss: 6.2210 avg training loss: 8.9795
batch: [6510/21305] batch time: 0.055 trainign loss: 7.5306 avg training loss: 8.9791
batch: [6520/21305] batch time: 0.057 trainign loss: 7.1754 avg training loss: 8.9788
batch: [6530/21305] batch time: 0.059 trainign loss: 6.2507 avg training loss: 8.9784
batch: [6540/21305] batch time: 0.057 trainign loss: 7.6028 avg training loss: 8.9779
batch: [6550/21305] batch time: 0.062 trainign loss: 6.2728 avg training loss: 8.9776
batch: [6560/21305] batch time: 0.056 trainign loss: 6.9393 avg training loss: 8.9771
batch: [6570/21305] batch time: 0.052 trainign loss: 7.0028 avg training loss: 8.9767
batch: [6580/21305] batch time: 0.063 trainign loss: 5.2841 avg training loss: 8.9764
batch: [6590/21305] batch time: 0.056 trainign loss: 6.9675 avg training loss: 8.9760
batch: [6600/21305] batch time: 0.056 trainign loss: 8.0907 avg training loss: 8.9758
batch: [6610/21305] batch time: 0.051 trainign loss: 8.5787 avg training loss: 8.9754
batch: [6620/21305] batch time: 0.056 trainign loss: 7.2546 avg training loss: 8.9751
batch: [6630/21305] batch time: 0.051 trainign loss: 6.9027 avg training loss: 8.9746
batch: [6640/21305] batch time: 0.056 trainign loss: 5.4542 avg training loss: 8.9742
batch: [6650/21305] batch time: 0.061 trainign loss: 6.7029 avg training loss: 8.9738
batch: [6660/21305] batch time: 0.063 trainign loss: 6.5761 avg training loss: 8.9734
batch: [6670/21305] batch time: 0.057 trainign loss: 6.9156 avg training loss: 8.9729
batch: [6680/21305] batch time: 0.056 trainign loss: 7.7574 avg training loss: 8.9727
batch: [6690/21305] batch time: 0.387 trainign loss: 7.7601 avg training loss: 8.9724
batch: [6700/21305] batch time: 0.056 trainign loss: 5.3866 avg training loss: 8.9718
batch: [6710/21305] batch time: 0.824 trainign loss: 5.9729 avg training loss: 8.9714
batch: [6720/21305] batch time: 0.686 trainign loss: 7.6551 avg training loss: 8.9710
batch: [6730/21305] batch time: 0.884 trainign loss: 6.6553 avg training loss: 8.9705
batch: [6740/21305] batch time: 0.692 trainign loss: 8.1244 avg training loss: 8.9702
batch: [6750/21305] batch time: 0.319 trainign loss: 8.3326 avg training loss: 8.9701
batch: [6760/21305] batch time: 0.528 trainign loss: 7.5316 avg training loss: 8.9698
batch: [6770/21305] batch time: 0.057 trainign loss: 6.2413 avg training loss: 8.9694
batch: [6780/21305] batch time: 0.420 trainign loss: 6.5000 avg training loss: 8.9690
batch: [6790/21305] batch time: 1.080 trainign loss: 7.4083 avg training loss: 8.9686
batch: [6800/21305] batch time: 0.061 trainign loss: 6.1208 avg training loss: 8.9681
batch: [6810/21305] batch time: 1.229 trainign loss: 6.7260 avg training loss: 8.9677
batch: [6820/21305] batch time: 0.058 trainign loss: 5.3608 avg training loss: 8.9672
batch: [6830/21305] batch time: 1.814 trainign loss: 7.7476 avg training loss: 8.9668
batch: [6840/21305] batch time: 0.059 trainign loss: 9.2519 avg training loss: 8.9658
batch: [6850/21305] batch time: 0.998 trainign loss: 6.7547 avg training loss: 8.9657
batch: [6860/21305] batch time: 0.056 trainign loss: 7.5921 avg training loss: 8.9657
batch: [6870/21305] batch time: 0.918 trainign loss: 7.3157 avg training loss: 8.9651
batch: [6880/21305] batch time: 0.057 trainign loss: 6.7371 avg training loss: 8.9647
batch: [6890/21305] batch time: 0.757 trainign loss: 8.4432 avg training loss: 8.9643
batch: [6900/21305] batch time: 0.681 trainign loss: 7.3025 avg training loss: 8.9639
batch: [6910/21305] batch time: 1.627 trainign loss: 6.7105 avg training loss: 8.9636
batch: [6920/21305] batch time: 0.056 trainign loss: 8.0546 avg training loss: 8.9633
batch: [6930/21305] batch time: 1.384 trainign loss: 6.0145 avg training loss: 8.9628
batch: [6940/21305] batch time: 0.378 trainign loss: 5.2484 avg training loss: 8.9623
batch: [6950/21305] batch time: 1.529 trainign loss: 6.7997 avg training loss: 8.9618
batch: [6960/21305] batch time: 0.063 trainign loss: 5.2144 avg training loss: 8.9613
batch: [6970/21305] batch time: 2.597 trainign loss: 4.1365 avg training loss: 8.9609
batch: [6980/21305] batch time: 0.053 trainign loss: 8.0721 avg training loss: 8.9607
batch: [6990/21305] batch time: 2.283 trainign loss: 8.1010 avg training loss: 8.9605
batch: [7000/21305] batch time: 0.054 trainign loss: 6.3936 avg training loss: 8.9602
batch: [7010/21305] batch time: 2.368 trainign loss: 7.1558 avg training loss: 8.9597
batch: [7020/21305] batch time: 0.062 trainign loss: 8.2055 avg training loss: 8.9594
batch: [7030/21305] batch time: 2.478 trainign loss: 7.1108 avg training loss: 8.9591
batch: [7040/21305] batch time: 0.052 trainign loss: 7.9057 avg training loss: 8.9588
batch: [7050/21305] batch time: 2.118 trainign loss: 6.2421 avg training loss: 8.9582
batch: [7060/21305] batch time: 0.056 trainign loss: 6.4163 avg training loss: 8.9579
batch: [7070/21305] batch time: 2.249 trainign loss: 7.5236 avg training loss: 8.9576
batch: [7080/21305] batch time: 0.057 trainign loss: 7.2614 avg training loss: 8.9572
batch: [7090/21305] batch time: 2.057 trainign loss: 6.8278 avg training loss: 8.9568
batch: [7100/21305] batch time: 0.058 trainign loss: 6.8904 avg training loss: 8.9565
batch: [7110/21305] batch time: 2.583 trainign loss: 7.0554 avg training loss: 8.9560
batch: [7120/21305] batch time: 0.052 trainign loss: 4.7502 avg training loss: 8.9551
batch: [7130/21305] batch time: 2.300 trainign loss: 8.2428 avg training loss: 8.9548
batch: [7140/21305] batch time: 0.058 trainign loss: 8.4884 avg training loss: 8.9544
batch: [7150/21305] batch time: 2.200 trainign loss: 6.4410 avg training loss: 8.9542
batch: [7160/21305] batch time: 0.056 trainign loss: 8.4490 avg training loss: 8.9537
batch: [7170/21305] batch time: 2.337 trainign loss: 6.6841 avg training loss: 8.9533
batch: [7180/21305] batch time: 0.052 trainign loss: 7.3163 avg training loss: 8.9530
batch: [7190/21305] batch time: 1.905 trainign loss: 6.1050 avg training loss: 8.9527
batch: [7200/21305] batch time: 0.061 trainign loss: 6.3232 avg training loss: 8.9522
batch: [7210/21305] batch time: 2.209 trainign loss: 6.8443 avg training loss: 8.9519
batch: [7220/21305] batch time: 0.056 trainign loss: 5.7414 avg training loss: 8.9513
batch: [7230/21305] batch time: 2.208 trainign loss: 5.3598 avg training loss: 8.9509
batch: [7240/21305] batch time: 0.055 trainign loss: 7.9908 avg training loss: 8.9505
batch: [7250/21305] batch time: 2.232 trainign loss: 6.4770 avg training loss: 8.9503
batch: [7260/21305] batch time: 0.062 trainign loss: 5.5108 avg training loss: 8.9498
batch: [7270/21305] batch time: 2.180 trainign loss: 7.6768 avg training loss: 8.9495
batch: [7280/21305] batch time: 0.054 trainign loss: 6.8654 avg training loss: 8.9493
batch: [7290/21305] batch time: 2.445 trainign loss: 6.6975 avg training loss: 8.9489
batch: [7300/21305] batch time: 0.063 trainign loss: 7.2807 avg training loss: 8.9485
batch: [7310/21305] batch time: 1.614 trainign loss: 7.5735 avg training loss: 8.9481
batch: [7320/21305] batch time: 0.056 trainign loss: 7.5240 avg training loss: 8.9478
batch: [7330/21305] batch time: 1.456 trainign loss: 6.0613 avg training loss: 8.9474
batch: [7340/21305] batch time: 0.063 trainign loss: 7.8422 avg training loss: 8.9470
batch: [7350/21305] batch time: 1.529 trainign loss: 7.6429 avg training loss: 8.9466
batch: [7360/21305] batch time: 0.057 trainign loss: 7.6861 avg training loss: 8.9462
batch: [7370/21305] batch time: 0.187 trainign loss: 3.1119 avg training loss: 8.9456
batch: [7380/21305] batch time: 0.057 trainign loss: 7.1964 avg training loss: 8.9451
batch: [7390/21305] batch time: 0.626 trainign loss: 7.8956 avg training loss: 8.9448
batch: [7400/21305] batch time: 0.056 trainign loss: 5.7652 avg training loss: 8.9443
batch: [7410/21305] batch time: 0.760 trainign loss: 8.0264 avg training loss: 8.9436
batch: [7420/21305] batch time: 0.063 trainign loss: 7.9594 avg training loss: 8.9434
batch: [7430/21305] batch time: 1.436 trainign loss: 7.6232 avg training loss: 8.9432
batch: [7440/21305] batch time: 0.062 trainign loss: 7.1226 avg training loss: 8.9429
batch: [7450/21305] batch time: 1.677 trainign loss: 7.5754 avg training loss: 8.9426
batch: [7460/21305] batch time: 0.055 trainign loss: 4.0001 avg training loss: 8.9420
batch: [7470/21305] batch time: 1.376 trainign loss: 3.9404 avg training loss: 8.9415
batch: [7480/21305] batch time: 0.062 trainign loss: 6.9821 avg training loss: 8.9411
batch: [7490/21305] batch time: 0.878 trainign loss: 7.6937 avg training loss: 8.9408
batch: [7500/21305] batch time: 0.056 trainign loss: 8.0840 avg training loss: 8.9406
batch: [7510/21305] batch time: 0.858 trainign loss: 6.8049 avg training loss: 8.9402
batch: [7520/21305] batch time: 0.056 trainign loss: 7.9337 avg training loss: 8.9399
batch: [7530/21305] batch time: 0.054 trainign loss: 6.5371 avg training loss: 8.9396
batch: [7540/21305] batch time: 0.061 trainign loss: 7.8492 avg training loss: 8.9392
batch: [7550/21305] batch time: 0.467 trainign loss: 8.1115 avg training loss: 8.9389
batch: [7560/21305] batch time: 0.056 trainign loss: 6.9143 avg training loss: 8.9386
batch: [7570/21305] batch time: 0.333 trainign loss: 7.5971 avg training loss: 8.9381
batch: [7580/21305] batch time: 0.056 trainign loss: 6.9831 avg training loss: 8.9379
batch: [7590/21305] batch time: 0.512 trainign loss: 8.1928 avg training loss: 8.9375
batch: [7600/21305] batch time: 0.056 trainign loss: 6.6228 avg training loss: 8.9372
batch: [7610/21305] batch time: 0.153 trainign loss: 6.5537 avg training loss: 8.9368
batch: [7620/21305] batch time: 0.056 trainign loss: 5.3944 avg training loss: 8.9362
batch: [7630/21305] batch time: 0.054 trainign loss: 8.0459 avg training loss: 8.9358
batch: [7640/21305] batch time: 0.058 trainign loss: 7.2212 avg training loss: 8.9354
batch: [7650/21305] batch time: 0.054 trainign loss: 6.1176 avg training loss: 8.9350
batch: [7660/21305] batch time: 0.057 trainign loss: 7.8127 avg training loss: 8.9345
batch: [7670/21305] batch time: 0.055 trainign loss: 7.7630 avg training loss: 8.9342
batch: [7680/21305] batch time: 0.056 trainign loss: 5.9626 avg training loss: 8.9338
batch: [7690/21305] batch time: 0.052 trainign loss: 5.6862 avg training loss: 8.9333
batch: [7700/21305] batch time: 0.056 trainign loss: 7.8054 avg training loss: 8.9328
batch: [7710/21305] batch time: 0.056 trainign loss: 5.7556 avg training loss: 8.9322
batch: [7720/21305] batch time: 0.056 trainign loss: 7.2554 avg training loss: 8.9318
batch: [7730/21305] batch time: 0.056 trainign loss: 5.1242 avg training loss: 8.9309
batch: [7740/21305] batch time: 0.056 trainign loss: 7.9190 avg training loss: 8.9306
batch: [7750/21305] batch time: 0.051 trainign loss: 8.6785 avg training loss: 8.9305
batch: [7760/21305] batch time: 0.058 trainign loss: 7.7610 avg training loss: 8.9304
batch: [7770/21305] batch time: 0.051 trainign loss: 6.2331 avg training loss: 8.9300
batch: [7780/21305] batch time: 0.056 trainign loss: 7.8511 avg training loss: 8.9297
batch: [7790/21305] batch time: 0.056 trainign loss: 7.4116 avg training loss: 8.9294
batch: [7800/21305] batch time: 0.063 trainign loss: 7.5327 avg training loss: 8.9290
batch: [7810/21305] batch time: 0.059 trainign loss: 6.9155 avg training loss: 8.9285
batch: [7820/21305] batch time: 0.056 trainign loss: 6.8992 avg training loss: 8.9282
batch: [7830/21305] batch time: 0.055 trainign loss: 6.8985 avg training loss: 8.9279
batch: [7840/21305] batch time: 0.056 trainign loss: 7.6462 avg training loss: 8.9276
batch: [7850/21305] batch time: 0.056 trainign loss: 6.0431 avg training loss: 8.9272
batch: [7860/21305] batch time: 0.062 trainign loss: 6.0289 avg training loss: 8.9268
batch: [7870/21305] batch time: 0.056 trainign loss: 6.4276 avg training loss: 8.9264
batch: [7880/21305] batch time: 0.056 trainign loss: 5.4500 avg training loss: 8.9260
batch: [7890/21305] batch time: 0.052 trainign loss: 6.8177 avg training loss: 8.9255
batch: [7900/21305] batch time: 0.063 trainign loss: 6.6148 avg training loss: 8.9248
batch: [7910/21305] batch time: 0.057 trainign loss: 8.4113 avg training loss: 8.9247
batch: [7920/21305] batch time: 0.056 trainign loss: 6.9073 avg training loss: 8.9245
batch: [7930/21305] batch time: 0.051 trainign loss: 7.6829 avg training loss: 8.9243
batch: [7940/21305] batch time: 0.060 trainign loss: 7.9359 avg training loss: 8.9241
batch: [7950/21305] batch time: 0.063 trainign loss: 7.5827 avg training loss: 8.9239
batch: [7960/21305] batch time: 0.056 trainign loss: 6.9660 avg training loss: 8.9235
batch: [7970/21305] batch time: 0.051 trainign loss: 7.0772 avg training loss: 8.9232
batch: [7980/21305] batch time: 0.056 trainign loss: 8.2053 avg training loss: 8.9227
batch: [7990/21305] batch time: 0.053 trainign loss: 8.1290 avg training loss: 8.9225
batch: [8000/21305] batch time: 0.062 trainign loss: 7.8700 avg training loss: 8.9222
batch: [8010/21305] batch time: 0.056 trainign loss: 6.6855 avg training loss: 8.9217
batch: [8020/21305] batch time: 0.056 trainign loss: 3.0312 avg training loss: 8.9211
batch: [8030/21305] batch time: 0.050 trainign loss: 6.3851 avg training loss: 8.9203
batch: [8040/21305] batch time: 0.061 trainign loss: 8.6316 avg training loss: 8.9201
batch: [8050/21305] batch time: 0.054 trainign loss: 8.9202 avg training loss: 8.9200
batch: [8060/21305] batch time: 0.056 trainign loss: 8.1165 avg training loss: 8.9199
batch: [8070/21305] batch time: 0.052 trainign loss: 7.3242 avg training loss: 8.9197
batch: [8080/21305] batch time: 0.056 trainign loss: 7.6506 avg training loss: 8.9195
batch: [8090/21305] batch time: 0.051 trainign loss: 7.7904 avg training loss: 8.9191
batch: [8100/21305] batch time: 0.062 trainign loss: 6.9641 avg training loss: 8.9188
batch: [8110/21305] batch time: 0.056 trainign loss: 8.0920 avg training loss: 8.9184
batch: [8120/21305] batch time: 0.056 trainign loss: 6.9493 avg training loss: 8.9180
batch: [8130/21305] batch time: 0.052 trainign loss: 6.8969 avg training loss: 8.9176
batch: [8140/21305] batch time: 0.056 trainign loss: 7.0781 avg training loss: 8.9173
batch: [8150/21305] batch time: 0.052 trainign loss: 6.5015 avg training loss: 8.9169
batch: [8160/21305] batch time: 0.057 trainign loss: 7.2712 avg training loss: 8.9165
batch: [8170/21305] batch time: 0.055 trainign loss: 7.4034 avg training loss: 8.9162
batch: [8180/21305] batch time: 0.063 trainign loss: 6.8709 avg training loss: 8.9158
batch: [8190/21305] batch time: 0.052 trainign loss: 7.5479 avg training loss: 8.9156
batch: [8200/21305] batch time: 0.056 trainign loss: 7.1724 avg training loss: 8.9153
batch: [8210/21305] batch time: 0.051 trainign loss: 6.9602 avg training loss: 8.9149
batch: [8220/21305] batch time: 0.056 trainign loss: 6.9330 avg training loss: 8.9146
batch: [8230/21305] batch time: 0.051 trainign loss: 7.7645 avg training loss: 8.9142
batch: [8240/21305] batch time: 0.063 trainign loss: 7.8364 avg training loss: 8.9139
batch: [8250/21305] batch time: 0.056 trainign loss: 6.8517 avg training loss: 8.9136
batch: [8260/21305] batch time: 0.056 trainign loss: 7.1173 avg training loss: 8.9132
batch: [8270/21305] batch time: 0.059 trainign loss: 8.2628 avg training loss: 8.9129
batch: [8280/21305] batch time: 0.059 trainign loss: 6.8994 avg training loss: 8.9126
batch: [8290/21305] batch time: 0.051 trainign loss: 6.6099 avg training loss: 8.9121
batch: [8300/21305] batch time: 0.056 trainign loss: 6.5212 avg training loss: 8.9118
batch: [8310/21305] batch time: 0.053 trainign loss: 8.0446 avg training loss: 8.9115
batch: [8320/21305] batch time: 0.058 trainign loss: 7.2067 avg training loss: 8.9112
batch: [8330/21305] batch time: 0.060 trainign loss: 7.6437 avg training loss: 8.9109
batch: [8340/21305] batch time: 0.061 trainign loss: 6.5473 avg training loss: 8.9104
batch: [8350/21305] batch time: 0.051 trainign loss: 5.5786 avg training loss: 8.9099
batch: [8360/21305] batch time: 0.056 trainign loss: 5.8588 avg training loss: 8.9095
batch: [8370/21305] batch time: 0.056 trainign loss: 8.3882 avg training loss: 8.9091
batch: [8380/21305] batch time: 0.062 trainign loss: 5.2622 avg training loss: 8.9087
batch: [8390/21305] batch time: 0.051 trainign loss: 4.6282 avg training loss: 8.9079
batch: [8400/21305] batch time: 0.056 trainign loss: 9.8179 avg training loss: 8.9071
batch: [8410/21305] batch time: 0.055 trainign loss: 7.6567 avg training loss: 8.9070
batch: [8420/21305] batch time: 0.062 trainign loss: 7.9536 avg training loss: 8.9062
batch: [8430/21305] batch time: 0.056 trainign loss: 8.6963 avg training loss: 8.9061
batch: [8440/21305] batch time: 0.058 trainign loss: 9.0116 avg training loss: 8.9060
batch: [8450/21305] batch time: 0.052 trainign loss: 8.2379 avg training loss: 8.9058
batch: [8460/21305] batch time: 0.058 trainign loss: 8.0094 avg training loss: 8.9056
batch: [8470/21305] batch time: 0.057 trainign loss: 8.1789 avg training loss: 8.9053
batch: [8480/21305] batch time: 0.058 trainign loss: 7.4577 avg training loss: 8.9050
batch: [8490/21305] batch time: 0.052 trainign loss: 7.3114 avg training loss: 8.9046
batch: [8500/21305] batch time: 0.058 trainign loss: 6.0403 avg training loss: 8.9042
batch: [8510/21305] batch time: 0.052 trainign loss: 6.4470 avg training loss: 8.9037
batch: [8520/21305] batch time: 0.058 trainign loss: 7.4690 avg training loss: 8.9033
batch: [8530/21305] batch time: 0.055 trainign loss: 7.7902 avg training loss: 8.9029
batch: [8540/21305] batch time: 0.057 trainign loss: 7.6249 avg training loss: 8.9026
batch: [8550/21305] batch time: 0.056 trainign loss: 8.4410 avg training loss: 8.9023
batch: [8560/21305] batch time: 0.056 trainign loss: 7.6967 avg training loss: 8.9020
batch: [8570/21305] batch time: 0.056 trainign loss: 6.3123 avg training loss: 8.9016
batch: [8580/21305] batch time: 0.056 trainign loss: 6.8735 avg training loss: 8.9013
batch: [8590/21305] batch time: 0.057 trainign loss: 7.8361 avg training loss: 8.9009
batch: [8600/21305] batch time: 0.056 trainign loss: 7.1791 avg training loss: 8.9006
batch: [8610/21305] batch time: 0.056 trainign loss: 6.7079 avg training loss: 8.9001
batch: [8620/21305] batch time: 0.062 trainign loss: 7.0828 avg training loss: 8.8998
batch: [8630/21305] batch time: 0.063 trainign loss: 5.5621 avg training loss: 8.8994
batch: [8640/21305] batch time: 0.056 trainign loss: 7.3339 avg training loss: 8.8991
batch: [8650/21305] batch time: 0.062 trainign loss: 5.8225 avg training loss: 8.8987
batch: [8660/21305] batch time: 0.054 trainign loss: 4.9746 avg training loss: 8.8982
batch: [8670/21305] batch time: 0.057 trainign loss: 6.2177 avg training loss: 8.8978
batch: [8680/21305] batch time: 0.056 trainign loss: 8.0252 avg training loss: 8.8975
batch: [8690/21305] batch time: 0.058 trainign loss: 7.0503 avg training loss: 8.8972
batch: [8700/21305] batch time: 0.051 trainign loss: 5.9166 avg training loss: 8.8968
batch: [8710/21305] batch time: 0.056 trainign loss: 1.5102 avg training loss: 8.8961
batch: [8720/21305] batch time: 0.060 trainign loss: 0.0005 avg training loss: 8.8944
batch: [8730/21305] batch time: 0.050 trainign loss: 0.0000 avg training loss: 8.8926
batch: [8740/21305] batch time: 0.057 trainign loss: 10.0387 avg training loss: 8.8929
batch: [8750/21305] batch time: 0.056 trainign loss: 9.4268 avg training loss: 8.8930
batch: [8760/21305] batch time: 0.063 trainign loss: 7.8968 avg training loss: 8.8928
batch: [8770/21305] batch time: 0.056 trainign loss: 7.2470 avg training loss: 8.8925
batch: [8780/21305] batch time: 0.056 trainign loss: 6.8274 avg training loss: 8.8921
batch: [8790/21305] batch time: 0.056 trainign loss: 7.5947 avg training loss: 8.8919
batch: [8800/21305] batch time: 0.397 trainign loss: 5.9746 avg training loss: 8.8914
batch: [8810/21305] batch time: 0.056 trainign loss: 8.4760 avg training loss: 8.8910
batch: [8820/21305] batch time: 2.059 trainign loss: 7.4292 avg training loss: 8.8909
batch: [8830/21305] batch time: 0.056 trainign loss: 7.8396 avg training loss: 8.8907
batch: [8840/21305] batch time: 2.254 trainign loss: 7.1042 avg training loss: 8.8903
batch: [8850/21305] batch time: 0.063 trainign loss: 7.1266 avg training loss: 8.8899
batch: [8860/21305] batch time: 2.510 trainign loss: 7.9445 avg training loss: 8.8897
batch: [8870/21305] batch time: 0.055 trainign loss: 7.5613 avg training loss: 8.8895
batch: [8880/21305] batch time: 2.220 trainign loss: 6.1583 avg training loss: 8.8891
batch: [8890/21305] batch time: 0.052 trainign loss: 7.9458 avg training loss: 8.8889
batch: [8900/21305] batch time: 2.600 trainign loss: 6.4893 avg training loss: 8.8884
batch: [8910/21305] batch time: 0.458 trainign loss: 6.2107 avg training loss: 8.8880
batch: [8920/21305] batch time: 1.774 trainign loss: 3.8019 avg training loss: 8.8875
batch: [8930/21305] batch time: 0.504 trainign loss: 8.3492 avg training loss: 8.8871
batch: [8940/21305] batch time: 0.767 trainign loss: 7.4765 avg training loss: 8.8869
batch: [8950/21305] batch time: 1.392 trainign loss: 7.1915 avg training loss: 8.8867
batch: [8960/21305] batch time: 0.583 trainign loss: 7.4025 avg training loss: 8.8864
batch: [8970/21305] batch time: 1.582 trainign loss: 6.9700 avg training loss: 8.8862
batch: [8980/21305] batch time: 0.272 trainign loss: 6.6620 avg training loss: 8.8857
batch: [8990/21305] batch time: 1.139 trainign loss: 7.1926 avg training loss: 8.8853
batch: [9000/21305] batch time: 0.326 trainign loss: 1.3676 avg training loss: 8.8846
batch: [9010/21305] batch time: 0.056 trainign loss: 8.8965 avg training loss: 8.8842
batch: [9020/21305] batch time: 0.054 trainign loss: 8.1394 avg training loss: 8.8841
batch: [9030/21305] batch time: 0.062 trainign loss: 7.5850 avg training loss: 8.8839
batch: [9040/21305] batch time: 0.052 trainign loss: 7.5061 avg training loss: 8.8837
batch: [9050/21305] batch time: 0.061 trainign loss: 6.9367 avg training loss: 8.8834
batch: [9060/21305] batch time: 0.052 trainign loss: 5.6452 avg training loss: 8.8829
batch: [9070/21305] batch time: 0.056 trainign loss: 3.8783 avg training loss: 8.8823
batch: [9080/21305] batch time: 0.056 trainign loss: 8.3221 avg training loss: 8.8821
batch: [9090/21305] batch time: 0.056 trainign loss: 6.7284 avg training loss: 8.8818
batch: [9100/21305] batch time: 0.057 trainign loss: 7.6573 avg training loss: 8.8815
batch: [9110/21305] batch time: 0.057 trainign loss: 6.0807 avg training loss: 8.8812
batch: [9120/21305] batch time: 0.062 trainign loss: 8.1125 avg training loss: 8.8807
batch: [9130/21305] batch time: 0.062 trainign loss: 5.7442 avg training loss: 8.8801
batch: [9140/21305] batch time: 0.051 trainign loss: 7.7660 avg training loss: 8.8796
batch: [9150/21305] batch time: 0.057 trainign loss: 6.9936 avg training loss: 8.8793
batch: [9160/21305] batch time: 0.055 trainign loss: 8.0245 avg training loss: 8.8790
batch: [9170/21305] batch time: 0.056 trainign loss: 6.4999 avg training loss: 8.8786
batch: [9180/21305] batch time: 0.051 trainign loss: 7.5063 avg training loss: 8.8784
batch: [9190/21305] batch time: 0.062 trainign loss: 7.7044 avg training loss: 8.8781
batch: [9200/21305] batch time: 1.002 trainign loss: 8.0343 avg training loss: 8.8778
batch: [9210/21305] batch time: 0.056 trainign loss: 6.8414 avg training loss: 8.8774
batch: [9220/21305] batch time: 0.331 trainign loss: 5.8125 avg training loss: 8.8770
batch: [9230/21305] batch time: 1.288 trainign loss: 7.3351 avg training loss: 8.8768
batch: [9240/21305] batch time: 0.278 trainign loss: 6.1218 avg training loss: 8.8764
batch: [9250/21305] batch time: 1.160 trainign loss: 7.6841 avg training loss: 8.8760
batch: [9260/21305] batch time: 0.741 trainign loss: 7.4951 avg training loss: 8.8757
batch: [9270/21305] batch time: 1.301 trainign loss: 7.3422 avg training loss: 8.8754
batch: [9280/21305] batch time: 1.300 trainign loss: 7.0042 avg training loss: 8.8750
batch: [9290/21305] batch time: 1.418 trainign loss: 5.7366 avg training loss: 8.8745
batch: [9300/21305] batch time: 0.720 trainign loss: 8.1060 avg training loss: 8.8740
batch: [9310/21305] batch time: 1.580 trainign loss: 7.9460 avg training loss: 8.8737
batch: [9320/21305] batch time: 0.277 trainign loss: 6.1612 avg training loss: 8.8733
batch: [9330/21305] batch time: 1.608 trainign loss: 6.0280 avg training loss: 8.8729
batch: [9340/21305] batch time: 0.056 trainign loss: 7.5290 avg training loss: 8.8727
batch: [9350/21305] batch time: 1.540 trainign loss: 7.8377 avg training loss: 8.8725
batch: [9360/21305] batch time: 0.056 trainign loss: 6.4174 avg training loss: 8.8721
batch: [9370/21305] batch time: 1.346 trainign loss: 5.8378 avg training loss: 8.8717
batch: [9380/21305] batch time: 0.056 trainign loss: 6.4208 avg training loss: 8.8713
batch: [9390/21305] batch time: 1.152 trainign loss: 7.3199 avg training loss: 8.8710
batch: [9400/21305] batch time: 0.055 trainign loss: 7.5480 avg training loss: 8.8707
batch: [9410/21305] batch time: 2.135 trainign loss: 6.2077 avg training loss: 8.8703
batch: [9420/21305] batch time: 0.056 trainign loss: 4.2782 avg training loss: 8.8698
batch: [9430/21305] batch time: 2.267 trainign loss: 7.5919 avg training loss: 8.8694
batch: [9440/21305] batch time: 0.061 trainign loss: 8.4320 avg training loss: 8.8691
batch: [9450/21305] batch time: 1.011 trainign loss: 5.1764 avg training loss: 8.8685
batch: [9460/21305] batch time: 0.063 trainign loss: 8.1075 avg training loss: 8.8682
batch: [9470/21305] batch time: 0.699 trainign loss: 8.6579 avg training loss: 8.8679
batch: [9480/21305] batch time: 0.055 trainign loss: 6.8613 avg training loss: 8.8677
batch: [9490/21305] batch time: 0.063 trainign loss: 8.3402 avg training loss: 8.8672
batch: [9500/21305] batch time: 0.257 trainign loss: 8.2024 avg training loss: 8.8671
batch: [9510/21305] batch time: 0.490 trainign loss: 6.7462 avg training loss: 8.8666
batch: [9520/21305] batch time: 0.144 trainign loss: 7.9867 avg training loss: 8.8664
batch: [9530/21305] batch time: 0.057 trainign loss: 7.6094 avg training loss: 8.8662
batch: [9540/21305] batch time: 0.204 trainign loss: 6.6609 avg training loss: 8.8658
batch: [9550/21305] batch time: 0.376 trainign loss: 5.6839 avg training loss: 8.8654
batch: [9560/21305] batch time: 0.509 trainign loss: 5.7298 avg training loss: 8.8650
batch: [9570/21305] batch time: 0.056 trainign loss: 7.8902 avg training loss: 8.8647
batch: [9580/21305] batch time: 0.838 trainign loss: 9.4053 avg training loss: 8.8641
batch: [9590/21305] batch time: 0.058 trainign loss: 7.7231 avg training loss: 8.8640
batch: [9600/21305] batch time: 1.168 trainign loss: 8.0368 avg training loss: 8.8638
batch: [9610/21305] batch time: 0.062 trainign loss: 6.9137 avg training loss: 8.8634
batch: [9620/21305] batch time: 0.343 trainign loss: 7.4548 avg training loss: 8.8631
batch: [9630/21305] batch time: 0.056 trainign loss: 8.7685 avg training loss: 8.8627
batch: [9640/21305] batch time: 0.096 trainign loss: 8.3906 avg training loss: 8.8625
batch: [9650/21305] batch time: 0.056 trainign loss: 7.0554 avg training loss: 8.8623
batch: [9660/21305] batch time: 1.251 trainign loss: 5.3575 avg training loss: 8.8618
batch: [9670/21305] batch time: 0.059 trainign loss: 7.2609 avg training loss: 8.8616
batch: [9680/21305] batch time: 0.482 trainign loss: 7.7818 avg training loss: 8.8613
batch: [9690/21305] batch time: 0.060 trainign loss: 7.2968 avg training loss: 8.8610
batch: [9700/21305] batch time: 0.979 trainign loss: 6.9186 avg training loss: 8.8607
batch: [9710/21305] batch time: 0.062 trainign loss: 6.9748 avg training loss: 8.8604
batch: [9720/21305] batch time: 1.776 trainign loss: 7.8307 avg training loss: 8.8601
batch: [9730/21305] batch time: 0.056 trainign loss: 7.4173 avg training loss: 8.8599
batch: [9740/21305] batch time: 1.463 trainign loss: 6.9534 avg training loss: 8.8595
batch: [9750/21305] batch time: 0.055 trainign loss: 7.8804 avg training loss: 8.8591
batch: [9760/21305] batch time: 1.770 trainign loss: 6.8980 avg training loss: 8.8587
batch: [9770/21305] batch time: 0.061 trainign loss: 7.5992 avg training loss: 8.8583
batch: [9780/21305] batch time: 1.601 trainign loss: 8.3640 avg training loss: 8.8581
batch: [9790/21305] batch time: 0.057 trainign loss: 7.2805 avg training loss: 8.8578
batch: [9800/21305] batch time: 1.147 trainign loss: 6.8778 avg training loss: 8.8574
batch: [9810/21305] batch time: 0.057 trainign loss: 8.2992 avg training loss: 8.8570
batch: [9820/21305] batch time: 2.323 trainign loss: 6.1961 avg training loss: 8.8566
batch: [9830/21305] batch time: 0.052 trainign loss: 6.0708 avg training loss: 8.8561
batch: [9840/21305] batch time: 2.250 trainign loss: 4.9654 avg training loss: 8.8555
batch: [9850/21305] batch time: 0.061 trainign loss: 7.5318 avg training loss: 8.8553
batch: [9860/21305] batch time: 2.264 trainign loss: 4.1323 avg training loss: 8.8549
batch: [9870/21305] batch time: 0.061 trainign loss: 5.8464 avg training loss: 8.8545
batch: [9880/21305] batch time: 2.185 trainign loss: 8.1344 avg training loss: 8.8540
batch: [9890/21305] batch time: 0.056 trainign loss: 7.3445 avg training loss: 8.8536
batch: [9900/21305] batch time: 2.488 trainign loss: 7.0336 avg training loss: 8.8534
batch: [9910/21305] batch time: 0.063 trainign loss: 7.5090 avg training loss: 8.8531
batch: [9920/21305] batch time: 1.895 trainign loss: 6.9091 avg training loss: 8.8528
batch: [9930/21305] batch time: 0.059 trainign loss: 6.4708 avg training loss: 8.8524
batch: [9940/21305] batch time: 1.855 trainign loss: 8.5220 avg training loss: 8.8521
batch: [9950/21305] batch time: 0.059 trainign loss: 7.0368 avg training loss: 8.8517
batch: [9960/21305] batch time: 2.391 trainign loss: 7.8812 avg training loss: 8.8514
batch: [9970/21305] batch time: 0.058 trainign loss: 7.6800 avg training loss: 8.8512
batch: [9980/21305] batch time: 1.596 trainign loss: 6.9004 avg training loss: 8.8509
batch: [9990/21305] batch time: 0.058 trainign loss: 7.5488 avg training loss: 8.8506
batch: [10000/21305] batch time: 1.807 trainign loss: 6.1405 avg training loss: 8.8503
batch: [10010/21305] batch time: 0.056 trainign loss: 7.0067 avg training loss: 8.8500
batch: [10020/21305] batch time: 0.685 trainign loss: 7.8679 avg training loss: 8.8497
batch: [10030/21305] batch time: 0.057 trainign loss: 6.9889 avg training loss: 8.8494
batch: [10040/21305] batch time: 1.089 trainign loss: 6.8303 avg training loss: 8.8491
batch: [10050/21305] batch time: 0.060 trainign loss: 7.0685 avg training loss: 8.8487
batch: [10060/21305] batch time: 1.048 trainign loss: 5.0502 avg training loss: 8.8482
batch: [10070/21305] batch time: 0.056 trainign loss: 6.8479 avg training loss: 8.8477
batch: [10080/21305] batch time: 0.576 trainign loss: 5.4356 avg training loss: 8.8474
batch: [10090/21305] batch time: 0.058 trainign loss: 8.7638 avg training loss: 8.8469
batch: [10100/21305] batch time: 0.739 trainign loss: 8.6062 avg training loss: 8.8465
batch: [10110/21305] batch time: 0.056 trainign loss: 6.9715 avg training loss: 8.8463
batch: [10120/21305] batch time: 0.866 trainign loss: 8.1101 avg training loss: 8.8460
batch: [10130/21305] batch time: 0.055 trainign loss: 6.8642 avg training loss: 8.8458
batch: [10140/21305] batch time: 0.512 trainign loss: 7.6456 avg training loss: 8.8455
batch: [10150/21305] batch time: 0.056 trainign loss: 7.3223 avg training loss: 8.8451
batch: [10160/21305] batch time: 0.871 trainign loss: 6.8156 avg training loss: 8.8449
batch: [10170/21305] batch time: 0.056 trainign loss: 7.5582 avg training loss: 8.8446
batch: [10180/21305] batch time: 0.872 trainign loss: 7.4817 avg training loss: 8.8443
batch: [10190/21305] batch time: 0.058 trainign loss: 7.6480 avg training loss: 8.8440
batch: [10200/21305] batch time: 0.594 trainign loss: 6.2562 avg training loss: 8.8436
batch: [10210/21305] batch time: 0.059 trainign loss: 6.9339 avg training loss: 8.8433
batch: [10220/21305] batch time: 0.840 trainign loss: 6.9679 avg training loss: 8.8427
batch: [10230/21305] batch time: 0.062 trainign loss: 5.9705 avg training loss: 8.8424
batch: [10240/21305] batch time: 0.920 trainign loss: 7.9170 avg training loss: 8.8420
batch: [10250/21305] batch time: 0.061 trainign loss: 8.1799 avg training loss: 8.8415
batch: [10260/21305] batch time: 0.865 trainign loss: 8.2732 avg training loss: 8.8413
batch: [10270/21305] batch time: 0.056 trainign loss: 7.5854 avg training loss: 8.8410
batch: [10280/21305] batch time: 0.311 trainign loss: 7.5082 avg training loss: 8.8407
batch: [10290/21305] batch time: 0.057 trainign loss: 6.7818 avg training loss: 8.8403
batch: [10300/21305] batch time: 0.056 trainign loss: 6.4727 avg training loss: 8.8399
batch: [10310/21305] batch time: 0.056 trainign loss: 5.9924 avg training loss: 8.8394
batch: [10320/21305] batch time: 0.765 trainign loss: 2.9316 avg training loss: 8.8389
batch: [10330/21305] batch time: 0.061 trainign loss: 7.4696 avg training loss: 8.8386
batch: [10340/21305] batch time: 0.779 trainign loss: 8.2459 avg training loss: 8.8384
batch: [10350/21305] batch time: 0.056 trainign loss: 8.1612 avg training loss: 8.8381
batch: [10360/21305] batch time: 0.552 trainign loss: 7.1939 avg training loss: 8.8379
batch: [10370/21305] batch time: 0.056 trainign loss: 7.0665 avg training loss: 8.8376
batch: [10380/21305] batch time: 0.704 trainign loss: 7.8106 avg training loss: 8.8374
batch: [10390/21305] batch time: 0.062 trainign loss: 6.8845 avg training loss: 8.8370
batch: [10400/21305] batch time: 0.251 trainign loss: 6.4716 avg training loss: 8.8365
batch: [10410/21305] batch time: 0.056 trainign loss: 7.4048 avg training loss: 8.8361
batch: [10420/21305] batch time: 1.011 trainign loss: 8.4876 avg training loss: 8.8359
batch: [10430/21305] batch time: 0.059 trainign loss: 5.8824 avg training loss: 8.8356
batch: [10440/21305] batch time: 0.054 trainign loss: 6.7998 avg training loss: 8.8351
batch: [10450/21305] batch time: 0.056 trainign loss: 6.9780 avg training loss: 8.8348
batch: [10460/21305] batch time: 0.353 trainign loss: 7.9413 avg training loss: 8.8345
batch: [10470/21305] batch time: 0.057 trainign loss: 6.0578 avg training loss: 8.8343
batch: [10480/21305] batch time: 0.054 trainign loss: 8.1565 avg training loss: 8.8338
batch: [10490/21305] batch time: 0.056 trainign loss: 7.3244 avg training loss: 8.8336
batch: [10500/21305] batch time: 0.161 trainign loss: 6.2680 avg training loss: 8.8333
batch: [10510/21305] batch time: 0.056 trainign loss: 5.5057 avg training loss: 8.8327
batch: [10520/21305] batch time: 0.702 trainign loss: 7.5380 avg training loss: 8.8324
batch: [10530/21305] batch time: 0.056 trainign loss: 7.3067 avg training loss: 8.8322
batch: [10540/21305] batch time: 1.437 trainign loss: 5.9754 avg training loss: 8.8319
batch: [10550/21305] batch time: 0.061 trainign loss: 6.5866 avg training loss: 8.8315
batch: [10560/21305] batch time: 1.086 trainign loss: 9.0252 avg training loss: 8.8307
batch: [10570/21305] batch time: 0.056 trainign loss: 8.2917 avg training loss: 8.8305
batch: [10580/21305] batch time: 2.145 trainign loss: 8.9156 avg training loss: 8.8305
batch: [10590/21305] batch time: 0.062 trainign loss: 8.2063 avg training loss: 8.8303
batch: [10600/21305] batch time: 0.745 trainign loss: 7.3114 avg training loss: 8.8301
batch: [10610/21305] batch time: 0.062 trainign loss: 5.4485 avg training loss: 8.8297
batch: [10620/21305] batch time: 0.425 trainign loss: 7.4713 avg training loss: 8.8294
batch: [10630/21305] batch time: 0.132 trainign loss: 6.8578 avg training loss: 8.8291
batch: [10640/21305] batch time: 0.803 trainign loss: 6.8919 avg training loss: 8.8286
batch: [10650/21305] batch time: 0.056 trainign loss: 8.4589 avg training loss: 8.8283
batch: [10660/21305] batch time: 0.055 trainign loss: 7.5809 avg training loss: 8.8280
batch: [10670/21305] batch time: 0.059 trainign loss: 5.3498 avg training loss: 8.8277
batch: [10680/21305] batch time: 0.051 trainign loss: 7.9010 avg training loss: 8.8273
batch: [10690/21305] batch time: 0.061 trainign loss: 7.2896 avg training loss: 8.8268
batch: [10700/21305] batch time: 0.062 trainign loss: 7.7723 avg training loss: 8.8264
batch: [10710/21305] batch time: 0.062 trainign loss: 6.8619 avg training loss: 8.8259
batch: [10720/21305] batch time: 0.054 trainign loss: 8.1972 avg training loss: 8.8253
batch: [10730/21305] batch time: 0.058 trainign loss: 8.4166 avg training loss: 8.8251
batch: [10740/21305] batch time: 0.056 trainign loss: 8.1971 avg training loss: 8.8249
batch: [10750/21305] batch time: 0.056 trainign loss: 7.4509 avg training loss: 8.8246
batch: [10760/21305] batch time: 0.061 trainign loss: 5.3054 avg training loss: 8.8242
batch: [10770/21305] batch time: 0.063 trainign loss: 6.9663 avg training loss: 8.8238
batch: [10780/21305] batch time: 0.056 trainign loss: 7.8071 avg training loss: 8.8236
batch: [10790/21305] batch time: 0.051 trainign loss: 7.3714 avg training loss: 8.8233
batch: [10800/21305] batch time: 0.056 trainign loss: 6.7634 avg training loss: 8.8230
batch: [10810/21305] batch time: 0.356 trainign loss: 7.7017 avg training loss: 8.8227
batch: [10820/21305] batch time: 0.062 trainign loss: 5.2222 avg training loss: 8.8222
batch: [10830/21305] batch time: 0.509 trainign loss: 2.0394 avg training loss: 8.8214
batch: [10840/21305] batch time: 0.559 trainign loss: 7.5680 avg training loss: 8.8213
batch: [10850/21305] batch time: 0.060 trainign loss: 8.1695 avg training loss: 8.8211
batch: [10860/21305] batch time: 1.360 trainign loss: 7.3602 avg training loss: 8.8209
batch: [10870/21305] batch time: 0.057 trainign loss: 6.3028 avg training loss: 8.8206
batch: [10880/21305] batch time: 0.772 trainign loss: 5.8275 avg training loss: 8.8202
batch: [10890/21305] batch time: 0.056 trainign loss: 7.5948 avg training loss: 8.8198
batch: [10900/21305] batch time: 0.086 trainign loss: 7.9047 avg training loss: 8.8195
batch: [10910/21305] batch time: 0.058 trainign loss: 7.4983 avg training loss: 8.8192
batch: [10920/21305] batch time: 0.362 trainign loss: 7.5974 avg training loss: 8.8189
batch: [10930/21305] batch time: 0.059 trainign loss: 5.5909 avg training loss: 8.8184
batch: [10940/21305] batch time: 0.962 trainign loss: 7.6959 avg training loss: 8.8180
batch: [10950/21305] batch time: 0.059 trainign loss: 6.7699 avg training loss: 8.8175
batch: [10960/21305] batch time: 1.287 trainign loss: 7.2293 avg training loss: 8.8173
batch: [10970/21305] batch time: 0.063 trainign loss: 7.4282 avg training loss: 8.8171
batch: [10980/21305] batch time: 0.390 trainign loss: 5.7301 avg training loss: 8.8166
batch: [10990/21305] batch time: 0.062 trainign loss: 7.6063 avg training loss: 8.8162
batch: [11000/21305] batch time: 0.056 trainign loss: 6.8981 avg training loss: 8.8158
batch: [11010/21305] batch time: 0.053 trainign loss: 6.6727 avg training loss: 8.8152
batch: [11020/21305] batch time: 0.187 trainign loss: 7.4616 avg training loss: 8.8150
batch: [11030/21305] batch time: 0.061 trainign loss: 6.0625 avg training loss: 8.8146
batch: [11040/21305] batch time: 0.890 trainign loss: 8.0289 avg training loss: 8.8142
batch: [11050/21305] batch time: 0.056 trainign loss: 6.8153 avg training loss: 8.8139
batch: [11060/21305] batch time: 1.450 trainign loss: 7.4356 avg training loss: 8.8136
batch: [11070/21305] batch time: 0.056 trainign loss: 5.8952 avg training loss: 8.8132
batch: [11080/21305] batch time: 1.380 trainign loss: 8.1374 avg training loss: 8.8128
batch: [11090/21305] batch time: 0.056 trainign loss: 8.2498 avg training loss: 8.8126
batch: [11100/21305] batch time: 1.348 trainign loss: 7.1010 avg training loss: 8.8123
batch: [11110/21305] batch time: 0.056 trainign loss: 7.7552 avg training loss: 8.8121
batch: [11120/21305] batch time: 0.571 trainign loss: 7.2653 avg training loss: 8.8118
batch: [11130/21305] batch time: 0.057 trainign loss: 5.8574 avg training loss: 8.8115
batch: [11140/21305] batch time: 0.541 trainign loss: 4.2704 avg training loss: 8.8110
batch: [11150/21305] batch time: 0.054 trainign loss: 7.1260 avg training loss: 8.8106
batch: [11160/21305] batch time: 0.062 trainign loss: 7.2987 avg training loss: 8.8102
batch: [11170/21305] batch time: 0.061 trainign loss: 4.6231 avg training loss: 8.8098
batch: [11180/21305] batch time: 0.429 trainign loss: 9.4260 avg training loss: 8.8091
batch: [11190/21305] batch time: 0.063 trainign loss: 8.4002 avg training loss: 8.8088
batch: [11200/21305] batch time: 0.696 trainign loss: 8.2493 avg training loss: 8.8087
batch: [11210/21305] batch time: 0.056 trainign loss: 6.3676 avg training loss: 8.8084
batch: [11220/21305] batch time: 0.058 trainign loss: 7.1647 avg training loss: 8.8081
batch: [11230/21305] batch time: 0.056 trainign loss: 6.6377 avg training loss: 8.8078
batch: [11240/21305] batch time: 0.056 trainign loss: 6.6025 avg training loss: 8.8074
batch: [11250/21305] batch time: 0.056 trainign loss: 1.6883 avg training loss: 8.8067
batch: [11260/21305] batch time: 0.056 trainign loss: 8.2549 avg training loss: 8.8065
batch: [11270/21305] batch time: 0.051 trainign loss: 7.3757 avg training loss: 8.8063
batch: [11280/21305] batch time: 0.058 trainign loss: 6.6702 avg training loss: 8.8060
batch: [11290/21305] batch time: 0.061 trainign loss: 7.4819 avg training loss: 8.8058
batch: [11300/21305] batch time: 0.056 trainign loss: 3.1764 avg training loss: 8.8053
batch: [11310/21305] batch time: 0.057 trainign loss: 6.2456 avg training loss: 8.8050
batch: [11320/21305] batch time: 0.056 trainign loss: 8.1427 avg training loss: 8.8045
batch: [11330/21305] batch time: 0.061 trainign loss: 8.5975 avg training loss: 8.8042
batch: [11340/21305] batch time: 0.205 trainign loss: 8.2529 avg training loss: 8.8041
batch: [11350/21305] batch time: 0.231 trainign loss: 7.7272 avg training loss: 8.8039
batch: [11360/21305] batch time: 1.165 trainign loss: 7.2517 avg training loss: 8.8035
batch: [11370/21305] batch time: 0.453 trainign loss: 7.1887 avg training loss: 8.8031
batch: [11380/21305] batch time: 1.947 trainign loss: 5.0669 avg training loss: 8.8027
batch: [11390/21305] batch time: 0.136 trainign loss: 7.9060 avg training loss: 8.8016
batch: [11400/21305] batch time: 1.348 trainign loss: 9.3607 avg training loss: 8.8017
batch: [11410/21305] batch time: 0.168 trainign loss: 8.5509 avg training loss: 8.8018
batch: [11420/21305] batch time: 1.363 trainign loss: 7.6775 avg training loss: 8.8016
batch: [11430/21305] batch time: 0.369 trainign loss: 7.6294 avg training loss: 8.8015
batch: [11440/21305] batch time: 0.921 trainign loss: 7.1760 avg training loss: 8.8013
batch: [11450/21305] batch time: 0.768 trainign loss: 6.9947 avg training loss: 8.8009
batch: [11460/21305] batch time: 0.713 trainign loss: 7.0288 avg training loss: 8.8006
batch: [11470/21305] batch time: 1.347 trainign loss: 7.4372 avg training loss: 8.8002
batch: [11480/21305] batch time: 1.180 trainign loss: 7.0057 avg training loss: 8.7999
batch: [11490/21305] batch time: 1.219 trainign loss: 7.4832 avg training loss: 8.7993
batch: [11500/21305] batch time: 1.309 trainign loss: 7.1999 avg training loss: 8.7991
batch: [11510/21305] batch time: 0.888 trainign loss: 6.9932 avg training loss: 8.7987
batch: [11520/21305] batch time: 1.400 trainign loss: 7.6693 avg training loss: 8.7984
batch: [11530/21305] batch time: 0.527 trainign loss: 6.7974 avg training loss: 8.7980
batch: [11540/21305] batch time: 1.552 trainign loss: 8.7266 avg training loss: 8.7978
batch: [11550/21305] batch time: 0.327 trainign loss: 6.0093 avg training loss: 8.7975
batch: [11560/21305] batch time: 1.696 trainign loss: 7.5395 avg training loss: 8.7973
batch: [11570/21305] batch time: 0.547 trainign loss: 5.7081 avg training loss: 8.7968
batch: [11580/21305] batch time: 1.012 trainign loss: 8.6002 avg training loss: 8.7965
batch: [11590/21305] batch time: 0.848 trainign loss: 7.4218 avg training loss: 8.7962
batch: [11600/21305] batch time: 1.280 trainign loss: 5.5564 avg training loss: 8.7958
batch: [11610/21305] batch time: 1.075 trainign loss: 6.3875 avg training loss: 8.7955
batch: [11620/21305] batch time: 1.247 trainign loss: 7.6680 avg training loss: 8.7951
batch: [11630/21305] batch time: 0.152 trainign loss: 7.7770 avg training loss: 8.7948
batch: [11640/21305] batch time: 1.400 trainign loss: 7.2594 avg training loss: 8.7945
batch: [11650/21305] batch time: 0.056 trainign loss: 8.2406 avg training loss: 8.7942
batch: [11660/21305] batch time: 1.592 trainign loss: 6.4204 avg training loss: 8.7939
batch: [11670/21305] batch time: 0.056 trainign loss: 7.5063 avg training loss: 8.7936
batch: [11680/21305] batch time: 0.649 trainign loss: 6.4494 avg training loss: 8.7932
batch: [11690/21305] batch time: 0.062 trainign loss: 6.8333 avg training loss: 8.7928
batch: [11700/21305] batch time: 1.005 trainign loss: 7.3625 avg training loss: 8.7924
batch: [11710/21305] batch time: 0.063 trainign loss: 5.8797 avg training loss: 8.7920
batch: [11720/21305] batch time: 1.973 trainign loss: 7.6645 avg training loss: 8.7917
batch: [11730/21305] batch time: 0.055 trainign loss: 5.7624 avg training loss: 8.7913
batch: [11740/21305] batch time: 1.620 trainign loss: 6.8175 avg training loss: 8.7909
batch: [11750/21305] batch time: 0.056 trainign loss: 7.5084 avg training loss: 8.7907
batch: [11760/21305] batch time: 2.067 trainign loss: 7.4063 avg training loss: 8.7905
batch: [11770/21305] batch time: 0.056 trainign loss: 7.5464 avg training loss: 8.7902
batch: [11780/21305] batch time: 2.263 trainign loss: 5.1326 avg training loss: 8.7898
batch: [11790/21305] batch time: 0.056 trainign loss: 1.6453 avg training loss: 8.7891
batch: [11800/21305] batch time: 2.517 trainign loss: 0.0004 avg training loss: 8.7875
batch: [11810/21305] batch time: 0.055 trainign loss: 8.3171 avg training loss: 8.7873
batch: [11820/21305] batch time: 2.119 trainign loss: 8.0182 avg training loss: 8.7873
batch: [11830/21305] batch time: 0.056 trainign loss: 8.5934 avg training loss: 8.7870
batch: [11840/21305] batch time: 2.400 trainign loss: 8.1342 avg training loss: 8.7867
batch: [11850/21305] batch time: 0.057 trainign loss: 6.0842 avg training loss: 8.7863
batch: [11860/21305] batch time: 2.224 trainign loss: 4.3783 avg training loss: 8.7859
batch: [11870/21305] batch time: 0.056 trainign loss: 7.0775 avg training loss: 8.7855
batch: [11880/21305] batch time: 2.203 trainign loss: 8.9875 avg training loss: 8.7846
batch: [11890/21305] batch time: 0.062 trainign loss: 8.4300 avg training loss: 8.7846
batch: [11900/21305] batch time: 2.141 trainign loss: 6.9100 avg training loss: 8.7844
batch: [11910/21305] batch time: 0.059 trainign loss: 8.0570 avg training loss: 8.7842
batch: [11920/21305] batch time: 1.435 trainign loss: 6.6666 avg training loss: 8.7839
batch: [11930/21305] batch time: 0.059 trainign loss: 6.3795 avg training loss: 8.7834
batch: [11940/21305] batch time: 1.022 trainign loss: 7.7824 avg training loss: 8.7833
batch: [11950/21305] batch time: 0.057 trainign loss: 7.4403 avg training loss: 8.7830
batch: [11960/21305] batch time: 2.322 trainign loss: 7.8879 avg training loss: 8.7826
batch: [11970/21305] batch time: 0.060 trainign loss: 6.8584 avg training loss: 8.7824
batch: [11980/21305] batch time: 1.606 trainign loss: 7.2390 avg training loss: 8.7820
batch: [11990/21305] batch time: 0.056 trainign loss: 6.3356 avg training loss: 8.7817
batch: [12000/21305] batch time: 1.427 trainign loss: 7.2989 avg training loss: 8.7814
batch: [12010/21305] batch time: 0.056 trainign loss: 7.9671 avg training loss: 8.7811
batch: [12020/21305] batch time: 0.323 trainign loss: 7.3601 avg training loss: 8.7809
batch: [12030/21305] batch time: 0.056 trainign loss: 6.6407 avg training loss: 8.7806
batch: [12040/21305] batch time: 0.056 trainign loss: 6.4030 avg training loss: 8.7803
batch: [12050/21305] batch time: 0.063 trainign loss: 6.9386 avg training loss: 8.7799
batch: [12060/21305] batch time: 0.051 trainign loss: 7.6511 avg training loss: 8.7796
batch: [12070/21305] batch time: 0.061 trainign loss: 3.6694 avg training loss: 8.7791
batch: [12080/21305] batch time: 0.234 trainign loss: 6.5390 avg training loss: 8.7783
batch: [12090/21305] batch time: 0.056 trainign loss: 6.3202 avg training loss: 8.7780
batch: [12100/21305] batch time: 0.240 trainign loss: 8.0922 avg training loss: 8.7778
batch: [12110/21305] batch time: 0.056 trainign loss: 8.0895 avg training loss: 8.7777
batch: [12120/21305] batch time: 1.497 trainign loss: 6.7134 avg training loss: 8.7772
batch: [12130/21305] batch time: 0.057 trainign loss: 7.6594 avg training loss: 8.7769
batch: [12140/21305] batch time: 2.021 trainign loss: 5.8085 avg training loss: 8.7762
batch: [12150/21305] batch time: 0.059 trainign loss: 8.2701 avg training loss: 8.7761
batch: [12160/21305] batch time: 1.922 trainign loss: 8.1806 avg training loss: 8.7760
batch: [12170/21305] batch time: 0.058 trainign loss: 7.6561 avg training loss: 8.7758
batch: [12180/21305] batch time: 1.401 trainign loss: 7.7390 avg training loss: 8.7756
batch: [12190/21305] batch time: 0.056 trainign loss: 7.6736 avg training loss: 8.7755
batch: [12200/21305] batch time: 1.167 trainign loss: 6.7742 avg training loss: 8.7752
batch: [12210/21305] batch time: 0.063 trainign loss: 7.9427 avg training loss: 8.7750
batch: [12220/21305] batch time: 1.986 trainign loss: 7.3146 avg training loss: 8.7747
batch: [12230/21305] batch time: 0.057 trainign loss: 7.9263 avg training loss: 8.7745
batch: [12240/21305] batch time: 1.316 trainign loss: 6.7646 avg training loss: 8.7742
batch: [12250/21305] batch time: 0.056 trainign loss: 6.6637 avg training loss: 8.7739
batch: [12260/21305] batch time: 1.343 trainign loss: 7.5022 avg training loss: 8.7736
batch: [12270/21305] batch time: 0.188 trainign loss: 6.0862 avg training loss: 8.7733
batch: [12280/21305] batch time: 0.622 trainign loss: 5.2628 avg training loss: 8.7727
batch: [12290/21305] batch time: 1.128 trainign loss: 7.3401 avg training loss: 8.7725
batch: [12300/21305] batch time: 1.393 trainign loss: 8.1708 avg training loss: 8.7723
batch: [12310/21305] batch time: 1.095 trainign loss: 7.5721 avg training loss: 8.7721
batch: [12320/21305] batch time: 0.245 trainign loss: 7.2831 avg training loss: 8.7718
batch: [12330/21305] batch time: 1.107 trainign loss: 7.3362 avg training loss: 8.7715
batch: [12340/21305] batch time: 0.271 trainign loss: 6.9902 avg training loss: 8.7713
batch: [12350/21305] batch time: 1.257 trainign loss: 7.6289 avg training loss: 8.7710
batch: [12360/21305] batch time: 0.556 trainign loss: 6.3507 avg training loss: 8.7706
batch: [12370/21305] batch time: 1.015 trainign loss: 6.0753 avg training loss: 8.7702
batch: [12380/21305] batch time: 0.360 trainign loss: 7.1096 avg training loss: 8.7699
batch: [12390/21305] batch time: 1.070 trainign loss: 8.1196 avg training loss: 8.7697
batch: [12400/21305] batch time: 0.308 trainign loss: 6.4344 avg training loss: 8.7694
batch: [12410/21305] batch time: 1.217 trainign loss: 8.1122 avg training loss: 8.7691
batch: [12420/21305] batch time: 0.123 trainign loss: 6.1357 avg training loss: 8.7687
batch: [12430/21305] batch time: 2.520 trainign loss: 5.6903 avg training loss: 8.7684
batch: [12440/21305] batch time: 0.056 trainign loss: 8.0164 avg training loss: 8.7678
batch: [12450/21305] batch time: 2.528 trainign loss: 7.8358 avg training loss: 8.7676
batch: [12460/21305] batch time: 0.056 trainign loss: 7.5732 avg training loss: 8.7674
batch: [12470/21305] batch time: 1.978 trainign loss: 6.0737 avg training loss: 8.7670
batch: [12480/21305] batch time: 0.056 trainign loss: 6.4468 avg training loss: 8.7667
batch: [12490/21305] batch time: 0.809 trainign loss: 6.6054 avg training loss: 8.7663
batch: [12500/21305] batch time: 0.054 trainign loss: 7.7796 avg training loss: 8.7658
batch: [12510/21305] batch time: 1.530 trainign loss: 8.0099 avg training loss: 8.7656
batch: [12520/21305] batch time: 0.056 trainign loss: 7.7904 avg training loss: 8.7653
batch: [12530/21305] batch time: 1.464 trainign loss: 7.7686 avg training loss: 8.7650
batch: [12540/21305] batch time: 0.062 trainign loss: 5.5850 avg training loss: 8.7645
batch: [12550/21305] batch time: 1.876 trainign loss: 7.3968 avg training loss: 8.7641
batch: [12560/21305] batch time: 0.063 trainign loss: 3.9699 avg training loss: 8.7637
batch: [12570/21305] batch time: 2.187 trainign loss: 0.0021 avg training loss: 8.7622
batch: [12580/21305] batch time: 0.054 trainign loss: 9.4115 avg training loss: 8.7625
batch: [12590/21305] batch time: 2.038 trainign loss: 6.3852 avg training loss: 8.7625
batch: [12600/21305] batch time: 0.062 trainign loss: 9.1402 avg training loss: 8.7624
batch: [12610/21305] batch time: 2.317 trainign loss: 5.5084 avg training loss: 8.7621
batch: [12620/21305] batch time: 0.061 trainign loss: 7.1128 avg training loss: 8.7619
batch: [12630/21305] batch time: 2.092 trainign loss: 6.8485 avg training loss: 8.7617
batch: [12640/21305] batch time: 0.058 trainign loss: 7.8750 avg training loss: 8.7614
batch: [12650/21305] batch time: 1.670 trainign loss: 6.2983 avg training loss: 8.7611
batch: [12660/21305] batch time: 0.062 trainign loss: 7.0252 avg training loss: 8.7608
batch: [12670/21305] batch time: 1.935 trainign loss: 8.1497 avg training loss: 8.7606
batch: [12680/21305] batch time: 0.489 trainign loss: 5.5508 avg training loss: 8.7602
batch: [12690/21305] batch time: 1.448 trainign loss: 8.3345 avg training loss: 8.7598
batch: [12700/21305] batch time: 1.028 trainign loss: 7.3915 avg training loss: 8.7595
batch: [12710/21305] batch time: 0.338 trainign loss: 6.7110 avg training loss: 8.7592
batch: [12720/21305] batch time: 1.951 trainign loss: 6.2144 avg training loss: 8.7589
batch: [12730/21305] batch time: 0.792 trainign loss: 6.2785 avg training loss: 8.7584
batch: [12740/21305] batch time: 1.721 trainign loss: 5.8017 avg training loss: 8.7582
batch: [12750/21305] batch time: 0.963 trainign loss: 4.5765 avg training loss: 8.7575
batch: [12760/21305] batch time: 2.106 trainign loss: 6.2314 avg training loss: 8.7573
batch: [12770/21305] batch time: 0.178 trainign loss: 6.9527 avg training loss: 8.7570
batch: [12780/21305] batch time: 1.549 trainign loss: 8.3683 avg training loss: 8.7568
batch: [12790/21305] batch time: 0.275 trainign loss: 6.4668 avg training loss: 8.7565
batch: [12800/21305] batch time: 1.989 trainign loss: 7.6627 avg training loss: 8.7563
batch: [12810/21305] batch time: 0.555 trainign loss: 7.8200 avg training loss: 8.7561
batch: [12820/21305] batch time: 1.362 trainign loss: 7.2549 avg training loss: 8.7557
batch: [12830/21305] batch time: 0.772 trainign loss: 5.1944 avg training loss: 8.7554
batch: [12840/21305] batch time: 1.678 trainign loss: 7.0505 avg training loss: 8.7551
batch: [12850/21305] batch time: 1.265 trainign loss: 7.4726 avg training loss: 8.7549
batch: [12860/21305] batch time: 0.948 trainign loss: 7.8761 avg training loss: 8.7546
batch: [12870/21305] batch time: 1.876 trainign loss: 7.1187 avg training loss: 8.7543
batch: [12880/21305] batch time: 0.433 trainign loss: 6.6174 avg training loss: 8.7539
batch: [12890/21305] batch time: 1.922 trainign loss: 6.8820 avg training loss: 8.7537
batch: [12900/21305] batch time: 0.749 trainign loss: 8.1346 avg training loss: 8.7535
batch: [12910/21305] batch time: 1.485 trainign loss: 7.2970 avg training loss: 8.7533
batch: [12920/21305] batch time: 0.391 trainign loss: 7.1488 avg training loss: 8.7529
batch: [12930/21305] batch time: 1.374 trainign loss: 6.5297 avg training loss: 8.7526
batch: [12940/21305] batch time: 1.586 trainign loss: 7.2197 avg training loss: 8.7523
batch: [12950/21305] batch time: 0.582 trainign loss: 5.4476 avg training loss: 8.7519
batch: [12960/21305] batch time: 1.930 trainign loss: 6.6476 avg training loss: 8.7516
batch: [12970/21305] batch time: 0.266 trainign loss: 5.7689 avg training loss: 8.7512
batch: [12980/21305] batch time: 1.318 trainign loss: 8.1164 avg training loss: 8.7508
batch: [12990/21305] batch time: 0.702 trainign loss: 6.6124 avg training loss: 8.7506
batch: [13000/21305] batch time: 1.540 trainign loss: 7.9580 avg training loss: 8.7503
batch: [13010/21305] batch time: 0.320 trainign loss: 7.6936 avg training loss: 8.7498
batch: [13020/21305] batch time: 1.296 trainign loss: 8.2893 avg training loss: 8.7497
batch: [13030/21305] batch time: 1.478 trainign loss: 7.8204 avg training loss: 8.7495
batch: [13040/21305] batch time: 0.563 trainign loss: 8.0459 avg training loss: 8.7492
batch: [13050/21305] batch time: 1.411 trainign loss: 3.7677 avg training loss: 8.7488
batch: [13060/21305] batch time: 0.799 trainign loss: 7.2688 avg training loss: 8.7485
batch: [13070/21305] batch time: 1.604 trainign loss: 7.8673 avg training loss: 8.7482
batch: [13080/21305] batch time: 0.056 trainign loss: 6.9945 avg training loss: 8.7479
batch: [13090/21305] batch time: 2.046 trainign loss: 6.2185 avg training loss: 8.7475
batch: [13100/21305] batch time: 0.382 trainign loss: 8.4127 avg training loss: 8.7470
batch: [13110/21305] batch time: 2.059 trainign loss: 7.7841 avg training loss: 8.7465
batch: [13120/21305] batch time: 0.056 trainign loss: 7.4576 avg training loss: 8.7464
batch: [13130/21305] batch time: 2.193 trainign loss: 6.6828 avg training loss: 8.7462
batch: [13140/21305] batch time: 0.055 trainign loss: 3.2749 avg training loss: 8.7457
batch: [13150/21305] batch time: 1.183 trainign loss: 7.6489 avg training loss: 8.7453
batch: [13160/21305] batch time: 0.060 trainign loss: 6.3258 avg training loss: 8.7451
batch: [13170/21305] batch time: 1.648 trainign loss: 7.5964 avg training loss: 8.7448
batch: [13180/21305] batch time: 0.059 trainign loss: 6.1172 avg training loss: 8.7445
batch: [13190/21305] batch time: 1.728 trainign loss: 7.4548 avg training loss: 8.7442
batch: [13200/21305] batch time: 0.056 trainign loss: 4.0698 avg training loss: 8.7436
batch: [13210/21305] batch time: 1.710 trainign loss: 0.0028 avg training loss: 8.7422
batch: [13220/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 8.7406
batch: [13230/21305] batch time: 2.110 trainign loss: 0.0000 avg training loss: 8.7390
batch: [13240/21305] batch time: 0.056 trainign loss: 8.9862 avg training loss: 8.7393
batch: [13250/21305] batch time: 2.353 trainign loss: 9.3250 avg training loss: 8.7394
batch: [13260/21305] batch time: 0.054 trainign loss: 8.8605 avg training loss: 8.7395
batch: [13270/21305] batch time: 2.309 trainign loss: 8.0802 avg training loss: 8.7393
batch: [13280/21305] batch time: 0.062 trainign loss: 8.4289 avg training loss: 8.7391
batch: [13290/21305] batch time: 2.235 trainign loss: 7.5551 avg training loss: 8.7388
batch: [13300/21305] batch time: 0.056 trainign loss: 7.9694 avg training loss: 8.7385
batch: [13310/21305] batch time: 2.085 trainign loss: 5.5719 avg training loss: 8.7382
batch: [13320/21305] batch time: 0.060 trainign loss: 6.2175 avg training loss: 8.7375
batch: [13330/21305] batch time: 2.476 trainign loss: 6.9542 avg training loss: 8.7373
batch: [13340/21305] batch time: 0.061 trainign loss: 8.5480 avg training loss: 8.7371
batch: [13350/21305] batch time: 2.053 trainign loss: 7.4500 avg training loss: 8.7370
batch: [13360/21305] batch time: 0.372 trainign loss: 8.8272 avg training loss: 8.7368
batch: [13370/21305] batch time: 1.061 trainign loss: 6.2967 avg training loss: 8.7366
batch: [13380/21305] batch time: 0.861 trainign loss: 5.2061 avg training loss: 8.7362
batch: [13390/21305] batch time: 0.419 trainign loss: 7.0112 avg training loss: 8.7358
batch: [13400/21305] batch time: 0.700 trainign loss: 5.6048 avg training loss: 8.7355
batch: [13410/21305] batch time: 1.169 trainign loss: 7.5697 avg training loss: 8.7354
batch: [13420/21305] batch time: 0.774 trainign loss: 7.2114 avg training loss: 8.7352
batch: [13430/21305] batch time: 0.733 trainign loss: 7.3765 avg training loss: 8.7349
batch: [13440/21305] batch time: 1.419 trainign loss: 6.3729 avg training loss: 8.7347
batch: [13450/21305] batch time: 0.464 trainign loss: 7.8926 avg training loss: 8.7345
batch: [13460/21305] batch time: 1.555 trainign loss: 7.1092 avg training loss: 8.7342
batch: [13470/21305] batch time: 0.336 trainign loss: 4.9581 avg training loss: 8.7338
batch: [13480/21305] batch time: 1.585 trainign loss: 4.8978 avg training loss: 8.7334
batch: [13490/21305] batch time: 0.146 trainign loss: 7.4935 avg training loss: 8.7331
batch: [13500/21305] batch time: 2.572 trainign loss: 7.4300 avg training loss: 8.7328
batch: [13510/21305] batch time: 0.096 trainign loss: 6.0391 avg training loss: 8.7324
batch: [13520/21305] batch time: 1.788 trainign loss: 6.6344 avg training loss: 8.7321
batch: [13530/21305] batch time: 1.257 trainign loss: 6.5074 avg training loss: 8.7317
batch: [13540/21305] batch time: 0.437 trainign loss: 4.0041 avg training loss: 8.7313
batch: [13550/21305] batch time: 1.604 trainign loss: 7.5013 avg training loss: 8.7310
batch: [13560/21305] batch time: 0.236 trainign loss: 6.7988 avg training loss: 8.7308
batch: [13570/21305] batch time: 2.288 trainign loss: 7.7998 avg training loss: 8.7304
batch: [13580/21305] batch time: 0.224 trainign loss: 8.0690 avg training loss: 8.7302
batch: [13590/21305] batch time: 2.161 trainign loss: 6.0728 avg training loss: 8.7299
batch: [13600/21305] batch time: 0.739 trainign loss: 7.1019 avg training loss: 8.7296
batch: [13610/21305] batch time: 1.254 trainign loss: 8.4753 avg training loss: 8.7293
batch: [13620/21305] batch time: 0.581 trainign loss: 7.3376 avg training loss: 8.7290
batch: [13630/21305] batch time: 0.991 trainign loss: 7.2685 avg training loss: 8.7287
batch: [13640/21305] batch time: 1.301 trainign loss: 7.1762 avg training loss: 8.7285
batch: [13650/21305] batch time: 1.540 trainign loss: 6.5707 avg training loss: 8.7282
batch: [13660/21305] batch time: 1.089 trainign loss: 6.3676 avg training loss: 8.7278
batch: [13670/21305] batch time: 1.662 trainign loss: 7.1242 avg training loss: 8.7275
batch: [13680/21305] batch time: 0.991 trainign loss: 5.9597 avg training loss: 8.7272
batch: [13690/21305] batch time: 1.238 trainign loss: 4.3469 avg training loss: 8.7267
batch: [13700/21305] batch time: 1.444 trainign loss: 8.3754 avg training loss: 8.7266
batch: [13710/21305] batch time: 1.958 trainign loss: 7.0050 avg training loss: 8.7264
batch: [13720/21305] batch time: 1.111 trainign loss: 7.2128 avg training loss: 8.7261
batch: [13730/21305] batch time: 1.958 trainign loss: 8.0298 avg training loss: 8.7259
batch: [13740/21305] batch time: 0.468 trainign loss: 6.3591 avg training loss: 8.7256
batch: [13750/21305] batch time: 2.088 trainign loss: 8.0747 avg training loss: 8.7253
batch: [13760/21305] batch time: 0.689 trainign loss: 7.9286 avg training loss: 8.7251
batch: [13770/21305] batch time: 1.034 trainign loss: 7.7097 avg training loss: 8.7249
batch: [13780/21305] batch time: 1.996 trainign loss: 6.6098 avg training loss: 8.7244
batch: [13790/21305] batch time: 0.166 trainign loss: 7.6758 avg training loss: 8.7243
batch: [13800/21305] batch time: 2.522 trainign loss: 6.3476 avg training loss: 8.7240
batch: [13810/21305] batch time: 0.052 trainign loss: 7.4483 avg training loss: 8.7237
batch: [13820/21305] batch time: 1.876 trainign loss: 8.2855 avg training loss: 8.7235
batch: [13830/21305] batch time: 0.061 trainign loss: 6.3545 avg training loss: 8.7231
batch: [13840/21305] batch time: 2.058 trainign loss: 6.8516 avg training loss: 8.7228
batch: [13850/21305] batch time: 0.062 trainign loss: 1.7961 avg training loss: 8.7222
batch: [13860/21305] batch time: 2.349 trainign loss: 6.9436 avg training loss: 8.7218
batch: [13870/21305] batch time: 0.055 trainign loss: 8.6406 avg training loss: 8.7215
batch: [13880/21305] batch time: 1.956 trainign loss: 6.9855 avg training loss: 8.7213
batch: [13890/21305] batch time: 0.060 trainign loss: 7.8604 avg training loss: 8.7211
batch: [13900/21305] batch time: 2.241 trainign loss: 7.0515 avg training loss: 8.7209
batch: [13910/21305] batch time: 0.057 trainign loss: 7.8542 avg training loss: 8.7205
batch: [13920/21305] batch time: 2.251 trainign loss: 4.9452 avg training loss: 8.7200
batch: [13930/21305] batch time: 0.056 trainign loss: 7.0179 avg training loss: 8.7194
batch: [13940/21305] batch time: 2.352 trainign loss: 8.2134 avg training loss: 8.7193
batch: [13950/21305] batch time: 0.055 trainign loss: 5.9548 avg training loss: 8.7190
batch: [13960/21305] batch time: 2.536 trainign loss: 6.8450 avg training loss: 8.7188
batch: [13970/21305] batch time: 0.056 trainign loss: 7.8826 avg training loss: 8.7186
batch: [13980/21305] batch time: 2.211 trainign loss: 7.5991 avg training loss: 8.7184
batch: [13990/21305] batch time: 0.062 trainign loss: 7.4096 avg training loss: 8.7181
batch: [14000/21305] batch time: 2.084 trainign loss: 7.4812 avg training loss: 8.7177
batch: [14010/21305] batch time: 0.057 trainign loss: 5.5375 avg training loss: 8.7172
batch: [14020/21305] batch time: 2.393 trainign loss: 7.9398 avg training loss: 8.7167
batch: [14030/21305] batch time: 0.056 trainign loss: 7.6978 avg training loss: 8.7165
batch: [14040/21305] batch time: 2.646 trainign loss: 7.7484 avg training loss: 8.7163
batch: [14050/21305] batch time: 0.063 trainign loss: 7.5930 avg training loss: 8.7161
batch: [14060/21305] batch time: 1.973 trainign loss: 7.1512 avg training loss: 8.7158
batch: [14070/21305] batch time: 0.789 trainign loss: 7.6039 avg training loss: 8.7156
batch: [14080/21305] batch time: 1.166 trainign loss: 7.0975 avg training loss: 8.7154
batch: [14090/21305] batch time: 0.909 trainign loss: 7.5969 avg training loss: 8.7151
batch: [14100/21305] batch time: 1.599 trainign loss: 7.1918 avg training loss: 8.7148
batch: [14110/21305] batch time: 0.630 trainign loss: 6.8756 avg training loss: 8.7146
batch: [14120/21305] batch time: 1.397 trainign loss: 8.2447 avg training loss: 8.7142
batch: [14130/21305] batch time: 1.267 trainign loss: 6.8349 avg training loss: 8.7137
batch: [14140/21305] batch time: 1.011 trainign loss: 7.2088 avg training loss: 8.7133
batch: [14150/21305] batch time: 1.674 trainign loss: 7.5699 avg training loss: 8.7131
batch: [14160/21305] batch time: 0.663 trainign loss: 7.6484 avg training loss: 8.7128
batch: [14170/21305] batch time: 1.553 trainign loss: 6.7664 avg training loss: 8.7126
batch: [14180/21305] batch time: 1.515 trainign loss: 6.6024 avg training loss: 8.7122
batch: [14190/21305] batch time: 0.726 trainign loss: 2.8286 avg training loss: 8.7116
batch: [14200/21305] batch time: 2.591 trainign loss: 8.0530 avg training loss: 8.7114
batch: [14210/21305] batch time: 0.056 trainign loss: 7.0089 avg training loss: 8.7112
batch: [14220/21305] batch time: 2.173 trainign loss: 8.2995 avg training loss: 8.7106
batch: [14230/21305] batch time: 0.052 trainign loss: 6.8011 avg training loss: 8.7104
batch: [14240/21305] batch time: 2.333 trainign loss: 8.0056 avg training loss: 8.7103
batch: [14250/21305] batch time: 0.057 trainign loss: 7.8445 avg training loss: 8.7101
batch: [14260/21305] batch time: 2.254 trainign loss: 6.6096 avg training loss: 8.7098
batch: [14270/21305] batch time: 0.055 trainign loss: 5.7942 avg training loss: 8.7095
batch: [14280/21305] batch time: 2.183 trainign loss: 5.7679 avg training loss: 8.7092
batch: [14290/21305] batch time: 0.053 trainign loss: 7.0049 avg training loss: 8.7088
batch: [14300/21305] batch time: 2.075 trainign loss: 7.5610 avg training loss: 8.7086
batch: [14310/21305] batch time: 0.056 trainign loss: 7.3074 avg training loss: 8.7082
batch: [14320/21305] batch time: 2.070 trainign loss: 3.5555 avg training loss: 8.7078
batch: [14330/21305] batch time: 0.051 trainign loss: 7.3837 avg training loss: 8.7075
batch: [14340/21305] batch time: 2.178 trainign loss: 6.9897 avg training loss: 8.7072
batch: [14350/21305] batch time: 0.056 trainign loss: 5.9604 avg training loss: 8.7070
batch: [14360/21305] batch time: 2.469 trainign loss: 6.8016 avg training loss: 8.7067
batch: [14370/21305] batch time: 0.057 trainign loss: 6.4630 avg training loss: 8.7065
batch: [14380/21305] batch time: 2.246 trainign loss: 7.0152 avg training loss: 8.7062
batch: [14390/21305] batch time: 0.056 trainign loss: 7.2532 avg training loss: 8.7060
batch: [14400/21305] batch time: 1.931 trainign loss: 6.4577 avg training loss: 8.7057
batch: [14410/21305] batch time: 0.056 trainign loss: 7.1714 avg training loss: 8.7054
batch: [14420/21305] batch time: 2.445 trainign loss: 7.0081 avg training loss: 8.7050
batch: [14430/21305] batch time: 0.057 trainign loss: 6.9374 avg training loss: 8.7047
batch: [14440/21305] batch time: 2.138 trainign loss: 7.4424 avg training loss: 8.7045
batch: [14450/21305] batch time: 0.054 trainign loss: 6.2005 avg training loss: 8.7042
batch: [14460/21305] batch time: 2.310 trainign loss: 7.7678 avg training loss: 8.7039
batch: [14470/21305] batch time: 0.062 trainign loss: 6.3746 avg training loss: 8.7036
batch: [14480/21305] batch time: 1.365 trainign loss: 4.9154 avg training loss: 8.7032
batch: [14490/21305] batch time: 0.057 trainign loss: 5.9228 avg training loss: 8.7028
batch: [14500/21305] batch time: 0.764 trainign loss: 2.7950 avg training loss: 8.7024
batch: [14510/21305] batch time: 0.063 trainign loss: 0.0007 avg training loss: 8.7009
batch: [14520/21305] batch time: 1.036 trainign loss: 0.0001 avg training loss: 8.6994
batch: [14530/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 8.6979
batch: [14540/21305] batch time: 1.855 trainign loss: 0.0000 avg training loss: 8.6963
batch: [14550/21305] batch time: 0.062 trainign loss: 0.0000 avg training loss: 8.6948
batch: [14560/21305] batch time: 1.796 trainign loss: 0.0000 avg training loss: 8.6933
batch: [14570/21305] batch time: 0.059 trainign loss: 0.0000 avg training loss: 8.6918
batch: [14580/21305] batch time: 1.903 trainign loss: 0.0000 avg training loss: 8.6903
batch: [14590/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 8.6887
batch: [14600/21305] batch time: 2.377 trainign loss: 8.9025 avg training loss: 8.6885
batch: [14610/21305] batch time: 0.056 trainign loss: 9.4823 avg training loss: 8.6886
batch: [14620/21305] batch time: 2.157 trainign loss: 8.2441 avg training loss: 8.6886
batch: [14630/21305] batch time: 0.056 trainign loss: 7.3692 avg training loss: 8.6884
batch: [14640/21305] batch time: 2.209 trainign loss: 6.6764 avg training loss: 8.6882
batch: [14650/21305] batch time: 0.052 trainign loss: 7.5831 avg training loss: 8.6879
batch: [14660/21305] batch time: 2.170 trainign loss: 6.4534 avg training loss: 8.6876
batch: [14670/21305] batch time: 0.054 trainign loss: 5.9297 avg training loss: 8.6873
batch: [14680/21305] batch time: 2.279 trainign loss: 3.6082 avg training loss: 8.6869
batch: [14690/21305] batch time: 0.056 trainign loss: 6.9763 avg training loss: 8.6865
batch: [14700/21305] batch time: 2.570 trainign loss: 5.7976 avg training loss: 8.6863
batch: [14710/21305] batch time: 0.056 trainign loss: 0.0060 avg training loss: 8.6850
batch: [14720/21305] batch time: 2.153 trainign loss: 9.6594 avg training loss: 8.6848
batch: [14730/21305] batch time: 0.057 trainign loss: 7.4390 avg training loss: 8.6847
batch: [14740/21305] batch time: 2.451 trainign loss: 6.8591 avg training loss: 8.6846
batch: [14750/21305] batch time: 0.056 trainign loss: 8.1528 avg training loss: 8.6844
batch: [14760/21305] batch time: 2.182 trainign loss: 7.6723 avg training loss: 8.6842
batch: [14770/21305] batch time: 0.062 trainign loss: 7.2650 avg training loss: 8.6840
batch: [14780/21305] batch time: 2.158 trainign loss: 6.7717 avg training loss: 8.6838
batch: [14790/21305] batch time: 0.056 trainign loss: 8.0151 avg training loss: 8.6835
batch: [14800/21305] batch time: 2.455 trainign loss: 7.2627 avg training loss: 8.6833
batch: [14810/21305] batch time: 0.062 trainign loss: 6.6088 avg training loss: 8.6829
batch: [14820/21305] batch time: 2.318 trainign loss: 6.8999 avg training loss: 8.6827
batch: [14830/21305] batch time: 0.057 trainign loss: 6.9312 avg training loss: 8.6825
batch: [14840/21305] batch time: 2.165 trainign loss: 6.3185 avg training loss: 8.6823
batch: [14850/21305] batch time: 0.055 trainign loss: 7.8442 avg training loss: 8.6820
batch: [14860/21305] batch time: 2.212 trainign loss: 7.8311 avg training loss: 8.6817
batch: [14870/21305] batch time: 0.063 trainign loss: 6.8338 avg training loss: 8.6815
batch: [14880/21305] batch time: 2.050 trainign loss: 7.6348 avg training loss: 8.6813
batch: [14890/21305] batch time: 0.056 trainign loss: 7.0536 avg training loss: 8.6810
batch: [14900/21305] batch time: 2.502 trainign loss: 7.0338 avg training loss: 8.6808
batch: [14910/21305] batch time: 0.062 trainign loss: 4.4217 avg training loss: 8.6804
batch: [14920/21305] batch time: 1.951 trainign loss: 6.3212 avg training loss: 8.6802
batch: [14930/21305] batch time: 0.059 trainign loss: 6.6362 avg training loss: 8.6798
batch: [14940/21305] batch time: 1.422 trainign loss: 7.5588 avg training loss: 8.6795
batch: [14950/21305] batch time: 0.062 trainign loss: 7.9051 avg training loss: 8.6792
batch: [14960/21305] batch time: 1.330 trainign loss: 8.2707 avg training loss: 8.6788
batch: [14970/21305] batch time: 0.058 trainign loss: 8.6699 avg training loss: 8.6784
batch: [14980/21305] batch time: 1.592 trainign loss: 8.4038 avg training loss: 8.6782
batch: [14990/21305] batch time: 0.056 trainign loss: 5.5133 avg training loss: 8.6778
batch: [15000/21305] batch time: 1.373 trainign loss: 6.8817 avg training loss: 8.6776
batch: [15010/21305] batch time: 0.054 trainign loss: 8.1038 avg training loss: 8.6774
batch: [15020/21305] batch time: 1.995 trainign loss: 6.1370 avg training loss: 8.6771
batch: [15030/21305] batch time: 0.062 trainign loss: 8.1463 avg training loss: 8.6768
batch: [15040/21305] batch time: 2.549 trainign loss: 3.9595 avg training loss: 8.6764
batch: [15050/21305] batch time: 0.057 trainign loss: 7.0615 avg training loss: 8.6762
batch: [15060/21305] batch time: 2.256 trainign loss: 6.8062 avg training loss: 8.6759
batch: [15070/21305] batch time: 0.062 trainign loss: 7.0359 avg training loss: 8.6755
batch: [15080/21305] batch time: 2.371 trainign loss: 4.0453 avg training loss: 8.6747
batch: [15090/21305] batch time: 0.060 trainign loss: 7.5713 avg training loss: 8.6747
batch: [15100/21305] batch time: 2.180 trainign loss: 6.4067 avg training loss: 8.6746
batch: [15110/21305] batch time: 0.056 trainign loss: 4.4682 avg training loss: 8.6743
batch: [15120/21305] batch time: 2.152 trainign loss: 8.1110 avg training loss: 8.6740
batch: [15130/21305] batch time: 0.062 trainign loss: 6.8158 avg training loss: 8.6736
batch: [15140/21305] batch time: 2.207 trainign loss: 8.4443 avg training loss: 8.6735
batch: [15150/21305] batch time: 0.059 trainign loss: 7.3399 avg training loss: 8.6733
batch: [15160/21305] batch time: 2.205 trainign loss: 7.2852 avg training loss: 8.6731
batch: [15170/21305] batch time: 0.056 trainign loss: 7.2040 avg training loss: 8.6727
batch: [15180/21305] batch time: 2.093 trainign loss: 6.1699 avg training loss: 8.6724
batch: [15190/21305] batch time: 0.056 trainign loss: 7.7471 avg training loss: 8.6722
batch: [15200/21305] batch time: 1.777 trainign loss: 7.8559 avg training loss: 8.6721
batch: [15210/21305] batch time: 0.056 trainign loss: 6.3606 avg training loss: 8.6717
batch: [15220/21305] batch time: 2.366 trainign loss: 6.7718 avg training loss: 8.6714
batch: [15230/21305] batch time: 0.061 trainign loss: 7.6161 avg training loss: 8.6711
batch: [15240/21305] batch time: 2.227 trainign loss: 7.1536 avg training loss: 8.6709
batch: [15250/21305] batch time: 0.054 trainign loss: 8.3082 avg training loss: 8.6707
batch: [15260/21305] batch time: 2.372 trainign loss: 6.6884 avg training loss: 8.6704
batch: [15270/21305] batch time: 0.056 trainign loss: 8.2230 avg training loss: 8.6701
batch: [15280/21305] batch time: 2.115 trainign loss: 6.9876 avg training loss: 8.6699
batch: [15290/21305] batch time: 0.063 trainign loss: 8.1373 avg training loss: 8.6696
batch: [15300/21305] batch time: 2.161 trainign loss: 7.3054 avg training loss: 8.6694
batch: [15310/21305] batch time: 0.057 trainign loss: 6.6552 avg training loss: 8.6689
batch: [15320/21305] batch time: 2.494 trainign loss: 7.9754 avg training loss: 8.6687
batch: [15330/21305] batch time: 0.057 trainign loss: 7.0825 avg training loss: 8.6685
batch: [15340/21305] batch time: 2.758 trainign loss: 6.2096 avg training loss: 8.6683
batch: [15350/21305] batch time: 0.063 trainign loss: 6.9375 avg training loss: 8.6680
batch: [15360/21305] batch time: 2.238 trainign loss: 8.2055 avg training loss: 8.6677
batch: [15370/21305] batch time: 0.056 trainign loss: 5.9181 avg training loss: 8.6673
batch: [15380/21305] batch time: 2.360 trainign loss: 6.5272 avg training loss: 8.6671
batch: [15390/21305] batch time: 0.062 trainign loss: 7.1129 avg training loss: 8.6668
batch: [15400/21305] batch time: 2.340 trainign loss: 7.7466 avg training loss: 8.6666
batch: [15410/21305] batch time: 0.056 trainign loss: 7.7099 avg training loss: 8.6664
batch: [15420/21305] batch time: 2.359 trainign loss: 7.5365 avg training loss: 8.6663
batch: [15430/21305] batch time: 0.056 trainign loss: 6.7800 avg training loss: 8.6660
batch: [15440/21305] batch time: 2.457 trainign loss: 8.0662 avg training loss: 8.6658
batch: [15450/21305] batch time: 0.061 trainign loss: 7.2813 avg training loss: 8.6655
batch: [15460/21305] batch time: 2.353 trainign loss: 7.5500 avg training loss: 8.6652
batch: [15470/21305] batch time: 0.056 trainign loss: 7.1513 avg training loss: 8.6650
batch: [15480/21305] batch time: 2.003 trainign loss: 7.6812 avg training loss: 8.6648
batch: [15490/21305] batch time: 0.592 trainign loss: 6.8999 avg training loss: 8.6646
batch: [15500/21305] batch time: 2.233 trainign loss: 5.8688 avg training loss: 8.6643
batch: [15510/21305] batch time: 0.062 trainign loss: 4.6101 avg training loss: 8.6638
batch: [15520/21305] batch time: 2.150 trainign loss: 9.0609 avg training loss: 8.6633
batch: [15530/21305] batch time: 0.054 trainign loss: 8.3197 avg training loss: 8.6631
batch: [15540/21305] batch time: 1.955 trainign loss: 8.2870 avg training loss: 8.6630
batch: [15550/21305] batch time: 0.279 trainign loss: 6.0209 avg training loss: 8.6628
batch: [15560/21305] batch time: 2.169 trainign loss: 8.9028 avg training loss: 8.6625
batch: [15570/21305] batch time: 0.062 trainign loss: 7.0528 avg training loss: 8.6624
batch: [15580/21305] batch time: 2.181 trainign loss: 6.9137 avg training loss: 8.6622
batch: [15590/21305] batch time: 0.057 trainign loss: 6.4030 avg training loss: 8.6619
batch: [15600/21305] batch time: 1.458 trainign loss: 7.2553 avg training loss: 8.6616
batch: [15610/21305] batch time: 0.062 trainign loss: 7.0132 avg training loss: 8.6613
batch: [15620/21305] batch time: 1.052 trainign loss: 6.5988 avg training loss: 8.6611
batch: [15630/21305] batch time: 0.056 trainign loss: 6.5196 avg training loss: 8.6609
batch: [15640/21305] batch time: 1.558 trainign loss: 7.7870 avg training loss: 8.6606
batch: [15650/21305] batch time: 0.059 trainign loss: 6.2040 avg training loss: 8.6604
batch: [15660/21305] batch time: 1.841 trainign loss: 5.8127 avg training loss: 8.6600
batch: [15670/21305] batch time: 0.061 trainign loss: 7.0693 avg training loss: 8.6597
batch: [15680/21305] batch time: 2.509 trainign loss: 7.5959 avg training loss: 8.6594
batch: [15690/21305] batch time: 0.056 trainign loss: 7.5131 avg training loss: 8.6592
batch: [15700/21305] batch time: 2.128 trainign loss: 7.1631 avg training loss: 8.6589
batch: [15710/21305] batch time: 0.055 trainign loss: 7.3418 avg training loss: 8.6586
batch: [15720/21305] batch time: 2.440 trainign loss: 5.2527 avg training loss: 8.6583
batch: [15730/21305] batch time: 0.054 trainign loss: 0.7553 avg training loss: 8.6575
batch: [15740/21305] batch time: 2.470 trainign loss: 13.1619 avg training loss: 8.6565
batch: [15750/21305] batch time: 0.058 trainign loss: 8.8170 avg training loss: 8.6566
batch: [15760/21305] batch time: 2.334 trainign loss: 8.6152 avg training loss: 8.6567
batch: [15770/21305] batch time: 0.056 trainign loss: 7.3699 avg training loss: 8.6566
batch: [15780/21305] batch time: 2.487 trainign loss: 7.0385 avg training loss: 8.6565
batch: [15790/21305] batch time: 0.052 trainign loss: 6.6787 avg training loss: 8.6563
batch: [15800/21305] batch time: 2.079 trainign loss: 5.6605 avg training loss: 8.6560
batch: [15810/21305] batch time: 0.056 trainign loss: 8.1451 avg training loss: 8.6556
batch: [15820/21305] batch time: 2.309 trainign loss: 6.4659 avg training loss: 8.6554
batch: [15830/21305] batch time: 0.062 trainign loss: 6.7059 avg training loss: 8.6552
batch: [15840/21305] batch time: 2.668 trainign loss: 6.8159 avg training loss: 8.6550
batch: [15850/21305] batch time: 0.056 trainign loss: 6.0176 avg training loss: 8.6547
batch: [15860/21305] batch time: 2.296 trainign loss: 8.5818 avg training loss: 8.6545
batch: [15870/21305] batch time: 0.056 trainign loss: 7.1489 avg training loss: 8.6543
batch: [15880/21305] batch time: 2.767 trainign loss: 7.6894 avg training loss: 8.6542
batch: [15890/21305] batch time: 0.062 trainign loss: 7.8289 avg training loss: 8.6540
batch: [15900/21305] batch time: 2.377 trainign loss: 6.2122 avg training loss: 8.6537
batch: [15910/21305] batch time: 0.056 trainign loss: 7.1789 avg training loss: 8.6535
batch: [15920/21305] batch time: 2.331 trainign loss: 7.3613 avg training loss: 8.6533
batch: [15930/21305] batch time: 0.055 trainign loss: 7.6745 avg training loss: 8.6529
batch: [15940/21305] batch time: 2.247 trainign loss: 7.0999 avg training loss: 8.6524
batch: [15950/21305] batch time: 0.059 trainign loss: 7.3910 avg training loss: 8.6522
batch: [15960/21305] batch time: 2.243 trainign loss: 5.6474 avg training loss: 8.6519
batch: [15970/21305] batch time: 0.053 trainign loss: 5.8250 avg training loss: 8.6517
batch: [15980/21305] batch time: 2.248 trainign loss: 6.2285 avg training loss: 8.6514
batch: [15990/21305] batch time: 0.056 trainign loss: 7.6741 avg training loss: 8.6512
batch: [16000/21305] batch time: 2.093 trainign loss: 7.4276 avg training loss: 8.6509
batch: [16010/21305] batch time: 0.056 trainign loss: 1.8514 avg training loss: 8.6503
batch: [16020/21305] batch time: 2.270 trainign loss: 6.9455 avg training loss: 8.6500
batch: [16030/21305] batch time: 0.061 trainign loss: 8.6262 avg training loss: 8.6497
batch: [16040/21305] batch time: 2.446 trainign loss: 5.5080 avg training loss: 8.6495
batch: [16050/21305] batch time: 0.051 trainign loss: 7.4428 avg training loss: 8.6493
batch: [16060/21305] batch time: 2.659 trainign loss: 6.7562 avg training loss: 8.6490
batch: [16070/21305] batch time: 0.056 trainign loss: 8.0253 avg training loss: 8.6489
batch: [16080/21305] batch time: 2.017 trainign loss: 7.4568 avg training loss: 8.6485
batch: [16090/21305] batch time: 0.057 trainign loss: 6.7604 avg training loss: 8.6483
batch: [16100/21305] batch time: 2.175 trainign loss: 6.4183 avg training loss: 8.6480
batch: [16110/21305] batch time: 0.232 trainign loss: 6.2639 avg training loss: 8.6478
batch: [16120/21305] batch time: 1.491 trainign loss: 5.4216 avg training loss: 8.6475
batch: [16130/21305] batch time: 1.033 trainign loss: 7.0913 avg training loss: 8.6471
batch: [16140/21305] batch time: 0.260 trainign loss: 7.5518 avg training loss: 8.6469
batch: [16150/21305] batch time: 2.334 trainign loss: 6.9555 avg training loss: 8.6467
batch: [16160/21305] batch time: 0.208 trainign loss: 5.2170 avg training loss: 8.6463
batch: [16170/21305] batch time: 1.961 trainign loss: 8.0652 avg training loss: 8.6461
batch: [16180/21305] batch time: 0.059 trainign loss: 7.6983 avg training loss: 8.6459
batch: [16190/21305] batch time: 2.071 trainign loss: 7.7645 avg training loss: 8.6457
batch: [16200/21305] batch time: 0.056 trainign loss: 6.2712 avg training loss: 8.6455
batch: [16210/21305] batch time: 1.095 trainign loss: 7.5829 avg training loss: 8.6453
batch: [16220/21305] batch time: 0.550 trainign loss: 7.2257 avg training loss: 8.6450
batch: [16230/21305] batch time: 1.507 trainign loss: 7.5251 avg training loss: 8.6448
batch: [16240/21305] batch time: 0.063 trainign loss: 7.9224 avg training loss: 8.6445
batch: [16250/21305] batch time: 1.966 trainign loss: 7.3684 avg training loss: 8.6443
batch: [16260/21305] batch time: 0.396 trainign loss: 7.4292 avg training loss: 8.6440
batch: [16270/21305] batch time: 2.476 trainign loss: 7.8729 avg training loss: 8.6438
batch: [16280/21305] batch time: 0.056 trainign loss: 7.9907 avg training loss: 8.6435
batch: [16290/21305] batch time: 2.309 trainign loss: 7.3153 avg training loss: 8.6433
batch: [16300/21305] batch time: 0.056 trainign loss: 7.4116 avg training loss: 8.6430
batch: [16310/21305] batch time: 2.521 trainign loss: 7.3559 avg training loss: 8.6427
batch: [16320/21305] batch time: 0.052 trainign loss: 7.7314 avg training loss: 8.6425
batch: [16330/21305] batch time: 2.065 trainign loss: 5.7650 avg training loss: 8.6421
batch: [16340/21305] batch time: 0.088 trainign loss: 5.6092 avg training loss: 8.6416
batch: [16350/21305] batch time: 2.172 trainign loss: 8.2443 avg training loss: 8.6414
batch: [16360/21305] batch time: 0.450 trainign loss: 6.7847 avg training loss: 8.6409
batch: [16370/21305] batch time: 1.401 trainign loss: 6.8629 avg training loss: 8.6407
batch: [16380/21305] batch time: 0.444 trainign loss: 7.6539 avg training loss: 8.6404
batch: [16390/21305] batch time: 1.159 trainign loss: 6.7937 avg training loss: 8.6402
batch: [16400/21305] batch time: 0.826 trainign loss: 8.1689 avg training loss: 8.6400
batch: [16410/21305] batch time: 0.428 trainign loss: 7.9737 avg training loss: 8.6398
batch: [16420/21305] batch time: 0.567 trainign loss: 7.2825 avg training loss: 8.6396
batch: [16430/21305] batch time: 0.626 trainign loss: 6.3032 avg training loss: 8.6392
batch: [16440/21305] batch time: 0.884 trainign loss: 7.7673 avg training loss: 8.6390
batch: [16450/21305] batch time: 0.059 trainign loss: 8.0874 avg training loss: 8.6387
batch: [16460/21305] batch time: 1.826 trainign loss: 7.5878 avg training loss: 8.6385
batch: [16470/21305] batch time: 0.056 trainign loss: 7.2425 avg training loss: 8.6383
batch: [16480/21305] batch time: 1.843 trainign loss: 6.4942 avg training loss: 8.6380
batch: [16490/21305] batch time: 0.053 trainign loss: 6.5588 avg training loss: 8.6377
batch: [16500/21305] batch time: 1.689 trainign loss: 6.1889 avg training loss: 8.6375
batch: [16510/21305] batch time: 0.568 trainign loss: 7.4075 avg training loss: 8.6372
batch: [16520/21305] batch time: 1.733 trainign loss: 7.0942 avg training loss: 8.6370
batch: [16530/21305] batch time: 0.057 trainign loss: 7.7870 avg training loss: 8.6367
batch: [16540/21305] batch time: 1.167 trainign loss: 6.6796 avg training loss: 8.6362
batch: [16550/21305] batch time: 0.056 trainign loss: 7.7996 avg training loss: 8.6360
batch: [16560/21305] batch time: 1.283 trainign loss: 7.6464 avg training loss: 8.6358
batch: [16570/21305] batch time: 0.057 trainign loss: 6.8602 avg training loss: 8.6355
batch: [16580/21305] batch time: 1.826 trainign loss: 8.0489 avg training loss: 8.6353
batch: [16590/21305] batch time: 0.062 trainign loss: 7.7509 avg training loss: 8.6350
batch: [16600/21305] batch time: 2.641 trainign loss: 7.4870 avg training loss: 8.6347
batch: [16610/21305] batch time: 0.056 trainign loss: 5.0313 avg training loss: 8.6342
batch: [16620/21305] batch time: 2.062 trainign loss: 6.6461 avg training loss: 8.6338
batch: [16630/21305] batch time: 0.052 trainign loss: 6.7687 avg training loss: 8.6335
batch: [16640/21305] batch time: 2.272 trainign loss: 8.6748 avg training loss: 8.6333
batch: [16650/21305] batch time: 0.051 trainign loss: 7.8201 avg training loss: 8.6333
batch: [16660/21305] batch time: 2.304 trainign loss: 7.5020 avg training loss: 8.6331
batch: [16670/21305] batch time: 0.062 trainign loss: 7.2621 avg training loss: 8.6328
batch: [16680/21305] batch time: 2.586 trainign loss: 7.7430 avg training loss: 8.6325
batch: [16690/21305] batch time: 0.056 trainign loss: 7.6468 avg training loss: 8.6323
batch: [16700/21305] batch time: 2.032 trainign loss: 7.3877 avg training loss: 8.6321
batch: [16710/21305] batch time: 0.057 trainign loss: 7.3605 avg training loss: 8.6319
batch: [16720/21305] batch time: 1.521 trainign loss: 7.8472 avg training loss: 8.6317
batch: [16730/21305] batch time: 0.055 trainign loss: 6.1057 avg training loss: 8.6315
batch: [16740/21305] batch time: 0.637 trainign loss: 7.6703 avg training loss: 8.6312
batch: [16750/21305] batch time: 0.057 trainign loss: 7.4382 avg training loss: 8.6311
batch: [16760/21305] batch time: 0.063 trainign loss: 6.6834 avg training loss: 8.6309
batch: [16770/21305] batch time: 0.051 trainign loss: 7.8925 avg training loss: 8.6306
batch: [16780/21305] batch time: 0.056 trainign loss: 6.9411 avg training loss: 8.6304
batch: [16790/21305] batch time: 0.053 trainign loss: 7.2896 avg training loss: 8.6301
batch: [16800/21305] batch time: 0.056 trainign loss: 2.9276 avg training loss: 8.6295
batch: [16810/21305] batch time: 0.052 trainign loss: 7.8700 avg training loss: 8.6293
batch: [16820/21305] batch time: 0.056 trainign loss: 6.7086 avg training loss: 8.6291
batch: [16830/21305] batch time: 0.053 trainign loss: 7.6477 avg training loss: 8.6289
batch: [16840/21305] batch time: 0.061 trainign loss: 8.1432 avg training loss: 8.6286
batch: [16850/21305] batch time: 0.337 trainign loss: 7.2902 avg training loss: 8.6284
batch: [16860/21305] batch time: 0.056 trainign loss: 7.8163 avg training loss: 8.6281
batch: [16870/21305] batch time: 0.348 trainign loss: 7.1328 avg training loss: 8.6278
batch: [16880/21305] batch time: 0.063 trainign loss: 6.7848 avg training loss: 8.6275
batch: [16890/21305] batch time: 0.052 trainign loss: 7.6385 avg training loss: 8.6272
batch: [16900/21305] batch time: 0.060 trainign loss: 5.4018 avg training loss: 8.6269
batch: [16910/21305] batch time: 0.056 trainign loss: 0.0069 avg training loss: 8.6257
batch: [16920/21305] batch time: 0.054 trainign loss: 9.1513 avg training loss: 8.6256
batch: [16930/21305] batch time: 0.058 trainign loss: 7.0398 avg training loss: 8.6256
batch: [16940/21305] batch time: 0.063 trainign loss: 8.0598 avg training loss: 8.6255
batch: [16950/21305] batch time: 0.056 trainign loss: 7.9512 avg training loss: 8.6253
batch: [16960/21305] batch time: 0.052 trainign loss: 5.6524 avg training loss: 8.6251
batch: [16970/21305] batch time: 0.063 trainign loss: 6.2389 avg training loss: 8.6248
batch: [16980/21305] batch time: 0.063 trainign loss: 5.0755 avg training loss: 8.6245
batch: [16990/21305] batch time: 0.056 trainign loss: 4.2620 avg training loss: 8.6240
batch: [17000/21305] batch time: 0.051 trainign loss: 3.0441 avg training loss: 8.6234
batch: [17010/21305] batch time: 0.056 trainign loss: 8.4097 avg training loss: 8.6233
batch: [17020/21305] batch time: 0.056 trainign loss: 1.3079 avg training loss: 8.6228
batch: [17030/21305] batch time: 0.056 trainign loss: 8.4166 avg training loss: 8.6224
batch: [17040/21305] batch time: 0.054 trainign loss: 8.5022 avg training loss: 8.6223
batch: [17050/21305] batch time: 0.056 trainign loss: 7.8024 avg training loss: 8.6221
batch: [17060/21305] batch time: 0.051 trainign loss: 7.9305 avg training loss: 8.6218
batch: [17070/21305] batch time: 0.056 trainign loss: 7.9795 avg training loss: 8.6214
batch: [17080/21305] batch time: 0.056 trainign loss: 7.8783 avg training loss: 8.6212
batch: [17090/21305] batch time: 0.056 trainign loss: 7.6034 avg training loss: 8.6211
batch: [17100/21305] batch time: 0.063 trainign loss: 3.2500 avg training loss: 8.6207
batch: [17110/21305] batch time: 0.060 trainign loss: 8.5169 avg training loss: 8.6207
batch: [17120/21305] batch time: 0.322 trainign loss: 7.8616 avg training loss: 8.6205
batch: [17130/21305] batch time: 0.061 trainign loss: 6.6961 avg training loss: 8.6203
batch: [17140/21305] batch time: 0.448 trainign loss: 6.6694 avg training loss: 8.6200
batch: [17150/21305] batch time: 0.059 trainign loss: 7.3312 avg training loss: 8.6198
batch: [17160/21305] batch time: 0.061 trainign loss: 7.5136 avg training loss: 8.6195
batch: [17170/21305] batch time: 0.056 trainign loss: 5.8687 avg training loss: 8.6189
batch: [17180/21305] batch time: 1.067 trainign loss: 7.8784 avg training loss: 8.6188
batch: [17190/21305] batch time: 0.062 trainign loss: 5.8985 avg training loss: 8.6185
batch: [17200/21305] batch time: 1.161 trainign loss: 7.0280 avg training loss: 8.6182
batch: [17210/21305] batch time: 0.056 trainign loss: 7.4286 avg training loss: 8.6179
batch: [17220/21305] batch time: 1.009 trainign loss: 7.7579 avg training loss: 8.6178
batch: [17230/21305] batch time: 0.058 trainign loss: 5.5315 avg training loss: 8.6174
batch: [17240/21305] batch time: 1.079 trainign loss: 7.2508 avg training loss: 8.6169
batch: [17250/21305] batch time: 0.056 trainign loss: 9.2094 avg training loss: 8.6165
batch: [17260/21305] batch time: 1.372 trainign loss: 8.3287 avg training loss: 8.6164
batch: [17270/21305] batch time: 0.063 trainign loss: 7.2878 avg training loss: 8.6164
batch: [17280/21305] batch time: 1.808 trainign loss: 7.9202 avg training loss: 8.6162
batch: [17290/21305] batch time: 0.056 trainign loss: 5.5779 avg training loss: 8.6159
batch: [17300/21305] batch time: 1.921 trainign loss: 7.1853 avg training loss: 8.6157
batch: [17310/21305] batch time: 0.061 trainign loss: 4.5551 avg training loss: 8.6152
batch: [17320/21305] batch time: 1.870 trainign loss: 12.4319 avg training loss: 8.6142
batch: [17330/21305] batch time: 0.149 trainign loss: 9.4852 avg training loss: 8.6144
batch: [17340/21305] batch time: 1.797 trainign loss: 9.0656 avg training loss: 8.6144
batch: [17350/21305] batch time: 0.732 trainign loss: 8.7420 avg training loss: 8.6140
batch: [17360/21305] batch time: 1.283 trainign loss: 6.9672 avg training loss: 8.6139
batch: [17370/21305] batch time: 0.943 trainign loss: 7.2323 avg training loss: 8.6136
batch: [17380/21305] batch time: 0.894 trainign loss: 7.6747 avg training loss: 8.6135
batch: [17390/21305] batch time: 0.551 trainign loss: 8.0522 avg training loss: 8.6134
batch: [17400/21305] batch time: 0.061 trainign loss: 7.7091 avg training loss: 8.6132
batch: [17410/21305] batch time: 1.256 trainign loss: 7.5929 avg training loss: 8.6130
batch: [17420/21305] batch time: 0.062 trainign loss: 6.5991 avg training loss: 8.6129
batch: [17430/21305] batch time: 0.295 trainign loss: 6.5781 avg training loss: 8.6126
batch: [17440/21305] batch time: 0.063 trainign loss: 7.3237 avg training loss: 8.6123
batch: [17450/21305] batch time: 0.816 trainign loss: 6.8549 avg training loss: 8.6121
batch: [17460/21305] batch time: 0.272 trainign loss: 5.8055 avg training loss: 8.6117
batch: [17470/21305] batch time: 0.056 trainign loss: 7.3267 avg training loss: 8.6114
batch: [17480/21305] batch time: 0.818 trainign loss: 6.6229 avg training loss: 8.6110
batch: [17490/21305] batch time: 0.063 trainign loss: 7.1600 avg training loss: 8.6108
batch: [17500/21305] batch time: 1.140 trainign loss: 6.8813 avg training loss: 8.6106
batch: [17510/21305] batch time: 0.393 trainign loss: 7.2866 avg training loss: 8.6104
batch: [17520/21305] batch time: 0.774 trainign loss: 6.0709 avg training loss: 8.6101
batch: [17530/21305] batch time: 0.062 trainign loss: 5.9409 avg training loss: 8.6098
batch: [17540/21305] batch time: 1.003 trainign loss: 7.1472 avg training loss: 8.6095
batch: [17550/21305] batch time: 0.056 trainign loss: 7.0315 avg training loss: 8.6093
batch: [17560/21305] batch time: 0.576 trainign loss: 6.8237 avg training loss: 8.6090
batch: [17570/21305] batch time: 0.757 trainign loss: 7.7845 avg training loss: 8.6087
batch: [17580/21305] batch time: 0.786 trainign loss: 2.8558 avg training loss: 8.6081
batch: [17590/21305] batch time: 0.106 trainign loss: 11.8598 avg training loss: 8.6070
batch: [17600/21305] batch time: 0.107 trainign loss: 9.0569 avg training loss: 8.6068
batch: [17610/21305] batch time: 0.069 trainign loss: 9.1782 avg training loss: 8.6068
batch: [17620/21305] batch time: 0.336 trainign loss: 6.6110 avg training loss: 8.6067
batch: [17630/21305] batch time: 0.265 trainign loss: 7.4654 avg training loss: 8.6066
batch: [17640/21305] batch time: 0.438 trainign loss: 6.1270 avg training loss: 8.6064
batch: [17650/21305] batch time: 0.061 trainign loss: 6.5113 avg training loss: 8.6061
batch: [17660/21305] batch time: 0.841 trainign loss: 6.7359 avg training loss: 8.6059
batch: [17670/21305] batch time: 0.612 trainign loss: 6.6252 avg training loss: 8.6056
batch: [17680/21305] batch time: 0.443 trainign loss: 7.2611 avg training loss: 8.6053
batch: [17690/21305] batch time: 1.709 trainign loss: 5.9646 avg training loss: 8.6050
batch: [17700/21305] batch time: 0.304 trainign loss: 6.6990 avg training loss: 8.6046
batch: [17710/21305] batch time: 1.235 trainign loss: 6.9072 avg training loss: 8.6044
batch: [17720/21305] batch time: 0.741 trainign loss: 7.1688 avg training loss: 8.6042
batch: [17730/21305] batch time: 1.315 trainign loss: 7.4137 avg training loss: 8.6039
batch: [17740/21305] batch time: 1.002 trainign loss: 6.1915 avg training loss: 8.6036
batch: [17750/21305] batch time: 0.716 trainign loss: 7.1393 avg training loss: 8.6034
batch: [17760/21305] batch time: 1.204 trainign loss: 6.7791 avg training loss: 8.6033
batch: [17770/21305] batch time: 0.474 trainign loss: 7.5128 avg training loss: 8.6030
batch: [17780/21305] batch time: 1.659 trainign loss: 6.6213 avg training loss: 8.6028
batch: [17790/21305] batch time: 0.715 trainign loss: 8.0241 avg training loss: 8.6025
batch: [17800/21305] batch time: 1.983 trainign loss: 6.5229 avg training loss: 8.6023
batch: [17810/21305] batch time: 0.054 trainign loss: 7.5306 avg training loss: 8.6021
batch: [17820/21305] batch time: 2.089 trainign loss: 7.1920 avg training loss: 8.6018
batch: [17830/21305] batch time: 0.439 trainign loss: 6.3725 avg training loss: 8.6016
batch: [17840/21305] batch time: 1.285 trainign loss: 8.1328 avg training loss: 8.6014
batch: [17850/21305] batch time: 1.727 trainign loss: 7.5511 avg training loss: 8.6013
batch: [17860/21305] batch time: 0.186 trainign loss: 7.3371 avg training loss: 8.6009
batch: [17870/21305] batch time: 2.637 trainign loss: 6.5199 avg training loss: 8.6007
batch: [17880/21305] batch time: 0.554 trainign loss: 7.2491 avg training loss: 8.6005
batch: [17890/21305] batch time: 1.641 trainign loss: 7.2972 avg training loss: 8.6002
batch: [17900/21305] batch time: 1.007 trainign loss: 7.1343 avg training loss: 8.5998
batch: [17910/21305] batch time: 0.972 trainign loss: 3.9268 avg training loss: 8.5993
batch: [17920/21305] batch time: 1.171 trainign loss: 8.0793 avg training loss: 8.5993
batch: [17930/21305] batch time: 1.259 trainign loss: 6.3478 avg training loss: 8.5991
batch: [17940/21305] batch time: 1.098 trainign loss: 7.5358 avg training loss: 8.5989
batch: [17950/21305] batch time: 0.358 trainign loss: 7.4736 avg training loss: 8.5984
batch: [17960/21305] batch time: 1.686 trainign loss: 7.1842 avg training loss: 8.5982
batch: [17970/21305] batch time: 0.411 trainign loss: 6.2131 avg training loss: 8.5979
batch: [17980/21305] batch time: 1.524 trainign loss: 2.7649 avg training loss: 8.5974
batch: [17990/21305] batch time: 0.326 trainign loss: 5.2322 avg training loss: 8.5972
batch: [18000/21305] batch time: 2.067 trainign loss: 8.5420 avg training loss: 8.5970
batch: [18010/21305] batch time: 0.367 trainign loss: 8.8074 avg training loss: 8.5968
batch: [18020/21305] batch time: 2.114 trainign loss: 5.1252 avg training loss: 8.5965
batch: [18030/21305] batch time: 0.166 trainign loss: 7.5666 avg training loss: 8.5963
batch: [18040/21305] batch time: 2.614 trainign loss: 6.9138 avg training loss: 8.5961
batch: [18050/21305] batch time: 0.088 trainign loss: 7.4260 avg training loss: 8.5959
batch: [18060/21305] batch time: 2.769 trainign loss: 8.2052 avg training loss: 8.5956
batch: [18070/21305] batch time: 0.057 trainign loss: 8.1527 avg training loss: 8.5955
batch: [18080/21305] batch time: 2.667 trainign loss: 7.3885 avg training loss: 8.5953
batch: [18090/21305] batch time: 0.056 trainign loss: 6.0980 avg training loss: 8.5950
batch: [18100/21305] batch time: 2.501 trainign loss: 6.6641 avg training loss: 8.5947
batch: [18110/21305] batch time: 0.061 trainign loss: 5.6743 avg training loss: 8.5944
batch: [18120/21305] batch time: 2.168 trainign loss: 7.1835 avg training loss: 8.5940
batch: [18130/21305] batch time: 0.062 trainign loss: 8.0599 avg training loss: 8.5938
batch: [18140/21305] batch time: 2.181 trainign loss: 8.2460 avg training loss: 8.5936
batch: [18150/21305] batch time: 0.055 trainign loss: 8.1272 avg training loss: 8.5935
batch: [18160/21305] batch time: 2.412 trainign loss: 5.9904 avg training loss: 8.5932
batch: [18170/21305] batch time: 0.063 trainign loss: 6.6166 avg training loss: 8.5930
batch: [18180/21305] batch time: 2.426 trainign loss: 0.1440 avg training loss: 8.5921
batch: [18190/21305] batch time: 0.053 trainign loss: 8.2355 avg training loss: 8.5920
batch: [18200/21305] batch time: 2.353 trainign loss: 8.4157 avg training loss: 8.5921
batch: [18210/21305] batch time: 0.055 trainign loss: 7.7272 avg training loss: 8.5918
batch: [18220/21305] batch time: 2.250 trainign loss: 5.1761 avg training loss: 8.5915
batch: [18230/21305] batch time: 0.056 trainign loss: 8.1094 avg training loss: 8.5914
batch: [18240/21305] batch time: 2.150 trainign loss: 6.4436 avg training loss: 8.5912
batch: [18250/21305] batch time: 0.051 trainign loss: 5.9046 avg training loss: 8.5909
batch: [18260/21305] batch time: 2.031 trainign loss: 3.9273 avg training loss: 8.5904
batch: [18270/21305] batch time: 0.053 trainign loss: 5.9325 avg training loss: 8.5901
batch: [18280/21305] batch time: 2.456 trainign loss: 7.2395 avg training loss: 8.5897
batch: [18290/21305] batch time: 0.058 trainign loss: 8.0142 avg training loss: 8.5897
batch: [18300/21305] batch time: 2.467 trainign loss: 7.0107 avg training loss: 8.5896
batch: [18310/21305] batch time: 0.061 trainign loss: 7.4838 avg training loss: 8.5893
batch: [18320/21305] batch time: 2.479 trainign loss: 6.2655 avg training loss: 8.5891
batch: [18330/21305] batch time: 0.056 trainign loss: 6.6373 avg training loss: 8.5888
batch: [18340/21305] batch time: 2.379 trainign loss: 7.4219 avg training loss: 8.5886
batch: [18350/21305] batch time: 0.053 trainign loss: 7.9265 avg training loss: 8.5883
batch: [18360/21305] batch time: 2.254 trainign loss: 6.5338 avg training loss: 8.5881
batch: [18370/21305] batch time: 0.053 trainign loss: 7.6807 avg training loss: 8.5878
batch: [18380/21305] batch time: 2.372 trainign loss: 6.9979 avg training loss: 8.5875
batch: [18390/21305] batch time: 0.061 trainign loss: 6.4341 avg training loss: 8.5872
batch: [18400/21305] batch time: 1.873 trainign loss: 7.5099 avg training loss: 8.5870
batch: [18410/21305] batch time: 0.062 trainign loss: 5.7672 avg training loss: 8.5867
batch: [18420/21305] batch time: 2.102 trainign loss: 7.1504 avg training loss: 8.5865
batch: [18430/21305] batch time: 0.062 trainign loss: 7.5007 avg training loss: 8.5862
batch: [18440/21305] batch time: 2.276 trainign loss: 0.1158 avg training loss: 8.5854
batch: [18450/21305] batch time: 0.054 trainign loss: 8.9375 avg training loss: 8.5850
batch: [18460/21305] batch time: 2.391 trainign loss: 8.6779 avg training loss: 8.5849
batch: [18470/21305] batch time: 0.062 trainign loss: 8.8094 avg training loss: 8.5850
batch: [18480/21305] batch time: 2.528 trainign loss: 7.1006 avg training loss: 8.5848
batch: [18490/21305] batch time: 0.062 trainign loss: 5.6698 avg training loss: 8.5844
batch: [18500/21305] batch time: 2.375 trainign loss: 0.0090 avg training loss: 8.5833
batch: [18510/21305] batch time: 0.056 trainign loss: 9.4631 avg training loss: 8.5833
batch: [18520/21305] batch time: 2.140 trainign loss: 9.4771 avg training loss: 8.5833
batch: [18530/21305] batch time: 0.051 trainign loss: 7.4939 avg training loss: 8.5833
batch: [18540/21305] batch time: 2.366 trainign loss: 7.3256 avg training loss: 8.5832
batch: [18550/21305] batch time: 0.056 trainign loss: 5.2610 avg training loss: 8.5829
batch: [18560/21305] batch time: 2.182 trainign loss: 7.9547 avg training loss: 8.5826
batch: [18570/21305] batch time: 0.052 trainign loss: 7.8349 avg training loss: 8.5824
batch: [18580/21305] batch time: 2.373 trainign loss: 7.2587 avg training loss: 8.5822
batch: [18590/21305] batch time: 0.063 trainign loss: 7.7102 avg training loss: 8.5819
batch: [18600/21305] batch time: 2.406 trainign loss: 6.2305 avg training loss: 8.5816
batch: [18610/21305] batch time: 0.056 trainign loss: 7.1442 avg training loss: 8.5811
batch: [18620/21305] batch time: 2.338 trainign loss: 8.3328 avg training loss: 8.5809
batch: [18630/21305] batch time: 0.052 trainign loss: 6.7494 avg training loss: 8.5807
batch: [18640/21305] batch time: 2.180 trainign loss: 5.2025 avg training loss: 8.5804
batch: [18650/21305] batch time: 0.061 trainign loss: 4.3871 avg training loss: 8.5801
batch: [18660/21305] batch time: 2.095 trainign loss: 0.0035 avg training loss: 8.5788
batch: [18670/21305] batch time: 0.056 trainign loss: 10.2000 avg training loss: 8.5786
batch: [18680/21305] batch time: 2.528 trainign loss: 8.8000 avg training loss: 8.5787
batch: [18690/21305] batch time: 0.062 trainign loss: 8.9258 avg training loss: 8.5787
batch: [18700/21305] batch time: 2.030 trainign loss: 8.0385 avg training loss: 8.5787
batch: [18710/21305] batch time: 0.063 trainign loss: 7.8839 avg training loss: 8.5786
batch: [18720/21305] batch time: 2.195 trainign loss: 6.8608 avg training loss: 8.5784
batch: [18730/21305] batch time: 0.055 trainign loss: 6.4758 avg training loss: 8.5782
batch: [18740/21305] batch time: 2.192 trainign loss: 8.3322 avg training loss: 8.5779
batch: [18750/21305] batch time: 0.057 trainign loss: 6.0895 avg training loss: 8.5777
batch: [18760/21305] batch time: 2.255 trainign loss: 5.9948 avg training loss: 8.5774
batch: [18770/21305] batch time: 0.059 trainign loss: 8.3958 avg training loss: 8.5770
batch: [18780/21305] batch time: 2.718 trainign loss: 7.6781 avg training loss: 8.5768
batch: [18790/21305] batch time: 0.057 trainign loss: 6.3121 avg training loss: 8.5766
batch: [18800/21305] batch time: 2.306 trainign loss: 8.4232 avg training loss: 8.5765
batch: [18810/21305] batch time: 0.052 trainign loss: 7.6793 avg training loss: 8.5764
batch: [18820/21305] batch time: 2.469 trainign loss: 5.9395 avg training loss: 8.5761
batch: [18830/21305] batch time: 0.060 trainign loss: 8.4308 avg training loss: 8.5759
batch: [18840/21305] batch time: 2.236 trainign loss: 7.0225 avg training loss: 8.5756
batch: [18850/21305] batch time: 0.056 trainign loss: 7.9475 avg training loss: 8.5753
batch: [18860/21305] batch time: 2.109 trainign loss: 7.4554 avg training loss: 8.5751
batch: [18870/21305] batch time: 0.052 trainign loss: 7.4297 avg training loss: 8.5748
batch: [18880/21305] batch time: 2.310 trainign loss: 5.2768 avg training loss: 8.5744
batch: [18890/21305] batch time: 0.062 trainign loss: 7.2244 avg training loss: 8.5742
batch: [18900/21305] batch time: 2.029 trainign loss: 6.3530 avg training loss: 8.5741
batch: [18910/21305] batch time: 0.063 trainign loss: 7.6744 avg training loss: 8.5739
batch: [18920/21305] batch time: 2.120 trainign loss: 8.2004 avg training loss: 8.5736
batch: [18930/21305] batch time: 0.060 trainign loss: 8.3257 avg training loss: 8.5735
batch: [18940/21305] batch time: 1.903 trainign loss: 7.1620 avg training loss: 8.5734
batch: [18950/21305] batch time: 0.374 trainign loss: 7.7874 avg training loss: 8.5731
batch: [18960/21305] batch time: 1.651 trainign loss: 7.3069 avg training loss: 8.5727
batch: [18970/21305] batch time: 0.132 trainign loss: 7.2153 avg training loss: 8.5725
batch: [18980/21305] batch time: 1.769 trainign loss: 8.6872 avg training loss: 8.5721
batch: [18990/21305] batch time: 0.357 trainign loss: 8.1496 avg training loss: 8.5719
batch: [19000/21305] batch time: 0.711 trainign loss: 7.0794 avg training loss: 8.5716
batch: [19010/21305] batch time: 0.278 trainign loss: 8.3492 avg training loss: 8.5715
batch: [19020/21305] batch time: 1.396 trainign loss: 7.1459 avg training loss: 8.5713
batch: [19030/21305] batch time: 1.638 trainign loss: 8.1691 avg training loss: 8.5710
batch: [19040/21305] batch time: 0.279 trainign loss: 7.5912 avg training loss: 8.5708
batch: [19050/21305] batch time: 2.131 trainign loss: 8.3003 avg training loss: 8.5705
batch: [19060/21305] batch time: 0.787 trainign loss: 6.2921 avg training loss: 8.5701
batch: [19070/21305] batch time: 1.582 trainign loss: 7.1987 avg training loss: 8.5698
batch: [19080/21305] batch time: 0.550 trainign loss: 7.4747 avg training loss: 8.5696
batch: [19090/21305] batch time: 2.199 trainign loss: 8.7166 avg training loss: 8.5695
batch: [19100/21305] batch time: 0.169 trainign loss: 8.3764 avg training loss: 8.5694
batch: [19110/21305] batch time: 1.985 trainign loss: 4.9372 avg training loss: 8.5691
batch: [19120/21305] batch time: 0.061 trainign loss: 7.5126 avg training loss: 8.5690
batch: [19130/21305] batch time: 1.778 trainign loss: 8.0828 avg training loss: 8.5688
batch: [19140/21305] batch time: 0.481 trainign loss: 6.8102 avg training loss: 8.5686
batch: [19150/21305] batch time: 0.336 trainign loss: 6.6762 avg training loss: 8.5683
batch: [19160/21305] batch time: 0.494 trainign loss: 7.5935 avg training loss: 8.5682
batch: [19170/21305] batch time: 0.062 trainign loss: 7.2067 avg training loss: 8.5679
batch: [19180/21305] batch time: 0.897 trainign loss: 5.6269 avg training loss: 8.5677
batch: [19190/21305] batch time: 0.056 trainign loss: 7.5991 avg training loss: 8.5674
batch: [19200/21305] batch time: 0.390 trainign loss: 5.2021 avg training loss: 8.5670
batch: [19210/21305] batch time: 0.062 trainign loss: 7.0569 avg training loss: 8.5667
batch: [19220/21305] batch time: 0.094 trainign loss: 7.7606 avg training loss: 8.5664
batch: [19230/21305] batch time: 0.056 trainign loss: 6.5901 avg training loss: 8.5662
batch: [19240/21305] batch time: 0.053 trainign loss: 6.3486 avg training loss: 8.5660
batch: [19250/21305] batch time: 0.056 trainign loss: 4.5425 avg training loss: 8.5656
batch: [19260/21305] batch time: 0.057 trainign loss: 5.3775 avg training loss: 8.5652
batch: [19270/21305] batch time: 0.062 trainign loss: 7.5511 avg training loss: 8.5649
batch: [19280/21305] batch time: 0.062 trainign loss: 8.9027 avg training loss: 8.5648
batch: [19290/21305] batch time: 0.062 trainign loss: 7.7359 avg training loss: 8.5646
batch: [19300/21305] batch time: 0.058 trainign loss: 8.0471 avg training loss: 8.5644
batch: [19310/21305] batch time: 0.053 trainign loss: 8.1746 avg training loss: 8.5643
batch: [19320/21305] batch time: 0.056 trainign loss: 8.0288 avg training loss: 8.5641
batch: [19330/21305] batch time: 0.056 trainign loss: 7.2755 avg training loss: 8.5639
batch: [19340/21305] batch time: 0.053 trainign loss: 6.0703 avg training loss: 8.5637
batch: [19350/21305] batch time: 0.056 trainign loss: 7.5783 avg training loss: 8.5635
batch: [19360/21305] batch time: 0.056 trainign loss: 6.5750 avg training loss: 8.5633
batch: [19370/21305] batch time: 0.052 trainign loss: 7.3079 avg training loss: 8.5631
batch: [19380/21305] batch time: 0.059 trainign loss: 6.9778 avg training loss: 8.5627
batch: [19390/21305] batch time: 0.062 trainign loss: 8.4456 avg training loss: 8.5626
batch: [19400/21305] batch time: 0.056 trainign loss: 7.5121 avg training loss: 8.5624
batch: [19410/21305] batch time: 0.056 trainign loss: 5.8695 avg training loss: 8.5621
batch: [19420/21305] batch time: 0.051 trainign loss: 7.4996 avg training loss: 8.5618
batch: [19430/21305] batch time: 0.058 trainign loss: 7.4662 avg training loss: 8.5617
batch: [19440/21305] batch time: 0.051 trainign loss: 7.7877 avg training loss: 8.5615
batch: [19450/21305] batch time: 0.063 trainign loss: 7.5548 avg training loss: 8.5612
batch: [19460/21305] batch time: 0.063 trainign loss: 6.9177 avg training loss: 8.5611
batch: [19470/21305] batch time: 0.056 trainign loss: 7.7859 avg training loss: 8.5609
batch: [19480/21305] batch time: 0.056 trainign loss: 6.4451 avg training loss: 8.5607
batch: [19490/21305] batch time: 0.056 trainign loss: 5.9063 avg training loss: 8.5604
batch: [19500/21305] batch time: 0.051 trainign loss: 7.2334 avg training loss: 8.5602
batch: [19510/21305] batch time: 0.063 trainign loss: 6.5694 avg training loss: 8.5600
batch: [19520/21305] batch time: 0.057 trainign loss: 7.4313 avg training loss: 8.5598
batch: [19530/21305] batch time: 0.062 trainign loss: 7.7319 avg training loss: 8.5595
batch: [19540/21305] batch time: 0.056 trainign loss: 7.0666 avg training loss: 8.5591
batch: [19550/21305] batch time: 0.056 trainign loss: 6.9948 avg training loss: 8.5588
batch: [19560/21305] batch time: 0.055 trainign loss: 7.6390 avg training loss: 8.5586
batch: [19570/21305] batch time: 0.056 trainign loss: 7.5112 avg training loss: 8.5584
batch: [19580/21305] batch time: 0.052 trainign loss: 5.9170 avg training loss: 8.5581
batch: [19590/21305] batch time: 0.058 trainign loss: 6.5506 avg training loss: 8.5577
batch: [19600/21305] batch time: 0.053 trainign loss: 6.9259 avg training loss: 8.5575
batch: [19610/21305] batch time: 0.057 trainign loss: 7.0088 avg training loss: 8.5573
batch: [19620/21305] batch time: 0.051 trainign loss: 7.6649 avg training loss: 8.5570
batch: [19630/21305] batch time: 0.058 trainign loss: 7.6869 avg training loss: 8.5569
batch: [19640/21305] batch time: 0.051 trainign loss: 7.9839 avg training loss: 8.5567
batch: [19650/21305] batch time: 0.056 trainign loss: 7.3144 avg training loss: 8.5564
batch: [19660/21305] batch time: 0.056 trainign loss: 7.6733 avg training loss: 8.5562
batch: [19670/21305] batch time: 0.056 trainign loss: 7.7516 avg training loss: 8.5560
batch: [19680/21305] batch time: 0.061 trainign loss: 0.0266 avg training loss: 8.5550
batch: [19690/21305] batch time: 0.056 trainign loss: 7.9565 avg training loss: 8.5551
batch: [19700/21305] batch time: 0.051 trainign loss: 8.9932 avg training loss: 8.5551
batch: [19710/21305] batch time: 0.062 trainign loss: 7.5953 avg training loss: 8.5549
batch: [19720/21305] batch time: 0.055 trainign loss: 7.3780 avg training loss: 8.5547
batch: [19730/21305] batch time: 0.063 trainign loss: 6.2899 avg training loss: 8.5545
batch: [19740/21305] batch time: 0.063 trainign loss: 6.2878 avg training loss: 8.5543
batch: [19750/21305] batch time: 0.056 trainign loss: 8.1193 avg training loss: 8.5541
batch: [19760/21305] batch time: 0.056 trainign loss: 6.9566 avg training loss: 8.5539
batch: [19770/21305] batch time: 0.062 trainign loss: 6.5774 avg training loss: 8.5537
batch: [19780/21305] batch time: 0.055 trainign loss: 6.6690 avg training loss: 8.5534
batch: [19790/21305] batch time: 0.056 trainign loss: 6.7625 avg training loss: 8.5531
batch: [19800/21305] batch time: 0.056 trainign loss: 6.6352 avg training loss: 8.5529
batch: [19810/21305] batch time: 0.056 trainign loss: 5.8151 avg training loss: 8.5525
batch: [19820/21305] batch time: 0.053 trainign loss: 7.2860 avg training loss: 8.5522
batch: [19830/21305] batch time: 0.058 trainign loss: 7.7169 avg training loss: 8.5521
batch: [19840/21305] batch time: 0.057 trainign loss: 7.1573 avg training loss: 8.5519
batch: [19850/21305] batch time: 0.062 trainign loss: 7.3197 avg training loss: 8.5516
batch: [19860/21305] batch time: 0.682 trainign loss: 8.2291 avg training loss: 8.5515
batch: [19870/21305] batch time: 0.056 trainign loss: 7.8570 avg training loss: 8.5513
batch: [19880/21305] batch time: 1.048 trainign loss: 5.9334 avg training loss: 8.5510
batch: [19890/21305] batch time: 0.057 trainign loss: 5.7713 avg training loss: 8.5507
batch: [19900/21305] batch time: 0.073 trainign loss: 10.2354 avg training loss: 8.5501
batch: [19910/21305] batch time: 0.056 trainign loss: 1.9141 avg training loss: 8.5493
batch: [19920/21305] batch time: 0.051 trainign loss: 7.8396 avg training loss: 8.5495
batch: [19930/21305] batch time: 0.056 trainign loss: 8.9814 avg training loss: 8.5495
batch: [19940/21305] batch time: 0.056 trainign loss: 6.7974 avg training loss: 8.5493
batch: [19950/21305] batch time: 0.056 trainign loss: 7.4150 avg training loss: 8.5490
batch: [19960/21305] batch time: 0.055 trainign loss: 7.5049 avg training loss: 8.5486
batch: [19970/21305] batch time: 0.056 trainign loss: 7.4511 avg training loss: 8.5484
batch: [19980/21305] batch time: 0.051 trainign loss: 8.2750 avg training loss: 8.5483
batch: [19990/21305] batch time: 0.056 trainign loss: 6.7640 avg training loss: 8.5481
batch: [20000/21305] batch time: 0.062 trainign loss: 7.6919 avg training loss: 8.5479
batch: [20010/21305] batch time: 0.056 trainign loss: 6.3881 avg training loss: 8.5476
batch: [20020/21305] batch time: 0.062 trainign loss: 1.4568 avg training loss: 8.5471
batch: [20030/21305] batch time: 0.056 trainign loss: 7.7868 avg training loss: 8.5467
batch: [20040/21305] batch time: 0.057 trainign loss: 6.4474 avg training loss: 8.5464
batch: [20050/21305] batch time: 0.061 trainign loss: 3.4149 avg training loss: 8.5460
batch: [20060/21305] batch time: 0.760 trainign loss: 3.9625 avg training loss: 8.5455
batch: [20070/21305] batch time: 0.060 trainign loss: 7.6024 avg training loss: 8.5453
batch: [20080/21305] batch time: 0.178 trainign loss: 7.6851 avg training loss: 8.5452
batch: [20090/21305] batch time: 0.057 trainign loss: 6.4295 avg training loss: 8.5450
batch: [20100/21305] batch time: 0.053 trainign loss: 10.5338 avg training loss: 8.5443
batch: [20110/21305] batch time: 0.056 trainign loss: 6.8601 avg training loss: 8.5442
batch: [20120/21305] batch time: 0.057 trainign loss: 8.5539 avg training loss: 8.5442
batch: [20130/21305] batch time: 0.057 trainign loss: 7.6598 avg training loss: 8.5441
batch: [20140/21305] batch time: 0.052 trainign loss: 7.5683 avg training loss: 8.5439
batch: [20150/21305] batch time: 0.057 trainign loss: 6.4660 avg training loss: 8.5437
batch: [20160/21305] batch time: 0.056 trainign loss: 7.8810 avg training loss: 8.5435
batch: [20170/21305] batch time: 0.057 trainign loss: 7.7908 avg training loss: 8.5433
batch: [20180/21305] batch time: 0.053 trainign loss: 6.7273 avg training loss: 8.5431
batch: [20190/21305] batch time: 0.057 trainign loss: 4.6762 avg training loss: 8.5427
batch: [20200/21305] batch time: 0.051 trainign loss: 5.8145 avg training loss: 8.5424
batch: [20210/21305] batch time: 0.056 trainign loss: 8.0198 avg training loss: 8.5422
batch: [20220/21305] batch time: 0.059 trainign loss: 7.8598 avg training loss: 8.5420
batch: [20230/21305] batch time: 0.207 trainign loss: 7.0669 avg training loss: 8.5418
batch: [20240/21305] batch time: 0.056 trainign loss: 5.8571 avg training loss: 8.5415
batch: [20250/21305] batch time: 0.051 trainign loss: 8.0366 avg training loss: 8.5412
batch: [20260/21305] batch time: 0.058 trainign loss: 7.7317 avg training loss: 8.5409
batch: [20270/21305] batch time: 0.241 trainign loss: 2.6089 avg training loss: 8.5405
batch: [20280/21305] batch time: 0.053 trainign loss: 7.3923 avg training loss: 8.5401
batch: [20290/21305] batch time: 0.052 trainign loss: 8.0600 avg training loss: 8.5400
batch: [20300/21305] batch time: 0.056 trainign loss: 8.0093 avg training loss: 8.5398
batch: [20310/21305] batch time: 0.201 trainign loss: 7.0525 avg training loss: 8.5397
batch: [20320/21305] batch time: 0.056 trainign loss: 7.1653 avg training loss: 8.5395
batch: [20330/21305] batch time: 0.874 trainign loss: 6.0165 avg training loss: 8.5391
batch: [20340/21305] batch time: 0.056 trainign loss: 6.6559 avg training loss: 8.5387
batch: [20350/21305] batch time: 1.155 trainign loss: 7.3367 avg training loss: 8.5386
batch: [20360/21305] batch time: 0.056 trainign loss: 8.0678 avg training loss: 8.5384
batch: [20370/21305] batch time: 0.872 trainign loss: 7.7657 avg training loss: 8.5381
batch: [20380/21305] batch time: 0.059 trainign loss: 6.6075 avg training loss: 8.5379
batch: [20390/21305] batch time: 0.057 trainign loss: 7.9196 avg training loss: 8.5376
batch: [20400/21305] batch time: 0.062 trainign loss: 7.3566 avg training loss: 8.5373
batch: [20410/21305] batch time: 0.056 trainign loss: 7.2191 avg training loss: 8.5371
batch: [20420/21305] batch time: 0.056 trainign loss: 7.0841 avg training loss: 8.5369
batch: [20430/21305] batch time: 0.352 trainign loss: 7.3561 avg training loss: 8.5364
batch: [20440/21305] batch time: 0.056 trainign loss: 7.0654 avg training loss: 8.5361
batch: [20450/21305] batch time: 0.054 trainign loss: 5.9550 avg training loss: 8.5360
batch: [20460/21305] batch time: 0.056 trainign loss: 8.5727 avg training loss: 8.5359
batch: [20470/21305] batch time: 0.769 trainign loss: 5.3338 avg training loss: 8.5356
batch: [20480/21305] batch time: 0.062 trainign loss: 0.0676 avg training loss: 8.5347
batch: [20490/21305] batch time: 0.533 trainign loss: 15.7773 avg training loss: 8.5338
batch: [20500/21305] batch time: 0.058 trainign loss: 9.3153 avg training loss: 8.5341
batch: [20510/21305] batch time: 0.056 trainign loss: 8.5742 avg training loss: 8.5342
batch: [20520/21305] batch time: 0.063 trainign loss: 8.0446 avg training loss: 8.5342
batch: [20530/21305] batch time: 0.497 trainign loss: 7.5724 avg training loss: 8.5338
batch: [20540/21305] batch time: 0.060 trainign loss: 8.5755 avg training loss: 8.5335
batch: [20550/21305] batch time: 0.777 trainign loss: 6.9239 avg training loss: 8.5333
batch: [20560/21305] batch time: 0.056 trainign loss: 7.1317 avg training loss: 8.5329
batch: [20570/21305] batch time: 0.606 trainign loss: 8.2772 avg training loss: 8.5328
batch: [20580/21305] batch time: 0.056 trainign loss: 7.5837 avg training loss: 8.5327
batch: [20590/21305] batch time: 0.678 trainign loss: 7.1589 avg training loss: 8.5325
batch: [20600/21305] batch time: 0.056 trainign loss: 7.3223 avg training loss: 8.5323
batch: [20610/21305] batch time: 0.050 trainign loss: 8.0130 avg training loss: 8.5322
batch: [20620/21305] batch time: 0.060 trainign loss: 7.5719 avg training loss: 8.5321
batch: [20630/21305] batch time: 0.051 trainign loss: 7.2237 avg training loss: 8.5318
batch: [20640/21305] batch time: 0.063 trainign loss: 7.3989 avg training loss: 8.5316
batch: [20650/21305] batch time: 0.058 trainign loss: 7.5312 avg training loss: 8.5314
batch: [20660/21305] batch time: 0.059 trainign loss: 6.4585 avg training loss: 8.5312
batch: [20670/21305] batch time: 0.056 trainign loss: 8.2896 avg training loss: 8.5310
batch: [20680/21305] batch time: 0.058 trainign loss: 6.1521 avg training loss: 8.5307
batch: [20690/21305] batch time: 0.056 trainign loss: 7.3818 avg training loss: 8.5305
batch: [20700/21305] batch time: 0.059 trainign loss: 7.6180 avg training loss: 8.5303
batch: [20710/21305] batch time: 0.058 trainign loss: 7.4955 avg training loss: 8.5301
batch: [20720/21305] batch time: 1.422 trainign loss: 7.5982 avg training loss: 8.5299
batch: [20730/21305] batch time: 0.056 trainign loss: 7.1972 avg training loss: 8.5298
batch: [20740/21305] batch time: 1.142 trainign loss: 7.2605 avg training loss: 8.5296
batch: [20750/21305] batch time: 0.056 trainign loss: 6.1686 avg training loss: 8.5294
batch: [20760/21305] batch time: 0.741 trainign loss: 8.1407 avg training loss: 8.5292
batch: [20770/21305] batch time: 0.056 trainign loss: 7.1609 avg training loss: 8.5291
batch: [20780/21305] batch time: 1.852 trainign loss: 7.3803 avg training loss: 8.5289
batch: [20790/21305] batch time: 0.059 trainign loss: 6.6313 avg training loss: 8.5287
batch: [20800/21305] batch time: 1.774 trainign loss: 7.0927 avg training loss: 8.5284
batch: [20810/21305] batch time: 0.060 trainign loss: 7.5489 avg training loss: 8.5283
batch: [20820/21305] batch time: 0.621 trainign loss: 7.0622 avg training loss: 8.5281
batch: [20830/21305] batch time: 0.055 trainign loss: 7.0178 avg training loss: 8.5279
batch: [20840/21305] batch time: 1.573 trainign loss: 6.1936 avg training loss: 8.5277
batch: [20850/21305] batch time: 0.054 trainign loss: 6.5264 avg training loss: 8.5273
batch: [20860/21305] batch time: 2.055 trainign loss: 7.0674 avg training loss: 8.5270
batch: [20870/21305] batch time: 0.060 trainign loss: 6.6399 avg training loss: 8.5268
batch: [20880/21305] batch time: 2.010 trainign loss: 6.4016 avg training loss: 8.5265
batch: [20890/21305] batch time: 0.059 trainign loss: 8.0423 avg training loss: 8.5263
batch: [20900/21305] batch time: 2.011 trainign loss: 5.1277 avg training loss: 8.5260
batch: [20910/21305] batch time: 0.056 trainign loss: 6.5295 avg training loss: 8.5256
batch: [20920/21305] batch time: 1.596 trainign loss: 5.2435 avg training loss: 8.5253
batch: [20930/21305] batch time: 0.058 trainign loss: 8.6026 avg training loss: 8.5249
batch: [20940/21305] batch time: 2.371 trainign loss: 2.6796 avg training loss: 8.5242
batch: [20950/21305] batch time: 0.054 trainign loss: 8.1058 avg training loss: 8.5242
batch: [20960/21305] batch time: 2.117 trainign loss: 7.7490 avg training loss: 8.5242
batch: [20970/21305] batch time: 0.055 trainign loss: 6.2207 avg training loss: 8.5241
batch: [20980/21305] batch time: 2.285 trainign loss: 7.9335 avg training loss: 8.5239
batch: [20990/21305] batch time: 0.056 trainign loss: 7.9707 avg training loss: 8.5237
batch: [21000/21305] batch time: 2.117 trainign loss: 6.9708 avg training loss: 8.5236
batch: [21010/21305] batch time: 0.056 trainign loss: 7.3182 avg training loss: 8.5234
batch: [21020/21305] batch time: 1.077 trainign loss: 6.8505 avg training loss: 8.5231
batch: [21030/21305] batch time: 0.061 trainign loss: 7.6016 avg training loss: 8.5230
batch: [21040/21305] batch time: 1.849 trainign loss: 8.1271 avg training loss: 8.5228
batch: [21050/21305] batch time: 0.056 trainign loss: 7.1158 avg training loss: 8.5226
batch: [21060/21305] batch time: 1.659 trainign loss: 6.8042 avg training loss: 8.5223
batch: [21070/21305] batch time: 0.062 trainign loss: 6.9666 avg training loss: 8.5220
batch: [21080/21305] batch time: 0.853 trainign loss: 7.3685 avg training loss: 8.5217
batch: [21090/21305] batch time: 0.062 trainign loss: 8.0356 avg training loss: 8.5214
batch: [21100/21305] batch time: 0.750 trainign loss: 4.7262 avg training loss: 8.5210
batch: [21110/21305] batch time: 0.056 trainign loss: 8.0808 avg training loss: 8.5208
batch: [21120/21305] batch time: 0.478 trainign loss: 8.5167 avg training loss: 8.5207
batch: [21130/21305] batch time: 0.062 trainign loss: 8.1223 avg training loss: 8.5205
batch: [21140/21305] batch time: 0.328 trainign loss: 7.5181 avg training loss: 8.5203
batch: [21150/21305] batch time: 0.056 trainign loss: 7.7900 avg training loss: 8.5201
batch: [21160/21305] batch time: 1.654 trainign loss: 7.5967 avg training loss: 8.5198
batch: [21170/21305] batch time: 0.056 trainign loss: 5.4180 avg training loss: 8.5192
batch: [21180/21305] batch time: 1.229 trainign loss: 8.0935 avg training loss: 8.5192
batch: [21190/21305] batch time: 0.058 trainign loss: 8.2136 avg training loss: 8.5192
batch: [21200/21305] batch time: 0.605 trainign loss: 7.3594 avg training loss: 8.5189
batch: [21210/21305] batch time: 0.058 trainign loss: 6.7211 avg training loss: 8.5188
batch: [21220/21305] batch time: 0.366 trainign loss: 6.7755 avg training loss: 8.5186
batch: [21230/21305] batch time: 0.056 trainign loss: 7.2551 avg training loss: 8.5183
batch: [21240/21305] batch time: 0.637 trainign loss: 5.9535 avg training loss: 8.5181
batch: [21250/21305] batch time: 0.056 trainign loss: 7.5588 avg training loss: 8.5179
batch: [21260/21305] batch time: 0.334 trainign loss: 8.1196 avg training loss: 8.5178
batch: [21270/21305] batch time: 0.056 trainign loss: 4.5599 avg training loss: 8.5175
batch: [21280/21305] batch time: 0.256 trainign loss: 6.8844 avg training loss: 8.5172
batch: [21290/21305] batch time: 0.058 trainign loss: 5.5036 avg training loss: 8.5168
batch: [21300/21305] batch time: 0.728 trainign loss: 8.2469 avg training loss: 8.5166
Epoch: 4
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.775 trainign loss: 8.1269 avg training loss: 8.5165
batch: [10/21305] batch time: 0.056 trainign loss: 10.0589 avg training loss: 8.5158
batch: [20/21305] batch time: 2.180 trainign loss: 8.1176 avg training loss: 8.5159
batch: [30/21305] batch time: 0.056 trainign loss: 7.8180 avg training loss: 8.5157
batch: [40/21305] batch time: 1.475 trainign loss: 8.1257 avg training loss: 8.5155
batch: [50/21305] batch time: 0.063 trainign loss: 5.2570 avg training loss: 8.5153
batch: [60/21305] batch time: 0.737 trainign loss: 8.0499 avg training loss: 8.5151
batch: [70/21305] batch time: 0.057 trainign loss: 8.0755 avg training loss: 8.5148
batch: [80/21305] batch time: 0.112 trainign loss: 7.2781 avg training loss: 8.5147
batch: [90/21305] batch time: 0.057 trainign loss: 7.5676 avg training loss: 8.5144
batch: [100/21305] batch time: 0.321 trainign loss: 7.5994 avg training loss: 8.5143
batch: [110/21305] batch time: 0.056 trainign loss: 6.8184 avg training loss: 8.5140
batch: [120/21305] batch time: 0.059 trainign loss: 7.4177 avg training loss: 8.5138
batch: [130/21305] batch time: 0.053 trainign loss: 7.9451 avg training loss: 8.5135
batch: [140/21305] batch time: 0.057 trainign loss: 6.2437 avg training loss: 8.5133
batch: [150/21305] batch time: 0.061 trainign loss: 8.6422 avg training loss: 8.5130
batch: [160/21305] batch time: 0.052 trainign loss: 7.7024 avg training loss: 8.5129
batch: [170/21305] batch time: 0.056 trainign loss: 7.5135 avg training loss: 8.5127
batch: [180/21305] batch time: 0.051 trainign loss: 6.9247 avg training loss: 8.5125
batch: [190/21305] batch time: 0.051 trainign loss: 6.7972 avg training loss: 8.5122
batch: [200/21305] batch time: 0.055 trainign loss: 0.0198 avg training loss: 8.5112
batch: [210/21305] batch time: 0.054 trainign loss: 10.0840 avg training loss: 8.5111
batch: [220/21305] batch time: 0.056 trainign loss: 8.6393 avg training loss: 8.5111
batch: [230/21305] batch time: 0.056 trainign loss: 7.5038 avg training loss: 8.5111
batch: [240/21305] batch time: 0.060 trainign loss: 8.2314 avg training loss: 8.5110
batch: [250/21305] batch time: 0.056 trainign loss: 8.2840 avg training loss: 8.5109
batch: [260/21305] batch time: 0.057 trainign loss: 7.7643 avg training loss: 8.5107
batch: [270/21305] batch time: 0.052 trainign loss: 6.0568 avg training loss: 8.5105
batch: [280/21305] batch time: 0.056 trainign loss: 6.6577 avg training loss: 8.5103
batch: [290/21305] batch time: 0.063 trainign loss: 7.7531 avg training loss: 8.5099
batch: [300/21305] batch time: 0.057 trainign loss: 7.6826 avg training loss: 8.5097
batch: [310/21305] batch time: 0.052 trainign loss: 6.5119 avg training loss: 8.5096
batch: [320/21305] batch time: 0.058 trainign loss: 7.7865 avg training loss: 8.5094
batch: [330/21305] batch time: 0.056 trainign loss: 7.2749 avg training loss: 8.5092
batch: [340/21305] batch time: 0.062 trainign loss: 8.1489 avg training loss: 8.5090
batch: [350/21305] batch time: 0.062 trainign loss: 8.3572 avg training loss: 8.5088
batch: [360/21305] batch time: 0.063 trainign loss: 8.0477 avg training loss: 8.5086
batch: [370/21305] batch time: 0.063 trainign loss: 6.8227 avg training loss: 8.5084
batch: [380/21305] batch time: 0.061 trainign loss: 7.0828 avg training loss: 8.5083
batch: [390/21305] batch time: 0.063 trainign loss: 5.4812 avg training loss: 8.5079
batch: [400/21305] batch time: 0.054 trainign loss: 7.2926 avg training loss: 8.5076
batch: [410/21305] batch time: 0.062 trainign loss: 8.5393 avg training loss: 8.5075
batch: [420/21305] batch time: 0.063 trainign loss: 7.9935 avg training loss: 8.5073
batch: [430/21305] batch time: 0.062 trainign loss: 7.3849 avg training loss: 8.5073
batch: [440/21305] batch time: 0.057 trainign loss: 6.7843 avg training loss: 8.5069
batch: [450/21305] batch time: 0.057 trainign loss: 6.1047 avg training loss: 8.5067
batch: [460/21305] batch time: 0.053 trainign loss: 7.6176 avg training loss: 8.5064
batch: [470/21305] batch time: 0.062 trainign loss: 7.0492 avg training loss: 8.5063
batch: [480/21305] batch time: 0.060 trainign loss: 8.2076 avg training loss: 8.5061
batch: [490/21305] batch time: 0.056 trainign loss: 7.5121 avg training loss: 8.5060
batch: [500/21305] batch time: 0.056 trainign loss: 5.3255 avg training loss: 8.5057
batch: [510/21305] batch time: 0.059 trainign loss: 6.2153 avg training loss: 8.5055
batch: [520/21305] batch time: 0.057 trainign loss: 5.0112 avg training loss: 8.5053
batch: [530/21305] batch time: 0.056 trainign loss: 7.0567 avg training loss: 8.5050
batch: [540/21305] batch time: 0.057 trainign loss: 7.3205 avg training loss: 8.5048
batch: [550/21305] batch time: 0.057 trainign loss: 5.6478 avg training loss: 8.5044
batch: [560/21305] batch time: 0.051 trainign loss: 6.8136 avg training loss: 8.5043
batch: [570/21305] batch time: 0.056 trainign loss: 6.9099 avg training loss: 8.5040
batch: [580/21305] batch time: 0.051 trainign loss: 7.0740 avg training loss: 8.5036
batch: [590/21305] batch time: 0.063 trainign loss: 7.4940 avg training loss: 8.5033
batch: [600/21305] batch time: 0.053 trainign loss: 6.9451 avg training loss: 8.5032
batch: [610/21305] batch time: 0.056 trainign loss: 8.3103 avg training loss: 8.5031
batch: [620/21305] batch time: 0.056 trainign loss: 7.6691 avg training loss: 8.5030
batch: [630/21305] batch time: 0.057 trainign loss: 6.9144 avg training loss: 8.5028
batch: [640/21305] batch time: 0.062 trainign loss: 7.6124 avg training loss: 8.5026
batch: [650/21305] batch time: 0.063 trainign loss: 6.8066 avg training loss: 8.5024
batch: [660/21305] batch time: 0.055 trainign loss: 6.5369 avg training loss: 8.5020
batch: [670/21305] batch time: 0.056 trainign loss: 9.2109 avg training loss: 8.5016
batch: [680/21305] batch time: 0.056 trainign loss: 8.5362 avg training loss: 8.5015
batch: [690/21305] batch time: 0.517 trainign loss: 6.9253 avg training loss: 8.5014
batch: [700/21305] batch time: 0.058 trainign loss: 8.3655 avg training loss: 8.5012
batch: [710/21305] batch time: 1.237 trainign loss: 6.8852 avg training loss: 8.5010
batch: [720/21305] batch time: 0.062 trainign loss: 7.1408 avg training loss: 8.5007
batch: [730/21305] batch time: 2.344 trainign loss: 8.0063 avg training loss: 8.5006
batch: [740/21305] batch time: 0.063 trainign loss: 8.2408 avg training loss: 8.5003
batch: [750/21305] batch time: 2.085 trainign loss: 7.3627 avg training loss: 8.5002
batch: [760/21305] batch time: 0.053 trainign loss: 6.9608 avg training loss: 8.4999
batch: [770/21305] batch time: 2.384 trainign loss: 5.9765 avg training loss: 8.4996
batch: [780/21305] batch time: 0.051 trainign loss: 7.6997 avg training loss: 8.4994
batch: [790/21305] batch time: 2.362 trainign loss: 7.8229 avg training loss: 8.4993
batch: [800/21305] batch time: 0.057 trainign loss: 7.3682 avg training loss: 8.4991
batch: [810/21305] batch time: 2.911 trainign loss: 7.0999 avg training loss: 8.4989
batch: [820/21305] batch time: 0.056 trainign loss: 7.3507 avg training loss: 8.4987
batch: [830/21305] batch time: 2.313 trainign loss: 6.9397 avg training loss: 8.4985
batch: [840/21305] batch time: 0.056 trainign loss: 7.4451 avg training loss: 8.4983
batch: [850/21305] batch time: 2.118 trainign loss: 6.5403 avg training loss: 8.4980
batch: [860/21305] batch time: 0.062 trainign loss: 5.9468 avg training loss: 8.4977
batch: [870/21305] batch time: 2.191 trainign loss: 5.6580 avg training loss: 8.4975
batch: [880/21305] batch time: 0.061 trainign loss: 7.8772 avg training loss: 8.4970
batch: [890/21305] batch time: 2.278 trainign loss: 8.6205 avg training loss: 8.4969
batch: [900/21305] batch time: 0.062 trainign loss: 6.9092 avg training loss: 8.4968
batch: [910/21305] batch time: 2.588 trainign loss: 7.4588 avg training loss: 8.4966
batch: [920/21305] batch time: 0.056 trainign loss: 5.3518 avg training loss: 8.4963
batch: [930/21305] batch time: 2.839 trainign loss: 6.9935 avg training loss: 8.4960
batch: [940/21305] batch time: 0.062 trainign loss: 5.1149 avg training loss: 8.4957
batch: [950/21305] batch time: 2.678 trainign loss: 0.0054 avg training loss: 8.4946
batch: [960/21305] batch time: 0.058 trainign loss: 8.0763 avg training loss: 8.4946
batch: [970/21305] batch time: 2.274 trainign loss: 8.5544 avg training loss: 8.4946
batch: [980/21305] batch time: 0.056 trainign loss: 8.0183 avg training loss: 8.4945
batch: [990/21305] batch time: 2.428 trainign loss: 7.3793 avg training loss: 8.4944
batch: [1000/21305] batch time: 0.062 trainign loss: 5.5062 avg training loss: 8.4941
batch: [1010/21305] batch time: 2.423 trainign loss: 7.9744 avg training loss: 8.4938
batch: [1020/21305] batch time: 0.062 trainign loss: 8.3316 avg training loss: 8.4936
batch: [1030/21305] batch time: 2.412 trainign loss: 7.6408 avg training loss: 8.4934
batch: [1040/21305] batch time: 0.062 trainign loss: 7.4180 avg training loss: 8.4933
batch: [1050/21305] batch time: 2.463 trainign loss: 7.1270 avg training loss: 8.4931
batch: [1060/21305] batch time: 0.056 trainign loss: 6.2658 avg training loss: 8.4927
batch: [1070/21305] batch time: 2.848 trainign loss: 5.4997 avg training loss: 8.4924
batch: [1080/21305] batch time: 0.056 trainign loss: 0.2745 avg training loss: 8.4917
batch: [1090/21305] batch time: 2.431 trainign loss: 9.4151 avg training loss: 8.4914
batch: [1100/21305] batch time: 0.056 trainign loss: 7.2146 avg training loss: 8.4912
batch: [1110/21305] batch time: 1.901 trainign loss: 7.6178 avg training loss: 8.4911
batch: [1120/21305] batch time: 0.056 trainign loss: 8.1586 avg training loss: 8.4910
batch: [1130/21305] batch time: 2.468 trainign loss: 6.6618 avg training loss: 8.4909
batch: [1140/21305] batch time: 0.056 trainign loss: 7.7812 avg training loss: 8.4907
batch: [1150/21305] batch time: 2.366 trainign loss: 7.5023 avg training loss: 8.4904
batch: [1160/21305] batch time: 0.056 trainign loss: 7.4535 avg training loss: 8.4902
batch: [1170/21305] batch time: 2.466 trainign loss: 7.0607 avg training loss: 8.4899
batch: [1180/21305] batch time: 0.055 trainign loss: 6.8504 avg training loss: 8.4897
batch: [1190/21305] batch time: 2.606 trainign loss: 7.4472 avg training loss: 8.4894
batch: [1200/21305] batch time: 0.061 trainign loss: 5.6853 avg training loss: 8.4891
batch: [1210/21305] batch time: 2.056 trainign loss: 7.5855 avg training loss: 8.4888
batch: [1220/21305] batch time: 0.057 trainign loss: 8.0745 avg training loss: 8.4887
batch: [1230/21305] batch time: 2.451 trainign loss: 6.7580 avg training loss: 8.4885
batch: [1240/21305] batch time: 0.056 trainign loss: 6.9948 avg training loss: 8.4882
batch: [1250/21305] batch time: 2.229 trainign loss: 7.3602 avg training loss: 8.4879
batch: [1260/21305] batch time: 0.054 trainign loss: 6.4679 avg training loss: 8.4877
batch: [1270/21305] batch time: 2.088 trainign loss: 7.1893 avg training loss: 8.4875
batch: [1280/21305] batch time: 0.056 trainign loss: 7.5349 avg training loss: 8.4873
batch: [1290/21305] batch time: 2.004 trainign loss: 6.8646 avg training loss: 8.4871
batch: [1300/21305] batch time: 0.056 trainign loss: 6.8002 avg training loss: 8.4868
batch: [1310/21305] batch time: 2.490 trainign loss: 7.5115 avg training loss: 8.4866
batch: [1320/21305] batch time: 0.058 trainign loss: 6.1303 avg training loss: 8.4863
batch: [1330/21305] batch time: 1.489 trainign loss: 7.4812 avg training loss: 8.4861
batch: [1340/21305] batch time: 0.056 trainign loss: 7.7046 avg training loss: 8.4859
batch: [1350/21305] batch time: 1.414 trainign loss: 7.5856 avg training loss: 8.4858
batch: [1360/21305] batch time: 0.058 trainign loss: 4.8868 avg training loss: 8.4854
batch: [1370/21305] batch time: 1.341 trainign loss: 8.4379 avg training loss: 8.4851
batch: [1380/21305] batch time: 0.058 trainign loss: 5.9283 avg training loss: 8.4849
batch: [1390/21305] batch time: 0.955 trainign loss: 7.4323 avg training loss: 8.4845
batch: [1400/21305] batch time: 0.059 trainign loss: 6.8665 avg training loss: 8.4843
batch: [1410/21305] batch time: 0.993 trainign loss: 7.2930 avg training loss: 8.4839
batch: [1420/21305] batch time: 0.056 trainign loss: 6.9403 avg training loss: 8.4835
batch: [1430/21305] batch time: 0.618 trainign loss: 6.1286 avg training loss: 8.4833
batch: [1440/21305] batch time: 0.062 trainign loss: 5.8452 avg training loss: 8.4830
batch: [1450/21305] batch time: 1.288 trainign loss: 7.0593 avg training loss: 8.4827
batch: [1460/21305] batch time: 0.062 trainign loss: 6.9790 avg training loss: 8.4826
batch: [1470/21305] batch time: 1.570 trainign loss: 6.2609 avg training loss: 8.4822
batch: [1480/21305] batch time: 0.056 trainign loss: 7.3535 avg training loss: 8.4820
batch: [1490/21305] batch time: 2.199 trainign loss: 7.4713 avg training loss: 8.4817
batch: [1500/21305] batch time: 0.062 trainign loss: 6.9889 avg training loss: 8.4815
batch: [1510/21305] batch time: 2.290 trainign loss: 7.2651 avg training loss: 8.4812
batch: [1520/21305] batch time: 0.062 trainign loss: 3.2213 avg training loss: 8.4807
batch: [1530/21305] batch time: 2.347 trainign loss: 12.4550 avg training loss: 8.4798
batch: [1540/21305] batch time: 0.057 trainign loss: 8.0244 avg training loss: 8.4799
batch: [1550/21305] batch time: 2.299 trainign loss: 5.6132 avg training loss: 8.4798
batch: [1560/21305] batch time: 0.053 trainign loss: 6.9595 avg training loss: 8.4796
batch: [1570/21305] batch time: 2.527 trainign loss: 6.3065 avg training loss: 8.4794
batch: [1580/21305] batch time: 0.062 trainign loss: 6.8966 avg training loss: 8.4791
batch: [1590/21305] batch time: 2.136 trainign loss: 8.6725 avg training loss: 8.4784
batch: [1600/21305] batch time: 0.056 trainign loss: 7.5406 avg training loss: 8.4783
batch: [1610/21305] batch time: 2.337 trainign loss: 7.6890 avg training loss: 8.4783
batch: [1620/21305] batch time: 0.063 trainign loss: 7.0665 avg training loss: 8.4781
batch: [1630/21305] batch time: 1.977 trainign loss: 8.1498 avg training loss: 8.4778
batch: [1640/21305] batch time: 0.056 trainign loss: 7.3355 avg training loss: 8.4776
batch: [1650/21305] batch time: 2.322 trainign loss: 6.3347 avg training loss: 8.4774
batch: [1660/21305] batch time: 0.056 trainign loss: 6.3467 avg training loss: 8.4771
batch: [1670/21305] batch time: 2.191 trainign loss: 6.8317 avg training loss: 8.4769
batch: [1680/21305] batch time: 0.057 trainign loss: 6.3541 avg training loss: 8.4766
batch: [1690/21305] batch time: 2.100 trainign loss: 6.9115 avg training loss: 8.4764
batch: [1700/21305] batch time: 0.056 trainign loss: 6.8931 avg training loss: 8.4762
batch: [1710/21305] batch time: 2.085 trainign loss: 5.2976 avg training loss: 8.4758
batch: [1720/21305] batch time: 0.057 trainign loss: 7.5858 avg training loss: 8.4755
batch: [1730/21305] batch time: 1.758 trainign loss: 6.8021 avg training loss: 8.4753
batch: [1740/21305] batch time: 0.058 trainign loss: 6.3522 avg training loss: 8.4750
batch: [1750/21305] batch time: 1.714 trainign loss: 6.7816 avg training loss: 8.4747
batch: [1760/21305] batch time: 0.061 trainign loss: 7.3463 avg training loss: 8.4745
batch: [1770/21305] batch time: 1.312 trainign loss: 6.4505 avg training loss: 8.4743
batch: [1780/21305] batch time: 0.062 trainign loss: 6.7290 avg training loss: 8.4740
batch: [1790/21305] batch time: 1.851 trainign loss: 6.9338 avg training loss: 8.4737
batch: [1800/21305] batch time: 0.057 trainign loss: 6.1859 avg training loss: 8.4735
batch: [1810/21305] batch time: 1.495 trainign loss: 6.8220 avg training loss: 8.4732
batch: [1820/21305] batch time: 0.063 trainign loss: 4.9535 avg training loss: 8.4728
batch: [1830/21305] batch time: 2.077 trainign loss: 7.6897 avg training loss: 8.4726
batch: [1840/21305] batch time: 0.058 trainign loss: 7.2363 avg training loss: 8.4724
batch: [1850/21305] batch time: 1.987 trainign loss: 6.7710 avg training loss: 8.4722
batch: [1860/21305] batch time: 0.063 trainign loss: 6.4003 avg training loss: 8.4719
batch: [1870/21305] batch time: 1.274 trainign loss: 6.8975 avg training loss: 8.4717
batch: [1880/21305] batch time: 0.058 trainign loss: 5.0520 avg training loss: 8.4714
batch: [1890/21305] batch time: 1.056 trainign loss: 5.1019 avg training loss: 8.4707
batch: [1900/21305] batch time: 0.057 trainign loss: 7.2200 avg training loss: 8.4705
batch: [1910/21305] batch time: 1.452 trainign loss: 7.3804 avg training loss: 8.4703
batch: [1920/21305] batch time: 0.057 trainign loss: 6.6315 avg training loss: 8.4700
batch: [1930/21305] batch time: 1.334 trainign loss: 3.0504 avg training loss: 8.4696
batch: [1940/21305] batch time: 0.449 trainign loss: 6.7150 avg training loss: 8.4694
batch: [1950/21305] batch time: 1.293 trainign loss: 6.6900 avg training loss: 8.4692
batch: [1960/21305] batch time: 0.233 trainign loss: 4.9580 avg training loss: 8.4689
batch: [1970/21305] batch time: 1.704 trainign loss: 7.1974 avg training loss: 8.4686
batch: [1980/21305] batch time: 0.056 trainign loss: 5.1418 avg training loss: 8.4683
batch: [1990/21305] batch time: 2.105 trainign loss: 6.5828 avg training loss: 8.4680
batch: [2000/21305] batch time: 0.347 trainign loss: 5.8587 avg training loss: 8.4677
batch: [2010/21305] batch time: 0.995 trainign loss: 7.4545 avg training loss: 8.4674
batch: [2020/21305] batch time: 1.164 trainign loss: 4.3050 avg training loss: 8.4671
batch: [2030/21305] batch time: 1.386 trainign loss: 6.4370 avg training loss: 8.4666
batch: [2040/21305] batch time: 1.176 trainign loss: 7.1996 avg training loss: 8.4665
batch: [2050/21305] batch time: 1.262 trainign loss: 6.2034 avg training loss: 8.4663
batch: [2060/21305] batch time: 0.751 trainign loss: 3.0857 avg training loss: 8.4659
batch: [2070/21305] batch time: 1.888 trainign loss: 1.3990 avg training loss: 8.4653
batch: [2080/21305] batch time: 0.232 trainign loss: 0.0350 avg training loss: 8.4644
batch: [2090/21305] batch time: 2.169 trainign loss: 9.1272 avg training loss: 8.4639
batch: [2100/21305] batch time: 0.057 trainign loss: 8.5832 avg training loss: 8.4640
batch: [2110/21305] batch time: 1.908 trainign loss: 7.0279 avg training loss: 8.4639
batch: [2120/21305] batch time: 0.058 trainign loss: 0.5187 avg training loss: 8.4632
batch: [2130/21305] batch time: 1.647 trainign loss: 0.0003 avg training loss: 8.4620
batch: [2140/21305] batch time: 0.060 trainign loss: 0.0000 avg training loss: 8.4607
batch: [2150/21305] batch time: 2.096 trainign loss: 8.5432 avg training loss: 8.4610
batch: [2160/21305] batch time: 0.063 trainign loss: 8.7918 avg training loss: 8.4610
batch: [2170/21305] batch time: 2.246 trainign loss: 8.3876 avg training loss: 8.4610
batch: [2180/21305] batch time: 0.062 trainign loss: 7.4620 avg training loss: 8.4608
batch: [2190/21305] batch time: 2.158 trainign loss: 6.1829 avg training loss: 8.4606
batch: [2200/21305] batch time: 0.057 trainign loss: 7.0642 avg training loss: 8.4603
batch: [2210/21305] batch time: 2.315 trainign loss: 6.1881 avg training loss: 8.4600
batch: [2220/21305] batch time: 0.161 trainign loss: 7.1601 avg training loss: 8.4598
batch: [2230/21305] batch time: 2.397 trainign loss: 7.0683 avg training loss: 8.4595
batch: [2240/21305] batch time: 0.063 trainign loss: 7.0352 avg training loss: 8.4592
batch: [2250/21305] batch time: 2.502 trainign loss: 6.8995 avg training loss: 8.4590
batch: [2260/21305] batch time: 0.056 trainign loss: 6.1633 avg training loss: 8.4586
batch: [2270/21305] batch time: 2.598 trainign loss: 4.5890 avg training loss: 8.4581
batch: [2280/21305] batch time: 0.056 trainign loss: 6.0948 avg training loss: 8.4576
batch: [2290/21305] batch time: 2.260 trainign loss: 4.0719 avg training loss: 8.4570
batch: [2300/21305] batch time: 0.056 trainign loss: 5.8889 avg training loss: 8.4569
batch: [2310/21305] batch time: 2.179 trainign loss: 7.4153 avg training loss: 8.4566
batch: [2320/21305] batch time: 0.056 trainign loss: 8.2391 avg training loss: 8.4564
batch: [2330/21305] batch time: 2.045 trainign loss: 7.8864 avg training loss: 8.4562
batch: [2340/21305] batch time: 0.463 trainign loss: 5.5949 avg training loss: 8.4560
batch: [2350/21305] batch time: 2.343 trainign loss: 5.7473 avg training loss: 8.4557
batch: [2360/21305] batch time: 0.062 trainign loss: 7.5547 avg training loss: 8.4555
batch: [2370/21305] batch time: 1.660 trainign loss: 5.0951 avg training loss: 8.4551
batch: [2380/21305] batch time: 0.505 trainign loss: 6.7309 avg training loss: 8.4548
batch: [2390/21305] batch time: 1.273 trainign loss: 6.7977 avg training loss: 8.4547
batch: [2400/21305] batch time: 0.061 trainign loss: 6.1975 avg training loss: 8.4543
batch: [2410/21305] batch time: 1.302 trainign loss: 7.9870 avg training loss: 8.4541
batch: [2420/21305] batch time: 0.062 trainign loss: 7.0418 avg training loss: 8.4540
batch: [2430/21305] batch time: 2.097 trainign loss: 5.6502 avg training loss: 8.4537
batch: [2440/21305] batch time: 0.056 trainign loss: 7.3171 avg training loss: 8.4535
batch: [2450/21305] batch time: 2.355 trainign loss: 7.3016 avg training loss: 8.4532
batch: [2460/21305] batch time: 0.056 trainign loss: 6.7518 avg training loss: 8.4529
batch: [2470/21305] batch time: 2.251 trainign loss: 7.1764 avg training loss: 8.4526
batch: [2480/21305] batch time: 0.053 trainign loss: 7.4794 avg training loss: 8.4524
batch: [2490/21305] batch time: 2.311 trainign loss: 6.9681 avg training loss: 8.4520
batch: [2500/21305] batch time: 0.056 trainign loss: 7.8537 avg training loss: 8.4518
batch: [2510/21305] batch time: 2.344 trainign loss: 5.7871 avg training loss: 8.4516
batch: [2520/21305] batch time: 0.056 trainign loss: 5.5350 avg training loss: 8.4514
batch: [2530/21305] batch time: 2.082 trainign loss: 5.6206 avg training loss: 8.4510
batch: [2540/21305] batch time: 0.061 trainign loss: 6.9631 avg training loss: 8.4508
batch: [2550/21305] batch time: 1.822 trainign loss: 6.0536 avg training loss: 8.4505
batch: [2560/21305] batch time: 0.378 trainign loss: 6.5410 avg training loss: 8.4503
batch: [2570/21305] batch time: 1.646 trainign loss: 7.3431 avg training loss: 8.4501
batch: [2580/21305] batch time: 0.710 trainign loss: 5.6056 avg training loss: 8.4497
batch: [2590/21305] batch time: 2.026 trainign loss: 6.3681 avg training loss: 8.4495
batch: [2600/21305] batch time: 0.523 trainign loss: 0.1115 avg training loss: 8.4486
batch: [2610/21305] batch time: 1.750 trainign loss: 8.6399 avg training loss: 8.4484
batch: [2620/21305] batch time: 0.595 trainign loss: 8.7588 avg training loss: 8.4485
batch: [2630/21305] batch time: 1.908 trainign loss: 7.3595 avg training loss: 8.4484
batch: [2640/21305] batch time: 0.464 trainign loss: 4.5534 avg training loss: 8.4481
batch: [2650/21305] batch time: 2.090 trainign loss: 8.3042 avg training loss: 8.4478
batch: [2660/21305] batch time: 0.063 trainign loss: 7.6564 avg training loss: 8.4476
batch: [2670/21305] batch time: 2.482 trainign loss: 6.8407 avg training loss: 8.4473
batch: [2680/21305] batch time: 0.056 trainign loss: 5.5851 avg training loss: 8.4470
batch: [2690/21305] batch time: 2.042 trainign loss: 7.7972 avg training loss: 8.4468
batch: [2700/21305] batch time: 0.056 trainign loss: 7.3657 avg training loss: 8.4465
batch: [2710/21305] batch time: 1.927 trainign loss: 5.6037 avg training loss: 8.4461
batch: [2720/21305] batch time: 1.168 trainign loss: 2.4272 avg training loss: 8.4456
batch: [2730/21305] batch time: 0.996 trainign loss: 7.7697 avg training loss: 8.4455
batch: [2740/21305] batch time: 0.955 trainign loss: 8.0605 avg training loss: 8.4453
batch: [2750/21305] batch time: 1.000 trainign loss: 7.5113 avg training loss: 8.4450
batch: [2760/21305] batch time: 1.366 trainign loss: 7.3263 avg training loss: 8.4448
batch: [2770/21305] batch time: 1.089 trainign loss: 7.2952 avg training loss: 8.4446
batch: [2780/21305] batch time: 1.712 trainign loss: 7.7073 avg training loss: 8.4444
batch: [2790/21305] batch time: 0.859 trainign loss: 7.0226 avg training loss: 8.4442
batch: [2800/21305] batch time: 1.661 trainign loss: 7.0972 avg training loss: 8.4440
batch: [2810/21305] batch time: 1.954 trainign loss: 6.4555 avg training loss: 8.4436
batch: [2820/21305] batch time: 0.056 trainign loss: 7.3734 avg training loss: 8.4432
batch: [2830/21305] batch time: 2.715 trainign loss: 7.5888 avg training loss: 8.4429
batch: [2840/21305] batch time: 0.059 trainign loss: 1.6065 avg training loss: 8.4425
batch: [2850/21305] batch time: 2.413 trainign loss: 6.0956 avg training loss: 8.4422
batch: [2860/21305] batch time: 0.445 trainign loss: 7.4866 avg training loss: 8.4420
batch: [2870/21305] batch time: 1.472 trainign loss: 7.9875 avg training loss: 8.4417
batch: [2880/21305] batch time: 0.940 trainign loss: 6.1921 avg training loss: 8.4414
batch: [2890/21305] batch time: 1.453 trainign loss: 7.3175 avg training loss: 8.4411
batch: [2900/21305] batch time: 0.572 trainign loss: 7.2219 avg training loss: 8.4409
batch: [2910/21305] batch time: 1.318 trainign loss: 7.5556 avg training loss: 8.4407
batch: [2920/21305] batch time: 0.860 trainign loss: 6.7408 avg training loss: 8.4406
batch: [2930/21305] batch time: 0.874 trainign loss: 5.8520 avg training loss: 8.4403
batch: [2940/21305] batch time: 1.649 trainign loss: 5.7977 avg training loss: 8.4400
batch: [2950/21305] batch time: 0.800 trainign loss: 0.9638 avg training loss: 8.4393
batch: [2960/21305] batch time: 0.991 trainign loss: 6.9586 avg training loss: 8.4392
batch: [2970/21305] batch time: 0.147 trainign loss: 7.0875 avg training loss: 8.4390
batch: [2980/21305] batch time: 0.061 trainign loss: 7.5696 avg training loss: 8.4389
batch: [2990/21305] batch time: 0.056 trainign loss: 7.0211 avg training loss: 8.4387
batch: [3000/21305] batch time: 0.062 trainign loss: 5.4226 avg training loss: 8.4385
batch: [3010/21305] batch time: 0.442 trainign loss: 8.1565 avg training loss: 8.4381
batch: [3020/21305] batch time: 0.199 trainign loss: 5.3533 avg training loss: 8.4378
batch: [3030/21305] batch time: 1.296 trainign loss: 6.4244 avg training loss: 8.4376
batch: [3040/21305] batch time: 0.470 trainign loss: 6.9837 avg training loss: 8.4374
batch: [3050/21305] batch time: 1.587 trainign loss: 7.4925 avg training loss: 8.4372
batch: [3060/21305] batch time: 0.502 trainign loss: 6.8058 avg training loss: 8.4370
batch: [3070/21305] batch time: 2.085 trainign loss: 6.7588 avg training loss: 8.4367
batch: [3080/21305] batch time: 0.056 trainign loss: 7.2856 avg training loss: 8.4364
batch: [3090/21305] batch time: 2.070 trainign loss: 6.8084 avg training loss: 8.4362
batch: [3100/21305] batch time: 0.074 trainign loss: 7.3395 avg training loss: 8.4359
batch: [3110/21305] batch time: 2.281 trainign loss: 8.0943 avg training loss: 8.4358
batch: [3120/21305] batch time: 0.535 trainign loss: 6.4293 avg training loss: 8.4355
batch: [3130/21305] batch time: 2.343 trainign loss: 0.8335 avg training loss: 8.4349
batch: [3140/21305] batch time: 0.057 trainign loss: 8.0360 avg training loss: 8.4348
batch: [3150/21305] batch time: 2.450 trainign loss: 7.0208 avg training loss: 8.4346
batch: [3160/21305] batch time: 0.056 trainign loss: 7.5296 avg training loss: 8.4344
batch: [3170/21305] batch time: 2.180 trainign loss: 6.9786 avg training loss: 8.4342
batch: [3180/21305] batch time: 0.226 trainign loss: 6.4440 avg training loss: 8.4340
batch: [3190/21305] batch time: 2.599 trainign loss: 6.8127 avg training loss: 8.4337
batch: [3200/21305] batch time: 0.056 trainign loss: 7.4413 avg training loss: 8.4335
batch: [3210/21305] batch time: 2.487 trainign loss: 7.3622 avg training loss: 8.4333
batch: [3220/21305] batch time: 0.063 trainign loss: 7.2733 avg training loss: 8.4331
batch: [3230/21305] batch time: 2.233 trainign loss: 7.4876 avg training loss: 8.4329
batch: [3240/21305] batch time: 0.056 trainign loss: 6.2991 avg training loss: 8.4327
batch: [3250/21305] batch time: 2.625 trainign loss: 6.8452 avg training loss: 8.4324
batch: [3260/21305] batch time: 0.053 trainign loss: 7.7022 avg training loss: 8.4322
batch: [3270/21305] batch time: 2.284 trainign loss: 6.3136 avg training loss: 8.4320
batch: [3280/21305] batch time: 0.056 trainign loss: 3.5373 avg training loss: 8.4316
batch: [3290/21305] batch time: 2.277 trainign loss: 6.4952 avg training loss: 8.4311
batch: [3300/21305] batch time: 0.056 trainign loss: 6.0477 avg training loss: 8.4308
batch: [3310/21305] batch time: 2.154 trainign loss: 1.1250 avg training loss: 8.4302
batch: [3320/21305] batch time: 0.056 trainign loss: 7.6964 avg training loss: 8.4301
batch: [3330/21305] batch time: 2.050 trainign loss: 7.2307 avg training loss: 8.4300
batch: [3340/21305] batch time: 0.553 trainign loss: 4.6434 avg training loss: 8.4298
batch: [3350/21305] batch time: 1.798 trainign loss: 5.6346 avg training loss: 8.4295
batch: [3360/21305] batch time: 0.656 trainign loss: 7.9029 avg training loss: 8.4290
batch: [3370/21305] batch time: 2.106 trainign loss: 4.1798 avg training loss: 8.4286
batch: [3380/21305] batch time: 0.062 trainign loss: 8.4152 avg training loss: 8.4284
batch: [3390/21305] batch time: 2.285 trainign loss: 7.5580 avg training loss: 8.4282
batch: [3400/21305] batch time: 0.568 trainign loss: 6.6442 avg training loss: 8.4281
batch: [3410/21305] batch time: 1.005 trainign loss: 6.1499 avg training loss: 8.4278
batch: [3420/21305] batch time: 0.946 trainign loss: 7.4083 avg training loss: 8.4275
batch: [3430/21305] batch time: 2.057 trainign loss: 7.0300 avg training loss: 8.4273
batch: [3440/21305] batch time: 0.060 trainign loss: 7.6363 avg training loss: 8.4272
batch: [3450/21305] batch time: 2.296 trainign loss: 6.3244 avg training loss: 8.4269
batch: [3460/21305] batch time: 0.056 trainign loss: 6.3047 avg training loss: 8.4266
batch: [3470/21305] batch time: 2.413 trainign loss: 7.4879 avg training loss: 8.4263
batch: [3480/21305] batch time: 0.062 trainign loss: 7.9384 avg training loss: 8.4262
batch: [3490/21305] batch time: 2.171 trainign loss: 7.3502 avg training loss: 8.4260
batch: [3500/21305] batch time: 0.061 trainign loss: 6.8795 avg training loss: 8.4258
batch: [3510/21305] batch time: 1.160 trainign loss: 7.2079 avg training loss: 8.4256
batch: [3520/21305] batch time: 0.063 trainign loss: 6.7565 avg training loss: 8.4254
batch: [3530/21305] batch time: 1.369 trainign loss: 7.2901 avg training loss: 8.4252
batch: [3540/21305] batch time: 0.056 trainign loss: 5.9569 avg training loss: 8.4249
batch: [3550/21305] batch time: 2.398 trainign loss: 7.4590 avg training loss: 8.4245
batch: [3560/21305] batch time: 0.063 trainign loss: 7.1894 avg training loss: 8.4241
batch: [3570/21305] batch time: 2.575 trainign loss: 7.8280 avg training loss: 8.4240
batch: [3580/21305] batch time: 0.056 trainign loss: 6.5995 avg training loss: 8.4238
batch: [3590/21305] batch time: 2.609 trainign loss: 4.8982 avg training loss: 8.4236
batch: [3600/21305] batch time: 0.056 trainign loss: 8.1382 avg training loss: 8.4232
batch: [3610/21305] batch time: 2.263 trainign loss: 8.2391 avg training loss: 8.4230
batch: [3620/21305] batch time: 0.063 trainign loss: 7.5899 avg training loss: 8.4229
batch: [3630/21305] batch time: 2.167 trainign loss: 7.3466 avg training loss: 8.4227
batch: [3640/21305] batch time: 0.057 trainign loss: 7.2019 avg training loss: 8.4225
batch: [3650/21305] batch time: 2.577 trainign loss: 6.4267 avg training loss: 8.4222
batch: [3660/21305] batch time: 0.053 trainign loss: 6.2633 avg training loss: 8.4220
batch: [3670/21305] batch time: 2.277 trainign loss: 5.8309 avg training loss: 8.4217
batch: [3680/21305] batch time: 0.056 trainign loss: 7.5960 avg training loss: 8.4215
batch: [3690/21305] batch time: 2.719 trainign loss: 7.0534 avg training loss: 8.4213
batch: [3700/21305] batch time: 0.062 trainign loss: 6.0632 avg training loss: 8.4211
batch: [3710/21305] batch time: 2.470 trainign loss: 5.9303 avg training loss: 8.4209
batch: [3720/21305] batch time: 0.059 trainign loss: 6.7686 avg training loss: 8.4206
batch: [3730/21305] batch time: 2.288 trainign loss: 6.3062 avg training loss: 8.4204
batch: [3740/21305] batch time: 0.057 trainign loss: 7.6829 avg training loss: 8.4202
batch: [3750/21305] batch time: 2.139 trainign loss: 7.4275 avg training loss: 8.4200
batch: [3760/21305] batch time: 0.062 trainign loss: 6.7517 avg training loss: 8.4197
batch: [3770/21305] batch time: 2.287 trainign loss: 4.9620 avg training loss: 8.4193
batch: [3780/21305] batch time: 0.063 trainign loss: 6.8428 avg training loss: 8.4191
batch: [3790/21305] batch time: 2.451 trainign loss: 5.1429 avg training loss: 8.4188
batch: [3800/21305] batch time: 0.056 trainign loss: 6.5424 avg training loss: 8.4186
batch: [3810/21305] batch time: 2.365 trainign loss: 7.6643 avg training loss: 8.4185
batch: [3820/21305] batch time: 0.053 trainign loss: 5.2172 avg training loss: 8.4182
batch: [3830/21305] batch time: 2.345 trainign loss: 6.7345 avg training loss: 8.4180
batch: [3840/21305] batch time: 0.056 trainign loss: 6.8236 avg training loss: 8.4178
batch: [3850/21305] batch time: 2.178 trainign loss: 6.6567 avg training loss: 8.4175
batch: [3860/21305] batch time: 0.063 trainign loss: 6.7256 avg training loss: 8.4173
batch: [3870/21305] batch time: 2.182 trainign loss: 5.5935 avg training loss: 8.4170
batch: [3880/21305] batch time: 0.057 trainign loss: 0.2039 avg training loss: 8.4162
batch: [3890/21305] batch time: 2.037 trainign loss: 9.3033 avg training loss: 8.4158
batch: [3900/21305] batch time: 0.056 trainign loss: 8.7278 avg training loss: 8.4158
batch: [3910/21305] batch time: 2.145 trainign loss: 8.0348 avg training loss: 8.4157
batch: [3920/21305] batch time: 0.058 trainign loss: 5.5896 avg training loss: 8.4155
batch: [3930/21305] batch time: 2.062 trainign loss: 7.2356 avg training loss: 8.4153
batch: [3940/21305] batch time: 0.063 trainign loss: 6.3117 avg training loss: 8.4151
batch: [3950/21305] batch time: 1.403 trainign loss: 6.7906 avg training loss: 8.4149
batch: [3960/21305] batch time: 0.063 trainign loss: 8.1484 avg training loss: 8.4147
batch: [3970/21305] batch time: 1.376 trainign loss: 7.4750 avg training loss: 8.4145
batch: [3980/21305] batch time: 0.057 trainign loss: 6.2903 avg training loss: 8.4143
batch: [3990/21305] batch time: 0.602 trainign loss: 5.9129 avg training loss: 8.4139
batch: [4000/21305] batch time: 0.056 trainign loss: 7.5226 avg training loss: 8.4136
batch: [4010/21305] batch time: 2.151 trainign loss: 7.1769 avg training loss: 8.4134
batch: [4020/21305] batch time: 0.056 trainign loss: 4.6501 avg training loss: 8.4131
batch: [4030/21305] batch time: 2.205 trainign loss: 7.3022 avg training loss: 8.4129
batch: [4040/21305] batch time: 0.062 trainign loss: 7.1389 avg training loss: 8.4127
batch: [4050/21305] batch time: 1.547 trainign loss: 4.7145 avg training loss: 8.4124
batch: [4060/21305] batch time: 0.056 trainign loss: 11.0532 avg training loss: 8.4115
batch: [4070/21305] batch time: 1.752 trainign loss: 9.0745 avg training loss: 8.4112
batch: [4080/21305] batch time: 0.056 trainign loss: 8.4920 avg training loss: 8.4113
batch: [4090/21305] batch time: 1.848 trainign loss: 8.1227 avg training loss: 8.4111
batch: [4100/21305] batch time: 0.058 trainign loss: 6.4156 avg training loss: 8.4109
batch: [4110/21305] batch time: 1.259 trainign loss: 5.5298 avg training loss: 8.4107
batch: [4120/21305] batch time: 0.061 trainign loss: 5.6279 avg training loss: 8.4103
batch: [4130/21305] batch time: 2.257 trainign loss: 6.4087 avg training loss: 8.4100
batch: [4140/21305] batch time: 0.062 trainign loss: 6.1106 avg training loss: 8.4096
batch: [4150/21305] batch time: 1.858 trainign loss: 8.0605 avg training loss: 8.4094
batch: [4160/21305] batch time: 0.063 trainign loss: 7.2147 avg training loss: 8.4092
batch: [4170/21305] batch time: 1.543 trainign loss: 6.1091 avg training loss: 8.4088
batch: [4180/21305] batch time: 0.059 trainign loss: 7.4080 avg training loss: 8.4086
batch: [4190/21305] batch time: 1.399 trainign loss: 7.3029 avg training loss: 8.4083
batch: [4200/21305] batch time: 0.057 trainign loss: 6.5705 avg training loss: 8.4081
batch: [4210/21305] batch time: 1.061 trainign loss: 1.4341 avg training loss: 8.4075
batch: [4220/21305] batch time: 0.058 trainign loss: 6.9997 avg training loss: 8.4073
batch: [4230/21305] batch time: 0.138 trainign loss: 6.9873 avg training loss: 8.4069
batch: [4240/21305] batch time: 0.056 trainign loss: 5.4972 avg training loss: 8.4066
batch: [4250/21305] batch time: 0.576 trainign loss: 0.5461 avg training loss: 8.4057
batch: [4260/21305] batch time: 0.056 trainign loss: 8.0996 avg training loss: 8.4057
batch: [4270/21305] batch time: 0.458 trainign loss: 9.1286 avg training loss: 8.4057
batch: [4280/21305] batch time: 0.061 trainign loss: 7.8796 avg training loss: 8.4057
batch: [4290/21305] batch time: 0.325 trainign loss: 7.3164 avg training loss: 8.4055
batch: [4300/21305] batch time: 0.062 trainign loss: 6.7459 avg training loss: 8.4054
batch: [4310/21305] batch time: 0.268 trainign loss: 7.4986 avg training loss: 8.4051
batch: [4320/21305] batch time: 0.056 trainign loss: 6.1834 avg training loss: 8.4049
batch: [4330/21305] batch time: 0.052 trainign loss: 4.6842 avg training loss: 8.4046
batch: [4340/21305] batch time: 0.056 trainign loss: 5.7560 avg training loss: 8.4042
batch: [4350/21305] batch time: 0.052 trainign loss: 6.2198 avg training loss: 8.4040
batch: [4360/21305] batch time: 0.057 trainign loss: 4.9597 avg training loss: 8.4036
batch: [4370/21305] batch time: 0.056 trainign loss: 8.2508 avg training loss: 8.4032
batch: [4380/21305] batch time: 0.062 trainign loss: 6.7140 avg training loss: 8.4031
batch: [4390/21305] batch time: 0.051 trainign loss: 6.5688 avg training loss: 8.4027
batch: [4400/21305] batch time: 0.063 trainign loss: 7.9956 avg training loss: 8.4027
batch: [4410/21305] batch time: 0.057 trainign loss: 7.1508 avg training loss: 8.4025
batch: [4420/21305] batch time: 0.062 trainign loss: 5.9770 avg training loss: 8.4023
batch: [4430/21305] batch time: 0.062 trainign loss: 1.5218 avg training loss: 8.4017
batch: [4440/21305] batch time: 0.696 trainign loss: 10.5254 avg training loss: 8.4012
batch: [4450/21305] batch time: 0.057 trainign loss: 7.3289 avg training loss: 8.4011
batch: [4460/21305] batch time: 0.476 trainign loss: 3.8998 avg training loss: 8.4007
batch: [4470/21305] batch time: 0.062 trainign loss: 10.0041 avg training loss: 8.4002
batch: [4480/21305] batch time: 0.057 trainign loss: 7.6728 avg training loss: 8.4001
batch: [4490/21305] batch time: 0.051 trainign loss: 4.7628 avg training loss: 8.3999
batch: [4500/21305] batch time: 0.056 trainign loss: 8.3218 avg training loss: 8.3997
batch: [4510/21305] batch time: 0.060 trainign loss: 7.3862 avg training loss: 8.3993
batch: [4520/21305] batch time: 0.051 trainign loss: 8.1543 avg training loss: 8.3992
batch: [4530/21305] batch time: 0.061 trainign loss: 7.9918 avg training loss: 8.3991
batch: [4540/21305] batch time: 0.056 trainign loss: 6.9642 avg training loss: 8.3989
batch: [4550/21305] batch time: 0.063 trainign loss: 6.7727 avg training loss: 8.3986
batch: [4560/21305] batch time: 0.054 trainign loss: 7.8591 avg training loss: 8.3984
batch: [4570/21305] batch time: 0.057 trainign loss: 7.3166 avg training loss: 8.3982
batch: [4580/21305] batch time: 0.056 trainign loss: 7.2765 avg training loss: 8.3981
batch: [4590/21305] batch time: 0.056 trainign loss: 5.7565 avg training loss: 8.3978
batch: [4600/21305] batch time: 0.052 trainign loss: 6.5943 avg training loss: 8.3976
batch: [4610/21305] batch time: 0.058 trainign loss: 6.5346 avg training loss: 8.3973
batch: [4620/21305] batch time: 0.056 trainign loss: 6.8701 avg training loss: 8.3970
batch: [4630/21305] batch time: 0.060 trainign loss: 6.2752 avg training loss: 8.3967
batch: [4640/21305] batch time: 0.062 trainign loss: 5.6827 avg training loss: 8.3965
batch: [4650/21305] batch time: 0.062 trainign loss: 6.4863 avg training loss: 8.3963
batch: [4660/21305] batch time: 0.056 trainign loss: 7.5421 avg training loss: 8.3961
batch: [4670/21305] batch time: 0.058 trainign loss: 7.0672 avg training loss: 8.3957
batch: [4680/21305] batch time: 0.055 trainign loss: 7.7522 avg training loss: 8.3956
batch: [4690/21305] batch time: 0.051 trainign loss: 5.6510 avg training loss: 8.3953
batch: [4700/21305] batch time: 0.062 trainign loss: 6.9775 avg training loss: 8.3950
batch: [4710/21305] batch time: 0.149 trainign loss: 7.7214 avg training loss: 8.3948
batch: [4720/21305] batch time: 0.062 trainign loss: 6.9538 avg training loss: 8.3947
batch: [4730/21305] batch time: 0.061 trainign loss: 6.6584 avg training loss: 8.3944
batch: [4740/21305] batch time: 0.062 trainign loss: 7.9890 avg training loss: 8.3941
batch: [4750/21305] batch time: 0.134 trainign loss: 7.1418 avg training loss: 8.3939
batch: [4760/21305] batch time: 0.056 trainign loss: 4.5121 avg training loss: 8.3936
batch: [4770/21305] batch time: 0.290 trainign loss: 6.8094 avg training loss: 8.3933
batch: [4780/21305] batch time: 0.062 trainign loss: 3.4150 avg training loss: 8.3929
batch: [4790/21305] batch time: 0.174 trainign loss: 6.5483 avg training loss: 8.3925
batch: [4800/21305] batch time: 0.058 trainign loss: 7.8660 avg training loss: 8.3923
batch: [4810/21305] batch time: 0.109 trainign loss: 5.8883 avg training loss: 8.3922
batch: [4820/21305] batch time: 0.057 trainign loss: 7.1971 avg training loss: 8.3920
batch: [4830/21305] batch time: 0.498 trainign loss: 6.4810 avg training loss: 8.3917
batch: [4840/21305] batch time: 0.057 trainign loss: 1.2634 avg training loss: 8.3912
batch: [4850/21305] batch time: 0.495 trainign loss: 10.8513 avg training loss: 8.3906
batch: [4860/21305] batch time: 0.057 trainign loss: 7.1693 avg training loss: 8.3905
batch: [4870/21305] batch time: 0.063 trainign loss: 8.7132 avg training loss: 8.3905
batch: [4880/21305] batch time: 0.056 trainign loss: 7.6088 avg training loss: 8.3904
batch: [4890/21305] batch time: 0.063 trainign loss: 6.9068 avg training loss: 8.3902
batch: [4900/21305] batch time: 0.590 trainign loss: 6.5029 avg training loss: 8.3898
batch: [4910/21305] batch time: 0.059 trainign loss: 5.2569 avg training loss: 8.3894
batch: [4920/21305] batch time: 0.876 trainign loss: 8.3255 avg training loss: 8.3894
batch: [4930/21305] batch time: 0.057 trainign loss: 5.8158 avg training loss: 8.3891
batch: [4940/21305] batch time: 0.626 trainign loss: 7.1528 avg training loss: 8.3887
batch: [4950/21305] batch time: 0.061 trainign loss: 5.6641 avg training loss: 8.3884
batch: [4960/21305] batch time: 0.698 trainign loss: 7.1236 avg training loss: 8.3881
batch: [4970/21305] batch time: 0.057 trainign loss: 6.4731 avg training loss: 8.3878
batch: [4980/21305] batch time: 0.819 trainign loss: 4.7141 avg training loss: 8.3875
batch: [4990/21305] batch time: 0.056 trainign loss: 0.0072 avg training loss: 8.3865
batch: [5000/21305] batch time: 0.408 trainign loss: 8.1003 avg training loss: 8.3863
batch: [5010/21305] batch time: 0.060 trainign loss: 8.4940 avg training loss: 8.3864
batch: [5020/21305] batch time: 0.967 trainign loss: 6.9958 avg training loss: 8.3862
batch: [5030/21305] batch time: 0.056 trainign loss: 6.2338 avg training loss: 8.3859
batch: [5040/21305] batch time: 0.613 trainign loss: 4.1626 avg training loss: 8.3856
batch: [5050/21305] batch time: 0.057 trainign loss: 7.0906 avg training loss: 8.3851
batch: [5060/21305] batch time: 0.551 trainign loss: 8.0880 avg training loss: 8.3850
batch: [5070/21305] batch time: 0.062 trainign loss: 5.3127 avg training loss: 8.3848
batch: [5080/21305] batch time: 0.899 trainign loss: 0.8163 avg training loss: 8.3838
batch: [5090/21305] batch time: 0.062 trainign loss: 7.2311 avg training loss: 8.3840
batch: [5100/21305] batch time: 1.918 trainign loss: 8.1407 avg training loss: 8.3839
batch: [5110/21305] batch time: 0.685 trainign loss: 7.0500 avg training loss: 8.3838
batch: [5120/21305] batch time: 1.339 trainign loss: 7.1394 avg training loss: 8.3836
batch: [5130/21305] batch time: 1.045 trainign loss: 7.0905 avg training loss: 8.3834
batch: [5140/21305] batch time: 0.672 trainign loss: 7.0657 avg training loss: 8.3830
batch: [5150/21305] batch time: 1.660 trainign loss: 7.7495 avg training loss: 8.3828
batch: [5160/21305] batch time: 0.253 trainign loss: 7.9632 avg training loss: 8.3828
batch: [5170/21305] batch time: 1.616 trainign loss: 7.4475 avg training loss: 8.3826
batch: [5180/21305] batch time: 0.370 trainign loss: 6.7692 avg training loss: 8.3824
batch: [5190/21305] batch time: 1.301 trainign loss: 6.9791 avg training loss: 8.3822
batch: [5200/21305] batch time: 0.778 trainign loss: 6.4692 avg training loss: 8.3820
batch: [5210/21305] batch time: 0.869 trainign loss: 6.4176 avg training loss: 8.3818
batch: [5220/21305] batch time: 1.508 trainign loss: 6.8879 avg training loss: 8.3815
batch: [5230/21305] batch time: 0.056 trainign loss: 6.6051 avg training loss: 8.3813
batch: [5240/21305] batch time: 1.702 trainign loss: 5.8033 avg training loss: 8.3811
batch: [5250/21305] batch time: 0.061 trainign loss: 6.2249 avg training loss: 8.3807
batch: [5260/21305] batch time: 1.195 trainign loss: 7.2987 avg training loss: 8.3806
batch: [5270/21305] batch time: 0.179 trainign loss: 7.2894 avg training loss: 8.3804
batch: [5280/21305] batch time: 0.480 trainign loss: 7.3823 avg training loss: 8.3803
batch: [5290/21305] batch time: 0.062 trainign loss: 5.9657 avg training loss: 8.3800
batch: [5300/21305] batch time: 0.210 trainign loss: 7.0864 avg training loss: 8.3797
batch: [5310/21305] batch time: 0.056 trainign loss: 7.3024 avg training loss: 8.3795
batch: [5320/21305] batch time: 0.892 trainign loss: 7.2965 avg training loss: 8.3793
batch: [5330/21305] batch time: 0.056 trainign loss: 6.8754 avg training loss: 8.3791
batch: [5340/21305] batch time: 1.548 trainign loss: 6.5465 avg training loss: 8.3788
batch: [5350/21305] batch time: 0.056 trainign loss: 6.0279 avg training loss: 8.3784
batch: [5360/21305] batch time: 1.323 trainign loss: 7.9399 avg training loss: 8.3781
batch: [5370/21305] batch time: 0.053 trainign loss: 7.9388 avg training loss: 8.3779
batch: [5380/21305] batch time: 2.247 trainign loss: 7.6637 avg training loss: 8.3778
batch: [5390/21305] batch time: 0.056 trainign loss: 5.7784 avg training loss: 8.3776
batch: [5400/21305] batch time: 2.354 trainign loss: 7.3965 avg training loss: 8.3774
batch: [5410/21305] batch time: 0.056 trainign loss: 7.0831 avg training loss: 8.3772
batch: [5420/21305] batch time: 2.162 trainign loss: 7.7660 avg training loss: 8.3769
batch: [5430/21305] batch time: 0.133 trainign loss: 6.6996 avg training loss: 8.3767
batch: [5440/21305] batch time: 1.794 trainign loss: 6.7478 avg training loss: 8.3764
batch: [5450/21305] batch time: 0.056 trainign loss: 5.7000 avg training loss: 8.3760
batch: [5460/21305] batch time: 2.258 trainign loss: 7.9269 avg training loss: 8.3758
batch: [5470/21305] batch time: 0.174 trainign loss: 5.9571 avg training loss: 8.3756
batch: [5480/21305] batch time: 2.087 trainign loss: 7.9662 avg training loss: 8.3754
batch: [5490/21305] batch time: 0.061 trainign loss: 6.1398 avg training loss: 8.3752
batch: [5500/21305] batch time: 1.426 trainign loss: 5.9730 avg training loss: 8.3750
batch: [5510/21305] batch time: 0.056 trainign loss: 2.4268 avg training loss: 8.3745
batch: [5520/21305] batch time: 1.258 trainign loss: 7.4641 avg training loss: 8.3742
batch: [5530/21305] batch time: 0.238 trainign loss: 7.2400 avg training loss: 8.3740
batch: [5540/21305] batch time: 2.152 trainign loss: 7.2203 avg training loss: 8.3737
batch: [5550/21305] batch time: 0.060 trainign loss: 8.1145 avg training loss: 8.3736
batch: [5560/21305] batch time: 1.149 trainign loss: 5.5587 avg training loss: 8.3732
batch: [5570/21305] batch time: 0.056 trainign loss: 3.0107 avg training loss: 8.3729
batch: [5580/21305] batch time: 0.544 trainign loss: 9.1534 avg training loss: 8.3725
batch: [5590/21305] batch time: 0.309 trainign loss: 7.6277 avg training loss: 8.3725
batch: [5600/21305] batch time: 1.383 trainign loss: 7.5392 avg training loss: 8.3723
batch: [5610/21305] batch time: 0.056 trainign loss: 7.9067 avg training loss: 8.3721
batch: [5620/21305] batch time: 1.661 trainign loss: 7.5749 avg training loss: 8.3720
batch: [5630/21305] batch time: 0.062 trainign loss: 7.0052 avg training loss: 8.3718
batch: [5640/21305] batch time: 0.620 trainign loss: 6.0919 avg training loss: 8.3716
batch: [5650/21305] batch time: 0.063 trainign loss: 8.0100 avg training loss: 8.3713
batch: [5660/21305] batch time: 1.019 trainign loss: 4.9899 avg training loss: 8.3711
batch: [5670/21305] batch time: 0.061 trainign loss: 7.2832 avg training loss: 8.3709
batch: [5680/21305] batch time: 0.335 trainign loss: 6.3773 avg training loss: 8.3706
batch: [5690/21305] batch time: 0.057 trainign loss: 6.2975 avg training loss: 8.3704
batch: [5700/21305] batch time: 0.566 trainign loss: 6.6531 avg training loss: 8.3701
batch: [5710/21305] batch time: 0.056 trainign loss: 5.3580 avg training loss: 8.3698
batch: [5720/21305] batch time: 0.061 trainign loss: 7.5047 avg training loss: 8.3693
batch: [5730/21305] batch time: 0.058 trainign loss: 7.6204 avg training loss: 8.3693
batch: [5740/21305] batch time: 0.056 trainign loss: 7.6509 avg training loss: 8.3692
batch: [5750/21305] batch time: 0.063 trainign loss: 6.7346 avg training loss: 8.3691
batch: [5760/21305] batch time: 0.055 trainign loss: 6.1030 avg training loss: 8.3689
batch: [5770/21305] batch time: 0.056 trainign loss: 7.1723 avg training loss: 8.3687
batch: [5780/21305] batch time: 0.062 trainign loss: 6.7662 avg training loss: 8.3685
batch: [5790/21305] batch time: 0.056 trainign loss: 7.9218 avg training loss: 8.3683
batch: [5800/21305] batch time: 0.100 trainign loss: 6.1524 avg training loss: 8.3681
batch: [5810/21305] batch time: 0.057 trainign loss: 8.0729 avg training loss: 8.3680
batch: [5820/21305] batch time: 0.052 trainign loss: 7.7767 avg training loss: 8.3678
batch: [5830/21305] batch time: 0.056 trainign loss: 6.6165 avg training loss: 8.3676
batch: [5840/21305] batch time: 0.051 trainign loss: 7.1715 avg training loss: 8.3674
batch: [5850/21305] batch time: 0.056 trainign loss: 6.9850 avg training loss: 8.3672
batch: [5860/21305] batch time: 0.051 trainign loss: 7.7499 avg training loss: 8.3670
batch: [5870/21305] batch time: 0.056 trainign loss: 7.0137 avg training loss: 8.3669
batch: [5880/21305] batch time: 0.056 trainign loss: 6.4905 avg training loss: 8.3666
batch: [5890/21305] batch time: 0.056 trainign loss: 6.8077 avg training loss: 8.3664
batch: [5900/21305] batch time: 0.057 trainign loss: 5.7096 avg training loss: 8.3662
batch: [5910/21305] batch time: 0.063 trainign loss: 5.9084 avg training loss: 8.3658
batch: [5920/21305] batch time: 0.056 trainign loss: 5.7037 avg training loss: 8.3654
batch: [5930/21305] batch time: 0.057 trainign loss: 7.2787 avg training loss: 8.3653
batch: [5940/21305] batch time: 0.054 trainign loss: 6.4134 avg training loss: 8.3651
batch: [5950/21305] batch time: 0.056 trainign loss: 5.8879 avg training loss: 8.3648
batch: [5960/21305] batch time: 0.062 trainign loss: 5.0158 avg training loss: 8.3644
batch: [5970/21305] batch time: 0.057 trainign loss: 7.9832 avg training loss: 8.3643
batch: [5980/21305] batch time: 0.063 trainign loss: 6.6511 avg training loss: 8.3642
batch: [5990/21305] batch time: 0.056 trainign loss: 7.6804 avg training loss: 8.3641
batch: [6000/21305] batch time: 0.058 trainign loss: 7.3223 avg training loss: 8.3640
batch: [6010/21305] batch time: 0.403 trainign loss: 6.1021 avg training loss: 8.3638
batch: [6020/21305] batch time: 0.056 trainign loss: 7.2547 avg training loss: 8.3635
batch: [6030/21305] batch time: 0.172 trainign loss: 6.4704 avg training loss: 8.3632
batch: [6040/21305] batch time: 0.056 trainign loss: 2.6754 avg training loss: 8.3627
batch: [6050/21305] batch time: 0.055 trainign loss: 7.1519 avg training loss: 8.3625
batch: [6060/21305] batch time: 0.056 trainign loss: 7.7548 avg training loss: 8.3622
batch: [6070/21305] batch time: 0.061 trainign loss: 3.3404 avg training loss: 8.3619
batch: [6080/21305] batch time: 0.052 trainign loss: 4.8728 avg training loss: 8.3615
batch: [6090/21305] batch time: 0.443 trainign loss: 7.3713 avg training loss: 8.3613
batch: [6100/21305] batch time: 0.059 trainign loss: 8.5432 avg training loss: 8.3612
batch: [6110/21305] batch time: 1.188 trainign loss: 6.2417 avg training loss: 8.3609
batch: [6120/21305] batch time: 0.060 trainign loss: 6.7415 avg training loss: 8.3607
batch: [6130/21305] batch time: 0.057 trainign loss: 7.4793 avg training loss: 8.3603
batch: [6140/21305] batch time: 0.056 trainign loss: 6.2070 avg training loss: 8.3601
batch: [6150/21305] batch time: 0.056 trainign loss: 6.6203 avg training loss: 8.3600
batch: [6160/21305] batch time: 0.360 trainign loss: 6.0910 avg training loss: 8.3598
batch: [6170/21305] batch time: 0.056 trainign loss: 5.0165 avg training loss: 8.3595
batch: [6180/21305] batch time: 0.053 trainign loss: 7.8359 avg training loss: 8.3592
batch: [6190/21305] batch time: 0.056 trainign loss: 7.9353 avg training loss: 8.3591
batch: [6200/21305] batch time: 0.055 trainign loss: 6.8907 avg training loss: 8.3590
batch: [6210/21305] batch time: 0.058 trainign loss: 6.8680 avg training loss: 8.3588
batch: [6220/21305] batch time: 0.263 trainign loss: 7.4681 avg training loss: 8.3585
batch: [6230/21305] batch time: 0.056 trainign loss: 6.9937 avg training loss: 8.3584
batch: [6240/21305] batch time: 0.752 trainign loss: 7.7713 avg training loss: 8.3581
batch: [6250/21305] batch time: 0.058 trainign loss: 6.9798 avg training loss: 8.3579
batch: [6260/21305] batch time: 0.100 trainign loss: 7.9989 avg training loss: 8.3577
batch: [6270/21305] batch time: 0.063 trainign loss: 6.9456 avg training loss: 8.3576
batch: [6280/21305] batch time: 0.909 trainign loss: 7.3505 avg training loss: 8.3574
batch: [6290/21305] batch time: 0.058 trainign loss: 4.4111 avg training loss: 8.3570
batch: [6300/21305] batch time: 1.914 trainign loss: 7.8245 avg training loss: 8.3569
batch: [6310/21305] batch time: 0.059 trainign loss: 7.0855 avg training loss: 8.3567
batch: [6320/21305] batch time: 0.994 trainign loss: 7.4298 avg training loss: 8.3565
batch: [6330/21305] batch time: 0.056 trainign loss: 6.9768 avg training loss: 8.3562
batch: [6340/21305] batch time: 1.148 trainign loss: 4.6776 avg training loss: 8.3559
batch: [6350/21305] batch time: 0.057 trainign loss: 7.0449 avg training loss: 8.3556
batch: [6360/21305] batch time: 1.795 trainign loss: 7.8463 avg training loss: 8.3555
batch: [6370/21305] batch time: 0.062 trainign loss: 6.6048 avg training loss: 8.3554
batch: [6380/21305] batch time: 1.544 trainign loss: 6.8316 avg training loss: 8.3551
batch: [6390/21305] batch time: 0.061 trainign loss: 5.0120 avg training loss: 8.3548
batch: [6400/21305] batch time: 1.178 trainign loss: 7.1215 avg training loss: 8.3545
batch: [6410/21305] batch time: 0.056 trainign loss: 7.9976 avg training loss: 8.3544
batch: [6420/21305] batch time: 1.174 trainign loss: 6.7085 avg training loss: 8.3540
batch: [6430/21305] batch time: 0.062 trainign loss: 5.2806 avg training loss: 8.3537
batch: [6440/21305] batch time: 0.230 trainign loss: 6.8958 avg training loss: 8.3533
batch: [6450/21305] batch time: 0.056 trainign loss: 7.6585 avg training loss: 8.3529
batch: [6460/21305] batch time: 0.478 trainign loss: 7.3822 avg training loss: 8.3529
batch: [6470/21305] batch time: 0.061 trainign loss: 7.8265 avg training loss: 8.3527
batch: [6480/21305] batch time: 0.060 trainign loss: 6.9768 avg training loss: 8.3525
batch: [6490/21305] batch time: 0.052 trainign loss: 6.4559 avg training loss: 8.3523
batch: [6500/21305] batch time: 0.061 trainign loss: 5.6445 avg training loss: 8.3521
batch: [6510/21305] batch time: 0.053 trainign loss: 7.1338 avg training loss: 8.3519
batch: [6520/21305] batch time: 0.063 trainign loss: 6.8306 avg training loss: 8.3517
batch: [6530/21305] batch time: 0.052 trainign loss: 5.7296 avg training loss: 8.3514
batch: [6540/21305] batch time: 0.059 trainign loss: 6.9735 avg training loss: 8.3511
batch: [6550/21305] batch time: 0.056 trainign loss: 6.1545 avg training loss: 8.3509
batch: [6560/21305] batch time: 0.063 trainign loss: 6.3990 avg training loss: 8.3506
batch: [6570/21305] batch time: 0.056 trainign loss: 6.5090 avg training loss: 8.3503
batch: [6580/21305] batch time: 0.060 trainign loss: 4.8071 avg training loss: 8.3501
batch: [6590/21305] batch time: 0.060 trainign loss: 6.5314 avg training loss: 8.3499
batch: [6600/21305] batch time: 0.058 trainign loss: 7.5676 avg training loss: 8.3497
batch: [6610/21305] batch time: 0.058 trainign loss: 7.9960 avg training loss: 8.3495
batch: [6620/21305] batch time: 0.060 trainign loss: 6.8254 avg training loss: 8.3493
batch: [6630/21305] batch time: 0.062 trainign loss: 6.6768 avg training loss: 8.3490
batch: [6640/21305] batch time: 0.054 trainign loss: 5.1796 avg training loss: 8.3487
batch: [6650/21305] batch time: 0.056 trainign loss: 6.2500 avg training loss: 8.3485
batch: [6660/21305] batch time: 0.056 trainign loss: 6.3278 avg training loss: 8.3482
batch: [6670/21305] batch time: 0.857 trainign loss: 6.7212 avg training loss: 8.3479
batch: [6680/21305] batch time: 0.061 trainign loss: 7.3225 avg training loss: 8.3478
batch: [6690/21305] batch time: 1.061 trainign loss: 7.2155 avg training loss: 8.3476
batch: [6700/21305] batch time: 0.056 trainign loss: 5.2120 avg training loss: 8.3472
batch: [6710/21305] batch time: 1.950 trainign loss: 5.7302 avg training loss: 8.3470
batch: [6720/21305] batch time: 1.148 trainign loss: 7.1840 avg training loss: 8.3467
batch: [6730/21305] batch time: 0.954 trainign loss: 6.1534 avg training loss: 8.3464
batch: [6740/21305] batch time: 1.631 trainign loss: 7.7068 avg training loss: 8.3463
batch: [6750/21305] batch time: 1.143 trainign loss: 7.8422 avg training loss: 8.3462
batch: [6760/21305] batch time: 1.727 trainign loss: 7.1368 avg training loss: 8.3460
batch: [6770/21305] batch time: 0.937 trainign loss: 5.9595 avg training loss: 8.3458
batch: [6780/21305] batch time: 1.123 trainign loss: 6.2757 avg training loss: 8.3455
batch: [6790/21305] batch time: 1.266 trainign loss: 7.0224 avg training loss: 8.3453
batch: [6800/21305] batch time: 1.371 trainign loss: 5.7946 avg training loss: 8.3450
batch: [6810/21305] batch time: 1.172 trainign loss: 6.7439 avg training loss: 8.3447
batch: [6820/21305] batch time: 1.044 trainign loss: 5.6601 avg training loss: 8.3445
batch: [6830/21305] batch time: 0.891 trainign loss: 7.0771 avg training loss: 8.3443
batch: [6840/21305] batch time: 0.576 trainign loss: 7.7788 avg training loss: 8.3437
batch: [6850/21305] batch time: 0.206 trainign loss: 6.0062 avg training loss: 8.3436
batch: [6860/21305] batch time: 0.124 trainign loss: 6.6661 avg training loss: 8.3435
batch: [6870/21305] batch time: 0.055 trainign loss: 7.0437 avg training loss: 8.3431
batch: [6880/21305] batch time: 0.052 trainign loss: 6.3560 avg training loss: 8.3429
batch: [6890/21305] batch time: 0.053 trainign loss: 7.9520 avg training loss: 8.3426
batch: [6900/21305] batch time: 0.058 trainign loss: 6.9538 avg training loss: 8.3424
batch: [6910/21305] batch time: 0.055 trainign loss: 6.2765 avg training loss: 8.3422
batch: [6920/21305] batch time: 0.051 trainign loss: 7.1697 avg training loss: 8.3420
batch: [6930/21305] batch time: 0.053 trainign loss: 6.3304 avg training loss: 8.3417
batch: [6940/21305] batch time: 0.056 trainign loss: 5.3973 avg training loss: 8.3414
batch: [6950/21305] batch time: 0.384 trainign loss: 6.4552 avg training loss: 8.3411
batch: [6960/21305] batch time: 0.056 trainign loss: 4.8994 avg training loss: 8.3408
batch: [6970/21305] batch time: 0.063 trainign loss: 4.6364 avg training loss: 8.3405
batch: [6980/21305] batch time: 0.056 trainign loss: 7.3214 avg training loss: 8.3403
batch: [6990/21305] batch time: 0.053 trainign loss: 7.6602 avg training loss: 8.3403
batch: [7000/21305] batch time: 0.062 trainign loss: 6.0379 avg training loss: 8.3401
batch: [7010/21305] batch time: 0.406 trainign loss: 6.7676 avg training loss: 8.3398
batch: [7020/21305] batch time: 0.056 trainign loss: 8.1532 avg training loss: 8.3396
batch: [7030/21305] batch time: 0.676 trainign loss: 6.8110 avg training loss: 8.3395
batch: [7040/21305] batch time: 0.062 trainign loss: 7.6062 avg training loss: 8.3393
batch: [7050/21305] batch time: 1.068 trainign loss: 6.1258 avg training loss: 8.3390
batch: [7060/21305] batch time: 0.058 trainign loss: 6.0854 avg training loss: 8.3388
batch: [7070/21305] batch time: 0.506 trainign loss: 7.1943 avg training loss: 8.3386
batch: [7080/21305] batch time: 0.062 trainign loss: 6.8548 avg training loss: 8.3384
batch: [7090/21305] batch time: 0.915 trainign loss: 6.3876 avg training loss: 8.3381
batch: [7100/21305] batch time: 0.056 trainign loss: 6.5251 avg training loss: 8.3379
batch: [7110/21305] batch time: 0.620 trainign loss: 6.5324 avg training loss: 8.3376
batch: [7120/21305] batch time: 0.061 trainign loss: 4.3916 avg training loss: 8.3370
batch: [7130/21305] batch time: 0.671 trainign loss: 7.9895 avg training loss: 8.3368
batch: [7140/21305] batch time: 0.062 trainign loss: 7.9285 avg training loss: 8.3366
batch: [7150/21305] batch time: 0.934 trainign loss: 5.4452 avg training loss: 8.3364
batch: [7160/21305] batch time: 0.058 trainign loss: 7.9294 avg training loss: 8.3361
batch: [7170/21305] batch time: 0.801 trainign loss: 6.2983 avg training loss: 8.3359
batch: [7180/21305] batch time: 0.056 trainign loss: 6.7899 avg training loss: 8.3357
batch: [7190/21305] batch time: 0.576 trainign loss: 5.2232 avg training loss: 8.3355
batch: [7200/21305] batch time: 0.062 trainign loss: 5.9447 avg training loss: 8.3351
batch: [7210/21305] batch time: 1.401 trainign loss: 6.5275 avg training loss: 8.3349
batch: [7220/21305] batch time: 0.056 trainign loss: 5.5488 avg training loss: 8.3346
batch: [7230/21305] batch time: 0.050 trainign loss: 5.0014 avg training loss: 8.3343
batch: [7240/21305] batch time: 0.056 trainign loss: 7.6789 avg training loss: 8.3341
batch: [7250/21305] batch time: 0.960 trainign loss: 6.2258 avg training loss: 8.3339
batch: [7260/21305] batch time: 0.057 trainign loss: 5.4759 avg training loss: 8.3336
batch: [7270/21305] batch time: 0.759 trainign loss: 7.2941 avg training loss: 8.3334
batch: [7280/21305] batch time: 0.058 trainign loss: 6.6777 avg training loss: 8.3333
batch: [7290/21305] batch time: 1.115 trainign loss: 5.8131 avg training loss: 8.3331
batch: [7300/21305] batch time: 0.081 trainign loss: 7.2135 avg training loss: 8.3328
batch: [7310/21305] batch time: 1.876 trainign loss: 7.4118 avg training loss: 8.3326
batch: [7320/21305] batch time: 0.063 trainign loss: 7.1809 avg training loss: 8.3324
batch: [7330/21305] batch time: 1.503 trainign loss: 5.9093 avg training loss: 8.3322
batch: [7340/21305] batch time: 0.063 trainign loss: 7.8067 avg training loss: 8.3319
batch: [7350/21305] batch time: 1.081 trainign loss: 7.2222 avg training loss: 8.3317
batch: [7360/21305] batch time: 0.058 trainign loss: 7.0782 avg training loss: 8.3314
batch: [7370/21305] batch time: 0.070 trainign loss: 3.0149 avg training loss: 8.3310
batch: [7380/21305] batch time: 0.062 trainign loss: 6.9923 avg training loss: 8.3308
batch: [7390/21305] batch time: 0.060 trainign loss: 7.2960 avg training loss: 8.3305
batch: [7400/21305] batch time: 0.056 trainign loss: 4.7323 avg training loss: 8.3302
batch: [7410/21305] batch time: 0.051 trainign loss: 7.9379 avg training loss: 8.3298
batch: [7420/21305] batch time: 0.062 trainign loss: 7.7639 avg training loss: 8.3297
batch: [7430/21305] batch time: 0.062 trainign loss: 6.8160 avg training loss: 8.3296
batch: [7440/21305] batch time: 0.061 trainign loss: 6.5552 avg training loss: 8.3293
batch: [7450/21305] batch time: 0.055 trainign loss: 7.1521 avg training loss: 8.3291
batch: [7460/21305] batch time: 0.056 trainign loss: 3.3785 avg training loss: 8.3287
batch: [7470/21305] batch time: 0.051 trainign loss: 4.1037 avg training loss: 8.3284
batch: [7480/21305] batch time: 0.056 trainign loss: 6.9030 avg training loss: 8.3282
batch: [7490/21305] batch time: 0.052 trainign loss: 6.9958 avg training loss: 8.3280
batch: [7500/21305] batch time: 0.058 trainign loss: 7.7723 avg training loss: 8.3279
batch: [7510/21305] batch time: 0.051 trainign loss: 6.4580 avg training loss: 8.3277
batch: [7520/21305] batch time: 0.056 trainign loss: 7.3589 avg training loss: 8.3275
batch: [7530/21305] batch time: 0.298 trainign loss: 6.0917 avg training loss: 8.3273
batch: [7540/21305] batch time: 0.056 trainign loss: 7.4588 avg training loss: 8.3270
batch: [7550/21305] batch time: 0.824 trainign loss: 7.7043 avg training loss: 8.3269
batch: [7560/21305] batch time: 0.061 trainign loss: 6.7380 avg training loss: 8.3267
batch: [7570/21305] batch time: 0.712 trainign loss: 7.1665 avg training loss: 8.3264
batch: [7580/21305] batch time: 0.063 trainign loss: 6.7104 avg training loss: 8.3263
batch: [7590/21305] batch time: 0.315 trainign loss: 7.7754 avg training loss: 8.3260
batch: [7600/21305] batch time: 0.308 trainign loss: 6.4051 avg training loss: 8.3258
batch: [7610/21305] batch time: 0.568 trainign loss: 6.3542 avg training loss: 8.3256
batch: [7620/21305] batch time: 0.320 trainign loss: 4.9448 avg training loss: 8.3252
batch: [7630/21305] batch time: 0.060 trainign loss: 7.8050 avg training loss: 8.3250
batch: [7640/21305] batch time: 0.056 trainign loss: 7.0534 avg training loss: 8.3248
batch: [7650/21305] batch time: 0.056 trainign loss: 5.4365 avg training loss: 8.3245
batch: [7660/21305] batch time: 0.063 trainign loss: 7.4290 avg training loss: 8.3242
batch: [7670/21305] batch time: 0.051 trainign loss: 7.5164 avg training loss: 8.3240
batch: [7680/21305] batch time: 0.062 trainign loss: 5.6347 avg training loss: 8.3238
batch: [7690/21305] batch time: 0.051 trainign loss: 5.3644 avg training loss: 8.3235
batch: [7700/21305] batch time: 0.058 trainign loss: 7.2134 avg training loss: 8.3232
batch: [7710/21305] batch time: 0.054 trainign loss: 5.5585 avg training loss: 8.3228
batch: [7720/21305] batch time: 0.099 trainign loss: 6.4247 avg training loss: 8.3225
batch: [7730/21305] batch time: 0.052 trainign loss: 4.6182 avg training loss: 8.3219
batch: [7740/21305] batch time: 0.749 trainign loss: 7.5255 avg training loss: 8.3216
batch: [7750/21305] batch time: 0.056 trainign loss: 7.9801 avg training loss: 8.3216
batch: [7760/21305] batch time: 1.487 trainign loss: 7.4389 avg training loss: 8.3215
batch: [7770/21305] batch time: 0.056 trainign loss: 5.1584 avg training loss: 8.3213
batch: [7780/21305] batch time: 2.213 trainign loss: 7.3986 avg training loss: 8.3210
batch: [7790/21305] batch time: 0.056 trainign loss: 7.3625 avg training loss: 8.3209
batch: [7800/21305] batch time: 2.372 trainign loss: 7.1717 avg training loss: 8.3207
batch: [7810/21305] batch time: 0.060 trainign loss: 6.3963 avg training loss: 8.3204
batch: [7820/21305] batch time: 2.275 trainign loss: 6.5886 avg training loss: 8.3202
batch: [7830/21305] batch time: 0.056 trainign loss: 6.4655 avg training loss: 8.3200
batch: [7840/21305] batch time: 2.435 trainign loss: 7.2705 avg training loss: 8.3198
batch: [7850/21305] batch time: 0.061 trainign loss: 5.4305 avg training loss: 8.3196
batch: [7860/21305] batch time: 2.009 trainign loss: 6.0147 avg training loss: 8.3193
batch: [7870/21305] batch time: 0.056 trainign loss: 6.4145 avg training loss: 8.3191
batch: [7880/21305] batch time: 2.496 trainign loss: 4.7693 avg training loss: 8.3188
batch: [7890/21305] batch time: 0.054 trainign loss: 6.3739 avg training loss: 8.3185
batch: [7900/21305] batch time: 2.116 trainign loss: 6.2368 avg training loss: 8.3180
batch: [7910/21305] batch time: 0.062 trainign loss: 7.7370 avg training loss: 8.3179
batch: [7920/21305] batch time: 2.102 trainign loss: 6.1668 avg training loss: 8.3178
batch: [7930/21305] batch time: 0.061 trainign loss: 6.7648 avg training loss: 8.3176
batch: [7940/21305] batch time: 2.113 trainign loss: 7.6016 avg training loss: 8.3175
batch: [7950/21305] batch time: 0.150 trainign loss: 7.3699 avg training loss: 8.3173
batch: [7960/21305] batch time: 2.280 trainign loss: 6.4636 avg training loss: 8.3171
batch: [7970/21305] batch time: 0.471 trainign loss: 6.6967 avg training loss: 8.3169
batch: [7980/21305] batch time: 1.862 trainign loss: 7.4685 avg training loss: 8.3166
batch: [7990/21305] batch time: 0.750 trainign loss: 7.6658 avg training loss: 8.3165
batch: [8000/21305] batch time: 1.390 trainign loss: 7.2892 avg training loss: 8.3163
batch: [8010/21305] batch time: 0.349 trainign loss: 6.0584 avg training loss: 8.3160
batch: [8020/21305] batch time: 0.660 trainign loss: 2.6493 avg training loss: 8.3155
batch: [8030/21305] batch time: 0.063 trainign loss: 6.2233 avg training loss: 8.3150
batch: [8040/21305] batch time: 1.078 trainign loss: 7.9585 avg training loss: 8.3148
batch: [8050/21305] batch time: 0.057 trainign loss: 8.6966 avg training loss: 8.3149
batch: [8060/21305] batch time: 1.262 trainign loss: 7.8205 avg training loss: 8.3148
batch: [8070/21305] batch time: 0.088 trainign loss: 6.6901 avg training loss: 8.3147
batch: [8080/21305] batch time: 1.144 trainign loss: 7.2699 avg training loss: 8.3146
batch: [8090/21305] batch time: 0.265 trainign loss: 7.4296 avg training loss: 8.3144
batch: [8100/21305] batch time: 0.826 trainign loss: 6.6059 avg training loss: 8.3141
batch: [8110/21305] batch time: 0.061 trainign loss: 7.5872 avg training loss: 8.3139
batch: [8120/21305] batch time: 0.056 trainign loss: 6.7500 avg training loss: 8.3137
batch: [8130/21305] batch time: 0.448 trainign loss: 6.7097 avg training loss: 8.3134
batch: [8140/21305] batch time: 0.063 trainign loss: 6.8292 avg training loss: 8.3133
batch: [8150/21305] batch time: 0.061 trainign loss: 6.1570 avg training loss: 8.3130
batch: [8160/21305] batch time: 0.050 trainign loss: 6.8642 avg training loss: 8.3128
batch: [8170/21305] batch time: 0.062 trainign loss: 6.8801 avg training loss: 8.3126
batch: [8180/21305] batch time: 0.054 trainign loss: 6.5348 avg training loss: 8.3123
batch: [8190/21305] batch time: 0.058 trainign loss: 7.2081 avg training loss: 8.3122
batch: [8200/21305] batch time: 0.056 trainign loss: 7.0193 avg training loss: 8.3120
batch: [8210/21305] batch time: 0.062 trainign loss: 6.4624 avg training loss: 8.3118
batch: [8220/21305] batch time: 0.055 trainign loss: 6.4838 avg training loss: 8.3116
batch: [8230/21305] batch time: 0.057 trainign loss: 7.2006 avg training loss: 8.3113
batch: [8240/21305] batch time: 0.056 trainign loss: 7.3921 avg training loss: 8.3111
batch: [8250/21305] batch time: 0.198 trainign loss: 6.4684 avg training loss: 8.3109
batch: [8260/21305] batch time: 0.969 trainign loss: 6.9211 avg training loss: 8.3106
batch: [8270/21305] batch time: 0.056 trainign loss: 7.8097 avg training loss: 8.3104
batch: [8280/21305] batch time: 1.495 trainign loss: 6.6661 avg training loss: 8.3103
batch: [8290/21305] batch time: 0.058 trainign loss: 6.1419 avg training loss: 8.3100
batch: [8300/21305] batch time: 0.808 trainign loss: 6.3364 avg training loss: 8.3098
batch: [8310/21305] batch time: 0.064 trainign loss: 7.7116 avg training loss: 8.3096
batch: [8320/21305] batch time: 1.273 trainign loss: 6.7887 avg training loss: 8.3094
batch: [8330/21305] batch time: 0.056 trainign loss: 7.1741 avg training loss: 8.3092
batch: [8340/21305] batch time: 1.046 trainign loss: 6.1544 avg training loss: 8.3089
batch: [8350/21305] batch time: 0.056 trainign loss: 5.3444 avg training loss: 8.3086
batch: [8360/21305] batch time: 1.251 trainign loss: 5.9341 avg training loss: 8.3083
batch: [8370/21305] batch time: 0.084 trainign loss: 7.8153 avg training loss: 8.3081
batch: [8380/21305] batch time: 0.714 trainign loss: 5.4325 avg training loss: 8.3079
batch: [8390/21305] batch time: 0.056 trainign loss: 4.9589 avg training loss: 8.3074
batch: [8400/21305] batch time: 0.771 trainign loss: 7.6864 avg training loss: 8.3068
batch: [8410/21305] batch time: 0.391 trainign loss: 6.8985 avg training loss: 8.3066
batch: [8420/21305] batch time: 0.486 trainign loss: 7.0362 avg training loss: 8.3062
batch: [8430/21305] batch time: 1.040 trainign loss: 8.3271 avg training loss: 8.3061
batch: [8440/21305] batch time: 0.862 trainign loss: 8.4580 avg training loss: 8.3060
batch: [8450/21305] batch time: 0.312 trainign loss: 7.5188 avg training loss: 8.3059
batch: [8460/21305] batch time: 0.946 trainign loss: 7.4994 avg training loss: 8.3058
batch: [8470/21305] batch time: 1.081 trainign loss: 7.9198 avg training loss: 8.3056
batch: [8480/21305] batch time: 0.688 trainign loss: 7.0315 avg training loss: 8.3054
batch: [8490/21305] batch time: 1.064 trainign loss: 6.8632 avg training loss: 8.3051
batch: [8500/21305] batch time: 1.734 trainign loss: 6.0508 avg training loss: 8.3048
batch: [8510/21305] batch time: 0.057 trainign loss: 6.1005 avg training loss: 8.3045
batch: [8520/21305] batch time: 1.821 trainign loss: 7.0525 avg training loss: 8.3042
batch: [8530/21305] batch time: 0.103 trainign loss: 7.0806 avg training loss: 8.3040
batch: [8540/21305] batch time: 1.995 trainign loss: 7.3899 avg training loss: 8.3038
batch: [8550/21305] batch time: 0.186 trainign loss: 7.9079 avg training loss: 8.3037
batch: [8560/21305] batch time: 1.085 trainign loss: 7.1489 avg training loss: 8.3035
batch: [8570/21305] batch time: 0.056 trainign loss: 5.9580 avg training loss: 8.3032
batch: [8580/21305] batch time: 0.776 trainign loss: 6.7572 avg training loss: 8.3030
batch: [8590/21305] batch time: 0.056 trainign loss: 7.5305 avg training loss: 8.3028
batch: [8600/21305] batch time: 0.570 trainign loss: 6.8595 avg training loss: 8.3026
batch: [8610/21305] batch time: 0.056 trainign loss: 6.3433 avg training loss: 8.3023
batch: [8620/21305] batch time: 1.137 trainign loss: 7.0710 avg training loss: 8.3021
batch: [8630/21305] batch time: 0.059 trainign loss: 5.4875 avg training loss: 8.3019
batch: [8640/21305] batch time: 0.378 trainign loss: 6.8426 avg training loss: 8.3017
batch: [8650/21305] batch time: 0.056 trainign loss: 4.9779 avg training loss: 8.3014
batch: [8660/21305] batch time: 0.662 trainign loss: 4.5253 avg training loss: 8.3011
batch: [8670/21305] batch time: 0.062 trainign loss: 5.9187 avg training loss: 8.3008
batch: [8680/21305] batch time: 0.063 trainign loss: 7.4965 avg training loss: 8.3007
batch: [8690/21305] batch time: 0.063 trainign loss: 6.2377 avg training loss: 8.3004
batch: [8700/21305] batch time: 0.051 trainign loss: 5.5193 avg training loss: 8.3002
batch: [8710/21305] batch time: 0.056 trainign loss: 1.6083 avg training loss: 8.2997
batch: [8720/21305] batch time: 0.056 trainign loss: 0.0008 avg training loss: 8.2986
batch: [8730/21305] batch time: 0.056 trainign loss: 0.0001 avg training loss: 8.2975
batch: [8740/21305] batch time: 0.052 trainign loss: 9.6507 avg training loss: 8.2975
batch: [8750/21305] batch time: 0.063 trainign loss: 8.9426 avg training loss: 8.2976
batch: [8760/21305] batch time: 0.059 trainign loss: 7.7764 avg training loss: 8.2975
batch: [8770/21305] batch time: 0.056 trainign loss: 6.8753 avg training loss: 8.2973
batch: [8780/21305] batch time: 0.056 trainign loss: 6.2446 avg training loss: 8.2970
batch: [8790/21305] batch time: 0.057 trainign loss: 6.7548 avg training loss: 8.2969
batch: [8800/21305] batch time: 0.057 trainign loss: 6.2492 avg training loss: 8.2966
batch: [8810/21305] batch time: 0.056 trainign loss: 7.5198 avg training loss: 8.2964
batch: [8820/21305] batch time: 0.053 trainign loss: 7.0065 avg training loss: 8.2963
batch: [8830/21305] batch time: 0.062 trainign loss: 7.3588 avg training loss: 8.2961
batch: [8840/21305] batch time: 0.051 trainign loss: 6.6361 avg training loss: 8.2959
batch: [8850/21305] batch time: 0.056 trainign loss: 6.5970 avg training loss: 8.2957
batch: [8860/21305] batch time: 0.056 trainign loss: 7.3471 avg training loss: 8.2955
batch: [8870/21305] batch time: 0.056 trainign loss: 7.0359 avg training loss: 8.2954
batch: [8880/21305] batch time: 0.056 trainign loss: 5.8461 avg training loss: 8.2951
batch: [8890/21305] batch time: 0.057 trainign loss: 7.3945 avg training loss: 8.2950
batch: [8900/21305] batch time: 0.056 trainign loss: 6.2884 avg training loss: 8.2947
batch: [8910/21305] batch time: 0.056 trainign loss: 5.6597 avg training loss: 8.2944
batch: [8920/21305] batch time: 0.051 trainign loss: 2.6853 avg training loss: 8.2940
batch: [8930/21305] batch time: 0.061 trainign loss: 7.8566 avg training loss: 8.2938
batch: [8940/21305] batch time: 0.143 trainign loss: 6.6201 avg training loss: 8.2936
batch: [8950/21305] batch time: 0.061 trainign loss: 6.2703 avg training loss: 8.2934
batch: [8960/21305] batch time: 0.055 trainign loss: 7.0145 avg training loss: 8.2932
batch: [8970/21305] batch time: 0.057 trainign loss: 6.7702 avg training loss: 8.2931
batch: [8980/21305] batch time: 0.112 trainign loss: 5.8399 avg training loss: 8.2928
batch: [8990/21305] batch time: 0.056 trainign loss: 6.5577 avg training loss: 8.2925
batch: [9000/21305] batch time: 0.156 trainign loss: 1.0583 avg training loss: 8.2920
batch: [9010/21305] batch time: 0.061 trainign loss: 8.5456 avg training loss: 8.2917
batch: [9020/21305] batch time: 0.056 trainign loss: 6.6385 avg training loss: 8.2916
batch: [9030/21305] batch time: 0.056 trainign loss: 6.8274 avg training loss: 8.2914
batch: [9040/21305] batch time: 0.052 trainign loss: 7.0612 avg training loss: 8.2913
batch: [9050/21305] batch time: 0.062 trainign loss: 6.4880 avg training loss: 8.2910
batch: [9060/21305] batch time: 0.061 trainign loss: 5.4466 avg training loss: 8.2908
batch: [9070/21305] batch time: 0.062 trainign loss: 3.2967 avg training loss: 8.2903
batch: [9080/21305] batch time: 0.051 trainign loss: 8.3087 avg training loss: 8.2902
batch: [9090/21305] batch time: 0.056 trainign loss: 6.6223 avg training loss: 8.2901
batch: [9100/21305] batch time: 0.059 trainign loss: 7.1336 avg training loss: 8.2899
batch: [9110/21305] batch time: 0.062 trainign loss: 5.9375 avg training loss: 8.2896
batch: [9120/21305] batch time: 0.056 trainign loss: 7.4454 avg training loss: 8.2894
batch: [9130/21305] batch time: 0.057 trainign loss: 5.4361 avg training loss: 8.2890
batch: [9140/21305] batch time: 0.056 trainign loss: 7.3077 avg training loss: 8.2887
batch: [9150/21305] batch time: 0.056 trainign loss: 6.4809 avg training loss: 8.2885
batch: [9160/21305] batch time: 0.056 trainign loss: 7.4096 avg training loss: 8.2883
batch: [9170/21305] batch time: 0.056 trainign loss: 6.3427 avg training loss: 8.2880
batch: [9180/21305] batch time: 0.063 trainign loss: 7.2665 avg training loss: 8.2879
batch: [9190/21305] batch time: 0.062 trainign loss: 7.1722 avg training loss: 8.2877
batch: [9200/21305] batch time: 0.056 trainign loss: 7.6666 avg training loss: 8.2875
batch: [9210/21305] batch time: 0.056 trainign loss: 6.4596 avg training loss: 8.2873
batch: [9220/21305] batch time: 0.054 trainign loss: 5.3600 avg training loss: 8.2870
batch: [9230/21305] batch time: 0.056 trainign loss: 6.9520 avg training loss: 8.2869
batch: [9240/21305] batch time: 0.056 trainign loss: 5.3413 avg training loss: 8.2866
batch: [9250/21305] batch time: 0.056 trainign loss: 7.1638 avg training loss: 8.2864
batch: [9260/21305] batch time: 0.056 trainign loss: 7.0246 avg training loss: 8.2862
batch: [9270/21305] batch time: 0.059 trainign loss: 6.7153 avg training loss: 8.2860
batch: [9280/21305] batch time: 0.051 trainign loss: 6.8470 avg training loss: 8.2857
batch: [9290/21305] batch time: 0.056 trainign loss: 5.3578 avg training loss: 8.2854
batch: [9300/21305] batch time: 0.055 trainign loss: 8.3056 avg training loss: 8.2851
batch: [9310/21305] batch time: 0.055 trainign loss: 7.5306 avg training loss: 8.2850
batch: [9320/21305] batch time: 0.056 trainign loss: 5.2512 avg training loss: 8.2847
batch: [9330/21305] batch time: 0.059 trainign loss: 5.3840 avg training loss: 8.2844
batch: [9340/21305] batch time: 0.056 trainign loss: 7.7109 avg training loss: 8.2843
batch: [9350/21305] batch time: 0.355 trainign loss: 7.7341 avg training loss: 8.2842
batch: [9360/21305] batch time: 0.056 trainign loss: 5.5803 avg training loss: 8.2840
batch: [9370/21305] batch time: 0.061 trainign loss: 5.5685 avg training loss: 8.2838
batch: [9380/21305] batch time: 0.053 trainign loss: 6.4546 avg training loss: 8.2835
batch: [9390/21305] batch time: 0.056 trainign loss: 6.8844 avg training loss: 8.2833
batch: [9400/21305] batch time: 0.056 trainign loss: 7.1628 avg training loss: 8.2832
batch: [9410/21305] batch time: 0.056 trainign loss: 6.0351 avg training loss: 8.2829
batch: [9420/21305] batch time: 0.056 trainign loss: 3.9845 avg training loss: 8.2826
batch: [9430/21305] batch time: 0.058 trainign loss: 7.3187 avg training loss: 8.2823
batch: [9440/21305] batch time: 0.051 trainign loss: 7.7087 avg training loss: 8.2822
batch: [9450/21305] batch time: 0.057 trainign loss: 5.3176 avg training loss: 8.2817
batch: [9460/21305] batch time: 0.056 trainign loss: 7.5514 avg training loss: 8.2816
batch: [9470/21305] batch time: 0.056 trainign loss: 8.2885 avg training loss: 8.2814
batch: [9480/21305] batch time: 0.063 trainign loss: 6.3914 avg training loss: 8.2812
batch: [9490/21305] batch time: 0.056 trainign loss: 7.8872 avg training loss: 8.2810
batch: [9500/21305] batch time: 0.056 trainign loss: 7.9132 avg training loss: 8.2809
batch: [9510/21305] batch time: 0.063 trainign loss: 6.2666 avg training loss: 8.2806
batch: [9520/21305] batch time: 0.056 trainign loss: 7.5455 avg training loss: 8.2804
batch: [9530/21305] batch time: 0.052 trainign loss: 7.4323 avg training loss: 8.2804
batch: [9540/21305] batch time: 0.061 trainign loss: 6.5597 avg training loss: 8.2801
batch: [9550/21305] batch time: 0.052 trainign loss: 5.0451 avg training loss: 8.2799
batch: [9560/21305] batch time: 0.056 trainign loss: 5.5341 avg training loss: 8.2795
batch: [9570/21305] batch time: 0.057 trainign loss: 7.2715 avg training loss: 8.2793
batch: [9580/21305] batch time: 0.062 trainign loss: 8.8970 avg training loss: 8.2790
batch: [9590/21305] batch time: 0.061 trainign loss: 7.7428 avg training loss: 8.2789
batch: [9600/21305] batch time: 0.052 trainign loss: 7.6874 avg training loss: 8.2788
batch: [9610/21305] batch time: 0.056 trainign loss: 6.9882 avg training loss: 8.2785
batch: [9620/21305] batch time: 0.053 trainign loss: 7.0711 avg training loss: 8.2784
batch: [9630/21305] batch time: 0.062 trainign loss: 8.1410 avg training loss: 8.2781
batch: [9640/21305] batch time: 0.056 trainign loss: 7.9762 avg training loss: 8.2780
batch: [9650/21305] batch time: 0.062 trainign loss: 6.8756 avg training loss: 8.2779
batch: [9660/21305] batch time: 0.056 trainign loss: 5.1457 avg training loss: 8.2776
batch: [9670/21305] batch time: 0.057 trainign loss: 6.6984 avg training loss: 8.2775
batch: [9680/21305] batch time: 0.833 trainign loss: 7.4252 avg training loss: 8.2773
batch: [9690/21305] batch time: 0.056 trainign loss: 6.9766 avg training loss: 8.2771
batch: [9700/21305] batch time: 1.471 trainign loss: 6.6327 avg training loss: 8.2770
batch: [9710/21305] batch time: 0.056 trainign loss: 6.6303 avg training loss: 8.2768
batch: [9720/21305] batch time: 1.510 trainign loss: 7.2564 avg training loss: 8.2766
batch: [9730/21305] batch time: 0.059 trainign loss: 6.9214 avg training loss: 8.2764
batch: [9740/21305] batch time: 1.813 trainign loss: 6.6368 avg training loss: 8.2762
batch: [9750/21305] batch time: 0.061 trainign loss: 7.6558 avg training loss: 8.2759
batch: [9760/21305] batch time: 2.701 trainign loss: 6.7884 avg training loss: 8.2757
batch: [9770/21305] batch time: 0.062 trainign loss: 7.2035 avg training loss: 8.2755
batch: [9780/21305] batch time: 2.602 trainign loss: 8.0628 avg training loss: 8.2754
batch: [9790/21305] batch time: 0.062 trainign loss: 6.8268 avg training loss: 8.2751
batch: [9800/21305] batch time: 2.042 trainign loss: 6.4487 avg training loss: 8.2749
batch: [9810/21305] batch time: 0.062 trainign loss: 7.8516 avg training loss: 8.2746
batch: [9820/21305] batch time: 2.071 trainign loss: 5.9827 avg training loss: 8.2744
batch: [9830/21305] batch time: 0.062 trainign loss: 5.7883 avg training loss: 8.2741
batch: [9840/21305] batch time: 2.369 trainign loss: 4.4993 avg training loss: 8.2737
batch: [9850/21305] batch time: 0.423 trainign loss: 7.5073 avg training loss: 8.2736
batch: [9860/21305] batch time: 2.212 trainign loss: 3.6222 avg training loss: 8.2733
batch: [9870/21305] batch time: 0.057 trainign loss: 5.5994 avg training loss: 8.2731
batch: [9880/21305] batch time: 2.288 trainign loss: 7.6689 avg training loss: 8.2728
batch: [9890/21305] batch time: 0.626 trainign loss: 6.8931 avg training loss: 8.2725
batch: [9900/21305] batch time: 2.096 trainign loss: 6.7645 avg training loss: 8.2724
batch: [9910/21305] batch time: 0.651 trainign loss: 6.9652 avg training loss: 8.2722
batch: [9920/21305] batch time: 1.852 trainign loss: 6.9468 avg training loss: 8.2720
batch: [9930/21305] batch time: 1.765 trainign loss: 5.9719 avg training loss: 8.2718
batch: [9940/21305] batch time: 0.751 trainign loss: 7.8763 avg training loss: 8.2716
batch: [9950/21305] batch time: 1.961 trainign loss: 6.9534 avg training loss: 8.2713
batch: [9960/21305] batch time: 1.318 trainign loss: 7.6367 avg training loss: 8.2712
batch: [9970/21305] batch time: 1.040 trainign loss: 7.3532 avg training loss: 8.2711
batch: [9980/21305] batch time: 1.785 trainign loss: 6.5619 avg training loss: 8.2709
batch: [9990/21305] batch time: 0.544 trainign loss: 7.1430 avg training loss: 8.2707
batch: [10000/21305] batch time: 2.219 trainign loss: 5.9391 avg training loss: 8.2705
batch: [10010/21305] batch time: 0.175 trainign loss: 6.8953 avg training loss: 8.2703
batch: [10020/21305] batch time: 2.210 trainign loss: 7.5888 avg training loss: 8.2702
batch: [10030/21305] batch time: 0.391 trainign loss: 6.6669 avg training loss: 8.2700
batch: [10040/21305] batch time: 2.039 trainign loss: 6.3948 avg training loss: 8.2698
batch: [10050/21305] batch time: 0.327 trainign loss: 6.3298 avg training loss: 8.2695
batch: [10060/21305] batch time: 2.008 trainign loss: 4.8580 avg training loss: 8.2692
batch: [10070/21305] batch time: 0.051 trainign loss: 7.0155 avg training loss: 8.2689
batch: [10080/21305] batch time: 1.287 trainign loss: 6.1317 avg training loss: 8.2688
batch: [10090/21305] batch time: 0.056 trainign loss: 8.9014 avg training loss: 8.2684
batch: [10100/21305] batch time: 1.332 trainign loss: 8.2787 avg training loss: 8.2683
batch: [10110/21305] batch time: 0.063 trainign loss: 6.9097 avg training loss: 8.2681
batch: [10120/21305] batch time: 1.108 trainign loss: 8.0211 avg training loss: 8.2680
batch: [10130/21305] batch time: 0.058 trainign loss: 7.1159 avg training loss: 8.2679
batch: [10140/21305] batch time: 1.528 trainign loss: 7.4909 avg training loss: 8.2678
batch: [10150/21305] batch time: 0.055 trainign loss: 6.8147 avg training loss: 8.2676
batch: [10160/21305] batch time: 1.367 trainign loss: 6.4334 avg training loss: 8.2674
batch: [10170/21305] batch time: 0.819 trainign loss: 7.3738 avg training loss: 8.2673
batch: [10180/21305] batch time: 1.225 trainign loss: 7.0209 avg training loss: 8.2671
batch: [10190/21305] batch time: 0.772 trainign loss: 7.3291 avg training loss: 8.2669
batch: [10200/21305] batch time: 0.761 trainign loss: 5.8619 avg training loss: 8.2666
batch: [10210/21305] batch time: 1.628 trainign loss: 6.4828 avg training loss: 8.2664
batch: [10220/21305] batch time: 0.274 trainign loss: 6.3146 avg training loss: 8.2661
batch: [10230/21305] batch time: 2.256 trainign loss: 5.8817 avg training loss: 8.2659
batch: [10240/21305] batch time: 0.057 trainign loss: 7.3170 avg training loss: 8.2656
batch: [10250/21305] batch time: 1.453 trainign loss: 7.5947 avg training loss: 8.2653
batch: [10260/21305] batch time: 0.316 trainign loss: 7.7883 avg training loss: 8.2652
batch: [10270/21305] batch time: 1.119 trainign loss: 7.5563 avg training loss: 8.2650
batch: [10280/21305] batch time: 1.000 trainign loss: 7.0462 avg training loss: 8.2648
batch: [10290/21305] batch time: 0.156 trainign loss: 6.5855 avg training loss: 8.2646
batch: [10300/21305] batch time: 0.767 trainign loss: 6.1215 avg training loss: 8.2643
batch: [10310/21305] batch time: 0.054 trainign loss: 6.0314 avg training loss: 8.2640
batch: [10320/21305] batch time: 1.119 trainign loss: 2.8266 avg training loss: 8.2637
batch: [10330/21305] batch time: 0.060 trainign loss: 7.2116 avg training loss: 8.2634
batch: [10340/21305] batch time: 1.997 trainign loss: 7.8478 avg training loss: 8.2633
batch: [10350/21305] batch time: 0.056 trainign loss: 7.7902 avg training loss: 8.2632
batch: [10360/21305] batch time: 2.065 trainign loss: 6.8452 avg training loss: 8.2631
batch: [10370/21305] batch time: 0.063 trainign loss: 6.5398 avg training loss: 8.2629
batch: [10380/21305] batch time: 2.237 trainign loss: 7.4057 avg training loss: 8.2627
batch: [10390/21305] batch time: 0.063 trainign loss: 7.0300 avg training loss: 8.2624
batch: [10400/21305] batch time: 2.315 trainign loss: 6.2348 avg training loss: 8.2622
batch: [10410/21305] batch time: 0.057 trainign loss: 6.8451 avg training loss: 8.2619
batch: [10420/21305] batch time: 2.146 trainign loss: 8.0524 avg training loss: 8.2617
batch: [10430/21305] batch time: 0.056 trainign loss: 5.7306 avg training loss: 8.2616
batch: [10440/21305] batch time: 2.331 trainign loss: 6.7998 avg training loss: 8.2612
batch: [10450/21305] batch time: 0.056 trainign loss: 6.6499 avg training loss: 8.2610
batch: [10460/21305] batch time: 2.363 trainign loss: 7.3578 avg training loss: 8.2608
batch: [10470/21305] batch time: 0.055 trainign loss: 5.7744 avg training loss: 8.2606
batch: [10480/21305] batch time: 2.712 trainign loss: 8.2900 avg training loss: 8.2603
batch: [10490/21305] batch time: 0.062 trainign loss: 7.2352 avg training loss: 8.2602
batch: [10500/21305] batch time: 2.343 trainign loss: 5.2646 avg training loss: 8.2599
batch: [10510/21305] batch time: 0.313 trainign loss: 5.0608 avg training loss: 8.2596
batch: [10520/21305] batch time: 2.313 trainign loss: 7.2355 avg training loss: 8.2594
batch: [10530/21305] batch time: 0.052 trainign loss: 7.0712 avg training loss: 8.2593
batch: [10540/21305] batch time: 2.351 trainign loss: 6.1077 avg training loss: 8.2590
batch: [10550/21305] batch time: 0.062 trainign loss: 5.2727 avg training loss: 8.2588
batch: [10560/21305] batch time: 2.573 trainign loss: 9.3685 avg training loss: 8.2582
batch: [10570/21305] batch time: 0.063 trainign loss: 7.5144 avg training loss: 8.2581
batch: [10580/21305] batch time: 2.447 trainign loss: 8.4402 avg training loss: 8.2581
batch: [10590/21305] batch time: 0.056 trainign loss: 7.7202 avg training loss: 8.2580
batch: [10600/21305] batch time: 2.195 trainign loss: 7.1192 avg training loss: 8.2579
batch: [10610/21305] batch time: 0.062 trainign loss: 4.4568 avg training loss: 8.2575
batch: [10620/21305] batch time: 2.278 trainign loss: 7.0678 avg training loss: 8.2574
batch: [10630/21305] batch time: 0.057 trainign loss: 6.7183 avg training loss: 8.2572
batch: [10640/21305] batch time: 2.307 trainign loss: 6.3068 avg training loss: 8.2568
batch: [10650/21305] batch time: 0.060 trainign loss: 8.1974 avg training loss: 8.2567
batch: [10660/21305] batch time: 2.524 trainign loss: 7.3311 avg training loss: 8.2565
batch: [10670/21305] batch time: 0.057 trainign loss: 4.8205 avg training loss: 8.2563
batch: [10680/21305] batch time: 2.094 trainign loss: 7.3854 avg training loss: 8.2561
batch: [10690/21305] batch time: 0.062 trainign loss: 6.9544 avg training loss: 8.2558
batch: [10700/21305] batch time: 2.356 trainign loss: 7.3172 avg training loss: 8.2555
batch: [10710/21305] batch time: 0.051 trainign loss: 6.7677 avg training loss: 8.2552
batch: [10720/21305] batch time: 2.275 trainign loss: 7.1134 avg training loss: 8.2548
batch: [10730/21305] batch time: 0.053 trainign loss: 7.9535 avg training loss: 8.2546
batch: [10740/21305] batch time: 2.188 trainign loss: 7.5834 avg training loss: 8.2545
batch: [10750/21305] batch time: 0.056 trainign loss: 7.1610 avg training loss: 8.2543
batch: [10760/21305] batch time: 2.394 trainign loss: 4.8839 avg training loss: 8.2541
batch: [10770/21305] batch time: 0.062 trainign loss: 6.6242 avg training loss: 8.2538
batch: [10780/21305] batch time: 2.460 trainign loss: 7.6037 avg training loss: 8.2537
batch: [10790/21305] batch time: 0.057 trainign loss: 6.8670 avg training loss: 8.2535
batch: [10800/21305] batch time: 2.224 trainign loss: 6.0595 avg training loss: 8.2533
batch: [10810/21305] batch time: 0.335 trainign loss: 7.2482 avg training loss: 8.2531
batch: [10820/21305] batch time: 1.981 trainign loss: 4.6015 avg training loss: 8.2527
batch: [10830/21305] batch time: 0.290 trainign loss: 1.8186 avg training loss: 8.2522
batch: [10840/21305] batch time: 1.747 trainign loss: 7.3886 avg training loss: 8.2522
batch: [10850/21305] batch time: 0.056 trainign loss: 7.5074 avg training loss: 8.2520
batch: [10860/21305] batch time: 2.070 trainign loss: 6.9476 avg training loss: 8.2519
batch: [10870/21305] batch time: 0.055 trainign loss: 5.8411 avg training loss: 8.2517
batch: [10880/21305] batch time: 1.381 trainign loss: 5.3933 avg training loss: 8.2514
batch: [10890/21305] batch time: 0.056 trainign loss: 7.2260 avg training loss: 8.2511
batch: [10900/21305] batch time: 2.126 trainign loss: 7.6324 avg training loss: 8.2510
batch: [10910/21305] batch time: 0.056 trainign loss: 7.1023 avg training loss: 8.2508
batch: [10920/21305] batch time: 2.090 trainign loss: 6.9231 avg training loss: 8.2506
batch: [10930/21305] batch time: 0.062 trainign loss: 5.5046 avg training loss: 8.2503
batch: [10940/21305] batch time: 2.106 trainign loss: 7.2382 avg training loss: 8.2500
batch: [10950/21305] batch time: 0.057 trainign loss: 5.8302 avg training loss: 8.2497
batch: [10960/21305] batch time: 2.282 trainign loss: 7.0936 avg training loss: 8.2495
batch: [10970/21305] batch time: 0.061 trainign loss: 7.0368 avg training loss: 8.2494
batch: [10980/21305] batch time: 2.368 trainign loss: 5.6414 avg training loss: 8.2491
batch: [10990/21305] batch time: 0.425 trainign loss: 7.2532 avg training loss: 8.2488
batch: [11000/21305] batch time: 1.473 trainign loss: 6.3744 avg training loss: 8.2486
batch: [11010/21305] batch time: 0.735 trainign loss: 6.5259 avg training loss: 8.2482
batch: [11020/21305] batch time: 2.212 trainign loss: 7.1993 avg training loss: 8.2481
batch: [11030/21305] batch time: 0.449 trainign loss: 5.7969 avg training loss: 8.2478
batch: [11040/21305] batch time: 1.770 trainign loss: 7.5263 avg training loss: 8.2476
batch: [11050/21305] batch time: 0.552 trainign loss: 6.8027 avg training loss: 8.2474
batch: [11060/21305] batch time: 0.975 trainign loss: 7.1373 avg training loss: 8.2472
batch: [11070/21305] batch time: 0.904 trainign loss: 5.5361 avg training loss: 8.2470
batch: [11080/21305] batch time: 1.574 trainign loss: 7.7870 avg training loss: 8.2467
batch: [11090/21305] batch time: 0.785 trainign loss: 7.6633 avg training loss: 8.2466
batch: [11100/21305] batch time: 2.809 trainign loss: 6.4909 avg training loss: 8.2464
batch: [11110/21305] batch time: 0.056 trainign loss: 7.3080 avg training loss: 8.2463
batch: [11120/21305] batch time: 2.527 trainign loss: 6.9570 avg training loss: 8.2461
batch: [11130/21305] batch time: 0.056 trainign loss: 5.1459 avg training loss: 8.2459
batch: [11140/21305] batch time: 2.361 trainign loss: 4.0174 avg training loss: 8.2456
batch: [11150/21305] batch time: 0.056 trainign loss: 6.9089 avg training loss: 8.2453
batch: [11160/21305] batch time: 2.474 trainign loss: 7.0884 avg training loss: 8.2451
batch: [11170/21305] batch time: 0.059 trainign loss: 3.7468 avg training loss: 8.2448
batch: [11180/21305] batch time: 2.695 trainign loss: 8.8337 avg training loss: 8.2443
batch: [11190/21305] batch time: 0.056 trainign loss: 7.8981 avg training loss: 8.2442
batch: [11200/21305] batch time: 2.494 trainign loss: 7.9761 avg training loss: 8.2441
batch: [11210/21305] batch time: 0.057 trainign loss: 5.9889 avg training loss: 8.2439
batch: [11220/21305] batch time: 2.428 trainign loss: 6.6996 avg training loss: 8.2437
batch: [11230/21305] batch time: 0.054 trainign loss: 6.0154 avg training loss: 8.2435
batch: [11240/21305] batch time: 2.287 trainign loss: 6.3392 avg training loss: 8.2433
batch: [11250/21305] batch time: 0.059 trainign loss: 0.9611 avg training loss: 8.2428
batch: [11260/21305] batch time: 2.223 trainign loss: 7.9249 avg training loss: 8.2426
batch: [11270/21305] batch time: 0.052 trainign loss: 7.0330 avg training loss: 8.2425
batch: [11280/21305] batch time: 2.392 trainign loss: 5.9887 avg training loss: 8.2423
batch: [11290/21305] batch time: 0.056 trainign loss: 7.3544 avg training loss: 8.2422
batch: [11300/21305] batch time: 2.324 trainign loss: 2.1693 avg training loss: 8.2418
batch: [11310/21305] batch time: 0.062 trainign loss: 6.8139 avg training loss: 8.2417
batch: [11320/21305] batch time: 2.072 trainign loss: 7.2141 avg training loss: 8.2414
batch: [11330/21305] batch time: 0.055 trainign loss: 8.3066 avg training loss: 8.2412
batch: [11340/21305] batch time: 2.233 trainign loss: 7.9564 avg training loss: 8.2411
batch: [11350/21305] batch time: 0.056 trainign loss: 7.4221 avg training loss: 8.2410
batch: [11360/21305] batch time: 1.918 trainign loss: 6.6897 avg training loss: 8.2408
batch: [11370/21305] batch time: 1.189 trainign loss: 6.7960 avg training loss: 8.2405
batch: [11380/21305] batch time: 1.653 trainign loss: 4.8151 avg training loss: 8.2402
batch: [11390/21305] batch time: 0.979 trainign loss: 7.3795 avg training loss: 8.2395
batch: [11400/21305] batch time: 1.333 trainign loss: 8.7737 avg training loss: 8.2396
batch: [11410/21305] batch time: 0.661 trainign loss: 7.5674 avg training loss: 8.2396
batch: [11420/21305] batch time: 1.687 trainign loss: 6.8036 avg training loss: 8.2394
batch: [11430/21305] batch time: 0.957 trainign loss: 7.2142 avg training loss: 8.2393
batch: [11440/21305] batch time: 1.601 trainign loss: 6.8431 avg training loss: 8.2392
batch: [11450/21305] batch time: 1.023 trainign loss: 6.5355 avg training loss: 8.2390
batch: [11460/21305] batch time: 1.561 trainign loss: 6.6039 avg training loss: 8.2388
batch: [11470/21305] batch time: 0.773 trainign loss: 7.0551 avg training loss: 8.2385
batch: [11480/21305] batch time: 1.691 trainign loss: 6.8707 avg training loss: 8.2383
batch: [11490/21305] batch time: 0.059 trainign loss: 6.9900 avg training loss: 8.2378
batch: [11500/21305] batch time: 1.790 trainign loss: 6.5892 avg training loss: 8.2377
batch: [11510/21305] batch time: 0.500 trainign loss: 6.2803 avg training loss: 8.2374
batch: [11520/21305] batch time: 2.612 trainign loss: 7.4030 avg training loss: 8.2372
batch: [11530/21305] batch time: 0.056 trainign loss: 6.2713 avg training loss: 8.2369
batch: [11540/21305] batch time: 2.367 trainign loss: 8.2751 avg training loss: 8.2368
batch: [11550/21305] batch time: 0.056 trainign loss: 5.2206 avg training loss: 8.2366
batch: [11560/21305] batch time: 2.406 trainign loss: 7.5057 avg training loss: 8.2365
batch: [11570/21305] batch time: 0.056 trainign loss: 5.5103 avg training loss: 8.2362
batch: [11580/21305] batch time: 2.291 trainign loss: 8.1441 avg training loss: 8.2360
batch: [11590/21305] batch time: 0.056 trainign loss: 7.2878 avg training loss: 8.2359
batch: [11600/21305] batch time: 2.567 trainign loss: 5.3670 avg training loss: 8.2357
batch: [11610/21305] batch time: 0.057 trainign loss: 6.0949 avg training loss: 8.2355
batch: [11620/21305] batch time: 2.288 trainign loss: 7.3062 avg training loss: 8.2352
batch: [11630/21305] batch time: 0.056 trainign loss: 7.3157 avg training loss: 8.2350
batch: [11640/21305] batch time: 2.487 trainign loss: 6.9292 avg training loss: 8.2348
batch: [11650/21305] batch time: 0.054 trainign loss: 7.8825 avg training loss: 8.2346
batch: [11660/21305] batch time: 2.378 trainign loss: 5.9961 avg training loss: 8.2344
batch: [11670/21305] batch time: 0.056 trainign loss: 7.2535 avg training loss: 8.2343
batch: [11680/21305] batch time: 2.641 trainign loss: 6.0261 avg training loss: 8.2341
batch: [11690/21305] batch time: 0.056 trainign loss: 6.2296 avg training loss: 8.2338
batch: [11700/21305] batch time: 2.207 trainign loss: 7.1339 avg training loss: 8.2335
batch: [11710/21305] batch time: 0.056 trainign loss: 5.4414 avg training loss: 8.2333
batch: [11720/21305] batch time: 2.263 trainign loss: 7.1837 avg training loss: 8.2331
batch: [11730/21305] batch time: 0.057 trainign loss: 5.4397 avg training loss: 8.2328
batch: [11740/21305] batch time: 2.168 trainign loss: 6.3312 avg training loss: 8.2326
batch: [11750/21305] batch time: 0.323 trainign loss: 7.3156 avg training loss: 8.2324
batch: [11760/21305] batch time: 1.809 trainign loss: 7.2095 avg training loss: 8.2323
batch: [11770/21305] batch time: 0.395 trainign loss: 6.9516 avg training loss: 8.2321
batch: [11780/21305] batch time: 2.064 trainign loss: 5.3106 avg training loss: 8.2319
batch: [11790/21305] batch time: 0.073 trainign loss: 1.2634 avg training loss: 8.2314
batch: [11800/21305] batch time: 2.168 trainign loss: 0.0006 avg training loss: 8.2303
batch: [11810/21305] batch time: 0.056 trainign loss: 8.0018 avg training loss: 8.2301
batch: [11820/21305] batch time: 2.030 trainign loss: 7.8741 avg training loss: 8.2302
batch: [11830/21305] batch time: 0.052 trainign loss: 8.0603 avg training loss: 8.2300
batch: [11840/21305] batch time: 2.367 trainign loss: 7.7652 avg training loss: 8.2298
batch: [11850/21305] batch time: 0.056 trainign loss: 6.0996 avg training loss: 8.2295
batch: [11860/21305] batch time: 2.735 trainign loss: 4.4363 avg training loss: 8.2293
batch: [11870/21305] batch time: 0.057 trainign loss: 6.4940 avg training loss: 8.2290
batch: [11880/21305] batch time: 2.379 trainign loss: 8.4551 avg training loss: 8.2285
batch: [11890/21305] batch time: 0.056 trainign loss: 7.9881 avg training loss: 8.2285
batch: [11900/21305] batch time: 2.027 trainign loss: 6.0245 avg training loss: 8.2283
batch: [11910/21305] batch time: 0.056 trainign loss: 8.0039 avg training loss: 8.2281
batch: [11920/21305] batch time: 2.266 trainign loss: 6.6520 avg training loss: 8.2280
batch: [11930/21305] batch time: 0.060 trainign loss: 5.7196 avg training loss: 8.2276
batch: [11940/21305] batch time: 2.619 trainign loss: 7.7520 avg training loss: 8.2276
batch: [11950/21305] batch time: 0.062 trainign loss: 6.9389 avg training loss: 8.2274
batch: [11960/21305] batch time: 2.492 trainign loss: 7.5569 avg training loss: 8.2272
batch: [11970/21305] batch time: 0.063 trainign loss: 6.7454 avg training loss: 8.2270
batch: [11980/21305] batch time: 2.572 trainign loss: 6.8227 avg training loss: 8.2268
batch: [11990/21305] batch time: 0.057 trainign loss: 5.9916 avg training loss: 8.2266
batch: [12000/21305] batch time: 2.408 trainign loss: 7.2088 avg training loss: 8.2264
batch: [12010/21305] batch time: 0.057 trainign loss: 7.6857 avg training loss: 8.2262
batch: [12020/21305] batch time: 2.390 trainign loss: 7.1712 avg training loss: 8.2261
batch: [12030/21305] batch time: 0.060 trainign loss: 6.3169 avg training loss: 8.2260
batch: [12040/21305] batch time: 1.886 trainign loss: 6.2734 avg training loss: 8.2257
batch: [12050/21305] batch time: 0.056 trainign loss: 6.7040 avg training loss: 8.2256
batch: [12060/21305] batch time: 2.414 trainign loss: 7.2869 avg training loss: 8.2253
batch: [12070/21305] batch time: 0.060 trainign loss: 3.9346 avg training loss: 8.2251
batch: [12080/21305] batch time: 2.289 trainign loss: 6.5009 avg training loss: 8.2244
batch: [12090/21305] batch time: 0.056 trainign loss: 6.1728 avg training loss: 8.2243
batch: [12100/21305] batch time: 2.529 trainign loss: 7.3297 avg training loss: 8.2241
batch: [12110/21305] batch time: 0.062 trainign loss: 7.6752 avg training loss: 8.2240
batch: [12120/21305] batch time: 2.447 trainign loss: 4.6271 avg training loss: 8.2237
batch: [12130/21305] batch time: 0.062 trainign loss: 7.5836 avg training loss: 8.2234
batch: [12140/21305] batch time: 2.557 trainign loss: 5.4201 avg training loss: 8.2231
batch: [12150/21305] batch time: 0.063 trainign loss: 8.0812 avg training loss: 8.2230
batch: [12160/21305] batch time: 2.398 trainign loss: 7.9407 avg training loss: 8.2229
batch: [12170/21305] batch time: 0.055 trainign loss: 7.5143 avg training loss: 8.2229
batch: [12180/21305] batch time: 2.479 trainign loss: 7.4437 avg training loss: 8.2228
batch: [12190/21305] batch time: 0.056 trainign loss: 7.5234 avg training loss: 8.2227
batch: [12200/21305] batch time: 2.158 trainign loss: 6.4554 avg training loss: 8.2226
batch: [12210/21305] batch time: 0.054 trainign loss: 7.5399 avg training loss: 8.2224
batch: [12220/21305] batch time: 2.420 trainign loss: 7.0079 avg training loss: 8.2222
batch: [12230/21305] batch time: 0.060 trainign loss: 7.6304 avg training loss: 8.2221
batch: [12240/21305] batch time: 2.242 trainign loss: 6.6220 avg training loss: 8.2219
batch: [12250/21305] batch time: 0.062 trainign loss: 6.5119 avg training loss: 8.2217
batch: [12260/21305] batch time: 2.250 trainign loss: 7.1362 avg training loss: 8.2216
batch: [12270/21305] batch time: 0.060 trainign loss: 5.5749 avg training loss: 8.2214
batch: [12280/21305] batch time: 1.715 trainign loss: 5.1659 avg training loss: 8.2210
batch: [12290/21305] batch time: 0.056 trainign loss: 7.1932 avg training loss: 8.2208
batch: [12300/21305] batch time: 2.195 trainign loss: 7.7874 avg training loss: 8.2207
batch: [12310/21305] batch time: 0.056 trainign loss: 7.0871 avg training loss: 8.2206
batch: [12320/21305] batch time: 1.490 trainign loss: 6.8859 avg training loss: 8.2204
batch: [12330/21305] batch time: 0.062 trainign loss: 7.2504 avg training loss: 8.2202
batch: [12340/21305] batch time: 1.114 trainign loss: 6.7394 avg training loss: 8.2201
batch: [12350/21305] batch time: 0.100 trainign loss: 7.3687 avg training loss: 8.2199
batch: [12360/21305] batch time: 1.107 trainign loss: 5.7595 avg training loss: 8.2197
batch: [12370/21305] batch time: 0.750 trainign loss: 5.6884 avg training loss: 8.2194
batch: [12380/21305] batch time: 0.066 trainign loss: 7.0351 avg training loss: 8.2192
batch: [12390/21305] batch time: 1.222 trainign loss: 7.7656 avg training loss: 8.2191
batch: [12400/21305] batch time: 0.780 trainign loss: 6.2354 avg training loss: 8.2189
batch: [12410/21305] batch time: 1.180 trainign loss: 7.8035 avg training loss: 8.2187
batch: [12420/21305] batch time: 0.708 trainign loss: 5.7984 avg training loss: 8.2185
batch: [12430/21305] batch time: 1.735 trainign loss: 5.8013 avg training loss: 8.2183
batch: [12440/21305] batch time: 0.223 trainign loss: 7.6012 avg training loss: 8.2180
batch: [12450/21305] batch time: 1.933 trainign loss: 7.8572 avg training loss: 8.2179
batch: [12460/21305] batch time: 0.093 trainign loss: 7.3075 avg training loss: 8.2178
batch: [12470/21305] batch time: 1.254 trainign loss: 5.8557 avg training loss: 8.2175
batch: [12480/21305] batch time: 0.694 trainign loss: 5.9606 avg training loss: 8.2173
batch: [12490/21305] batch time: 0.756 trainign loss: 6.5855 avg training loss: 8.2171
batch: [12500/21305] batch time: 0.839 trainign loss: 7.5072 avg training loss: 8.2168
batch: [12510/21305] batch time: 0.174 trainign loss: 7.8437 avg training loss: 8.2167
batch: [12520/21305] batch time: 0.821 trainign loss: 7.3636 avg training loss: 8.2166
batch: [12530/21305] batch time: 0.617 trainign loss: 7.1094 avg training loss: 8.2164
batch: [12540/21305] batch time: 0.267 trainign loss: 5.4758 avg training loss: 8.2161
batch: [12550/21305] batch time: 1.041 trainign loss: 7.2038 avg training loss: 8.2159
batch: [12560/21305] batch time: 0.719 trainign loss: 4.8531 avg training loss: 8.2156
batch: [12570/21305] batch time: 1.758 trainign loss: 0.0064 avg training loss: 8.2147
batch: [12580/21305] batch time: 0.877 trainign loss: 8.5772 avg training loss: 8.2148
batch: [12590/21305] batch time: 1.352 trainign loss: 5.4509 avg training loss: 8.2147
batch: [12600/21305] batch time: 1.269 trainign loss: 8.4432 avg training loss: 8.2147
batch: [12610/21305] batch time: 1.226 trainign loss: 5.1637 avg training loss: 8.2145
batch: [12620/21305] batch time: 1.095 trainign loss: 6.7571 avg training loss: 8.2144
batch: [12630/21305] batch time: 1.399 trainign loss: 6.4389 avg training loss: 8.2143
batch: [12640/21305] batch time: 1.822 trainign loss: 7.3679 avg training loss: 8.2141
batch: [12650/21305] batch time: 0.204 trainign loss: 5.8242 avg training loss: 8.2139
batch: [12660/21305] batch time: 2.299 trainign loss: 6.7371 avg training loss: 8.2137
batch: [12670/21305] batch time: 0.061 trainign loss: 7.6945 avg training loss: 8.2135
batch: [12680/21305] batch time: 2.281 trainign loss: 5.0438 avg training loss: 8.2133
batch: [12690/21305] batch time: 0.062 trainign loss: 8.0877 avg training loss: 8.2130
batch: [12700/21305] batch time: 2.361 trainign loss: 7.0137 avg training loss: 8.2128
batch: [12710/21305] batch time: 0.056 trainign loss: 6.3433 avg training loss: 8.2126
batch: [12720/21305] batch time: 2.659 trainign loss: 5.8767 avg training loss: 8.2124
batch: [12730/21305] batch time: 0.056 trainign loss: 6.1609 avg training loss: 8.2121
batch: [12740/21305] batch time: 2.140 trainign loss: 5.7958 avg training loss: 8.2120
batch: [12750/21305] batch time: 0.057 trainign loss: 4.4165 avg training loss: 8.2116
batch: [12760/21305] batch time: 2.005 trainign loss: 6.7705 avg training loss: 8.2115
batch: [12770/21305] batch time: 0.053 trainign loss: 6.3099 avg training loss: 8.2113
batch: [12780/21305] batch time: 2.320 trainign loss: 8.1123 avg training loss: 8.2112
batch: [12790/21305] batch time: 0.056 trainign loss: 5.9677 avg training loss: 8.2110
batch: [12800/21305] batch time: 2.640 trainign loss: 7.2908 avg training loss: 8.2109
batch: [12810/21305] batch time: 0.060 trainign loss: 7.5073 avg training loss: 8.2107
batch: [12820/21305] batch time: 2.318 trainign loss: 6.9475 avg training loss: 8.2105
batch: [12830/21305] batch time: 0.057 trainign loss: 4.9256 avg training loss: 8.2103
batch: [12840/21305] batch time: 2.247 trainign loss: 6.7744 avg training loss: 8.2101
batch: [12850/21305] batch time: 0.055 trainign loss: 7.2949 avg training loss: 8.2100
batch: [12860/21305] batch time: 2.110 trainign loss: 7.3610 avg training loss: 8.2098
batch: [12870/21305] batch time: 0.057 trainign loss: 6.5671 avg training loss: 8.2096
batch: [12880/21305] batch time: 2.393 trainign loss: 6.2466 avg training loss: 8.2093
batch: [12890/21305] batch time: 0.053 trainign loss: 6.3935 avg training loss: 8.2092
batch: [12900/21305] batch time: 2.638 trainign loss: 7.5629 avg training loss: 8.2090
batch: [12910/21305] batch time: 0.054 trainign loss: 6.5488 avg training loss: 8.2088
batch: [12920/21305] batch time: 2.153 trainign loss: 6.5557 avg training loss: 8.2086
batch: [12930/21305] batch time: 0.643 trainign loss: 6.2627 avg training loss: 8.2084
batch: [12940/21305] batch time: 2.488 trainign loss: 6.7605 avg training loss: 8.2082
batch: [12950/21305] batch time: 0.061 trainign loss: 5.4728 avg training loss: 8.2078
batch: [12960/21305] batch time: 2.470 trainign loss: 6.7265 avg training loss: 8.2077
batch: [12970/21305] batch time: 0.056 trainign loss: 5.8751 avg training loss: 8.2075
batch: [12980/21305] batch time: 2.329 trainign loss: 7.1923 avg training loss: 8.2072
batch: [12990/21305] batch time: 0.060 trainign loss: 6.7312 avg training loss: 8.2071
batch: [13000/21305] batch time: 2.702 trainign loss: 7.3478 avg training loss: 8.2069
batch: [13010/21305] batch time: 0.050 trainign loss: 6.9195 avg training loss: 8.2066
batch: [13020/21305] batch time: 2.362 trainign loss: 7.7247 avg training loss: 8.2064
batch: [13030/21305] batch time: 0.060 trainign loss: 7.0566 avg training loss: 8.2063
batch: [13040/21305] batch time: 2.297 trainign loss: 7.3263 avg training loss: 8.2061
batch: [13050/21305] batch time: 0.056 trainign loss: 2.4940 avg training loss: 8.2057
batch: [13060/21305] batch time: 2.305 trainign loss: 6.7997 avg training loss: 8.2055
batch: [13070/21305] batch time: 0.919 trainign loss: 7.0732 avg training loss: 8.2053
batch: [13080/21305] batch time: 1.112 trainign loss: 6.6309 avg training loss: 8.2051
batch: [13090/21305] batch time: 1.457 trainign loss: 5.2973 avg training loss: 8.2049
batch: [13100/21305] batch time: 0.907 trainign loss: 7.6986 avg training loss: 8.2045
batch: [13110/21305] batch time: 2.004 trainign loss: 7.3841 avg training loss: 8.2041
batch: [13120/21305] batch time: 1.083 trainign loss: 6.8653 avg training loss: 8.2041
batch: [13130/21305] batch time: 1.803 trainign loss: 5.8869 avg training loss: 8.2039
batch: [13140/21305] batch time: 1.040 trainign loss: 2.2942 avg training loss: 8.2035
batch: [13150/21305] batch time: 1.433 trainign loss: 7.5031 avg training loss: 8.2033
batch: [13160/21305] batch time: 0.195 trainign loss: 6.1979 avg training loss: 8.2032
batch: [13170/21305] batch time: 1.993 trainign loss: 7.0645 avg training loss: 8.2030
batch: [13180/21305] batch time: 0.057 trainign loss: 5.4104 avg training loss: 8.2027
batch: [13190/21305] batch time: 2.168 trainign loss: 6.7943 avg training loss: 8.2025
batch: [13200/21305] batch time: 0.056 trainign loss: 3.6351 avg training loss: 8.2021
batch: [13210/21305] batch time: 1.925 trainign loss: 0.0041 avg training loss: 8.2012
batch: [13220/21305] batch time: 0.062 trainign loss: 0.0001 avg training loss: 8.2001
batch: [13230/21305] batch time: 2.682 trainign loss: 0.0001 avg training loss: 8.1990
batch: [13240/21305] batch time: 0.053 trainign loss: 8.5010 avg training loss: 8.1991
batch: [13250/21305] batch time: 2.308 trainign loss: 8.7695 avg training loss: 8.1991
batch: [13260/21305] batch time: 0.061 trainign loss: 8.3517 avg training loss: 8.1992
batch: [13270/21305] batch time: 2.483 trainign loss: 7.3090 avg training loss: 8.1991
batch: [13280/21305] batch time: 0.052 trainign loss: 7.8244 avg training loss: 8.1989
batch: [13290/21305] batch time: 2.431 trainign loss: 7.2509 avg training loss: 8.1987
batch: [13300/21305] batch time: 0.055 trainign loss: 7.4110 avg training loss: 8.1985
batch: [13310/21305] batch time: 2.106 trainign loss: 5.4150 avg training loss: 8.1983
batch: [13320/21305] batch time: 0.439 trainign loss: 5.6864 avg training loss: 8.1978
batch: [13330/21305] batch time: 2.553 trainign loss: 7.3733 avg training loss: 8.1977
batch: [13340/21305] batch time: 0.056 trainign loss: 7.5966 avg training loss: 8.1975
batch: [13350/21305] batch time: 1.962 trainign loss: 6.4008 avg training loss: 8.1974
batch: [13360/21305] batch time: 0.057 trainign loss: 8.3747 avg training loss: 8.1973
batch: [13370/21305] batch time: 2.457 trainign loss: 6.1200 avg training loss: 8.1971
batch: [13380/21305] batch time: 0.055 trainign loss: 4.6763 avg training loss: 8.1968
batch: [13390/21305] batch time: 1.958 trainign loss: 6.9349 avg training loss: 8.1965
batch: [13400/21305] batch time: 0.060 trainign loss: 4.7825 avg training loss: 8.1963
batch: [13410/21305] batch time: 2.597 trainign loss: 7.4427 avg training loss: 8.1963
batch: [13420/21305] batch time: 0.056 trainign loss: 6.7634 avg training loss: 8.1961
batch: [13430/21305] batch time: 2.619 trainign loss: 7.1446 avg training loss: 8.1959
batch: [13440/21305] batch time: 0.057 trainign loss: 6.2416 avg training loss: 8.1958
batch: [13450/21305] batch time: 2.306 trainign loss: 7.4823 avg training loss: 8.1957
batch: [13460/21305] batch time: 0.056 trainign loss: 6.8945 avg training loss: 8.1955
batch: [13470/21305] batch time: 2.486 trainign loss: 3.6835 avg training loss: 8.1952
batch: [13480/21305] batch time: 0.053 trainign loss: 4.6147 avg training loss: 8.1949
batch: [13490/21305] batch time: 2.417 trainign loss: 7.1297 avg training loss: 8.1947
batch: [13500/21305] batch time: 0.056 trainign loss: 6.8299 avg training loss: 8.1945
batch: [13510/21305] batch time: 2.409 trainign loss: 5.8486 avg training loss: 8.1943
batch: [13520/21305] batch time: 0.056 trainign loss: 6.4303 avg training loss: 8.1941
batch: [13530/21305] batch time: 2.241 trainign loss: 6.6507 avg training loss: 8.1939
batch: [13540/21305] batch time: 0.059 trainign loss: 4.2001 avg training loss: 8.1936
batch: [13550/21305] batch time: 2.432 trainign loss: 7.4133 avg training loss: 8.1935
batch: [13560/21305] batch time: 0.056 trainign loss: 6.5012 avg training loss: 8.1933
batch: [13570/21305] batch time: 2.208 trainign loss: 7.2635 avg training loss: 8.1931
batch: [13580/21305] batch time: 0.060 trainign loss: 7.9312 avg training loss: 8.1929
batch: [13590/21305] batch time: 2.031 trainign loss: 5.3081 avg training loss: 8.1928
batch: [13600/21305] batch time: 0.060 trainign loss: 6.6253 avg training loss: 8.1925
batch: [13610/21305] batch time: 2.388 trainign loss: 8.1347 avg training loss: 8.1923
batch: [13620/21305] batch time: 0.061 trainign loss: 7.1256 avg training loss: 8.1922
batch: [13630/21305] batch time: 2.302 trainign loss: 6.9511 avg training loss: 8.1920
batch: [13640/21305] batch time: 0.056 trainign loss: 7.2741 avg training loss: 8.1919
batch: [13650/21305] batch time: 2.412 trainign loss: 6.6024 avg training loss: 8.1917
batch: [13660/21305] batch time: 0.062 trainign loss: 6.2540 avg training loss: 8.1915
batch: [13670/21305] batch time: 2.447 trainign loss: 7.0440 avg training loss: 8.1913
batch: [13680/21305] batch time: 0.061 trainign loss: 6.0388 avg training loss: 8.1911
batch: [13690/21305] batch time: 2.347 trainign loss: 4.1712 avg training loss: 8.1908
batch: [13700/21305] batch time: 0.056 trainign loss: 8.3060 avg training loss: 8.1908
batch: [13710/21305] batch time: 2.450 trainign loss: 7.1248 avg training loss: 8.1907
batch: [13720/21305] batch time: 0.052 trainign loss: 6.8528 avg training loss: 8.1905
batch: [13730/21305] batch time: 2.518 trainign loss: 7.6196 avg training loss: 8.1904
batch: [13740/21305] batch time: 0.056 trainign loss: 6.0859 avg training loss: 8.1902
batch: [13750/21305] batch time: 1.991 trainign loss: 7.5115 avg training loss: 8.1900
batch: [13760/21305] batch time: 0.056 trainign loss: 7.5917 avg training loss: 8.1899
batch: [13770/21305] batch time: 2.181 trainign loss: 7.1936 avg training loss: 8.1898
batch: [13780/21305] batch time: 0.056 trainign loss: 5.9978 avg training loss: 8.1895
batch: [13790/21305] batch time: 2.316 trainign loss: 7.5262 avg training loss: 8.1894
batch: [13800/21305] batch time: 0.051 trainign loss: 6.5218 avg training loss: 8.1893
batch: [13810/21305] batch time: 2.180 trainign loss: 6.8465 avg training loss: 8.1890
batch: [13820/21305] batch time: 0.057 trainign loss: 7.7942 avg training loss: 8.1889
batch: [13830/21305] batch time: 2.108 trainign loss: 6.5190 avg training loss: 8.1887
batch: [13840/21305] batch time: 0.697 trainign loss: 6.2286 avg training loss: 8.1884
batch: [13850/21305] batch time: 2.076 trainign loss: 1.4086 avg training loss: 8.1880
batch: [13860/21305] batch time: 0.061 trainign loss: 6.5951 avg training loss: 8.1877
batch: [13870/21305] batch time: 2.583 trainign loss: 8.2548 avg training loss: 8.1876
batch: [13880/21305] batch time: 0.062 trainign loss: 6.5350 avg training loss: 8.1875
batch: [13890/21305] batch time: 2.283 trainign loss: 7.3724 avg training loss: 8.1873
batch: [13900/21305] batch time: 0.057 trainign loss: 6.4380 avg training loss: 8.1872
batch: [13910/21305] batch time: 2.466 trainign loss: 7.7147 avg training loss: 8.1869
batch: [13920/21305] batch time: 0.057 trainign loss: 4.7624 avg training loss: 8.1866
batch: [13930/21305] batch time: 2.479 trainign loss: 6.9042 avg training loss: 8.1862
batch: [13940/21305] batch time: 0.055 trainign loss: 8.0114 avg training loss: 8.1862
batch: [13950/21305] batch time: 2.605 trainign loss: 5.3317 avg training loss: 8.1860
batch: [13960/21305] batch time: 0.063 trainign loss: 6.3931 avg training loss: 8.1859
batch: [13970/21305] batch time: 2.126 trainign loss: 7.5191 avg training loss: 8.1858
batch: [13980/21305] batch time: 0.056 trainign loss: 7.3643 avg training loss: 8.1856
batch: [13990/21305] batch time: 2.163 trainign loss: 6.9952 avg training loss: 8.1855
batch: [14000/21305] batch time: 0.061 trainign loss: 6.7299 avg training loss: 8.1852
batch: [14010/21305] batch time: 2.210 trainign loss: 5.2231 avg training loss: 8.1848
batch: [14020/21305] batch time: 0.062 trainign loss: 7.4428 avg training loss: 8.1845
batch: [14030/21305] batch time: 2.559 trainign loss: 7.2873 avg training loss: 8.1844
batch: [14040/21305] batch time: 0.052 trainign loss: 7.7163 avg training loss: 8.1842
batch: [14050/21305] batch time: 2.795 trainign loss: 7.4128 avg training loss: 8.1841
batch: [14060/21305] batch time: 0.056 trainign loss: 6.5940 avg training loss: 8.1840
batch: [14070/21305] batch time: 2.262 trainign loss: 7.2943 avg training loss: 8.1838
batch: [14080/21305] batch time: 0.052 trainign loss: 6.8697 avg training loss: 8.1837
batch: [14090/21305] batch time: 2.065 trainign loss: 7.2053 avg training loss: 8.1835
batch: [14100/21305] batch time: 0.056 trainign loss: 6.8542 avg training loss: 8.1833
batch: [14110/21305] batch time: 2.311 trainign loss: 6.6783 avg training loss: 8.1832
batch: [14120/21305] batch time: 0.062 trainign loss: 7.8616 avg training loss: 8.1829
batch: [14130/21305] batch time: 1.906 trainign loss: 6.3884 avg training loss: 8.1826
batch: [14140/21305] batch time: 0.056 trainign loss: 6.6580 avg training loss: 8.1823
batch: [14150/21305] batch time: 0.487 trainign loss: 6.9917 avg training loss: 8.1821
batch: [14160/21305] batch time: 0.057 trainign loss: 7.3316 avg training loss: 8.1820
batch: [14170/21305] batch time: 0.307 trainign loss: 6.9465 avg training loss: 8.1819
batch: [14180/21305] batch time: 0.056 trainign loss: 6.4102 avg training loss: 8.1816
batch: [14190/21305] batch time: 0.054 trainign loss: 2.2856 avg training loss: 8.1812
batch: [14200/21305] batch time: 0.057 trainign loss: 7.7516 avg training loss: 8.1811
batch: [14210/21305] batch time: 0.050 trainign loss: 6.6509 avg training loss: 8.1810
batch: [14220/21305] batch time: 0.056 trainign loss: 8.0795 avg training loss: 8.1806
batch: [14230/21305] batch time: 0.052 trainign loss: 6.7241 avg training loss: 8.1805
batch: [14240/21305] batch time: 0.056 trainign loss: 7.4772 avg training loss: 8.1804
batch: [14250/21305] batch time: 0.053 trainign loss: 7.3015 avg training loss: 8.1803
batch: [14260/21305] batch time: 0.056 trainign loss: 5.5831 avg training loss: 8.1801
batch: [14270/21305] batch time: 0.061 trainign loss: 5.5336 avg training loss: 8.1798
batch: [14280/21305] batch time: 0.056 trainign loss: 5.4999 avg training loss: 8.1796
batch: [14290/21305] batch time: 0.051 trainign loss: 6.7713 avg training loss: 8.1794
batch: [14300/21305] batch time: 0.057 trainign loss: 7.1751 avg training loss: 8.1793
batch: [14310/21305] batch time: 0.057 trainign loss: 6.8737 avg training loss: 8.1791
batch: [14320/21305] batch time: 0.056 trainign loss: 2.7235 avg training loss: 8.1787
batch: [14330/21305] batch time: 0.051 trainign loss: 7.4198 avg training loss: 8.1785
batch: [14340/21305] batch time: 0.056 trainign loss: 6.6579 avg training loss: 8.1784
batch: [14350/21305] batch time: 0.056 trainign loss: 5.5889 avg training loss: 8.1783
batch: [14360/21305] batch time: 0.058 trainign loss: 6.3774 avg training loss: 8.1781
batch: [14370/21305] batch time: 0.055 trainign loss: 6.3830 avg training loss: 8.1779
batch: [14380/21305] batch time: 0.058 trainign loss: 6.8461 avg training loss: 8.1778
batch: [14390/21305] batch time: 0.056 trainign loss: 7.0174 avg training loss: 8.1777
batch: [14400/21305] batch time: 0.057 trainign loss: 6.3062 avg training loss: 8.1775
batch: [14410/21305] batch time: 0.054 trainign loss: 6.8344 avg training loss: 8.1772
batch: [14420/21305] batch time: 0.056 trainign loss: 6.6610 avg training loss: 8.1770
batch: [14430/21305] batch time: 0.054 trainign loss: 6.9809 avg training loss: 8.1768
batch: [14440/21305] batch time: 0.054 trainign loss: 7.1833 avg training loss: 8.1767
batch: [14450/21305] batch time: 0.051 trainign loss: 5.9063 avg training loss: 8.1765
batch: [14460/21305] batch time: 0.053 trainign loss: 7.1700 avg training loss: 8.1763
batch: [14470/21305] batch time: 0.058 trainign loss: 6.1019 avg training loss: 8.1761
batch: [14480/21305] batch time: 0.057 trainign loss: 4.9537 avg training loss: 8.1759
batch: [14490/21305] batch time: 0.052 trainign loss: 5.5494 avg training loss: 8.1756
batch: [14500/21305] batch time: 0.062 trainign loss: 1.9753 avg training loss: 8.1752
batch: [14510/21305] batch time: 0.062 trainign loss: 0.0011 avg training loss: 8.1742
batch: [14520/21305] batch time: 0.051 trainign loss: 0.0001 avg training loss: 8.1732
batch: [14530/21305] batch time: 0.057 trainign loss: 0.0000 avg training loss: 8.1721
batch: [14540/21305] batch time: 0.051 trainign loss: 0.0000 avg training loss: 8.1711
batch: [14550/21305] batch time: 0.057 trainign loss: 0.0000 avg training loss: 8.1700
batch: [14560/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 8.1690
batch: [14570/21305] batch time: 0.062 trainign loss: 0.0000 avg training loss: 8.1679
batch: [14580/21305] batch time: 0.057 trainign loss: 0.0000 avg training loss: 8.1669
batch: [14590/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 8.1659
batch: [14600/21305] batch time: 0.054 trainign loss: 9.0527 avg training loss: 8.1658
batch: [14610/21305] batch time: 0.056 trainign loss: 8.8388 avg training loss: 8.1659
batch: [14620/21305] batch time: 0.050 trainign loss: 7.7517 avg training loss: 8.1658
batch: [14630/21305] batch time: 0.055 trainign loss: 6.5493 avg training loss: 8.1656
batch: [14640/21305] batch time: 0.052 trainign loss: 6.2694 avg training loss: 8.1655
batch: [14650/21305] batch time: 0.062 trainign loss: 7.4889 avg training loss: 8.1654
batch: [14660/21305] batch time: 0.051 trainign loss: 6.4802 avg training loss: 8.1652
batch: [14670/21305] batch time: 0.055 trainign loss: 5.5381 avg training loss: 8.1650
batch: [14680/21305] batch time: 0.056 trainign loss: 2.4721 avg training loss: 8.1647
batch: [14690/21305] batch time: 0.056 trainign loss: 6.8770 avg training loss: 8.1645
batch: [14700/21305] batch time: 0.058 trainign loss: 4.6003 avg training loss: 8.1642
batch: [14710/21305] batch time: 0.057 trainign loss: 0.0054 avg training loss: 8.1633
batch: [14720/21305] batch time: 0.055 trainign loss: 9.5560 avg training loss: 8.1632
batch: [14730/21305] batch time: 0.057 trainign loss: 7.1077 avg training loss: 8.1632
batch: [14740/21305] batch time: 0.057 trainign loss: 5.9781 avg training loss: 8.1631
batch: [14750/21305] batch time: 0.056 trainign loss: 7.6755 avg training loss: 8.1630
batch: [14760/21305] batch time: 0.051 trainign loss: 7.2838 avg training loss: 8.1628
batch: [14770/21305] batch time: 0.057 trainign loss: 7.0517 avg training loss: 8.1627
batch: [14780/21305] batch time: 0.056 trainign loss: 6.6142 avg training loss: 8.1626
batch: [14790/21305] batch time: 0.056 trainign loss: 7.4962 avg training loss: 8.1624
batch: [14800/21305] batch time: 0.053 trainign loss: 6.8205 avg training loss: 8.1623
batch: [14810/21305] batch time: 0.061 trainign loss: 6.4076 avg training loss: 8.1620
batch: [14820/21305] batch time: 0.054 trainign loss: 6.9486 avg training loss: 8.1619
batch: [14830/21305] batch time: 0.063 trainign loss: 6.5084 avg training loss: 8.1618
batch: [14840/21305] batch time: 0.056 trainign loss: 5.6528 avg training loss: 8.1616
batch: [14850/21305] batch time: 0.056 trainign loss: 7.3905 avg training loss: 8.1614
batch: [14860/21305] batch time: 0.063 trainign loss: 7.4127 avg training loss: 8.1612
batch: [14870/21305] batch time: 0.062 trainign loss: 6.8513 avg training loss: 8.1611
batch: [14880/21305] batch time: 0.053 trainign loss: 7.0220 avg training loss: 8.1610
batch: [14890/21305] batch time: 0.056 trainign loss: 6.5452 avg training loss: 8.1607
batch: [14900/21305] batch time: 0.057 trainign loss: 6.6531 avg training loss: 8.1606
batch: [14910/21305] batch time: 0.056 trainign loss: 3.7714 avg training loss: 8.1603
batch: [14920/21305] batch time: 0.054 trainign loss: 6.2983 avg training loss: 8.1602
batch: [14930/21305] batch time: 0.056 trainign loss: 6.4076 avg training loss: 8.1600
batch: [14940/21305] batch time: 0.060 trainign loss: 7.0200 avg training loss: 8.1597
batch: [14950/21305] batch time: 0.053 trainign loss: 7.4829 avg training loss: 8.1596
batch: [14960/21305] batch time: 0.062 trainign loss: 7.7047 avg training loss: 8.1593
batch: [14970/21305] batch time: 0.063 trainign loss: 8.0521 avg training loss: 8.1591
batch: [14980/21305] batch time: 0.063 trainign loss: 7.9818 avg training loss: 8.1590
batch: [14990/21305] batch time: 0.062 trainign loss: 5.4476 avg training loss: 8.1587
batch: [15000/21305] batch time: 0.063 trainign loss: 6.2058 avg training loss: 8.1585
batch: [15010/21305] batch time: 0.063 trainign loss: 7.9198 avg training loss: 8.1584
batch: [15020/21305] batch time: 0.063 trainign loss: 5.7616 avg training loss: 8.1582
batch: [15030/21305] batch time: 0.062 trainign loss: 7.5881 avg training loss: 8.1580
batch: [15040/21305] batch time: 0.063 trainign loss: 3.1493 avg training loss: 8.1578
batch: [15050/21305] batch time: 0.063 trainign loss: 7.2092 avg training loss: 8.1577
batch: [15060/21305] batch time: 0.063 trainign loss: 6.3877 avg training loss: 8.1575
batch: [15070/21305] batch time: 0.062 trainign loss: 6.2731 avg training loss: 8.1572
batch: [15080/21305] batch time: 0.063 trainign loss: 4.0441 avg training loss: 8.1566
batch: [15090/21305] batch time: 0.063 trainign loss: 7.5497 avg training loss: 8.1566
batch: [15100/21305] batch time: 0.071 trainign loss: 6.0937 avg training loss: 8.1565
batch: [15110/21305] batch time: 0.072 trainign loss: 3.1000 avg training loss: 8.1562
batch: [15120/21305] batch time: 0.063 trainign loss: 7.5080 avg training loss: 8.1561
batch: [15130/21305] batch time: 0.068 trainign loss: 6.8785 avg training loss: 8.1558
batch: [15140/21305] batch time: 0.071 trainign loss: 7.7857 avg training loss: 8.1558
batch: [15150/21305] batch time: 0.063 trainign loss: 7.0712 avg training loss: 8.1556
batch: [15160/21305] batch time: 0.063 trainign loss: 6.8859 avg training loss: 8.1555
batch: [15170/21305] batch time: 0.059 trainign loss: 6.7656 avg training loss: 8.1553
batch: [15180/21305] batch time: 0.056 trainign loss: 5.8371 avg training loss: 8.1550
batch: [15190/21305] batch time: 0.056 trainign loss: 7.1963 avg training loss: 8.1548
batch: [15200/21305] batch time: 0.059 trainign loss: 7.4685 avg training loss: 8.1548
batch: [15210/21305] batch time: 0.056 trainign loss: 6.1623 avg training loss: 8.1545
batch: [15220/21305] batch time: 0.056 trainign loss: 6.3867 avg training loss: 8.1543
batch: [15230/21305] batch time: 0.062 trainign loss: 7.1816 avg training loss: 8.1542
batch: [15240/21305] batch time: 0.054 trainign loss: 6.8988 avg training loss: 8.1541
batch: [15250/21305] batch time: 0.052 trainign loss: 7.6761 avg training loss: 8.1539
batch: [15260/21305] batch time: 0.054 trainign loss: 6.1951 avg training loss: 8.1537
batch: [15270/21305] batch time: 0.053 trainign loss: 7.8524 avg training loss: 8.1535
batch: [15280/21305] batch time: 0.056 trainign loss: 6.7645 avg training loss: 8.1534
batch: [15290/21305] batch time: 0.056 trainign loss: 7.5507 avg training loss: 8.1532
batch: [15300/21305] batch time: 0.051 trainign loss: 6.9213 avg training loss: 8.1530
batch: [15310/21305] batch time: 0.056 trainign loss: 6.2391 avg training loss: 8.1527
batch: [15320/21305] batch time: 0.051 trainign loss: 7.4631 avg training loss: 8.1525
batch: [15330/21305] batch time: 0.057 trainign loss: 6.7162 avg training loss: 8.1524
batch: [15340/21305] batch time: 0.062 trainign loss: 5.6695 avg training loss: 8.1522
batch: [15350/21305] batch time: 0.057 trainign loss: 6.6822 avg training loss: 8.1520
batch: [15360/21305] batch time: 0.050 trainign loss: 7.8394 avg training loss: 8.1519
batch: [15370/21305] batch time: 0.056 trainign loss: 5.6486 avg training loss: 8.1516
batch: [15380/21305] batch time: 0.057 trainign loss: 6.3706 avg training loss: 8.1514
batch: [15390/21305] batch time: 0.056 trainign loss: 7.0654 avg training loss: 8.1513
batch: [15400/21305] batch time: 0.053 trainign loss: 7.3240 avg training loss: 8.1512
batch: [15410/21305] batch time: 0.056 trainign loss: 7.1824 avg training loss: 8.1511
batch: [15420/21305] batch time: 0.050 trainign loss: 6.8774 avg training loss: 8.1509
batch: [15430/21305] batch time: 0.056 trainign loss: 6.4568 avg training loss: 8.1507
batch: [15440/21305] batch time: 0.058 trainign loss: 7.6104 avg training loss: 8.1506
batch: [15450/21305] batch time: 0.063 trainign loss: 7.2324 avg training loss: 8.1504
batch: [15460/21305] batch time: 0.051 trainign loss: 7.3370 avg training loss: 8.1503
batch: [15470/21305] batch time: 0.056 trainign loss: 6.7052 avg training loss: 8.1501
batch: [15480/21305] batch time: 0.056 trainign loss: 7.2201 avg training loss: 8.1500
batch: [15490/21305] batch time: 0.056 trainign loss: 6.3017 avg training loss: 8.1498
batch: [15500/21305] batch time: 0.217 trainign loss: 5.2020 avg training loss: 8.1495
batch: [15510/21305] batch time: 0.060 trainign loss: 4.5966 avg training loss: 8.1492
batch: [15520/21305] batch time: 0.052 trainign loss: 8.2949 avg training loss: 8.1489
batch: [15530/21305] batch time: 0.059 trainign loss: 8.0135 avg training loss: 8.1488
batch: [15540/21305] batch time: 0.061 trainign loss: 7.7766 avg training loss: 8.1487
batch: [15550/21305] batch time: 1.114 trainign loss: 5.8671 avg training loss: 8.1486
batch: [15560/21305] batch time: 0.062 trainign loss: 8.5763 avg training loss: 8.1484
batch: [15570/21305] batch time: 0.927 trainign loss: 7.3687 avg training loss: 8.1484
batch: [15580/21305] batch time: 0.058 trainign loss: 6.8038 avg training loss: 8.1482
batch: [15590/21305] batch time: 1.476 trainign loss: 6.3052 avg training loss: 8.1481
batch: [15600/21305] batch time: 0.062 trainign loss: 6.9292 avg training loss: 8.1479
batch: [15610/21305] batch time: 0.875 trainign loss: 6.8680 avg training loss: 8.1477
batch: [15620/21305] batch time: 0.059 trainign loss: 6.3624 avg training loss: 8.1476
batch: [15630/21305] batch time: 0.143 trainign loss: 6.2085 avg training loss: 8.1474
batch: [15640/21305] batch time: 0.059 trainign loss: 7.6281 avg training loss: 8.1473
batch: [15650/21305] batch time: 0.057 trainign loss: 5.7723 avg training loss: 8.1471
batch: [15660/21305] batch time: 0.055 trainign loss: 5.4791 avg training loss: 8.1469
batch: [15670/21305] batch time: 0.062 trainign loss: 6.8663 avg training loss: 8.1467
batch: [15680/21305] batch time: 0.057 trainign loss: 7.4592 avg training loss: 8.1465
batch: [15690/21305] batch time: 0.056 trainign loss: 7.2170 avg training loss: 8.1464
batch: [15700/21305] batch time: 0.051 trainign loss: 6.8218 avg training loss: 8.1462
batch: [15710/21305] batch time: 0.062 trainign loss: 6.6413 avg training loss: 8.1459
batch: [15720/21305] batch time: 0.069 trainign loss: 5.2682 avg training loss: 8.1457
batch: [15730/21305] batch time: 0.242 trainign loss: 0.3077 avg training loss: 8.1451
batch: [15740/21305] batch time: 0.132 trainign loss: 13.0064 avg training loss: 8.1445
batch: [15750/21305] batch time: 0.347 trainign loss: 8.1811 avg training loss: 8.1446
batch: [15760/21305] batch time: 0.700 trainign loss: 7.6635 avg training loss: 8.1446
batch: [15770/21305] batch time: 0.057 trainign loss: 7.0417 avg training loss: 8.1446
batch: [15780/21305] batch time: 0.154 trainign loss: 6.7946 avg training loss: 8.1445
batch: [15790/21305] batch time: 0.267 trainign loss: 6.5570 avg training loss: 8.1444
batch: [15800/21305] batch time: 0.058 trainign loss: 4.7370 avg training loss: 8.1441
batch: [15810/21305] batch time: 1.316 trainign loss: 8.2305 avg training loss: 8.1439
batch: [15820/21305] batch time: 0.056 trainign loss: 6.6513 avg training loss: 8.1438
batch: [15830/21305] batch time: 2.303 trainign loss: 6.3552 avg training loss: 8.1437
batch: [15840/21305] batch time: 0.055 trainign loss: 6.3757 avg training loss: 8.1435
batch: [15850/21305] batch time: 2.096 trainign loss: 5.4420 avg training loss: 8.1433
batch: [15860/21305] batch time: 0.062 trainign loss: 8.1982 avg training loss: 8.1432
batch: [15870/21305] batch time: 2.007 trainign loss: 6.3967 avg training loss: 8.1431
batch: [15880/21305] batch time: 0.056 trainign loss: 7.6269 avg training loss: 8.1430
batch: [15890/21305] batch time: 1.620 trainign loss: 7.5379 avg training loss: 8.1429
batch: [15900/21305] batch time: 0.056 trainign loss: 6.0327 avg training loss: 8.1428
batch: [15910/21305] batch time: 0.739 trainign loss: 7.0962 avg training loss: 8.1426
batch: [15920/21305] batch time: 0.056 trainign loss: 7.1401 avg training loss: 8.1425
batch: [15930/21305] batch time: 1.137 trainign loss: 7.2114 avg training loss: 8.1423
batch: [15940/21305] batch time: 0.056 trainign loss: 6.8598 avg training loss: 8.1419
batch: [15950/21305] batch time: 1.118 trainign loss: 7.3887 avg training loss: 8.1419
batch: [15960/21305] batch time: 0.053 trainign loss: 5.1960 avg training loss: 8.1417
batch: [15970/21305] batch time: 1.687 trainign loss: 5.7184 avg training loss: 8.1415
batch: [15980/21305] batch time: 0.056 trainign loss: 6.4488 avg training loss: 8.1414
batch: [15990/21305] batch time: 1.407 trainign loss: 7.4648 avg training loss: 8.1413
batch: [16000/21305] batch time: 0.056 trainign loss: 7.1618 avg training loss: 8.1411
batch: [16010/21305] batch time: 1.710 trainign loss: 0.8312 avg training loss: 8.1406
batch: [16020/21305] batch time: 0.367 trainign loss: 7.1186 avg training loss: 8.1405
batch: [16030/21305] batch time: 1.977 trainign loss: 7.9845 avg training loss: 8.1404
batch: [16040/21305] batch time: 0.227 trainign loss: 5.5410 avg training loss: 8.1402
batch: [16050/21305] batch time: 2.237 trainign loss: 7.0114 avg training loss: 8.1401
batch: [16060/21305] batch time: 0.409 trainign loss: 6.5205 avg training loss: 8.1399
batch: [16070/21305] batch time: 1.137 trainign loss: 7.7941 avg training loss: 8.1398
batch: [16080/21305] batch time: 0.312 trainign loss: 7.1279 avg training loss: 8.1397
batch: [16090/21305] batch time: 0.724 trainign loss: 6.3425 avg training loss: 8.1395
batch: [16100/21305] batch time: 0.063 trainign loss: 6.2028 avg training loss: 8.1393
batch: [16110/21305] batch time: 0.454 trainign loss: 5.9789 avg training loss: 8.1391
batch: [16120/21305] batch time: 0.056 trainign loss: 5.0990 avg training loss: 8.1390
batch: [16130/21305] batch time: 1.264 trainign loss: 6.4981 avg training loss: 8.1387
batch: [16140/21305] batch time: 0.058 trainign loss: 7.2210 avg training loss: 8.1385
batch: [16150/21305] batch time: 1.087 trainign loss: 6.7156 avg training loss: 8.1384
batch: [16160/21305] batch time: 0.056 trainign loss: 4.8511 avg training loss: 8.1381
batch: [16170/21305] batch time: 0.344 trainign loss: 7.6939 avg training loss: 8.1380
batch: [16180/21305] batch time: 0.062 trainign loss: 7.4149 avg training loss: 8.1379
batch: [16190/21305] batch time: 0.810 trainign loss: 7.2761 avg training loss: 8.1378
batch: [16200/21305] batch time: 0.056 trainign loss: 5.7104 avg training loss: 8.1376
batch: [16210/21305] batch time: 0.053 trainign loss: 7.1020 avg training loss: 8.1375
batch: [16220/21305] batch time: 0.056 trainign loss: 6.7708 avg training loss: 8.1373
batch: [16230/21305] batch time: 0.321 trainign loss: 7.2185 avg training loss: 8.1372
batch: [16240/21305] batch time: 0.056 trainign loss: 7.3967 avg training loss: 8.1370
batch: [16250/21305] batch time: 0.516 trainign loss: 7.0773 avg training loss: 8.1368
batch: [16260/21305] batch time: 0.053 trainign loss: 7.0611 avg training loss: 8.1366
batch: [16270/21305] batch time: 0.053 trainign loss: 7.4912 avg training loss: 8.1365
batch: [16280/21305] batch time: 0.052 trainign loss: 7.4323 avg training loss: 8.1364
batch: [16290/21305] batch time: 0.137 trainign loss: 6.8130 avg training loss: 8.1362
batch: [16300/21305] batch time: 0.062 trainign loss: 6.9823 avg training loss: 8.1360
batch: [16310/21305] batch time: 0.057 trainign loss: 7.0393 avg training loss: 8.1358
batch: [16320/21305] batch time: 0.057 trainign loss: 6.9897 avg training loss: 8.1356
batch: [16330/21305] batch time: 0.061 trainign loss: 5.5901 avg training loss: 8.1354
batch: [16340/21305] batch time: 0.056 trainign loss: 5.5170 avg training loss: 8.1350
batch: [16350/21305] batch time: 0.062 trainign loss: 7.6581 avg training loss: 8.1348
batch: [16360/21305] batch time: 0.058 trainign loss: 6.0329 avg training loss: 8.1346
batch: [16370/21305] batch time: 0.056 trainign loss: 6.5750 avg training loss: 8.1344
batch: [16380/21305] batch time: 0.056 trainign loss: 7.1122 avg training loss: 8.1342
batch: [16390/21305] batch time: 0.056 trainign loss: 6.4477 avg training loss: 8.1340
batch: [16400/21305] batch time: 0.063 trainign loss: 7.5766 avg training loss: 8.1339
batch: [16410/21305] batch time: 0.060 trainign loss: 7.5333 avg training loss: 8.1337
batch: [16420/21305] batch time: 0.059 trainign loss: 6.9123 avg training loss: 8.1336
batch: [16430/21305] batch time: 0.056 trainign loss: 5.9001 avg training loss: 8.1333
batch: [16440/21305] batch time: 0.057 trainign loss: 7.2651 avg training loss: 8.1332
batch: [16450/21305] batch time: 0.061 trainign loss: 7.6946 avg training loss: 8.1330
batch: [16460/21305] batch time: 0.056 trainign loss: 7.2179 avg training loss: 8.1328
batch: [16470/21305] batch time: 0.052 trainign loss: 7.0126 avg training loss: 8.1327
batch: [16480/21305] batch time: 0.056 trainign loss: 6.2269 avg training loss: 8.1326
batch: [16490/21305] batch time: 0.057 trainign loss: 5.7416 avg training loss: 8.1323
batch: [16500/21305] batch time: 0.056 trainign loss: 6.0962 avg training loss: 8.1321
batch: [16510/21305] batch time: 0.056 trainign loss: 6.9432 avg training loss: 8.1320
batch: [16520/21305] batch time: 0.056 trainign loss: 6.7905 avg training loss: 8.1318
batch: [16530/21305] batch time: 0.056 trainign loss: 7.0215 avg training loss: 8.1316
batch: [16540/21305] batch time: 0.056 trainign loss: 6.3145 avg training loss: 8.1313
batch: [16550/21305] batch time: 0.052 trainign loss: 7.4771 avg training loss: 8.1311
batch: [16560/21305] batch time: 0.056 trainign loss: 7.2653 avg training loss: 8.1310
batch: [16570/21305] batch time: 0.062 trainign loss: 6.2289 avg training loss: 8.1308
batch: [16580/21305] batch time: 0.062 trainign loss: 7.4586 avg training loss: 8.1306
batch: [16590/21305] batch time: 0.056 trainign loss: 7.0598 avg training loss: 8.1304
batch: [16600/21305] batch time: 0.057 trainign loss: 6.6881 avg training loss: 8.1302
batch: [16610/21305] batch time: 0.055 trainign loss: 5.7499 avg training loss: 8.1298
batch: [16620/21305] batch time: 0.056 trainign loss: 5.7758 avg training loss: 8.1295
batch: [16630/21305] batch time: 0.056 trainign loss: 6.2169 avg training loss: 8.1292
batch: [16640/21305] batch time: 0.056 trainign loss: 7.8284 avg training loss: 8.1291
batch: [16650/21305] batch time: 0.056 trainign loss: 7.1694 avg training loss: 8.1290
batch: [16660/21305] batch time: 0.056 trainign loss: 7.1190 avg training loss: 8.1289
batch: [16670/21305] batch time: 0.057 trainign loss: 6.9650 avg training loss: 8.1286
batch: [16680/21305] batch time: 0.056 trainign loss: 7.3979 avg training loss: 8.1285
batch: [16690/21305] batch time: 0.054 trainign loss: 7.2986 avg training loss: 8.1284
batch: [16700/21305] batch time: 0.058 trainign loss: 7.0398 avg training loss: 8.1282
batch: [16710/21305] batch time: 0.052 trainign loss: 6.8793 avg training loss: 8.1281
batch: [16720/21305] batch time: 0.062 trainign loss: 7.2001 avg training loss: 8.1279
batch: [16730/21305] batch time: 0.056 trainign loss: 5.5438 avg training loss: 8.1277
batch: [16740/21305] batch time: 0.057 trainign loss: 6.9731 avg training loss: 8.1275
batch: [16750/21305] batch time: 0.057 trainign loss: 6.9605 avg training loss: 8.1274
batch: [16760/21305] batch time: 0.655 trainign loss: 6.2800 avg training loss: 8.1273
batch: [16770/21305] batch time: 0.222 trainign loss: 7.3852 avg training loss: 8.1271
batch: [16780/21305] batch time: 0.645 trainign loss: 6.6337 avg training loss: 8.1269
batch: [16790/21305] batch time: 0.380 trainign loss: 6.8460 avg training loss: 8.1267
batch: [16800/21305] batch time: 0.057 trainign loss: 2.9537 avg training loss: 8.1263
batch: [16810/21305] batch time: 0.395 trainign loss: 7.6828 avg training loss: 8.1262
batch: [16820/21305] batch time: 0.345 trainign loss: 6.1655 avg training loss: 8.1261
batch: [16830/21305] batch time: 0.204 trainign loss: 6.9454 avg training loss: 8.1259
batch: [16840/21305] batch time: 0.056 trainign loss: 7.7041 avg training loss: 8.1257
batch: [16850/21305] batch time: 0.108 trainign loss: 7.1077 avg training loss: 8.1255
batch: [16860/21305] batch time: 0.057 trainign loss: 7.2211 avg training loss: 8.1254
batch: [16870/21305] batch time: 0.059 trainign loss: 6.9095 avg training loss: 8.1251
batch: [16880/21305] batch time: 0.097 trainign loss: 6.7275 avg training loss: 8.1249
batch: [16890/21305] batch time: 0.058 trainign loss: 6.9552 avg training loss: 8.1247
batch: [16900/21305] batch time: 0.491 trainign loss: 4.8009 avg training loss: 8.1245
batch: [16910/21305] batch time: 0.056 trainign loss: 0.0073 avg training loss: 8.1236
batch: [16920/21305] batch time: 0.533 trainign loss: 8.7440 avg training loss: 8.1236
batch: [16930/21305] batch time: 0.062 trainign loss: 5.6201 avg training loss: 8.1235
batch: [16940/21305] batch time: 0.641 trainign loss: 7.3986 avg training loss: 8.1234
batch: [16950/21305] batch time: 0.056 trainign loss: 7.6005 avg training loss: 8.1233
batch: [16960/21305] batch time: 0.400 trainign loss: 4.9334 avg training loss: 8.1231
batch: [16970/21305] batch time: 0.056 trainign loss: 6.1036 avg training loss: 8.1229
batch: [16980/21305] batch time: 0.060 trainign loss: 4.0645 avg training loss: 8.1226
batch: [16990/21305] batch time: 0.055 trainign loss: 3.5380 avg training loss: 8.1223
batch: [17000/21305] batch time: 0.335 trainign loss: 2.4637 avg training loss: 8.1218
batch: [17010/21305] batch time: 0.056 trainign loss: 8.0571 avg training loss: 8.1218
batch: [17020/21305] batch time: 0.057 trainign loss: 0.6271 avg training loss: 8.1214
batch: [17030/21305] batch time: 0.057 trainign loss: 8.3469 avg training loss: 8.1211
batch: [17040/21305] batch time: 0.056 trainign loss: 7.8274 avg training loss: 8.1211
batch: [17050/21305] batch time: 0.055 trainign loss: 7.3490 avg training loss: 8.1209
batch: [17060/21305] batch time: 0.056 trainign loss: 7.4566 avg training loss: 8.1207
batch: [17070/21305] batch time: 0.056 trainign loss: 7.6737 avg training loss: 8.1204
batch: [17080/21305] batch time: 0.056 trainign loss: 7.4226 avg training loss: 8.1203
batch: [17090/21305] batch time: 0.054 trainign loss: 6.5704 avg training loss: 8.1202
batch: [17100/21305] batch time: 0.056 trainign loss: 2.3300 avg training loss: 8.1198
batch: [17110/21305] batch time: 0.051 trainign loss: 8.0704 avg training loss: 8.1198
batch: [17120/21305] batch time: 0.056 trainign loss: 7.4307 avg training loss: 8.1197
batch: [17130/21305] batch time: 0.057 trainign loss: 6.1472 avg training loss: 8.1196
batch: [17140/21305] batch time: 0.063 trainign loss: 6.1118 avg training loss: 8.1194
batch: [17150/21305] batch time: 0.056 trainign loss: 6.8251 avg training loss: 8.1192
batch: [17160/21305] batch time: 0.056 trainign loss: 6.8776 avg training loss: 8.1190
batch: [17170/21305] batch time: 0.056 trainign loss: 5.5283 avg training loss: 8.1186
batch: [17180/21305] batch time: 0.589 trainign loss: 7.6587 avg training loss: 8.1185
batch: [17190/21305] batch time: 0.057 trainign loss: 5.8281 avg training loss: 8.1183
batch: [17200/21305] batch time: 0.706 trainign loss: 6.6270 avg training loss: 8.1181
batch: [17210/21305] batch time: 0.055 trainign loss: 7.0247 avg training loss: 8.1179
batch: [17220/21305] batch time: 0.617 trainign loss: 7.0442 avg training loss: 8.1178
batch: [17230/21305] batch time: 0.061 trainign loss: 5.3841 avg training loss: 8.1175
batch: [17240/21305] batch time: 1.371 trainign loss: 6.6595 avg training loss: 8.1172
batch: [17250/21305] batch time: 0.052 trainign loss: 8.5077 avg training loss: 8.1169
batch: [17260/21305] batch time: 0.710 trainign loss: 7.8168 avg training loss: 8.1169
batch: [17270/21305] batch time: 0.056 trainign loss: 6.7560 avg training loss: 8.1168
batch: [17280/21305] batch time: 0.286 trainign loss: 7.3737 avg training loss: 8.1167
batch: [17290/21305] batch time: 0.061 trainign loss: 5.3500 avg training loss: 8.1166
batch: [17300/21305] batch time: 0.822 trainign loss: 6.6291 avg training loss: 8.1164
batch: [17310/21305] batch time: 0.056 trainign loss: 4.0473 avg training loss: 8.1160
batch: [17320/21305] batch time: 0.518 trainign loss: 12.0566 avg training loss: 8.1153
batch: [17330/21305] batch time: 0.063 trainign loss: 9.0044 avg training loss: 8.1154
batch: [17340/21305] batch time: 1.035 trainign loss: 8.4609 avg training loss: 8.1154
batch: [17350/21305] batch time: 0.053 trainign loss: 8.3685 avg training loss: 8.1151
batch: [17360/21305] batch time: 1.445 trainign loss: 6.5476 avg training loss: 8.1150
batch: [17370/21305] batch time: 0.056 trainign loss: 6.6874 avg training loss: 8.1148
batch: [17380/21305] batch time: 1.565 trainign loss: 7.3644 avg training loss: 8.1148
batch: [17390/21305] batch time: 0.062 trainign loss: 7.6563 avg training loss: 8.1147
batch: [17400/21305] batch time: 0.715 trainign loss: 7.2950 avg training loss: 8.1146
batch: [17410/21305] batch time: 0.053 trainign loss: 7.1527 avg training loss: 8.1145
batch: [17420/21305] batch time: 0.722 trainign loss: 6.2440 avg training loss: 8.1144
batch: [17430/21305] batch time: 0.092 trainign loss: 5.8672 avg training loss: 8.1142
batch: [17440/21305] batch time: 1.858 trainign loss: 6.9855 avg training loss: 8.1140
batch: [17450/21305] batch time: 0.397 trainign loss: 6.4529 avg training loss: 8.1138
batch: [17460/21305] batch time: 1.322 trainign loss: 5.2621 avg training loss: 8.1135
batch: [17470/21305] batch time: 0.720 trainign loss: 7.3495 avg training loss: 8.1133
batch: [17480/21305] batch time: 1.258 trainign loss: 5.9847 avg training loss: 8.1131
batch: [17490/21305] batch time: 0.052 trainign loss: 6.9506 avg training loss: 8.1130
batch: [17500/21305] batch time: 1.759 trainign loss: 6.3940 avg training loss: 8.1128
batch: [17510/21305] batch time: 0.057 trainign loss: 6.6581 avg training loss: 8.1127
batch: [17520/21305] batch time: 1.272 trainign loss: 5.8069 avg training loss: 8.1124
batch: [17530/21305] batch time: 1.608 trainign loss: 5.4320 avg training loss: 8.1122
batch: [17540/21305] batch time: 0.056 trainign loss: 6.6435 avg training loss: 8.1120
batch: [17550/21305] batch time: 2.539 trainign loss: 6.8124 avg training loss: 8.1119
batch: [17560/21305] batch time: 0.061 trainign loss: 6.0293 avg training loss: 8.1116
batch: [17570/21305] batch time: 2.226 trainign loss: 7.2364 avg training loss: 8.1114
batch: [17580/21305] batch time: 0.063 trainign loss: 1.9803 avg training loss: 8.1110
batch: [17590/21305] batch time: 2.201 trainign loss: 11.2681 avg training loss: 8.1101
batch: [17600/21305] batch time: 0.057 trainign loss: 8.6906 avg training loss: 8.1100
batch: [17610/21305] batch time: 0.510 trainign loss: 8.8396 avg training loss: 8.1100
batch: [17620/21305] batch time: 0.057 trainign loss: 6.2100 avg training loss: 8.1099
batch: [17630/21305] batch time: 0.965 trainign loss: 7.1990 avg training loss: 8.1098
batch: [17640/21305] batch time: 0.056 trainign loss: 5.8336 avg training loss: 8.1097
batch: [17650/21305] batch time: 0.667 trainign loss: 5.9288 avg training loss: 8.1095
batch: [17660/21305] batch time: 0.056 trainign loss: 6.7060 avg training loss: 8.1094
batch: [17670/21305] batch time: 1.591 trainign loss: 6.2185 avg training loss: 8.1092
batch: [17680/21305] batch time: 0.058 trainign loss: 6.9567 avg training loss: 8.1090
batch: [17690/21305] batch time: 2.663 trainign loss: 6.0992 avg training loss: 8.1088
batch: [17700/21305] batch time: 0.061 trainign loss: 6.7161 avg training loss: 8.1085
batch: [17710/21305] batch time: 2.089 trainign loss: 6.9307 avg training loss: 8.1084
batch: [17720/21305] batch time: 0.057 trainign loss: 6.8192 avg training loss: 8.1083
batch: [17730/21305] batch time: 2.247 trainign loss: 7.1758 avg training loss: 8.1081
batch: [17740/21305] batch time: 0.268 trainign loss: 6.1642 avg training loss: 8.1079
batch: [17750/21305] batch time: 1.176 trainign loss: 6.9686 avg training loss: 8.1078
batch: [17760/21305] batch time: 0.844 trainign loss: 6.6874 avg training loss: 8.1077
batch: [17770/21305] batch time: 0.300 trainign loss: 7.1345 avg training loss: 8.1076
batch: [17780/21305] batch time: 1.343 trainign loss: 6.4185 avg training loss: 8.1074
batch: [17790/21305] batch time: 0.236 trainign loss: 7.4949 avg training loss: 8.1073
batch: [17800/21305] batch time: 1.569 trainign loss: 6.1666 avg training loss: 8.1071
batch: [17810/21305] batch time: 0.130 trainign loss: 7.2129 avg training loss: 8.1069
batch: [17820/21305] batch time: 0.778 trainign loss: 6.8541 avg training loss: 8.1068
batch: [17830/21305] batch time: 0.912 trainign loss: 5.9243 avg training loss: 8.1066
batch: [17840/21305] batch time: 0.141 trainign loss: 7.6010 avg training loss: 8.1065
batch: [17850/21305] batch time: 0.426 trainign loss: 7.1405 avg training loss: 8.1064
batch: [17860/21305] batch time: 0.056 trainign loss: 6.9836 avg training loss: 8.1062
batch: [17870/21305] batch time: 0.122 trainign loss: 6.1643 avg training loss: 8.1060
batch: [17880/21305] batch time: 0.190 trainign loss: 6.9014 avg training loss: 8.1059
batch: [17890/21305] batch time: 0.201 trainign loss: 6.9690 avg training loss: 8.1057
batch: [17900/21305] batch time: 1.325 trainign loss: 6.6355 avg training loss: 8.1055
batch: [17910/21305] batch time: 0.055 trainign loss: 3.7258 avg training loss: 8.1051
batch: [17920/21305] batch time: 1.275 trainign loss: 8.2252 avg training loss: 8.1051
batch: [17930/21305] batch time: 0.532 trainign loss: 6.2812 avg training loss: 8.1050
batch: [17940/21305] batch time: 0.433 trainign loss: 6.8760 avg training loss: 8.1048
batch: [17950/21305] batch time: 0.056 trainign loss: 7.2932 avg training loss: 8.1046
batch: [17960/21305] batch time: 1.665 trainign loss: 7.2289 avg training loss: 8.1044
batch: [17970/21305] batch time: 0.055 trainign loss: 6.1132 avg training loss: 8.1043
batch: [17980/21305] batch time: 2.019 trainign loss: 1.6247 avg training loss: 8.1038
batch: [17990/21305] batch time: 0.056 trainign loss: 5.1435 avg training loss: 8.1037
batch: [18000/21305] batch time: 2.477 trainign loss: 7.5448 avg training loss: 8.1035
batch: [18010/21305] batch time: 0.057 trainign loss: 8.4099 avg training loss: 8.1034
batch: [18020/21305] batch time: 1.940 trainign loss: 5.0959 avg training loss: 8.1032
batch: [18030/21305] batch time: 0.055 trainign loss: 7.0380 avg training loss: 8.1030
batch: [18040/21305] batch time: 1.904 trainign loss: 6.4172 avg training loss: 8.1029
batch: [18050/21305] batch time: 0.053 trainign loss: 7.0118 avg training loss: 8.1027
batch: [18060/21305] batch time: 2.395 trainign loss: 7.5481 avg training loss: 8.1026
batch: [18070/21305] batch time: 0.056 trainign loss: 7.6567 avg training loss: 8.1025
batch: [18080/21305] batch time: 2.013 trainign loss: 7.0565 avg training loss: 8.1023
batch: [18090/21305] batch time: 0.063 trainign loss: 5.7687 avg training loss: 8.1021
batch: [18100/21305] batch time: 1.930 trainign loss: 6.1465 avg training loss: 8.1019
batch: [18110/21305] batch time: 0.060 trainign loss: 5.7432 avg training loss: 8.1017
batch: [18120/21305] batch time: 2.005 trainign loss: 6.9436 avg training loss: 8.1014
batch: [18130/21305] batch time: 0.056 trainign loss: 7.5960 avg training loss: 8.1013
batch: [18140/21305] batch time: 2.226 trainign loss: 7.8764 avg training loss: 8.1011
batch: [18150/21305] batch time: 0.056 trainign loss: 7.8460 avg training loss: 8.1011
batch: [18160/21305] batch time: 1.806 trainign loss: 6.0950 avg training loss: 8.1009
batch: [18170/21305] batch time: 0.057 trainign loss: 5.9916 avg training loss: 8.1007
batch: [18180/21305] batch time: 2.261 trainign loss: 0.1308 avg training loss: 8.1001
batch: [18190/21305] batch time: 0.054 trainign loss: 7.9523 avg training loss: 8.1001
batch: [18200/21305] batch time: 2.097 trainign loss: 8.0800 avg training loss: 8.1001
batch: [18210/21305] batch time: 0.056 trainign loss: 7.1176 avg training loss: 8.1000
batch: [18220/21305] batch time: 1.808 trainign loss: 4.7033 avg training loss: 8.0998
batch: [18230/21305] batch time: 0.062 trainign loss: 7.9461 avg training loss: 8.0996
batch: [18240/21305] batch time: 1.762 trainign loss: 6.4711 avg training loss: 8.0995
batch: [18250/21305] batch time: 0.056 trainign loss: 6.2133 avg training loss: 8.0994
batch: [18260/21305] batch time: 0.933 trainign loss: 2.5665 avg training loss: 8.0990
batch: [18270/21305] batch time: 0.056 trainign loss: 6.3445 avg training loss: 8.0989
batch: [18280/21305] batch time: 0.443 trainign loss: 6.6148 avg training loss: 8.0986
batch: [18290/21305] batch time: 0.056 trainign loss: 7.7067 avg training loss: 8.0985
batch: [18300/21305] batch time: 0.353 trainign loss: 6.8514 avg training loss: 8.0985
batch: [18310/21305] batch time: 0.056 trainign loss: 7.1923 avg training loss: 8.0983
batch: [18320/21305] batch time: 0.875 trainign loss: 5.4290 avg training loss: 8.0982
batch: [18330/21305] batch time: 0.056 trainign loss: 6.3793 avg training loss: 8.0979
batch: [18340/21305] batch time: 1.239 trainign loss: 7.4140 avg training loss: 8.0979
batch: [18350/21305] batch time: 0.056 trainign loss: 7.4125 avg training loss: 8.0977
batch: [18360/21305] batch time: 0.906 trainign loss: 6.0583 avg training loss: 8.0975
batch: [18370/21305] batch time: 0.062 trainign loss: 7.3231 avg training loss: 8.0973
batch: [18380/21305] batch time: 1.041 trainign loss: 6.7869 avg training loss: 8.0972
batch: [18390/21305] batch time: 0.056 trainign loss: 6.4046 avg training loss: 8.0969
batch: [18400/21305] batch time: 0.644 trainign loss: 7.5044 avg training loss: 8.0969
batch: [18410/21305] batch time: 0.056 trainign loss: 4.9531 avg training loss: 8.0966
batch: [18420/21305] batch time: 1.179 trainign loss: 6.7565 avg training loss: 8.0965
batch: [18430/21305] batch time: 0.056 trainign loss: 6.8967 avg training loss: 8.0963
batch: [18440/21305] batch time: 1.477 trainign loss: 0.1071 avg training loss: 8.0957
batch: [18450/21305] batch time: 0.056 trainign loss: 7.4805 avg training loss: 8.0954
batch: [18460/21305] batch time: 1.481 trainign loss: 7.8820 avg training loss: 8.0954
batch: [18470/21305] batch time: 0.197 trainign loss: 7.8484 avg training loss: 8.0954
batch: [18480/21305] batch time: 1.392 trainign loss: 6.4253 avg training loss: 8.0952
batch: [18490/21305] batch time: 0.062 trainign loss: 4.4406 avg training loss: 8.0949
batch: [18500/21305] batch time: 1.406 trainign loss: 0.0059 avg training loss: 8.0940
batch: [18510/21305] batch time: 0.056 trainign loss: 8.7141 avg training loss: 8.0940
batch: [18520/21305] batch time: 0.590 trainign loss: 8.9725 avg training loss: 8.0940
batch: [18530/21305] batch time: 0.056 trainign loss: 6.9683 avg training loss: 8.0940
batch: [18540/21305] batch time: 0.762 trainign loss: 6.7150 avg training loss: 8.0939
batch: [18550/21305] batch time: 0.619 trainign loss: 4.2280 avg training loss: 8.0936
batch: [18560/21305] batch time: 0.706 trainign loss: 7.4507 avg training loss: 8.0934
batch: [18570/21305] batch time: 0.141 trainign loss: 7.3958 avg training loss: 8.0933
batch: [18580/21305] batch time: 0.818 trainign loss: 7.0723 avg training loss: 8.0931
batch: [18590/21305] batch time: 0.544 trainign loss: 7.1479 avg training loss: 8.0929
batch: [18600/21305] batch time: 2.106 trainign loss: 5.8607 avg training loss: 8.0927
batch: [18610/21305] batch time: 0.056 trainign loss: 6.9176 avg training loss: 8.0924
batch: [18620/21305] batch time: 2.351 trainign loss: 8.0314 avg training loss: 8.0923
batch: [18630/21305] batch time: 0.054 trainign loss: 6.4699 avg training loss: 8.0921
batch: [18640/21305] batch time: 2.131 trainign loss: 5.6681 avg training loss: 8.0920
batch: [18650/21305] batch time: 0.063 trainign loss: 3.9852 avg training loss: 8.0917
batch: [18660/21305] batch time: 2.240 trainign loss: 0.0044 avg training loss: 8.0908
batch: [18670/21305] batch time: 0.057 trainign loss: 8.4140 avg training loss: 8.0906
batch: [18680/21305] batch time: 2.461 trainign loss: 8.1406 avg training loss: 8.0906
batch: [18690/21305] batch time: 0.056 trainign loss: 8.3693 avg training loss: 8.0907
batch: [18700/21305] batch time: 2.268 trainign loss: 7.4950 avg training loss: 8.0906
batch: [18710/21305] batch time: 0.059 trainign loss: 7.1180 avg training loss: 8.0905
batch: [18720/21305] batch time: 2.333 trainign loss: 6.3317 avg training loss: 8.0903
batch: [18730/21305] batch time: 0.057 trainign loss: 5.6517 avg training loss: 8.0901
batch: [18740/21305] batch time: 2.468 trainign loss: 7.7673 avg training loss: 8.0900
batch: [18750/21305] batch time: 0.062 trainign loss: 5.9143 avg training loss: 8.0898
batch: [18760/21305] batch time: 2.447 trainign loss: 5.3457 avg training loss: 8.0896
batch: [18770/21305] batch time: 0.055 trainign loss: 7.7972 avg training loss: 8.0893
batch: [18780/21305] batch time: 2.140 trainign loss: 7.2992 avg training loss: 8.0891
batch: [18790/21305] batch time: 0.063 trainign loss: 6.2766 avg training loss: 8.0890
batch: [18800/21305] batch time: 2.318 trainign loss: 7.7710 avg training loss: 8.0889
batch: [18810/21305] batch time: 0.056 trainign loss: 7.4614 avg training loss: 8.0888
batch: [18820/21305] batch time: 2.480 trainign loss: 5.3944 avg training loss: 8.0887
batch: [18830/21305] batch time: 0.063 trainign loss: 7.7781 avg training loss: 8.0885
batch: [18840/21305] batch time: 2.252 trainign loss: 6.6320 avg training loss: 8.0884
batch: [18850/21305] batch time: 0.056 trainign loss: 7.5381 avg training loss: 8.0881
batch: [18860/21305] batch time: 2.502 trainign loss: 7.0067 avg training loss: 8.0880
batch: [18870/21305] batch time: 0.056 trainign loss: 6.9314 avg training loss: 8.0878
batch: [18880/21305] batch time: 2.577 trainign loss: 4.7623 avg training loss: 8.0874
batch: [18890/21305] batch time: 0.061 trainign loss: 7.3634 avg training loss: 8.0874
batch: [18900/21305] batch time: 2.099 trainign loss: 6.1584 avg training loss: 8.0873
batch: [18910/21305] batch time: 0.056 trainign loss: 6.9702 avg training loss: 8.0871
batch: [18920/21305] batch time: 2.329 trainign loss: 7.4149 avg training loss: 8.0870
batch: [18930/21305] batch time: 0.063 trainign loss: 7.7725 avg training loss: 8.0868
batch: [18940/21305] batch time: 2.139 trainign loss: 6.8675 avg training loss: 8.0867
batch: [18950/21305] batch time: 0.056 trainign loss: 7.2326 avg training loss: 8.0866
batch: [18960/21305] batch time: 2.085 trainign loss: 7.0004 avg training loss: 8.0863
batch: [18970/21305] batch time: 0.331 trainign loss: 6.6449 avg training loss: 8.0861
batch: [18980/21305] batch time: 2.023 trainign loss: 8.5603 avg training loss: 8.0858
batch: [18990/21305] batch time: 0.604 trainign loss: 7.6148 avg training loss: 8.0857
batch: [19000/21305] batch time: 1.916 trainign loss: 6.0374 avg training loss: 8.0855
batch: [19010/21305] batch time: 0.319 trainign loss: 7.9068 avg training loss: 8.0853
batch: [19020/21305] batch time: 1.561 trainign loss: 6.9351 avg training loss: 8.0852
batch: [19030/21305] batch time: 1.678 trainign loss: 7.6708 avg training loss: 8.0850
batch: [19040/21305] batch time: 0.597 trainign loss: 7.1354 avg training loss: 8.0849
batch: [19050/21305] batch time: 1.406 trainign loss: 7.7685 avg training loss: 8.0847
batch: [19060/21305] batch time: 1.231 trainign loss: 5.9575 avg training loss: 8.0844
batch: [19070/21305] batch time: 1.249 trainign loss: 7.2084 avg training loss: 8.0842
batch: [19080/21305] batch time: 1.298 trainign loss: 6.7561 avg training loss: 8.0841
batch: [19090/21305] batch time: 1.511 trainign loss: 8.1934 avg training loss: 8.0840
batch: [19100/21305] batch time: 1.015 trainign loss: 7.7609 avg training loss: 8.0839
batch: [19110/21305] batch time: 1.655 trainign loss: 4.2038 avg training loss: 8.0837
batch: [19120/21305] batch time: 0.600 trainign loss: 6.9200 avg training loss: 8.0836
batch: [19130/21305] batch time: 1.612 trainign loss: 7.6228 avg training loss: 8.0835
batch: [19140/21305] batch time: 0.814 trainign loss: 6.4334 avg training loss: 8.0833
batch: [19150/21305] batch time: 1.431 trainign loss: 6.2823 avg training loss: 8.0831
batch: [19160/21305] batch time: 0.614 trainign loss: 7.0659 avg training loss: 8.0830
batch: [19170/21305] batch time: 1.710 trainign loss: 6.8527 avg training loss: 8.0828
batch: [19180/21305] batch time: 0.056 trainign loss: 5.1049 avg training loss: 8.0827
batch: [19190/21305] batch time: 2.364 trainign loss: 6.9421 avg training loss: 8.0824
batch: [19200/21305] batch time: 0.057 trainign loss: 4.5573 avg training loss: 8.0821
batch: [19210/21305] batch time: 2.162 trainign loss: 6.8480 avg training loss: 8.0820
batch: [19220/21305] batch time: 0.186 trainign loss: 7.3217 avg training loss: 8.0817
batch: [19230/21305] batch time: 2.024 trainign loss: 6.5191 avg training loss: 8.0816
batch: [19240/21305] batch time: 0.063 trainign loss: 6.5910 avg training loss: 8.0815
batch: [19250/21305] batch time: 2.352 trainign loss: 3.7360 avg training loss: 8.0812
batch: [19260/21305] batch time: 0.056 trainign loss: 5.2836 avg training loss: 8.0809
batch: [19270/21305] batch time: 2.044 trainign loss: 7.5724 avg training loss: 8.0808
batch: [19280/21305] batch time: 0.062 trainign loss: 8.4701 avg training loss: 8.0807
batch: [19290/21305] batch time: 1.213 trainign loss: 7.2260 avg training loss: 8.0806
batch: [19300/21305] batch time: 0.063 trainign loss: 7.5842 avg training loss: 8.0805
batch: [19310/21305] batch time: 0.978 trainign loss: 7.7332 avg training loss: 8.0804
batch: [19320/21305] batch time: 0.063 trainign loss: 7.6713 avg training loss: 8.0803
batch: [19330/21305] batch time: 1.222 trainign loss: 6.9636 avg training loss: 8.0802
batch: [19340/21305] batch time: 0.522 trainign loss: 5.5545 avg training loss: 8.0800
batch: [19350/21305] batch time: 0.997 trainign loss: 7.2786 avg training loss: 8.0799
batch: [19360/21305] batch time: 0.370 trainign loss: 6.6441 avg training loss: 8.0797
batch: [19370/21305] batch time: 0.422 trainign loss: 6.9181 avg training loss: 8.0796
batch: [19380/21305] batch time: 0.189 trainign loss: 6.7493 avg training loss: 8.0793
batch: [19390/21305] batch time: 0.085 trainign loss: 8.1242 avg training loss: 8.0792
batch: [19400/21305] batch time: 0.677 trainign loss: 7.3601 avg training loss: 8.0792
batch: [19410/21305] batch time: 0.057 trainign loss: 5.4713 avg training loss: 8.0790
batch: [19420/21305] batch time: 0.052 trainign loss: 6.9122 avg training loss: 8.0788
batch: [19430/21305] batch time: 0.062 trainign loss: 7.1554 avg training loss: 8.0786
batch: [19440/21305] batch time: 0.054 trainign loss: 7.5126 avg training loss: 8.0785
batch: [19450/21305] batch time: 0.302 trainign loss: 7.2193 avg training loss: 8.0784
batch: [19460/21305] batch time: 0.062 trainign loss: 6.6464 avg training loss: 8.0783
batch: [19470/21305] batch time: 0.356 trainign loss: 7.4033 avg training loss: 8.0782
batch: [19480/21305] batch time: 0.056 trainign loss: 6.1386 avg training loss: 8.0780
batch: [19490/21305] batch time: 1.495 trainign loss: 5.3404 avg training loss: 8.0778
batch: [19500/21305] batch time: 0.056 trainign loss: 7.2109 avg training loss: 8.0777
batch: [19510/21305] batch time: 1.694 trainign loss: 6.4760 avg training loss: 8.0776
batch: [19520/21305] batch time: 0.062 trainign loss: 6.9386 avg training loss: 8.0775
batch: [19530/21305] batch time: 2.040 trainign loss: 7.2542 avg training loss: 8.0773
batch: [19540/21305] batch time: 0.062 trainign loss: 6.7235 avg training loss: 8.0770
batch: [19550/21305] batch time: 1.908 trainign loss: 6.6015 avg training loss: 8.0768
batch: [19560/21305] batch time: 0.056 trainign loss: 7.2542 avg training loss: 8.0766
batch: [19570/21305] batch time: 1.525 trainign loss: 7.0539 avg training loss: 8.0765
batch: [19580/21305] batch time: 0.056 trainign loss: 5.6480 avg training loss: 8.0763
batch: [19590/21305] batch time: 1.340 trainign loss: 6.0757 avg training loss: 8.0760
batch: [19600/21305] batch time: 0.056 trainign loss: 7.0483 avg training loss: 8.0758
batch: [19610/21305] batch time: 1.776 trainign loss: 6.6941 avg training loss: 8.0757
batch: [19620/21305] batch time: 0.056 trainign loss: 6.9243 avg training loss: 8.0755
batch: [19630/21305] batch time: 1.632 trainign loss: 7.2450 avg training loss: 8.0754
batch: [19640/21305] batch time: 0.058 trainign loss: 7.6331 avg training loss: 8.0753
batch: [19650/21305] batch time: 2.015 trainign loss: 6.8785 avg training loss: 8.0751
batch: [19660/21305] batch time: 0.056 trainign loss: 6.8533 avg training loss: 8.0750
batch: [19670/21305] batch time: 2.401 trainign loss: 6.8584 avg training loss: 8.0748
batch: [19680/21305] batch time: 0.056 trainign loss: 0.0545 avg training loss: 8.0741
batch: [19690/21305] batch time: 2.456 trainign loss: 7.9827 avg training loss: 8.0741
batch: [19700/21305] batch time: 0.056 trainign loss: 8.5857 avg training loss: 8.0741
batch: [19710/21305] batch time: 2.164 trainign loss: 7.1860 avg training loss: 8.0740
batch: [19720/21305] batch time: 0.055 trainign loss: 6.6219 avg training loss: 8.0738
batch: [19730/21305] batch time: 2.418 trainign loss: 5.9801 avg training loss: 8.0737
batch: [19740/21305] batch time: 0.055 trainign loss: 5.9689 avg training loss: 8.0735
batch: [19750/21305] batch time: 2.168 trainign loss: 7.5885 avg training loss: 8.0733
batch: [19760/21305] batch time: 0.058 trainign loss: 6.5833 avg training loss: 8.0732
batch: [19770/21305] batch time: 2.215 trainign loss: 6.1285 avg training loss: 8.0731
batch: [19780/21305] batch time: 0.060 trainign loss: 5.2694 avg training loss: 8.0728
batch: [19790/21305] batch time: 2.497 trainign loss: 6.5521 avg training loss: 8.0726
batch: [19800/21305] batch time: 0.057 trainign loss: 6.2413 avg training loss: 8.0725
batch: [19810/21305] batch time: 1.717 trainign loss: 5.5975 avg training loss: 8.0722
batch: [19820/21305] batch time: 0.058 trainign loss: 7.0614 avg training loss: 8.0720
batch: [19830/21305] batch time: 2.295 trainign loss: 7.3241 avg training loss: 8.0719
batch: [19840/21305] batch time: 0.057 trainign loss: 6.6557 avg training loss: 8.0717
batch: [19850/21305] batch time: 1.843 trainign loss: 6.9973 avg training loss: 8.0715
batch: [19860/21305] batch time: 0.062 trainign loss: 7.7944 avg training loss: 8.0714
batch: [19870/21305] batch time: 1.078 trainign loss: 7.3324 avg training loss: 8.0713
batch: [19880/21305] batch time: 0.056 trainign loss: 5.4465 avg training loss: 8.0711
batch: [19890/21305] batch time: 0.053 trainign loss: 5.3379 avg training loss: 8.0709
batch: [19900/21305] batch time: 0.061 trainign loss: 9.5719 avg training loss: 8.0705
batch: [19910/21305] batch time: 0.057 trainign loss: 1.7240 avg training loss: 8.0698
batch: [19920/21305] batch time: 0.056 trainign loss: 7.9019 avg training loss: 8.0699
batch: [19930/21305] batch time: 0.051 trainign loss: 8.3569 avg training loss: 8.0700
batch: [19940/21305] batch time: 0.056 trainign loss: 6.5076 avg training loss: 8.0698
batch: [19950/21305] batch time: 0.056 trainign loss: 7.2206 avg training loss: 8.0696
batch: [19960/21305] batch time: 0.057 trainign loss: 7.2083 avg training loss: 8.0693
batch: [19970/21305] batch time: 0.051 trainign loss: 7.2478 avg training loss: 8.0692
batch: [19980/21305] batch time: 0.056 trainign loss: 7.7222 avg training loss: 8.0691
batch: [19990/21305] batch time: 0.057 trainign loss: 6.3678 avg training loss: 8.0690
batch: [20000/21305] batch time: 0.056 trainign loss: 7.0951 avg training loss: 8.0687
batch: [20010/21305] batch time: 0.056 trainign loss: 6.0188 avg training loss: 8.0686
batch: [20020/21305] batch time: 0.057 trainign loss: 1.1195 avg training loss: 8.0682
batch: [20030/21305] batch time: 0.056 trainign loss: 7.3582 avg training loss: 8.0679
batch: [20040/21305] batch time: 0.058 trainign loss: 5.8190 avg training loss: 8.0676
batch: [20050/21305] batch time: 0.056 trainign loss: 3.4655 avg training loss: 8.0674
batch: [20060/21305] batch time: 0.057 trainign loss: 3.9677 avg training loss: 8.0670
batch: [20070/21305] batch time: 0.058 trainign loss: 7.3861 avg training loss: 8.0669
batch: [20080/21305] batch time: 0.057 trainign loss: 7.0944 avg training loss: 8.0669
batch: [20090/21305] batch time: 0.062 trainign loss: 4.7852 avg training loss: 8.0667
batch: [20100/21305] batch time: 0.056 trainign loss: 11.0560 avg training loss: 8.0661
batch: [20110/21305] batch time: 0.054 trainign loss: 6.8003 avg training loss: 8.0661
batch: [20120/21305] batch time: 0.056 trainign loss: 8.0251 avg training loss: 8.0661
batch: [20130/21305] batch time: 0.052 trainign loss: 7.2855 avg training loss: 8.0660
batch: [20140/21305] batch time: 0.062 trainign loss: 7.2224 avg training loss: 8.0659
batch: [20150/21305] batch time: 0.055 trainign loss: 6.2690 avg training loss: 8.0658
batch: [20160/21305] batch time: 0.056 trainign loss: 7.4744 avg training loss: 8.0656
batch: [20170/21305] batch time: 0.051 trainign loss: 7.5541 avg training loss: 8.0655
batch: [20180/21305] batch time: 0.057 trainign loss: 6.5494 avg training loss: 8.0654
batch: [20190/21305] batch time: 0.053 trainign loss: 3.7239 avg training loss: 8.0651
batch: [20200/21305] batch time: 0.057 trainign loss: 5.4281 avg training loss: 8.0649
batch: [20210/21305] batch time: 0.058 trainign loss: 7.7109 avg training loss: 8.0647
batch: [20220/21305] batch time: 0.061 trainign loss: 7.5582 avg training loss: 8.0646
batch: [20230/21305] batch time: 0.063 trainign loss: 6.7156 avg training loss: 8.0644
batch: [20240/21305] batch time: 0.060 trainign loss: 5.4774 avg training loss: 8.0642
batch: [20250/21305] batch time: 0.057 trainign loss: 7.5734 avg training loss: 8.0640
batch: [20260/21305] batch time: 0.062 trainign loss: 7.4303 avg training loss: 8.0639
batch: [20270/21305] batch time: 0.052 trainign loss: 1.2010 avg training loss: 8.0635
batch: [20280/21305] batch time: 0.051 trainign loss: 7.5738 avg training loss: 8.0633
batch: [20290/21305] batch time: 0.052 trainign loss: 7.5372 avg training loss: 8.0633
batch: [20300/21305] batch time: 0.069 trainign loss: 7.4524 avg training loss: 8.0631
batch: [20310/21305] batch time: 0.052 trainign loss: 7.1871 avg training loss: 8.0630
batch: [20320/21305] batch time: 0.062 trainign loss: 6.7000 avg training loss: 8.0629
batch: [20330/21305] batch time: 0.056 trainign loss: 4.9554 avg training loss: 8.0626
batch: [20340/21305] batch time: 0.062 trainign loss: 7.6793 avg training loss: 8.0624
batch: [20350/21305] batch time: 0.057 trainign loss: 7.1391 avg training loss: 8.0623
batch: [20360/21305] batch time: 0.056 trainign loss: 7.4930 avg training loss: 8.0621
batch: [20370/21305] batch time: 0.056 trainign loss: 7.3962 avg training loss: 8.0619
batch: [20380/21305] batch time: 0.056 trainign loss: 6.6238 avg training loss: 8.0618
batch: [20390/21305] batch time: 0.061 trainign loss: 7.3084 avg training loss: 8.0617
batch: [20400/21305] batch time: 0.056 trainign loss: 6.9406 avg training loss: 8.0615
batch: [20410/21305] batch time: 0.050 trainign loss: 6.9105 avg training loss: 8.0613
batch: [20420/21305] batch time: 0.062 trainign loss: 6.7655 avg training loss: 8.0612
batch: [20430/21305] batch time: 0.061 trainign loss: 7.2393 avg training loss: 8.0609
batch: [20440/21305] batch time: 0.056 trainign loss: 6.7720 avg training loss: 8.0606
batch: [20450/21305] batch time: 0.060 trainign loss: 6.0868 avg training loss: 8.0606
batch: [20460/21305] batch time: 0.056 trainign loss: 7.7879 avg training loss: 8.0605
batch: [20470/21305] batch time: 0.056 trainign loss: 5.0384 avg training loss: 8.0603
batch: [20480/21305] batch time: 0.056 trainign loss: 0.0651 avg training loss: 8.0596
batch: [20490/21305] batch time: 0.051 trainign loss: 14.6507 avg training loss: 8.0590
batch: [20500/21305] batch time: 0.056 trainign loss: 9.0114 avg training loss: 8.0592
batch: [20510/21305] batch time: 0.053 trainign loss: 8.1511 avg training loss: 8.0592
batch: [20520/21305] batch time: 0.057 trainign loss: 7.2837 avg training loss: 8.0592
batch: [20530/21305] batch time: 0.056 trainign loss: 6.7813 avg training loss: 8.0590
batch: [20540/21305] batch time: 0.056 trainign loss: 8.8103 avg training loss: 8.0586
batch: [20550/21305] batch time: 0.051 trainign loss: 7.2182 avg training loss: 8.0586
batch: [20560/21305] batch time: 0.841 trainign loss: 6.7826 avg training loss: 8.0583
batch: [20570/21305] batch time: 0.056 trainign loss: 7.8112 avg training loss: 8.0583
batch: [20580/21305] batch time: 0.433 trainign loss: 6.9664 avg training loss: 8.0582
batch: [20590/21305] batch time: 0.052 trainign loss: 6.6543 avg training loss: 8.0580
batch: [20600/21305] batch time: 0.542 trainign loss: 6.9884 avg training loss: 8.0579
batch: [20610/21305] batch time: 0.054 trainign loss: 7.6421 avg training loss: 8.0578
batch: [20620/21305] batch time: 0.057 trainign loss: 7.2435 avg training loss: 8.0577
batch: [20630/21305] batch time: 0.057 trainign loss: 7.0717 avg training loss: 8.0576
batch: [20640/21305] batch time: 0.617 trainign loss: 7.2207 avg training loss: 8.0575
batch: [20650/21305] batch time: 0.056 trainign loss: 7.3605 avg training loss: 8.0574
batch: [20660/21305] batch time: 0.057 trainign loss: 6.1136 avg training loss: 8.0572
batch: [20670/21305] batch time: 0.051 trainign loss: 7.9239 avg training loss: 8.0571
batch: [20680/21305] batch time: 0.056 trainign loss: 5.8660 avg training loss: 8.0569
batch: [20690/21305] batch time: 0.059 trainign loss: 7.1808 avg training loss: 8.0568
batch: [20700/21305] batch time: 0.062 trainign loss: 7.4785 avg training loss: 8.0566
batch: [20710/21305] batch time: 0.051 trainign loss: 7.2380 avg training loss: 8.0565
batch: [20720/21305] batch time: 0.055 trainign loss: 7.2997 avg training loss: 8.0564
batch: [20730/21305] batch time: 0.053 trainign loss: 6.9389 avg training loss: 8.0563
batch: [20740/21305] batch time: 0.870 trainign loss: 7.0107 avg training loss: 8.0562
batch: [20750/21305] batch time: 0.057 trainign loss: 5.9691 avg training loss: 8.0561
batch: [20760/21305] batch time: 0.056 trainign loss: 7.6201 avg training loss: 8.0560
batch: [20770/21305] batch time: 0.050 trainign loss: 6.9687 avg training loss: 8.0559
batch: [20780/21305] batch time: 0.288 trainign loss: 7.0540 avg training loss: 8.0558
batch: [20790/21305] batch time: 0.056 trainign loss: 6.2475 avg training loss: 8.0556
batch: [20800/21305] batch time: 0.110 trainign loss: 6.6614 avg training loss: 8.0554
batch: [20810/21305] batch time: 0.056 trainign loss: 7.4297 avg training loss: 8.0553
batch: [20820/21305] batch time: 0.686 trainign loss: 6.6105 avg training loss: 8.0552
batch: [20830/21305] batch time: 0.056 trainign loss: 6.8197 avg training loss: 8.0551
batch: [20840/21305] batch time: 1.332 trainign loss: 5.9956 avg training loss: 8.0549
batch: [20850/21305] batch time: 0.062 trainign loss: 6.3362 avg training loss: 8.0546
batch: [20860/21305] batch time: 1.163 trainign loss: 6.6748 avg training loss: 8.0545
batch: [20870/21305] batch time: 0.062 trainign loss: 6.3320 avg training loss: 8.0543
batch: [20880/21305] batch time: 1.332 trainign loss: 6.1130 avg training loss: 8.0541
batch: [20890/21305] batch time: 0.057 trainign loss: 7.6788 avg training loss: 8.0540
batch: [20900/21305] batch time: 1.811 trainign loss: 4.9928 avg training loss: 8.0538
batch: [20910/21305] batch time: 0.056 trainign loss: 6.2527 avg training loss: 8.0535
batch: [20920/21305] batch time: 0.962 trainign loss: 4.9558 avg training loss: 8.0533
batch: [20930/21305] batch time: 0.056 trainign loss: 8.3887 avg training loss: 8.0530
batch: [20940/21305] batch time: 0.061 trainign loss: 2.3543 avg training loss: 8.0525
batch: [20950/21305] batch time: 0.056 trainign loss: 7.8097 avg training loss: 8.0526
batch: [20960/21305] batch time: 0.053 trainign loss: 7.5379 avg training loss: 8.0526
batch: [20970/21305] batch time: 0.062 trainign loss: 5.9277 avg training loss: 8.0525
batch: [20980/21305] batch time: 0.055 trainign loss: 7.6130 avg training loss: 8.0524
batch: [20990/21305] batch time: 0.058 trainign loss: 7.6574 avg training loss: 8.0523
batch: [21000/21305] batch time: 0.056 trainign loss: 6.6521 avg training loss: 8.0522
batch: [21010/21305] batch time: 0.061 trainign loss: 7.0092 avg training loss: 8.0521
batch: [21020/21305] batch time: 0.052 trainign loss: 6.4980 avg training loss: 8.0519
batch: [21030/21305] batch time: 0.056 trainign loss: 7.2646 avg training loss: 8.0518
batch: [21040/21305] batch time: 0.063 trainign loss: 7.7558 avg training loss: 8.0517
batch: [21050/21305] batch time: 0.058 trainign loss: 6.7555 avg training loss: 8.0516
batch: [21060/21305] batch time: 0.056 trainign loss: 6.5447 avg training loss: 8.0514
batch: [21070/21305] batch time: 0.063 trainign loss: 6.4323 avg training loss: 8.0512
batch: [21080/21305] batch time: 0.063 trainign loss: 7.0949 avg training loss: 8.0509
batch: [21090/21305] batch time: 0.061 trainign loss: 7.7551 avg training loss: 8.0508
batch: [21100/21305] batch time: 0.051 trainign loss: 4.8101 avg training loss: 8.0505
batch: [21110/21305] batch time: 0.057 trainign loss: 7.9414 avg training loss: 8.0504
batch: [21120/21305] batch time: 0.057 trainign loss: 7.9363 avg training loss: 8.0503
batch: [21130/21305] batch time: 0.057 trainign loss: 7.5819 avg training loss: 8.0502
batch: [21140/21305] batch time: 0.058 trainign loss: 6.9843 avg training loss: 8.0500
batch: [21150/21305] batch time: 0.063 trainign loss: 7.2529 avg training loss: 8.0498
batch: [21160/21305] batch time: 0.050 trainign loss: 7.0694 avg training loss: 8.0496
batch: [21170/21305] batch time: 0.056 trainign loss: 4.9910 avg training loss: 8.0492
batch: [21180/21305] batch time: 0.056 trainign loss: 7.8731 avg training loss: 8.0492
batch: [21190/21305] batch time: 0.056 trainign loss: 7.6120 avg training loss: 8.0492
batch: [21200/21305] batch time: 0.055 trainign loss: 7.0541 avg training loss: 8.0490
batch: [21210/21305] batch time: 0.057 trainign loss: 6.9530 avg training loss: 8.0489
batch: [21220/21305] batch time: 0.209 trainign loss: 6.5150 avg training loss: 8.0488
batch: [21230/21305] batch time: 0.056 trainign loss: 6.9316 avg training loss: 8.0486
batch: [21240/21305] batch time: 0.050 trainign loss: 5.6470 avg training loss: 8.0484
batch: [21250/21305] batch time: 0.056 trainign loss: 7.1984 avg training loss: 8.0483
batch: [21260/21305] batch time: 0.883 trainign loss: 7.4855 avg training loss: 8.0482
batch: [21270/21305] batch time: 0.056 trainign loss: 3.6625 avg training loss: 8.0479
batch: [21280/21305] batch time: 1.309 trainign loss: 6.3407 avg training loss: 8.0477
batch: [21290/21305] batch time: 0.056 trainign loss: 5.0671 avg training loss: 8.0474
batch: [21300/21305] batch time: 0.053 trainign loss: 7.7312 avg training loss: 8.0473
Epoch: 5
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.627 trainign loss: 7.3403 avg training loss: 8.0472
batch: [10/21305] batch time: 0.051 trainign loss: 9.0152 avg training loss: 8.0468
batch: [20/21305] batch time: 2.136 trainign loss: 8.0656 avg training loss: 8.0468
batch: [30/21305] batch time: 0.056 trainign loss: 7.6525 avg training loss: 8.0467
batch: [40/21305] batch time: 1.986 trainign loss: 7.5685 avg training loss: 8.0466
batch: [50/21305] batch time: 0.056 trainign loss: 4.2423 avg training loss: 8.0464
batch: [60/21305] batch time: 1.650 trainign loss: 7.6426 avg training loss: 8.0463
batch: [70/21305] batch time: 0.062 trainign loss: 7.7594 avg training loss: 8.0461
batch: [80/21305] batch time: 0.823 trainign loss: 7.0231 avg training loss: 8.0460
batch: [90/21305] batch time: 0.056 trainign loss: 7.2249 avg training loss: 8.0459
batch: [100/21305] batch time: 0.974 trainign loss: 7.1992 avg training loss: 8.0457
batch: [110/21305] batch time: 0.062 trainign loss: 6.6251 avg training loss: 8.0456
batch: [120/21305] batch time: 0.982 trainign loss: 6.9245 avg training loss: 8.0454
batch: [130/21305] batch time: 0.056 trainign loss: 7.5402 avg training loss: 8.0452
batch: [140/21305] batch time: 1.220 trainign loss: 5.7489 avg training loss: 8.0451
batch: [150/21305] batch time: 0.057 trainign loss: 7.9441 avg training loss: 8.0449
batch: [160/21305] batch time: 2.041 trainign loss: 7.3182 avg training loss: 8.0448
batch: [170/21305] batch time: 0.051 trainign loss: 6.9912 avg training loss: 8.0447
batch: [180/21305] batch time: 1.840 trainign loss: 6.5372 avg training loss: 8.0445
batch: [190/21305] batch time: 0.063 trainign loss: 5.5746 avg training loss: 8.0443
batch: [200/21305] batch time: 1.591 trainign loss: 0.0084 avg training loss: 8.0435
batch: [210/21305] batch time: 0.060 trainign loss: 8.7299 avg training loss: 8.0434
batch: [220/21305] batch time: 1.989 trainign loss: 8.0870 avg training loss: 8.0435
batch: [230/21305] batch time: 0.056 trainign loss: 7.1122 avg training loss: 8.0434
batch: [240/21305] batch time: 1.917 trainign loss: 7.9351 avg training loss: 8.0434
batch: [250/21305] batch time: 0.061 trainign loss: 7.7252 avg training loss: 8.0433
batch: [260/21305] batch time: 1.831 trainign loss: 7.0993 avg training loss: 8.0432
batch: [270/21305] batch time: 0.057 trainign loss: 5.1701 avg training loss: 8.0430
batch: [280/21305] batch time: 2.295 trainign loss: 6.3927 avg training loss: 8.0428
batch: [290/21305] batch time: 0.056 trainign loss: 7.4637 avg training loss: 8.0426
batch: [300/21305] batch time: 1.125 trainign loss: 7.4251 avg training loss: 8.0425
batch: [310/21305] batch time: 0.062 trainign loss: 6.5392 avg training loss: 8.0424
batch: [320/21305] batch time: 1.075 trainign loss: 7.5405 avg training loss: 8.0423
batch: [330/21305] batch time: 0.061 trainign loss: 6.8908 avg training loss: 8.0421
batch: [340/21305] batch time: 1.076 trainign loss: 7.8936 avg training loss: 8.0420
batch: [350/21305] batch time: 0.055 trainign loss: 8.1496 avg training loss: 8.0420
batch: [360/21305] batch time: 0.835 trainign loss: 7.6152 avg training loss: 8.0418
batch: [370/21305] batch time: 0.056 trainign loss: 6.3360 avg training loss: 8.0417
batch: [380/21305] batch time: 0.278 trainign loss: 6.7909 avg training loss: 8.0416
batch: [390/21305] batch time: 0.565 trainign loss: 4.5953 avg training loss: 8.0413
batch: [400/21305] batch time: 1.174 trainign loss: 6.9936 avg training loss: 8.0411
batch: [410/21305] batch time: 0.829 trainign loss: 8.1210 avg training loss: 8.0410
batch: [420/21305] batch time: 1.419 trainign loss: 7.6813 avg training loss: 8.0409
batch: [430/21305] batch time: 0.504 trainign loss: 7.3199 avg training loss: 8.0409
batch: [440/21305] batch time: 0.828 trainign loss: 6.6672 avg training loss: 8.0407
batch: [450/21305] batch time: 0.056 trainign loss: 5.6314 avg training loss: 8.0405
batch: [460/21305] batch time: 1.382 trainign loss: 7.2001 avg training loss: 8.0403
batch: [470/21305] batch time: 0.058 trainign loss: 6.5551 avg training loss: 8.0402
batch: [480/21305] batch time: 1.507 trainign loss: 7.7173 avg training loss: 8.0401
batch: [490/21305] batch time: 0.056 trainign loss: 7.1016 avg training loss: 8.0400
batch: [500/21305] batch time: 1.286 trainign loss: 4.6550 avg training loss: 8.0398
batch: [510/21305] batch time: 0.056 trainign loss: 5.9550 avg training loss: 8.0397
batch: [520/21305] batch time: 0.803 trainign loss: 4.5578 avg training loss: 8.0395
batch: [530/21305] batch time: 0.061 trainign loss: 6.6112 avg training loss: 8.0394
batch: [540/21305] batch time: 0.960 trainign loss: 6.8122 avg training loss: 8.0392
batch: [550/21305] batch time: 0.056 trainign loss: 5.2628 avg training loss: 8.0389
batch: [560/21305] batch time: 0.344 trainign loss: 6.4779 avg training loss: 8.0388
batch: [570/21305] batch time: 0.056 trainign loss: 6.4696 avg training loss: 8.0386
batch: [580/21305] batch time: 0.270 trainign loss: 6.8574 avg training loss: 8.0384
batch: [590/21305] batch time: 0.062 trainign loss: 7.0878 avg training loss: 8.0382
batch: [600/21305] batch time: 0.391 trainign loss: 6.4267 avg training loss: 8.0381
batch: [610/21305] batch time: 0.056 trainign loss: 7.8047 avg training loss: 8.0380
batch: [620/21305] batch time: 0.444 trainign loss: 7.2158 avg training loss: 8.0379
batch: [630/21305] batch time: 0.057 trainign loss: 6.5875 avg training loss: 8.0378
batch: [640/21305] batch time: 0.205 trainign loss: 7.3578 avg training loss: 8.0376
batch: [650/21305] batch time: 0.062 trainign loss: 6.4867 avg training loss: 8.0375
batch: [660/21305] batch time: 0.056 trainign loss: 6.0309 avg training loss: 8.0372
batch: [670/21305] batch time: 0.546 trainign loss: 9.0044 avg training loss: 8.0369
batch: [680/21305] batch time: 0.427 trainign loss: 8.4852 avg training loss: 8.0369
batch: [690/21305] batch time: 0.056 trainign loss: 6.7477 avg training loss: 8.0369
batch: [700/21305] batch time: 0.056 trainign loss: 8.0856 avg training loss: 8.0367
batch: [710/21305] batch time: 0.056 trainign loss: 6.8246 avg training loss: 8.0365
batch: [720/21305] batch time: 0.057 trainign loss: 6.7307 avg training loss: 8.0364
batch: [730/21305] batch time: 0.056 trainign loss: 7.7201 avg training loss: 8.0363
batch: [740/21305] batch time: 0.209 trainign loss: 7.8786 avg training loss: 8.0361
batch: [750/21305] batch time: 0.058 trainign loss: 7.3562 avg training loss: 8.0360
batch: [760/21305] batch time: 1.077 trainign loss: 6.8004 avg training loss: 8.0359
batch: [770/21305] batch time: 0.056 trainign loss: 5.6419 avg training loss: 8.0357
batch: [780/21305] batch time: 1.076 trainign loss: 7.7265 avg training loss: 8.0356
batch: [790/21305] batch time: 0.056 trainign loss: 7.5843 avg training loss: 8.0355
batch: [800/21305] batch time: 0.755 trainign loss: 7.3433 avg training loss: 8.0354
batch: [810/21305] batch time: 0.056 trainign loss: 6.7722 avg training loss: 8.0353
batch: [820/21305] batch time: 1.643 trainign loss: 7.5533 avg training loss: 8.0352
batch: [830/21305] batch time: 0.062 trainign loss: 6.7080 avg training loss: 8.0351
batch: [840/21305] batch time: 2.176 trainign loss: 7.1067 avg training loss: 8.0349
batch: [850/21305] batch time: 0.057 trainign loss: 6.0699 avg training loss: 8.0347
batch: [860/21305] batch time: 0.052 trainign loss: 5.8940 avg training loss: 8.0345
batch: [870/21305] batch time: 0.060 trainign loss: 5.7940 avg training loss: 8.0344
batch: [880/21305] batch time: 0.056 trainign loss: 7.5992 avg training loss: 8.0340
batch: [890/21305] batch time: 0.056 trainign loss: 8.6105 avg training loss: 8.0340
batch: [900/21305] batch time: 0.057 trainign loss: 6.7655 avg training loss: 8.0340
batch: [910/21305] batch time: 0.056 trainign loss: 7.1448 avg training loss: 8.0339
batch: [920/21305] batch time: 0.056 trainign loss: 4.8593 avg training loss: 8.0336
batch: [930/21305] batch time: 0.057 trainign loss: 6.9556 avg training loss: 8.0335
batch: [940/21305] batch time: 0.055 trainign loss: 4.6419 avg training loss: 8.0333
batch: [950/21305] batch time: 0.056 trainign loss: 0.0048 avg training loss: 8.0324
batch: [960/21305] batch time: 0.062 trainign loss: 8.4771 avg training loss: 8.0325
batch: [970/21305] batch time: 0.056 trainign loss: 8.2101 avg training loss: 8.0325
batch: [980/21305] batch time: 0.056 trainign loss: 7.5799 avg training loss: 8.0325
batch: [990/21305] batch time: 0.056 trainign loss: 7.2576 avg training loss: 8.0324
batch: [1000/21305] batch time: 0.062 trainign loss: 4.8374 avg training loss: 8.0322
batch: [1010/21305] batch time: 0.056 trainign loss: 7.9217 avg training loss: 8.0320
batch: [1020/21305] batch time: 0.050 trainign loss: 7.9120 avg training loss: 8.0319
batch: [1030/21305] batch time: 0.061 trainign loss: 7.3919 avg training loss: 8.0318
batch: [1040/21305] batch time: 0.056 trainign loss: 7.3003 avg training loss: 8.0317
batch: [1050/21305] batch time: 0.059 trainign loss: 6.9643 avg training loss: 8.0316
batch: [1060/21305] batch time: 0.054 trainign loss: 6.2036 avg training loss: 8.0314
batch: [1070/21305] batch time: 0.063 trainign loss: 5.0706 avg training loss: 8.0312
batch: [1080/21305] batch time: 0.051 trainign loss: 0.1224 avg training loss: 8.0306
batch: [1090/21305] batch time: 0.058 trainign loss: 8.7402 avg training loss: 8.0304
batch: [1100/21305] batch time: 0.051 trainign loss: 6.5452 avg training loss: 8.0303
batch: [1110/21305] batch time: 0.056 trainign loss: 7.3130 avg training loss: 8.0302
batch: [1120/21305] batch time: 0.053 trainign loss: 8.0463 avg training loss: 8.0301
batch: [1130/21305] batch time: 0.056 trainign loss: 6.5940 avg training loss: 8.0301
batch: [1140/21305] batch time: 0.056 trainign loss: 7.6467 avg training loss: 8.0300
batch: [1150/21305] batch time: 0.057 trainign loss: 7.2106 avg training loss: 8.0298
batch: [1160/21305] batch time: 0.056 trainign loss: 7.3816 avg training loss: 8.0296
batch: [1170/21305] batch time: 0.062 trainign loss: 7.0221 avg training loss: 8.0295
batch: [1180/21305] batch time: 0.054 trainign loss: 6.9082 avg training loss: 8.0294
batch: [1190/21305] batch time: 0.056 trainign loss: 7.4603 avg training loss: 8.0293
batch: [1200/21305] batch time: 0.051 trainign loss: 5.3076 avg training loss: 8.0290
batch: [1210/21305] batch time: 0.056 trainign loss: 7.4302 avg training loss: 8.0289
batch: [1220/21305] batch time: 0.056 trainign loss: 7.8459 avg training loss: 8.0288
batch: [1230/21305] batch time: 0.061 trainign loss: 6.5487 avg training loss: 8.0287
batch: [1240/21305] batch time: 0.051 trainign loss: 6.8288 avg training loss: 8.0285
batch: [1250/21305] batch time: 0.056 trainign loss: 7.1787 avg training loss: 8.0283
batch: [1260/21305] batch time: 0.056 trainign loss: 6.0067 avg training loss: 8.0282
batch: [1270/21305] batch time: 0.056 trainign loss: 6.8853 avg training loss: 8.0280
batch: [1280/21305] batch time: 0.051 trainign loss: 7.2964 avg training loss: 8.0279
batch: [1290/21305] batch time: 0.056 trainign loss: 6.6474 avg training loss: 8.0277
batch: [1300/21305] batch time: 0.056 trainign loss: 6.7127 avg training loss: 8.0276
batch: [1310/21305] batch time: 0.368 trainign loss: 7.2947 avg training loss: 8.0274
batch: [1320/21305] batch time: 0.057 trainign loss: 6.0201 avg training loss: 8.0273
batch: [1330/21305] batch time: 1.548 trainign loss: 7.2518 avg training loss: 8.0271
batch: [1340/21305] batch time: 0.056 trainign loss: 7.2953 avg training loss: 8.0270
batch: [1350/21305] batch time: 0.565 trainign loss: 7.1602 avg training loss: 8.0269
batch: [1360/21305] batch time: 0.052 trainign loss: 4.7348 avg training loss: 8.0267
batch: [1370/21305] batch time: 0.203 trainign loss: 8.0897 avg training loss: 8.0264
batch: [1380/21305] batch time: 0.061 trainign loss: 6.2319 avg training loss: 8.0263
batch: [1390/21305] batch time: 0.340 trainign loss: 6.8511 avg training loss: 8.0261
batch: [1400/21305] batch time: 0.051 trainign loss: 6.4591 avg training loss: 8.0259
batch: [1410/21305] batch time: 0.251 trainign loss: 7.0666 avg training loss: 8.0256
batch: [1420/21305] batch time: 0.051 trainign loss: 6.6597 avg training loss: 8.0254
batch: [1430/21305] batch time: 0.659 trainign loss: 5.9168 avg training loss: 8.0252
batch: [1440/21305] batch time: 0.057 trainign loss: 5.7840 avg training loss: 8.0251
batch: [1450/21305] batch time: 0.910 trainign loss: 6.7293 avg training loss: 8.0249
batch: [1460/21305] batch time: 0.057 trainign loss: 6.6983 avg training loss: 8.0248
batch: [1470/21305] batch time: 1.406 trainign loss: 5.9918 avg training loss: 8.0246
batch: [1480/21305] batch time: 0.061 trainign loss: 7.0763 avg training loss: 8.0244
batch: [1490/21305] batch time: 1.585 trainign loss: 7.1775 avg training loss: 8.0242
batch: [1500/21305] batch time: 0.054 trainign loss: 6.7890 avg training loss: 8.0241
batch: [1510/21305] batch time: 2.271 trainign loss: 6.8951 avg training loss: 8.0239
batch: [1520/21305] batch time: 0.056 trainign loss: 2.9062 avg training loss: 8.0235
batch: [1530/21305] batch time: 2.403 trainign loss: 12.1654 avg training loss: 8.0229
batch: [1540/21305] batch time: 0.057 trainign loss: 7.7897 avg training loss: 8.0230
batch: [1550/21305] batch time: 2.293 trainign loss: 5.7884 avg training loss: 8.0229
batch: [1560/21305] batch time: 0.061 trainign loss: 6.4647 avg training loss: 8.0228
batch: [1570/21305] batch time: 2.480 trainign loss: 6.1215 avg training loss: 8.0227
batch: [1580/21305] batch time: 0.056 trainign loss: 6.4202 avg training loss: 8.0224
batch: [1590/21305] batch time: 2.375 trainign loss: 8.3714 avg training loss: 8.0220
batch: [1600/21305] batch time: 0.055 trainign loss: 7.4511 avg training loss: 8.0220
batch: [1610/21305] batch time: 2.412 trainign loss: 7.4321 avg training loss: 8.0219
batch: [1620/21305] batch time: 0.059 trainign loss: 6.6516 avg training loss: 8.0218
batch: [1630/21305] batch time: 2.368 trainign loss: 8.0904 avg training loss: 8.0216
batch: [1640/21305] batch time: 0.062 trainign loss: 7.1942 avg training loss: 8.0215
batch: [1650/21305] batch time: 2.427 trainign loss: 6.0850 avg training loss: 8.0214
batch: [1660/21305] batch time: 0.063 trainign loss: 6.4674 avg training loss: 8.0213
batch: [1670/21305] batch time: 2.424 trainign loss: 6.6524 avg training loss: 8.0211
batch: [1680/21305] batch time: 0.055 trainign loss: 6.1279 avg training loss: 8.0209
batch: [1690/21305] batch time: 2.043 trainign loss: 6.7681 avg training loss: 8.0208
batch: [1700/21305] batch time: 0.056 trainign loss: 6.7538 avg training loss: 8.0207
batch: [1710/21305] batch time: 2.639 trainign loss: 4.9807 avg training loss: 8.0205
batch: [1720/21305] batch time: 0.190 trainign loss: 7.5844 avg training loss: 8.0203
batch: [1730/21305] batch time: 2.137 trainign loss: 6.6169 avg training loss: 8.0201
batch: [1740/21305] batch time: 0.405 trainign loss: 6.3804 avg training loss: 8.0200
batch: [1750/21305] batch time: 1.464 trainign loss: 6.7978 avg training loss: 8.0198
batch: [1760/21305] batch time: 0.061 trainign loss: 7.1949 avg training loss: 8.0197
batch: [1770/21305] batch time: 2.337 trainign loss: 6.1512 avg training loss: 8.0195
batch: [1780/21305] batch time: 0.056 trainign loss: 6.5405 avg training loss: 8.0194
batch: [1790/21305] batch time: 2.312 trainign loss: 6.8759 avg training loss: 8.0192
batch: [1800/21305] batch time: 0.105 trainign loss: 6.3046 avg training loss: 8.0191
batch: [1810/21305] batch time: 2.412 trainign loss: 6.4056 avg training loss: 8.0189
batch: [1820/21305] batch time: 0.495 trainign loss: 4.6426 avg training loss: 8.0186
batch: [1830/21305] batch time: 2.135 trainign loss: 7.5747 avg training loss: 8.0185
batch: [1840/21305] batch time: 0.552 trainign loss: 7.0880 avg training loss: 8.0184
batch: [1850/21305] batch time: 1.590 trainign loss: 6.7538 avg training loss: 8.0182
batch: [1860/21305] batch time: 0.761 trainign loss: 6.4844 avg training loss: 8.0181
batch: [1870/21305] batch time: 1.117 trainign loss: 6.8123 avg training loss: 8.0179
batch: [1880/21305] batch time: 1.590 trainign loss: 4.7727 avg training loss: 8.0177
batch: [1890/21305] batch time: 0.353 trainign loss: 4.7456 avg training loss: 8.0173
batch: [1900/21305] batch time: 1.845 trainign loss: 7.5240 avg training loss: 8.0172
batch: [1910/21305] batch time: 0.654 trainign loss: 7.1557 avg training loss: 8.0171
batch: [1920/21305] batch time: 1.910 trainign loss: 6.4479 avg training loss: 8.0170
batch: [1930/21305] batch time: 0.845 trainign loss: 2.7410 avg training loss: 8.0167
batch: [1940/21305] batch time: 0.553 trainign loss: 6.4219 avg training loss: 8.0165
batch: [1950/21305] batch time: 1.610 trainign loss: 6.4672 avg training loss: 8.0163
batch: [1960/21305] batch time: 1.944 trainign loss: 5.1061 avg training loss: 8.0162
batch: [1970/21305] batch time: 0.055 trainign loss: 7.0290 avg training loss: 8.0160
batch: [1980/21305] batch time: 2.101 trainign loss: 5.1720 avg training loss: 8.0158
batch: [1990/21305] batch time: 0.056 trainign loss: 6.5576 avg training loss: 8.0157
batch: [2000/21305] batch time: 2.457 trainign loss: 5.7175 avg training loss: 8.0154
batch: [2010/21305] batch time: 0.056 trainign loss: 6.8960 avg training loss: 8.0152
batch: [2020/21305] batch time: 2.225 trainign loss: 3.1307 avg training loss: 8.0150
batch: [2030/21305] batch time: 0.060 trainign loss: 6.2438 avg training loss: 8.0147
batch: [2040/21305] batch time: 2.213 trainign loss: 7.1987 avg training loss: 8.0146
batch: [2050/21305] batch time: 0.056 trainign loss: 6.1138 avg training loss: 8.0145
batch: [2060/21305] batch time: 2.792 trainign loss: 2.7193 avg training loss: 8.0142
batch: [2070/21305] batch time: 0.056 trainign loss: 1.3110 avg training loss: 8.0138
batch: [2080/21305] batch time: 2.432 trainign loss: 0.0376 avg training loss: 8.0132
batch: [2090/21305] batch time: 0.056 trainign loss: 8.7314 avg training loss: 8.0129
batch: [2100/21305] batch time: 2.175 trainign loss: 8.1881 avg training loss: 8.0129
batch: [2110/21305] batch time: 0.057 trainign loss: 6.7682 avg training loss: 8.0128
batch: [2120/21305] batch time: 2.132 trainign loss: 0.2658 avg training loss: 8.0123
batch: [2130/21305] batch time: 0.061 trainign loss: 0.0003 avg training loss: 8.0114
batch: [2140/21305] batch time: 2.667 trainign loss: 0.0000 avg training loss: 8.0105
batch: [2150/21305] batch time: 0.054 trainign loss: 8.1087 avg training loss: 8.0107
batch: [2160/21305] batch time: 2.305 trainign loss: 8.2445 avg training loss: 8.0108
batch: [2170/21305] batch time: 0.056 trainign loss: 7.7973 avg training loss: 8.0107
batch: [2180/21305] batch time: 2.412 trainign loss: 7.3627 avg training loss: 8.0106
batch: [2190/21305] batch time: 0.056 trainign loss: 6.3671 avg training loss: 8.0104
batch: [2200/21305] batch time: 2.728 trainign loss: 6.7516 avg training loss: 8.0103
batch: [2210/21305] batch time: 0.053 trainign loss: 6.2194 avg training loss: 8.0101
batch: [2220/21305] batch time: 2.455 trainign loss: 7.0185 avg training loss: 8.0100
batch: [2230/21305] batch time: 0.060 trainign loss: 6.7382 avg training loss: 8.0098
batch: [2240/21305] batch time: 2.351 trainign loss: 6.9385 avg training loss: 8.0095
batch: [2250/21305] batch time: 0.056 trainign loss: 6.4221 avg training loss: 8.0094
batch: [2260/21305] batch time: 2.377 trainign loss: 6.0215 avg training loss: 8.0092
batch: [2270/21305] batch time: 0.056 trainign loss: 4.2533 avg training loss: 8.0088
batch: [2280/21305] batch time: 2.311 trainign loss: 6.2240 avg training loss: 8.0085
batch: [2290/21305] batch time: 0.061 trainign loss: 4.1440 avg training loss: 8.0081
batch: [2300/21305] batch time: 2.323 trainign loss: 6.0716 avg training loss: 8.0081
batch: [2310/21305] batch time: 0.062 trainign loss: 7.6781 avg training loss: 8.0079
batch: [2320/21305] batch time: 1.802 trainign loss: 7.8620 avg training loss: 8.0078
batch: [2330/21305] batch time: 0.623 trainign loss: 7.4519 avg training loss: 8.0077
batch: [2340/21305] batch time: 0.530 trainign loss: 5.5895 avg training loss: 8.0075
batch: [2350/21305] batch time: 0.930 trainign loss: 5.7137 avg training loss: 8.0074
batch: [2360/21305] batch time: 0.968 trainign loss: 7.4830 avg training loss: 8.0072
batch: [2370/21305] batch time: 1.012 trainign loss: 5.0485 avg training loss: 8.0070
batch: [2380/21305] batch time: 0.343 trainign loss: 6.5531 avg training loss: 8.0068
batch: [2390/21305] batch time: 1.129 trainign loss: 6.8645 avg training loss: 8.0068
batch: [2400/21305] batch time: 0.530 trainign loss: 5.8159 avg training loss: 8.0065
batch: [2410/21305] batch time: 1.513 trainign loss: 7.7856 avg training loss: 8.0064
batch: [2420/21305] batch time: 1.731 trainign loss: 6.8805 avg training loss: 8.0063
batch: [2430/21305] batch time: 0.656 trainign loss: 5.4447 avg training loss: 8.0061
batch: [2440/21305] batch time: 1.198 trainign loss: 6.8954 avg training loss: 8.0060
batch: [2450/21305] batch time: 1.098 trainign loss: 7.1179 avg training loss: 8.0058
batch: [2460/21305] batch time: 0.820 trainign loss: 6.5696 avg training loss: 8.0056
batch: [2470/21305] batch time: 1.685 trainign loss: 7.0194 avg training loss: 8.0054
batch: [2480/21305] batch time: 0.245 trainign loss: 7.4789 avg training loss: 8.0052
batch: [2490/21305] batch time: 2.458 trainign loss: 6.9254 avg training loss: 8.0050
batch: [2500/21305] batch time: 0.062 trainign loss: 7.7813 avg training loss: 8.0049
batch: [2510/21305] batch time: 2.385 trainign loss: 5.6798 avg training loss: 8.0048
batch: [2520/21305] batch time: 0.061 trainign loss: 5.3399 avg training loss: 8.0046
batch: [2530/21305] batch time: 1.816 trainign loss: 5.3165 avg training loss: 8.0044
batch: [2540/21305] batch time: 0.061 trainign loss: 7.2361 avg training loss: 8.0043
batch: [2550/21305] batch time: 1.328 trainign loss: 6.1299 avg training loss: 8.0041
batch: [2560/21305] batch time: 1.240 trainign loss: 6.6937 avg training loss: 8.0041
batch: [2570/21305] batch time: 0.758 trainign loss: 7.4377 avg training loss: 8.0040
batch: [2580/21305] batch time: 1.416 trainign loss: 5.4230 avg training loss: 8.0038
batch: [2590/21305] batch time: 1.380 trainign loss: 6.1421 avg training loss: 8.0036
batch: [2600/21305] batch time: 2.420 trainign loss: 0.2849 avg training loss: 8.0031
batch: [2610/21305] batch time: 0.056 trainign loss: 8.4795 avg training loss: 8.0029
batch: [2620/21305] batch time: 2.069 trainign loss: 8.5253 avg training loss: 8.0030
batch: [2630/21305] batch time: 0.056 trainign loss: 7.4022 avg training loss: 8.0029
batch: [2640/21305] batch time: 2.488 trainign loss: 4.4652 avg training loss: 8.0027
batch: [2650/21305] batch time: 0.536 trainign loss: 8.2210 avg training loss: 8.0026
batch: [2660/21305] batch time: 1.215 trainign loss: 7.5648 avg training loss: 8.0024
batch: [2670/21305] batch time: 0.800 trainign loss: 6.6764 avg training loss: 8.0023
batch: [2680/21305] batch time: 1.029 trainign loss: 5.1651 avg training loss: 8.0020
batch: [2690/21305] batch time: 1.054 trainign loss: 7.6861 avg training loss: 8.0019
batch: [2700/21305] batch time: 1.300 trainign loss: 7.3828 avg training loss: 8.0018
batch: [2710/21305] batch time: 0.275 trainign loss: 5.3836 avg training loss: 8.0015
batch: [2720/21305] batch time: 2.000 trainign loss: 2.2809 avg training loss: 8.0012
batch: [2730/21305] batch time: 0.057 trainign loss: 8.1032 avg training loss: 8.0011
batch: [2740/21305] batch time: 2.032 trainign loss: 7.6013 avg training loss: 8.0010
batch: [2750/21305] batch time: 0.056 trainign loss: 7.1612 avg training loss: 8.0008
batch: [2760/21305] batch time: 2.004 trainign loss: 7.1015 avg training loss: 8.0007
batch: [2770/21305] batch time: 0.510 trainign loss: 7.2694 avg training loss: 8.0006
batch: [2780/21305] batch time: 1.363 trainign loss: 7.6182 avg training loss: 8.0005
batch: [2790/21305] batch time: 0.843 trainign loss: 7.0700 avg training loss: 8.0004
batch: [2800/21305] batch time: 0.909 trainign loss: 7.0319 avg training loss: 8.0002
batch: [2810/21305] batch time: 2.428 trainign loss: 6.3730 avg training loss: 8.0000
batch: [2820/21305] batch time: 0.053 trainign loss: 7.2997 avg training loss: 7.9997
batch: [2830/21305] batch time: 2.736 trainign loss: 7.4772 avg training loss: 7.9996
batch: [2840/21305] batch time: 0.052 trainign loss: 1.5696 avg training loss: 7.9993
batch: [2850/21305] batch time: 2.348 trainign loss: 5.8020 avg training loss: 7.9991
batch: [2860/21305] batch time: 0.060 trainign loss: 7.1103 avg training loss: 7.9989
batch: [2870/21305] batch time: 2.203 trainign loss: 8.0573 avg training loss: 7.9987
batch: [2880/21305] batch time: 0.062 trainign loss: 6.6964 avg training loss: 7.9986
batch: [2890/21305] batch time: 2.321 trainign loss: 7.0577 avg training loss: 7.9984
batch: [2900/21305] batch time: 0.303 trainign loss: 7.0245 avg training loss: 7.9982
batch: [2910/21305] batch time: 1.549 trainign loss: 7.3798 avg training loss: 7.9982
batch: [2920/21305] batch time: 0.592 trainign loss: 6.4983 avg training loss: 7.9981
batch: [2930/21305] batch time: 1.799 trainign loss: 5.7087 avg training loss: 7.9979
batch: [2940/21305] batch time: 0.497 trainign loss: 5.4352 avg training loss: 7.9976
batch: [2950/21305] batch time: 1.388 trainign loss: 0.8388 avg training loss: 7.9972
batch: [2960/21305] batch time: 0.681 trainign loss: 7.0618 avg training loss: 7.9971
batch: [2970/21305] batch time: 1.218 trainign loss: 6.3585 avg training loss: 7.9970
batch: [2980/21305] batch time: 1.624 trainign loss: 7.1440 avg training loss: 7.9969
batch: [2990/21305] batch time: 1.204 trainign loss: 6.6858 avg training loss: 7.9968
batch: [3000/21305] batch time: 1.597 trainign loss: 4.9096 avg training loss: 7.9966
batch: [3010/21305] batch time: 1.171 trainign loss: 7.9677 avg training loss: 7.9964
batch: [3020/21305] batch time: 1.754 trainign loss: 5.3715 avg training loss: 7.9962
batch: [3030/21305] batch time: 1.043 trainign loss: 6.1299 avg training loss: 7.9961
batch: [3040/21305] batch time: 1.210 trainign loss: 6.4995 avg training loss: 7.9959
batch: [3050/21305] batch time: 0.905 trainign loss: 7.3044 avg training loss: 7.9958
batch: [3060/21305] batch time: 1.234 trainign loss: 6.7494 avg training loss: 7.9956
batch: [3070/21305] batch time: 1.206 trainign loss: 6.6110 avg training loss: 7.9954
batch: [3080/21305] batch time: 0.407 trainign loss: 7.2724 avg training loss: 7.9953
batch: [3090/21305] batch time: 1.862 trainign loss: 6.6384 avg training loss: 7.9951
batch: [3100/21305] batch time: 0.080 trainign loss: 7.0351 avg training loss: 7.9950
batch: [3110/21305] batch time: 2.364 trainign loss: 7.6410 avg training loss: 7.9949
batch: [3120/21305] batch time: 0.060 trainign loss: 6.1948 avg training loss: 7.9946
batch: [3130/21305] batch time: 2.088 trainign loss: 0.9914 avg training loss: 7.9942
batch: [3140/21305] batch time: 1.337 trainign loss: 7.6843 avg training loss: 7.9941
batch: [3150/21305] batch time: 0.714 trainign loss: 6.7140 avg training loss: 7.9940
batch: [3160/21305] batch time: 1.080 trainign loss: 7.0560 avg training loss: 7.9939
batch: [3170/21305] batch time: 1.726 trainign loss: 6.8038 avg training loss: 7.9937
batch: [3180/21305] batch time: 0.063 trainign loss: 6.1871 avg training loss: 7.9936
batch: [3190/21305] batch time: 2.390 trainign loss: 6.5614 avg training loss: 7.9935
batch: [3200/21305] batch time: 0.056 trainign loss: 7.1016 avg training loss: 7.9933
batch: [3210/21305] batch time: 2.567 trainign loss: 7.0735 avg training loss: 7.9932
batch: [3220/21305] batch time: 0.056 trainign loss: 6.8841 avg training loss: 7.9930
batch: [3230/21305] batch time: 2.445 trainign loss: 7.2717 avg training loss: 7.9928
batch: [3240/21305] batch time: 0.062 trainign loss: 6.1631 avg training loss: 7.9928
batch: [3250/21305] batch time: 2.322 trainign loss: 6.4402 avg training loss: 7.9925
batch: [3260/21305] batch time: 0.056 trainign loss: 7.4150 avg training loss: 7.9924
batch: [3270/21305] batch time: 2.010 trainign loss: 5.8887 avg training loss: 7.9923
batch: [3280/21305] batch time: 0.055 trainign loss: 3.2397 avg training loss: 7.9919
batch: [3290/21305] batch time: 2.269 trainign loss: 7.0119 avg training loss: 7.9916
batch: [3300/21305] batch time: 0.057 trainign loss: 5.9878 avg training loss: 7.9914
batch: [3310/21305] batch time: 1.931 trainign loss: 1.0956 avg training loss: 7.9910
batch: [3320/21305] batch time: 0.053 trainign loss: 7.4055 avg training loss: 7.9910
batch: [3330/21305] batch time: 2.389 trainign loss: 6.6652 avg training loss: 7.9909
batch: [3340/21305] batch time: 0.062 trainign loss: 4.6728 avg training loss: 7.9907
batch: [3350/21305] batch time: 2.314 trainign loss: 5.4651 avg training loss: 7.9905
batch: [3360/21305] batch time: 0.059 trainign loss: 7.4715 avg training loss: 7.9902
batch: [3370/21305] batch time: 2.302 trainign loss: 4.0481 avg training loss: 7.9899
batch: [3380/21305] batch time: 0.057 trainign loss: 8.4074 avg training loss: 7.9898
batch: [3390/21305] batch time: 1.979 trainign loss: 7.4194 avg training loss: 7.9897
batch: [3400/21305] batch time: 0.056 trainign loss: 6.6764 avg training loss: 7.9896
batch: [3410/21305] batch time: 1.999 trainign loss: 5.9509 avg training loss: 7.9895
batch: [3420/21305] batch time: 0.056 trainign loss: 6.9716 avg training loss: 7.9892
batch: [3430/21305] batch time: 1.802 trainign loss: 6.8763 avg training loss: 7.9891
batch: [3440/21305] batch time: 0.062 trainign loss: 7.4599 avg training loss: 7.9890
batch: [3450/21305] batch time: 1.894 trainign loss: 5.8967 avg training loss: 7.9888
batch: [3460/21305] batch time: 0.056 trainign loss: 6.0973 avg training loss: 7.9886
batch: [3470/21305] batch time: 1.327 trainign loss: 7.3077 avg training loss: 7.9884
batch: [3480/21305] batch time: 0.056 trainign loss: 7.5706 avg training loss: 7.9884
batch: [3490/21305] batch time: 1.698 trainign loss: 7.0915 avg training loss: 7.9882
batch: [3500/21305] batch time: 0.056 trainign loss: 6.5887 avg training loss: 7.9881
batch: [3510/21305] batch time: 1.973 trainign loss: 6.9300 avg training loss: 7.9880
batch: [3520/21305] batch time: 0.063 trainign loss: 6.5080 avg training loss: 7.9878
batch: [3530/21305] batch time: 2.067 trainign loss: 7.0014 avg training loss: 7.9877
batch: [3540/21305] batch time: 0.056 trainign loss: 5.7984 avg training loss: 7.9875
batch: [3550/21305] batch time: 2.228 trainign loss: 6.9574 avg training loss: 7.9872
batch: [3560/21305] batch time: 0.057 trainign loss: 6.9931 avg training loss: 7.9869
batch: [3570/21305] batch time: 2.148 trainign loss: 7.9477 avg training loss: 7.9869
batch: [3580/21305] batch time: 0.057 trainign loss: 6.3995 avg training loss: 7.9868
batch: [3590/21305] batch time: 2.368 trainign loss: 4.8081 avg training loss: 7.9866
batch: [3600/21305] batch time: 0.056 trainign loss: 7.8062 avg training loss: 7.9863
batch: [3610/21305] batch time: 2.662 trainign loss: 8.1386 avg training loss: 7.9862
batch: [3620/21305] batch time: 0.056 trainign loss: 7.4178 avg training loss: 7.9862
batch: [3630/21305] batch time: 2.464 trainign loss: 7.1578 avg training loss: 7.9861
batch: [3640/21305] batch time: 0.056 trainign loss: 6.7782 avg training loss: 7.9859
batch: [3650/21305] batch time: 2.495 trainign loss: 6.0237 avg training loss: 7.9857
batch: [3660/21305] batch time: 0.062 trainign loss: 6.1604 avg training loss: 7.9856
batch: [3670/21305] batch time: 2.017 trainign loss: 5.6138 avg training loss: 7.9854
batch: [3680/21305] batch time: 0.057 trainign loss: 7.4466 avg training loss: 7.9853
batch: [3690/21305] batch time: 2.090 trainign loss: 6.6587 avg training loss: 7.9851
batch: [3700/21305] batch time: 0.063 trainign loss: 5.9271 avg training loss: 7.9850
batch: [3710/21305] batch time: 2.506 trainign loss: 5.6946 avg training loss: 7.9848
batch: [3720/21305] batch time: 0.057 trainign loss: 6.5479 avg training loss: 7.9846
batch: [3730/21305] batch time: 2.619 trainign loss: 6.3275 avg training loss: 7.9845
batch: [3740/21305] batch time: 0.056 trainign loss: 7.4749 avg training loss: 7.9844
batch: [3750/21305] batch time: 2.353 trainign loss: 7.2050 avg training loss: 7.9843
batch: [3760/21305] batch time: 0.053 trainign loss: 6.6130 avg training loss: 7.9840
batch: [3770/21305] batch time: 2.177 trainign loss: 4.7723 avg training loss: 7.9838
batch: [3780/21305] batch time: 0.055 trainign loss: 6.7970 avg training loss: 7.9837
batch: [3790/21305] batch time: 2.240 trainign loss: 5.1143 avg training loss: 7.9835
batch: [3800/21305] batch time: 0.056 trainign loss: 6.3128 avg training loss: 7.9834
batch: [3810/21305] batch time: 2.494 trainign loss: 7.4154 avg training loss: 7.9832
batch: [3820/21305] batch time: 0.056 trainign loss: 5.1001 avg training loss: 7.9830
batch: [3830/21305] batch time: 2.533 trainign loss: 6.5260 avg training loss: 7.9829
batch: [3840/21305] batch time: 0.053 trainign loss: 6.4804 avg training loss: 7.9828
batch: [3850/21305] batch time: 2.305 trainign loss: 6.4417 avg training loss: 7.9826
batch: [3860/21305] batch time: 0.056 trainign loss: 6.6777 avg training loss: 7.9825
batch: [3870/21305] batch time: 2.329 trainign loss: 5.5027 avg training loss: 7.9823
batch: [3880/21305] batch time: 0.056 trainign loss: 0.1775 avg training loss: 7.9817
batch: [3890/21305] batch time: 2.117 trainign loss: 9.5607 avg training loss: 7.9814
batch: [3900/21305] batch time: 0.057 trainign loss: 8.4728 avg training loss: 7.9815
batch: [3910/21305] batch time: 2.403 trainign loss: 7.7483 avg training loss: 7.9814
batch: [3920/21305] batch time: 0.060 trainign loss: 5.5221 avg training loss: 7.9813
batch: [3930/21305] batch time: 2.279 trainign loss: 6.8039 avg training loss: 7.9812
batch: [3940/21305] batch time: 0.057 trainign loss: 6.0522 avg training loss: 7.9810
batch: [3950/21305] batch time: 2.227 trainign loss: 6.5226 avg training loss: 7.9809
batch: [3960/21305] batch time: 0.057 trainign loss: 7.7098 avg training loss: 7.9807
batch: [3970/21305] batch time: 2.645 trainign loss: 7.1830 avg training loss: 7.9806
batch: [3980/21305] batch time: 0.057 trainign loss: 5.9899 avg training loss: 7.9805
batch: [3990/21305] batch time: 2.570 trainign loss: 5.7147 avg training loss: 7.9802
batch: [4000/21305] batch time: 0.056 trainign loss: 7.5369 avg training loss: 7.9800
batch: [4010/21305] batch time: 2.560 trainign loss: 7.2170 avg training loss: 7.9799
batch: [4020/21305] batch time: 0.054 trainign loss: 4.6396 avg training loss: 7.9797
batch: [4030/21305] batch time: 2.329 trainign loss: 7.2024 avg training loss: 7.9796
batch: [4040/21305] batch time: 0.056 trainign loss: 6.8438 avg training loss: 7.9795
batch: [4050/21305] batch time: 2.929 trainign loss: 4.4837 avg training loss: 7.9792
batch: [4060/21305] batch time: 0.057 trainign loss: 10.4447 avg training loss: 7.9785
batch: [4070/21305] batch time: 2.073 trainign loss: 8.8179 avg training loss: 7.9784
batch: [4080/21305] batch time: 0.062 trainign loss: 8.2419 avg training loss: 7.9784
batch: [4090/21305] batch time: 1.954 trainign loss: 7.9042 avg training loss: 7.9783
batch: [4100/21305] batch time: 0.063 trainign loss: 6.5669 avg training loss: 7.9783
batch: [4110/21305] batch time: 2.221 trainign loss: 5.5038 avg training loss: 7.9781
batch: [4120/21305] batch time: 0.062 trainign loss: 5.6687 avg training loss: 7.9779
batch: [4130/21305] batch time: 2.205 trainign loss: 6.6799 avg training loss: 7.9777
batch: [4140/21305] batch time: 0.056 trainign loss: 5.6951 avg training loss: 7.9774
batch: [4150/21305] batch time: 2.397 trainign loss: 8.1914 avg training loss: 7.9773
batch: [4160/21305] batch time: 0.059 trainign loss: 7.2244 avg training loss: 7.9772
batch: [4170/21305] batch time: 2.088 trainign loss: 5.8987 avg training loss: 7.9770
batch: [4180/21305] batch time: 0.057 trainign loss: 7.4890 avg training loss: 7.9769
batch: [4190/21305] batch time: 2.369 trainign loss: 7.1793 avg training loss: 7.9767
batch: [4200/21305] batch time: 0.063 trainign loss: 6.2506 avg training loss: 7.9766
batch: [4210/21305] batch time: 2.217 trainign loss: 1.6561 avg training loss: 7.9762
batch: [4220/21305] batch time: 0.056 trainign loss: 7.1454 avg training loss: 7.9760
batch: [4230/21305] batch time: 2.196 trainign loss: 6.7824 avg training loss: 7.9758
batch: [4240/21305] batch time: 0.052 trainign loss: 5.5391 avg training loss: 7.9757
batch: [4250/21305] batch time: 2.346 trainign loss: 0.5194 avg training loss: 7.9750
batch: [4260/21305] batch time: 0.056 trainign loss: 7.8712 avg training loss: 7.9750
batch: [4270/21305] batch time: 1.603 trainign loss: 8.8341 avg training loss: 7.9750
batch: [4280/21305] batch time: 0.063 trainign loss: 7.6574 avg training loss: 7.9751
batch: [4290/21305] batch time: 1.347 trainign loss: 6.9877 avg training loss: 7.9749
batch: [4300/21305] batch time: 0.058 trainign loss: 6.5807 avg training loss: 7.9748
batch: [4310/21305] batch time: 2.675 trainign loss: 7.0218 avg training loss: 7.9747
batch: [4320/21305] batch time: 0.056 trainign loss: 6.0212 avg training loss: 7.9745
batch: [4330/21305] batch time: 2.358 trainign loss: 4.7698 avg training loss: 7.9743
batch: [4340/21305] batch time: 0.056 trainign loss: 5.4933 avg training loss: 7.9741
batch: [4350/21305] batch time: 2.277 trainign loss: 6.3212 avg training loss: 7.9739
batch: [4360/21305] batch time: 0.056 trainign loss: 5.3156 avg training loss: 7.9737
batch: [4370/21305] batch time: 2.213 trainign loss: 7.8233 avg training loss: 7.9734
batch: [4380/21305] batch time: 0.057 trainign loss: 6.8338 avg training loss: 7.9734
batch: [4390/21305] batch time: 2.466 trainign loss: 6.5995 avg training loss: 7.9731
batch: [4400/21305] batch time: 0.056 trainign loss: 7.5927 avg training loss: 7.9731
batch: [4410/21305] batch time: 2.582 trainign loss: 6.8165 avg training loss: 7.9730
batch: [4420/21305] batch time: 0.056 trainign loss: 5.7791 avg training loss: 7.9728
batch: [4430/21305] batch time: 2.221 trainign loss: 1.7264 avg training loss: 7.9724
batch: [4440/21305] batch time: 0.056 trainign loss: 9.6847 avg training loss: 7.9720
batch: [4450/21305] batch time: 2.376 trainign loss: 7.2884 avg training loss: 7.9719
batch: [4460/21305] batch time: 0.062 trainign loss: 4.5321 avg training loss: 7.9717
batch: [4470/21305] batch time: 2.212 trainign loss: 9.5177 avg training loss: 7.9714
batch: [4480/21305] batch time: 0.056 trainign loss: 7.7236 avg training loss: 7.9714
batch: [4490/21305] batch time: 2.511 trainign loss: 4.5634 avg training loss: 7.9712
batch: [4500/21305] batch time: 0.056 trainign loss: 8.1735 avg training loss: 7.9711
batch: [4510/21305] batch time: 2.195 trainign loss: 7.5129 avg training loss: 7.9709
batch: [4520/21305] batch time: 0.062 trainign loss: 7.9091 avg training loss: 7.9708
batch: [4530/21305] batch time: 2.301 trainign loss: 7.8043 avg training loss: 7.9707
batch: [4540/21305] batch time: 0.056 trainign loss: 6.9155 avg training loss: 7.9706
batch: [4550/21305] batch time: 2.474 trainign loss: 6.3030 avg training loss: 7.9705
batch: [4560/21305] batch time: 0.056 trainign loss: 7.5832 avg training loss: 7.9703
batch: [4570/21305] batch time: 2.322 trainign loss: 7.4050 avg training loss: 7.9702
batch: [4580/21305] batch time: 0.057 trainign loss: 7.3678 avg training loss: 7.9701
batch: [4590/21305] batch time: 2.060 trainign loss: 5.5234 avg training loss: 7.9699
batch: [4600/21305] batch time: 0.062 trainign loss: 6.5974 avg training loss: 7.9698
batch: [4610/21305] batch time: 2.149 trainign loss: 6.2363 avg training loss: 7.9696
batch: [4620/21305] batch time: 0.056 trainign loss: 6.7896 avg training loss: 7.9694
batch: [4630/21305] batch time: 2.109 trainign loss: 6.2285 avg training loss: 7.9693
batch: [4640/21305] batch time: 0.057 trainign loss: 5.6603 avg training loss: 7.9691
batch: [4650/21305] batch time: 2.173 trainign loss: 6.4698 avg training loss: 7.9690
batch: [4660/21305] batch time: 0.057 trainign loss: 7.3271 avg training loss: 7.9689
batch: [4670/21305] batch time: 2.305 trainign loss: 6.9760 avg training loss: 7.9686
batch: [4680/21305] batch time: 0.056 trainign loss: 7.6704 avg training loss: 7.9685
batch: [4690/21305] batch time: 1.628 trainign loss: 5.6382 avg training loss: 7.9684
batch: [4700/21305] batch time: 0.063 trainign loss: 6.6614 avg training loss: 7.9681
batch: [4710/21305] batch time: 2.156 trainign loss: 7.4905 avg training loss: 7.9680
batch: [4720/21305] batch time: 0.058 trainign loss: 6.7449 avg training loss: 7.9679
batch: [4730/21305] batch time: 1.618 trainign loss: 6.3767 avg training loss: 7.9678
batch: [4740/21305] batch time: 0.056 trainign loss: 7.8629 avg training loss: 7.9676
batch: [4750/21305] batch time: 1.977 trainign loss: 7.0826 avg training loss: 7.9674
batch: [4760/21305] batch time: 0.057 trainign loss: 4.4944 avg training loss: 7.9672
batch: [4770/21305] batch time: 1.836 trainign loss: 6.6844 avg training loss: 7.9670
batch: [4780/21305] batch time: 0.061 trainign loss: 3.1504 avg training loss: 7.9668
batch: [4790/21305] batch time: 1.900 trainign loss: 6.9312 avg training loss: 7.9665
batch: [4800/21305] batch time: 0.062 trainign loss: 7.7085 avg training loss: 7.9664
batch: [4810/21305] batch time: 1.425 trainign loss: 5.6752 avg training loss: 7.9663
batch: [4820/21305] batch time: 0.501 trainign loss: 7.0665 avg training loss: 7.9662
batch: [4830/21305] batch time: 1.873 trainign loss: 6.1406 avg training loss: 7.9660
batch: [4840/21305] batch time: 0.556 trainign loss: 1.2389 avg training loss: 7.9656
batch: [4850/21305] batch time: 1.254 trainign loss: 10.4376 avg training loss: 7.9652
batch: [4860/21305] batch time: 0.635 trainign loss: 7.4931 avg training loss: 7.9653
batch: [4870/21305] batch time: 2.505 trainign loss: 8.6082 avg training loss: 7.9653
batch: [4880/21305] batch time: 0.054 trainign loss: 7.7635 avg training loss: 7.9653
batch: [4890/21305] batch time: 2.621 trainign loss: 6.9721 avg training loss: 7.9651
batch: [4900/21305] batch time: 0.053 trainign loss: 6.3602 avg training loss: 7.9649
batch: [4910/21305] batch time: 2.344 trainign loss: 4.9103 avg training loss: 7.9646
batch: [4920/21305] batch time: 0.056 trainign loss: 8.1576 avg training loss: 7.9646
batch: [4930/21305] batch time: 1.965 trainign loss: 5.9055 avg training loss: 7.9644
batch: [4940/21305] batch time: 0.060 trainign loss: 7.2207 avg training loss: 7.9642
batch: [4950/21305] batch time: 2.312 trainign loss: 5.5718 avg training loss: 7.9641
batch: [4960/21305] batch time: 0.056 trainign loss: 7.0311 avg training loss: 7.9638
batch: [4970/21305] batch time: 2.740 trainign loss: 6.7435 avg training loss: 7.9637
batch: [4980/21305] batch time: 0.063 trainign loss: 5.1769 avg training loss: 7.9636
batch: [4990/21305] batch time: 2.174 trainign loss: 0.0129 avg training loss: 7.9628
batch: [5000/21305] batch time: 0.060 trainign loss: 7.8316 avg training loss: 7.9627
batch: [5010/21305] batch time: 2.123 trainign loss: 8.4586 avg training loss: 7.9628
batch: [5020/21305] batch time: 0.056 trainign loss: 6.9866 avg training loss: 7.9627
batch: [5030/21305] batch time: 2.498 trainign loss: 6.3504 avg training loss: 7.9625
batch: [5040/21305] batch time: 0.062 trainign loss: 4.0751 avg training loss: 7.9623
batch: [5050/21305] batch time: 2.197 trainign loss: 6.4396 avg training loss: 7.9620
batch: [5060/21305] batch time: 0.056 trainign loss: 7.8788 avg training loss: 7.9618
batch: [5070/21305] batch time: 2.402 trainign loss: 5.6050 avg training loss: 7.9617
batch: [5080/21305] batch time: 0.060 trainign loss: 0.7703 avg training loss: 7.9610
batch: [5090/21305] batch time: 2.039 trainign loss: 7.1094 avg training loss: 7.9611
batch: [5100/21305] batch time: 0.055 trainign loss: 7.7134 avg training loss: 7.9610
batch: [5110/21305] batch time: 2.046 trainign loss: 6.8298 avg training loss: 7.9610
batch: [5120/21305] batch time: 0.056 trainign loss: 6.8400 avg training loss: 7.9609
batch: [5130/21305] batch time: 2.265 trainign loss: 6.9783 avg training loss: 7.9607
batch: [5140/21305] batch time: 0.056 trainign loss: 6.9294 avg training loss: 7.9604
batch: [5150/21305] batch time: 2.194 trainign loss: 7.5764 avg training loss: 7.9603
batch: [5160/21305] batch time: 0.057 trainign loss: 7.5579 avg training loss: 7.9603
batch: [5170/21305] batch time: 2.263 trainign loss: 7.3359 avg training loss: 7.9602
batch: [5180/21305] batch time: 0.146 trainign loss: 6.7342 avg training loss: 7.9600
batch: [5190/21305] batch time: 1.046 trainign loss: 6.8045 avg training loss: 7.9599
batch: [5200/21305] batch time: 0.986 trainign loss: 6.2549 avg training loss: 7.9598
batch: [5210/21305] batch time: 0.727 trainign loss: 6.1826 avg training loss: 7.9596
batch: [5220/21305] batch time: 1.206 trainign loss: 6.7712 avg training loss: 7.9595
batch: [5230/21305] batch time: 0.056 trainign loss: 6.5463 avg training loss: 7.9594
batch: [5240/21305] batch time: 1.025 trainign loss: 5.0939 avg training loss: 7.9592
batch: [5250/21305] batch time: 0.243 trainign loss: 6.0297 avg training loss: 7.9589
batch: [5260/21305] batch time: 1.232 trainign loss: 7.1072 avg training loss: 7.9589
batch: [5270/21305] batch time: 0.063 trainign loss: 7.3005 avg training loss: 7.9588
batch: [5280/21305] batch time: 1.206 trainign loss: 7.1939 avg training loss: 7.9587
batch: [5290/21305] batch time: 0.981 trainign loss: 5.7916 avg training loss: 7.9585
batch: [5300/21305] batch time: 1.392 trainign loss: 6.9238 avg training loss: 7.9583
batch: [5310/21305] batch time: 0.204 trainign loss: 7.0689 avg training loss: 7.9582
batch: [5320/21305] batch time: 1.762 trainign loss: 7.1006 avg training loss: 7.9581
batch: [5330/21305] batch time: 0.755 trainign loss: 6.6265 avg training loss: 7.9579
batch: [5340/21305] batch time: 1.209 trainign loss: 6.4936 avg training loss: 7.9577
batch: [5350/21305] batch time: 0.409 trainign loss: 5.7316 avg training loss: 7.9574
batch: [5360/21305] batch time: 0.614 trainign loss: 7.6954 avg training loss: 7.9572
batch: [5370/21305] batch time: 1.313 trainign loss: 7.5171 avg training loss: 7.9571
batch: [5380/21305] batch time: 1.160 trainign loss: 7.3078 avg training loss: 7.9570
batch: [5390/21305] batch time: 0.990 trainign loss: 5.5149 avg training loss: 7.9568
batch: [5400/21305] batch time: 1.111 trainign loss: 7.1762 avg training loss: 7.9568
batch: [5410/21305] batch time: 1.705 trainign loss: 6.8884 avg training loss: 7.9566
batch: [5420/21305] batch time: 0.267 trainign loss: 7.4960 avg training loss: 7.9564
batch: [5430/21305] batch time: 1.747 trainign loss: 6.3127 avg training loss: 7.9563
batch: [5440/21305] batch time: 0.253 trainign loss: 6.5139 avg training loss: 7.9561
batch: [5450/21305] batch time: 1.760 trainign loss: 5.6666 avg training loss: 7.9558
batch: [5460/21305] batch time: 0.752 trainign loss: 7.6744 avg training loss: 7.9557
batch: [5470/21305] batch time: 1.022 trainign loss: 5.7718 avg training loss: 7.9556
batch: [5480/21305] batch time: 0.903 trainign loss: 7.5590 avg training loss: 7.9554
batch: [5490/21305] batch time: 0.703 trainign loss: 5.8611 avg training loss: 7.9553
batch: [5500/21305] batch time: 1.471 trainign loss: 5.9241 avg training loss: 7.9551
batch: [5510/21305] batch time: 0.297 trainign loss: 2.5114 avg training loss: 7.9548
batch: [5520/21305] batch time: 1.934 trainign loss: 7.5407 avg training loss: 7.9546
batch: [5530/21305] batch time: 0.135 trainign loss: 7.0753 avg training loss: 7.9545
batch: [5540/21305] batch time: 2.070 trainign loss: 7.0107 avg training loss: 7.9543
batch: [5550/21305] batch time: 0.270 trainign loss: 7.9479 avg training loss: 7.9542
batch: [5560/21305] batch time: 2.027 trainign loss: 5.3968 avg training loss: 7.9540
batch: [5570/21305] batch time: 0.371 trainign loss: 2.7343 avg training loss: 7.9538
batch: [5580/21305] batch time: 2.144 trainign loss: 8.5459 avg training loss: 7.9535
batch: [5590/21305] batch time: 0.056 trainign loss: 7.7171 avg training loss: 7.9535
batch: [5600/21305] batch time: 2.268 trainign loss: 7.1630 avg training loss: 7.9534
batch: [5610/21305] batch time: 0.054 trainign loss: 7.6312 avg training loss: 7.9532
batch: [5620/21305] batch time: 2.266 trainign loss: 7.4739 avg training loss: 7.9532
batch: [5630/21305] batch time: 0.063 trainign loss: 6.9723 avg training loss: 7.9530
batch: [5640/21305] batch time: 2.496 trainign loss: 6.1385 avg training loss: 7.9529
batch: [5650/21305] batch time: 0.056 trainign loss: 7.6956 avg training loss: 7.9528
batch: [5660/21305] batch time: 2.213 trainign loss: 4.8466 avg training loss: 7.9526
batch: [5670/21305] batch time: 0.056 trainign loss: 7.3080 avg training loss: 7.9525
batch: [5680/21305] batch time: 2.337 trainign loss: 6.2641 avg training loss: 7.9523
batch: [5690/21305] batch time: 0.061 trainign loss: 6.5357 avg training loss: 7.9523
batch: [5700/21305] batch time: 2.438 trainign loss: 6.5071 avg training loss: 7.9521
batch: [5710/21305] batch time: 0.057 trainign loss: 5.1064 avg training loss: 7.9519
batch: [5720/21305] batch time: 2.136 trainign loss: 7.1163 avg training loss: 7.9515
batch: [5730/21305] batch time: 0.057 trainign loss: 7.7046 avg training loss: 7.9515
batch: [5740/21305] batch time: 2.363 trainign loss: 7.4708 avg training loss: 7.9515
batch: [5750/21305] batch time: 0.056 trainign loss: 6.7716 avg training loss: 7.9514
batch: [5760/21305] batch time: 2.276 trainign loss: 5.6723 avg training loss: 7.9513
batch: [5770/21305] batch time: 0.057 trainign loss: 6.7610 avg training loss: 7.9511
batch: [5780/21305] batch time: 2.192 trainign loss: 6.5741 avg training loss: 7.9510
batch: [5790/21305] batch time: 0.056 trainign loss: 7.8299 avg training loss: 7.9509
batch: [5800/21305] batch time: 2.196 trainign loss: 5.9325 avg training loss: 7.9508
batch: [5810/21305] batch time: 0.056 trainign loss: 7.9316 avg training loss: 7.9507
batch: [5820/21305] batch time: 2.085 trainign loss: 7.6126 avg training loss: 7.9506
batch: [5830/21305] batch time: 0.056 trainign loss: 6.3532 avg training loss: 7.9504
batch: [5840/21305] batch time: 2.411 trainign loss: 6.8045 avg training loss: 7.9503
batch: [5850/21305] batch time: 0.059 trainign loss: 6.6662 avg training loss: 7.9502
batch: [5860/21305] batch time: 2.534 trainign loss: 7.5492 avg training loss: 7.9501
batch: [5870/21305] batch time: 0.054 trainign loss: 7.0982 avg training loss: 7.9500
batch: [5880/21305] batch time: 2.456 trainign loss: 6.2617 avg training loss: 7.9498
batch: [5890/21305] batch time: 0.056 trainign loss: 6.6613 avg training loss: 7.9497
batch: [5900/21305] batch time: 2.639 trainign loss: 5.4257 avg training loss: 7.9495
batch: [5910/21305] batch time: 0.057 trainign loss: 5.7794 avg training loss: 7.9492
batch: [5920/21305] batch time: 1.707 trainign loss: 5.4731 avg training loss: 7.9490
batch: [5930/21305] batch time: 0.057 trainign loss: 7.1191 avg training loss: 7.9489
batch: [5940/21305] batch time: 2.640 trainign loss: 6.0253 avg training loss: 7.9488
batch: [5950/21305] batch time: 0.056 trainign loss: 5.6940 avg training loss: 7.9486
batch: [5960/21305] batch time: 2.613 trainign loss: 4.9958 avg training loss: 7.9483
batch: [5970/21305] batch time: 0.061 trainign loss: 8.0103 avg training loss: 7.9482
batch: [5980/21305] batch time: 2.057 trainign loss: 6.4049 avg training loss: 7.9482
batch: [5990/21305] batch time: 0.056 trainign loss: 7.6055 avg training loss: 7.9481
batch: [6000/21305] batch time: 1.640 trainign loss: 7.2648 avg training loss: 7.9481
batch: [6010/21305] batch time: 0.056 trainign loss: 6.0433 avg training loss: 7.9479
batch: [6020/21305] batch time: 1.760 trainign loss: 7.0145 avg training loss: 7.9478
batch: [6030/21305] batch time: 0.058 trainign loss: 6.1393 avg training loss: 7.9475
batch: [6040/21305] batch time: 1.045 trainign loss: 2.4422 avg training loss: 7.9472
batch: [6050/21305] batch time: 0.061 trainign loss: 6.8440 avg training loss: 7.9470
batch: [6060/21305] batch time: 1.054 trainign loss: 7.5029 avg training loss: 7.9468
batch: [6070/21305] batch time: 0.057 trainign loss: 3.0639 avg training loss: 7.9466
batch: [6080/21305] batch time: 0.994 trainign loss: 5.1487 avg training loss: 7.9463
batch: [6090/21305] batch time: 0.061 trainign loss: 6.9266 avg training loss: 7.9462
batch: [6100/21305] batch time: 0.828 trainign loss: 8.2999 avg training loss: 7.9461
batch: [6110/21305] batch time: 0.413 trainign loss: 5.7326 avg training loss: 7.9460
batch: [6120/21305] batch time: 0.799 trainign loss: 6.5662 avg training loss: 7.9458
batch: [6130/21305] batch time: 0.741 trainign loss: 7.1864 avg training loss: 7.9455
batch: [6140/21305] batch time: 0.733 trainign loss: 6.2134 avg training loss: 7.9454
batch: [6150/21305] batch time: 0.057 trainign loss: 6.2301 avg training loss: 7.9453
batch: [6160/21305] batch time: 0.132 trainign loss: 6.0816 avg training loss: 7.9452
batch: [6170/21305] batch time: 0.063 trainign loss: 5.0974 avg training loss: 7.9450
batch: [6180/21305] batch time: 0.141 trainign loss: 7.5066 avg training loss: 7.9448
batch: [6190/21305] batch time: 0.059 trainign loss: 7.9667 avg training loss: 7.9448
batch: [6200/21305] batch time: 0.196 trainign loss: 6.9676 avg training loss: 7.9447
batch: [6210/21305] batch time: 0.267 trainign loss: 6.3415 avg training loss: 7.9445
batch: [6220/21305] batch time: 0.856 trainign loss: 7.2470 avg training loss: 7.9444
batch: [6230/21305] batch time: 0.292 trainign loss: 6.8578 avg training loss: 7.9443
batch: [6240/21305] batch time: 0.579 trainign loss: 7.6841 avg training loss: 7.9441
batch: [6250/21305] batch time: 0.557 trainign loss: 7.0018 avg training loss: 7.9440
batch: [6260/21305] batch time: 0.617 trainign loss: 7.7490 avg training loss: 7.9439
batch: [6270/21305] batch time: 0.062 trainign loss: 6.8629 avg training loss: 7.9438
batch: [6280/21305] batch time: 0.056 trainign loss: 7.1065 avg training loss: 7.9437
batch: [6290/21305] batch time: 0.384 trainign loss: 4.3885 avg training loss: 7.9435
batch: [6300/21305] batch time: 0.062 trainign loss: 7.7710 avg training loss: 7.9434
batch: [6310/21305] batch time: 0.056 trainign loss: 6.9249 avg training loss: 7.9433
batch: [6320/21305] batch time: 0.056 trainign loss: 7.2884 avg training loss: 7.9432
batch: [6330/21305] batch time: 0.280 trainign loss: 6.6945 avg training loss: 7.9430
batch: [6340/21305] batch time: 0.056 trainign loss: 4.2255 avg training loss: 7.9428
batch: [6350/21305] batch time: 0.298 trainign loss: 7.1373 avg training loss: 7.9426
batch: [6360/21305] batch time: 0.062 trainign loss: 7.5387 avg training loss: 7.9425
batch: [6370/21305] batch time: 0.056 trainign loss: 6.2585 avg training loss: 7.9424
batch: [6380/21305] batch time: 0.056 trainign loss: 6.7486 avg training loss: 7.9423
batch: [6390/21305] batch time: 0.056 trainign loss: 4.5980 avg training loss: 7.9421
batch: [6400/21305] batch time: 0.058 trainign loss: 7.1746 avg training loss: 7.9419
batch: [6410/21305] batch time: 0.057 trainign loss: 7.7273 avg training loss: 7.9418
batch: [6420/21305] batch time: 0.051 trainign loss: 6.7352 avg training loss: 7.9416
batch: [6430/21305] batch time: 0.058 trainign loss: 5.1799 avg training loss: 7.9414
batch: [6440/21305] batch time: 0.051 trainign loss: 6.4435 avg training loss: 7.9411
batch: [6450/21305] batch time: 0.056 trainign loss: 7.7545 avg training loss: 7.9408
batch: [6460/21305] batch time: 0.057 trainign loss: 6.9904 avg training loss: 7.9408
batch: [6470/21305] batch time: 0.057 trainign loss: 7.3972 avg training loss: 7.9406
batch: [6480/21305] batch time: 0.056 trainign loss: 6.7653 avg training loss: 7.9405
batch: [6490/21305] batch time: 0.063 trainign loss: 6.5513 avg training loss: 7.9404
batch: [6500/21305] batch time: 0.052 trainign loss: 5.8949 avg training loss: 7.9403
batch: [6510/21305] batch time: 0.056 trainign loss: 6.9297 avg training loss: 7.9401
batch: [6520/21305] batch time: 0.061 trainign loss: 6.6365 avg training loss: 7.9400
batch: [6530/21305] batch time: 0.056 trainign loss: 5.4230 avg training loss: 7.9398
batch: [6540/21305] batch time: 0.056 trainign loss: 7.1454 avg training loss: 7.9396
batch: [6550/21305] batch time: 0.056 trainign loss: 5.7165 avg training loss: 7.9395
batch: [6560/21305] batch time: 0.056 trainign loss: 6.3944 avg training loss: 7.9393
batch: [6570/21305] batch time: 0.056 trainign loss: 6.4907 avg training loss: 7.9392
batch: [6580/21305] batch time: 0.053 trainign loss: 5.0991 avg training loss: 7.9390
batch: [6590/21305] batch time: 0.059 trainign loss: 6.2163 avg training loss: 7.9389
batch: [6600/21305] batch time: 0.056 trainign loss: 7.5143 avg training loss: 7.9388
batch: [6610/21305] batch time: 0.063 trainign loss: 7.7216 avg training loss: 7.9386
batch: [6620/21305] batch time: 0.054 trainign loss: 6.7739 avg training loss: 7.9385
batch: [6630/21305] batch time: 0.056 trainign loss: 6.4513 avg training loss: 7.9383
batch: [6640/21305] batch time: 0.056 trainign loss: 4.9142 avg training loss: 7.9381
batch: [6650/21305] batch time: 0.277 trainign loss: 6.2775 avg training loss: 7.9380
batch: [6660/21305] batch time: 0.056 trainign loss: 5.9781 avg training loss: 7.9378
batch: [6670/21305] batch time: 0.062 trainign loss: 6.5570 avg training loss: 7.9376
batch: [6680/21305] batch time: 0.055 trainign loss: 7.6312 avg training loss: 7.9376
batch: [6690/21305] batch time: 0.618 trainign loss: 7.1107 avg training loss: 7.9375
batch: [6700/21305] batch time: 0.053 trainign loss: 5.0603 avg training loss: 7.9372
batch: [6710/21305] batch time: 0.454 trainign loss: 5.7574 avg training loss: 7.9371
batch: [6720/21305] batch time: 0.060 trainign loss: 6.8114 avg training loss: 7.9369
batch: [6730/21305] batch time: 0.081 trainign loss: 6.0034 avg training loss: 7.9367
batch: [6740/21305] batch time: 0.056 trainign loss: 7.7614 avg training loss: 7.9366
batch: [6750/21305] batch time: 0.840 trainign loss: 7.8326 avg training loss: 7.9366
batch: [6760/21305] batch time: 0.058 trainign loss: 6.8231 avg training loss: 7.9365
batch: [6770/21305] batch time: 0.207 trainign loss: 5.7059 avg training loss: 7.9363
batch: [6780/21305] batch time: 0.056 trainign loss: 5.9217 avg training loss: 7.9361
batch: [6790/21305] batch time: 0.463 trainign loss: 6.9855 avg training loss: 7.9360
batch: [6800/21305] batch time: 0.061 trainign loss: 5.7397 avg training loss: 7.9358
batch: [6810/21305] batch time: 0.055 trainign loss: 6.6351 avg training loss: 7.9356
batch: [6820/21305] batch time: 0.060 trainign loss: 5.5244 avg training loss: 7.9355
batch: [6830/21305] batch time: 0.431 trainign loss: 6.4955 avg training loss: 7.9353
batch: [6840/21305] batch time: 0.060 trainign loss: 8.1203 avg training loss: 7.9348
batch: [6850/21305] batch time: 0.854 trainign loss: 6.1544 avg training loss: 7.9348
batch: [6860/21305] batch time: 0.349 trainign loss: 6.4826 avg training loss: 7.9347
batch: [6870/21305] batch time: 0.205 trainign loss: 6.9820 avg training loss: 7.9345
batch: [6880/21305] batch time: 0.308 trainign loss: 6.1222 avg training loss: 7.9343
batch: [6890/21305] batch time: 0.981 trainign loss: 7.7228 avg training loss: 7.9341
batch: [6900/21305] batch time: 0.288 trainign loss: 6.8799 avg training loss: 7.9340
batch: [6910/21305] batch time: 0.740 trainign loss: 6.1750 avg training loss: 7.9339
batch: [6920/21305] batch time: 0.056 trainign loss: 6.9743 avg training loss: 7.9338
batch: [6930/21305] batch time: 2.041 trainign loss: 5.7159 avg training loss: 7.9335
batch: [6940/21305] batch time: 0.057 trainign loss: 5.3449 avg training loss: 7.9333
batch: [6950/21305] batch time: 1.637 trainign loss: 6.1256 avg training loss: 7.9331
batch: [6960/21305] batch time: 0.060 trainign loss: 4.6178 avg training loss: 7.9328
batch: [6970/21305] batch time: 1.962 trainign loss: 4.4624 avg training loss: 7.9327
batch: [6980/21305] batch time: 0.062 trainign loss: 6.9551 avg training loss: 7.9326
batch: [6990/21305] batch time: 0.145 trainign loss: 7.4204 avg training loss: 7.9326
batch: [7000/21305] batch time: 0.056 trainign loss: 5.8197 avg training loss: 7.9324
batch: [7010/21305] batch time: 0.461 trainign loss: 6.3291 avg training loss: 7.9322
batch: [7020/21305] batch time: 0.052 trainign loss: 7.8986 avg training loss: 7.9321
batch: [7030/21305] batch time: 0.059 trainign loss: 6.6375 avg training loss: 7.9320
batch: [7040/21305] batch time: 0.057 trainign loss: 7.2013 avg training loss: 7.9319
batch: [7050/21305] batch time: 1.748 trainign loss: 5.9455 avg training loss: 7.9316
batch: [7060/21305] batch time: 0.062 trainign loss: 5.8255 avg training loss: 7.9315
batch: [7070/21305] batch time: 2.104 trainign loss: 6.9852 avg training loss: 7.9314
batch: [7080/21305] batch time: 0.052 trainign loss: 6.6115 avg training loss: 7.9312
batch: [7090/21305] batch time: 2.158 trainign loss: 6.3328 avg training loss: 7.9311
batch: [7100/21305] batch time: 0.431 trainign loss: 6.3552 avg training loss: 7.9309
batch: [7110/21305] batch time: 1.995 trainign loss: 6.4208 avg training loss: 7.9307
batch: [7120/21305] batch time: 0.061 trainign loss: 4.1324 avg training loss: 7.9303
batch: [7130/21305] batch time: 2.274 trainign loss: 7.6093 avg training loss: 7.9301
batch: [7140/21305] batch time: 0.515 trainign loss: 7.7710 avg training loss: 7.9300
batch: [7150/21305] batch time: 1.387 trainign loss: 5.5056 avg training loss: 7.9299
batch: [7160/21305] batch time: 0.312 trainign loss: 7.4527 avg training loss: 7.9297
batch: [7170/21305] batch time: 1.239 trainign loss: 5.9617 avg training loss: 7.9294
batch: [7180/21305] batch time: 0.456 trainign loss: 6.5738 avg training loss: 7.9293
batch: [7190/21305] batch time: 1.749 trainign loss: 4.8046 avg training loss: 7.9292
batch: [7200/21305] batch time: 0.636 trainign loss: 5.7580 avg training loss: 7.9289
batch: [7210/21305] batch time: 1.319 trainign loss: 6.5394 avg training loss: 7.9288
batch: [7220/21305] batch time: 0.831 trainign loss: 5.3971 avg training loss: 7.9286
batch: [7230/21305] batch time: 1.242 trainign loss: 5.1677 avg training loss: 7.9284
batch: [7240/21305] batch time: 1.642 trainign loss: 7.5571 avg training loss: 7.9282
batch: [7250/21305] batch time: 1.149 trainign loss: 6.0085 avg training loss: 7.9282
batch: [7260/21305] batch time: 1.157 trainign loss: 5.4429 avg training loss: 7.9279
batch: [7270/21305] batch time: 1.001 trainign loss: 7.1111 avg training loss: 7.9278
batch: [7280/21305] batch time: 1.239 trainign loss: 6.4486 avg training loss: 7.9277
batch: [7290/21305] batch time: 0.769 trainign loss: 5.8063 avg training loss: 7.9276
batch: [7300/21305] batch time: 2.207 trainign loss: 6.9450 avg training loss: 7.9274
batch: [7310/21305] batch time: 0.056 trainign loss: 7.3297 avg training loss: 7.9273
batch: [7320/21305] batch time: 2.460 trainign loss: 7.1001 avg training loss: 7.9272
batch: [7330/21305] batch time: 0.054 trainign loss: 5.5320 avg training loss: 7.9270
batch: [7340/21305] batch time: 2.427 trainign loss: 7.3841 avg training loss: 7.9268
batch: [7350/21305] batch time: 0.055 trainign loss: 7.0536 avg training loss: 7.9267
batch: [7360/21305] batch time: 1.810 trainign loss: 6.8174 avg training loss: 7.9265
batch: [7370/21305] batch time: 0.057 trainign loss: 2.5837 avg training loss: 7.9262
batch: [7380/21305] batch time: 2.157 trainign loss: 6.9235 avg training loss: 7.9260
batch: [7390/21305] batch time: 0.318 trainign loss: 7.1639 avg training loss: 7.9259
batch: [7400/21305] batch time: 2.602 trainign loss: 4.8097 avg training loss: 7.9257
batch: [7410/21305] batch time: 0.056 trainign loss: 7.3037 avg training loss: 7.9254
batch: [7420/21305] batch time: 2.330 trainign loss: 7.2347 avg training loss: 7.9253
batch: [7430/21305] batch time: 0.057 trainign loss: 6.6882 avg training loss: 7.9252
batch: [7440/21305] batch time: 2.180 trainign loss: 6.2886 avg training loss: 7.9251
batch: [7450/21305] batch time: 0.051 trainign loss: 6.9150 avg training loss: 7.9249
batch: [7460/21305] batch time: 2.705 trainign loss: 3.0986 avg training loss: 7.9246
batch: [7470/21305] batch time: 0.057 trainign loss: 4.3603 avg training loss: 7.9244
batch: [7480/21305] batch time: 1.953 trainign loss: 6.3933 avg training loss: 7.9242
batch: [7490/21305] batch time: 0.062 trainign loss: 6.5983 avg training loss: 7.9241
batch: [7500/21305] batch time: 2.102 trainign loss: 7.6228 avg training loss: 7.9240
batch: [7510/21305] batch time: 0.056 trainign loss: 6.2238 avg training loss: 7.9239
batch: [7520/21305] batch time: 2.154 trainign loss: 7.3084 avg training loss: 7.9238
batch: [7530/21305] batch time: 0.056 trainign loss: 5.8693 avg training loss: 7.9236
batch: [7540/21305] batch time: 2.316 trainign loss: 7.4113 avg training loss: 7.9235
batch: [7550/21305] batch time: 0.059 trainign loss: 7.6083 avg training loss: 7.9234
batch: [7560/21305] batch time: 2.289 trainign loss: 6.4936 avg training loss: 7.9233
batch: [7570/21305] batch time: 0.063 trainign loss: 6.9865 avg training loss: 7.9231
batch: [7580/21305] batch time: 2.491 trainign loss: 6.7283 avg training loss: 7.9230
batch: [7590/21305] batch time: 0.055 trainign loss: 7.6179 avg training loss: 7.9229
batch: [7600/21305] batch time: 2.580 trainign loss: 6.1055 avg training loss: 7.9227
batch: [7610/21305] batch time: 0.056 trainign loss: 6.1076 avg training loss: 7.9225
batch: [7620/21305] batch time: 1.914 trainign loss: 4.7827 avg training loss: 7.9223
batch: [7630/21305] batch time: 0.056 trainign loss: 7.6407 avg training loss: 7.9221
batch: [7640/21305] batch time: 2.487 trainign loss: 6.8132 avg training loss: 7.9220
batch: [7650/21305] batch time: 0.057 trainign loss: 5.5384 avg training loss: 7.9218
batch: [7660/21305] batch time: 2.085 trainign loss: 7.1449 avg training loss: 7.9216
batch: [7670/21305] batch time: 0.057 trainign loss: 7.2604 avg training loss: 7.9215
batch: [7680/21305] batch time: 2.733 trainign loss: 5.6310 avg training loss: 7.9213
batch: [7690/21305] batch time: 0.062 trainign loss: 5.2048 avg training loss: 7.9211
batch: [7700/21305] batch time: 2.775 trainign loss: 7.2528 avg training loss: 7.9209
batch: [7710/21305] batch time: 0.051 trainign loss: 5.3851 avg training loss: 7.9206
batch: [7720/21305] batch time: 2.534 trainign loss: 6.1878 avg training loss: 7.9205
batch: [7730/21305] batch time: 0.056 trainign loss: 4.6412 avg training loss: 7.9200
batch: [7740/21305] batch time: 2.267 trainign loss: 7.8965 avg training loss: 7.9199
batch: [7750/21305] batch time: 0.054 trainign loss: 8.1522 avg training loss: 7.9200
batch: [7760/21305] batch time: 2.562 trainign loss: 7.2665 avg training loss: 7.9199
batch: [7770/21305] batch time: 0.055 trainign loss: 5.0956 avg training loss: 7.9198
batch: [7780/21305] batch time: 1.780 trainign loss: 7.4669 avg training loss: 7.9196
batch: [7790/21305] batch time: 0.056 trainign loss: 7.4902 avg training loss: 7.9196
batch: [7800/21305] batch time: 2.241 trainign loss: 7.2548 avg training loss: 7.9195
batch: [7810/21305] batch time: 0.053 trainign loss: 6.1835 avg training loss: 7.9192
batch: [7820/21305] batch time: 2.303 trainign loss: 6.7570 avg training loss: 7.9191
batch: [7830/21305] batch time: 0.060 trainign loss: 6.3863 avg training loss: 7.9191
batch: [7840/21305] batch time: 2.499 trainign loss: 7.0159 avg training loss: 7.9190
batch: [7850/21305] batch time: 0.052 trainign loss: 5.3499 avg training loss: 7.9188
batch: [7860/21305] batch time: 2.177 trainign loss: 5.7179 avg training loss: 7.9186
batch: [7870/21305] batch time: 0.059 trainign loss: 6.2950 avg training loss: 7.9184
batch: [7880/21305] batch time: 2.465 trainign loss: 4.6269 avg training loss: 7.9183
batch: [7890/21305] batch time: 0.521 trainign loss: 6.4777 avg training loss: 7.9181
batch: [7900/21305] batch time: 1.855 trainign loss: 5.6365 avg training loss: 7.9178
batch: [7910/21305] batch time: 1.922 trainign loss: 7.7237 avg training loss: 7.9177
batch: [7920/21305] batch time: 0.084 trainign loss: 6.1074 avg training loss: 7.9176
batch: [7930/21305] batch time: 2.409 trainign loss: 6.3789 avg training loss: 7.9175
batch: [7940/21305] batch time: 0.056 trainign loss: 7.5508 avg training loss: 7.9174
batch: [7950/21305] batch time: 2.157 trainign loss: 7.2591 avg training loss: 7.9174
batch: [7960/21305] batch time: 0.056 trainign loss: 6.3779 avg training loss: 7.9172
batch: [7970/21305] batch time: 2.320 trainign loss: 6.5226 avg training loss: 7.9171
batch: [7980/21305] batch time: 0.056 trainign loss: 7.2785 avg training loss: 7.9169
batch: [7990/21305] batch time: 2.161 trainign loss: 7.3932 avg training loss: 7.9168
batch: [8000/21305] batch time: 0.059 trainign loss: 6.8247 avg training loss: 7.9166
batch: [8010/21305] batch time: 2.000 trainign loss: 5.7626 avg training loss: 7.9164
batch: [8020/21305] batch time: 0.278 trainign loss: 2.7140 avg training loss: 7.9161
batch: [8030/21305] batch time: 1.702 trainign loss: 5.8306 avg training loss: 7.9157
batch: [8040/21305] batch time: 0.585 trainign loss: 7.7164 avg training loss: 7.9156
batch: [8050/21305] batch time: 1.557 trainign loss: 8.3021 avg training loss: 7.9156
batch: [8060/21305] batch time: 1.376 trainign loss: 7.3300 avg training loss: 7.9156
batch: [8070/21305] batch time: 1.266 trainign loss: 6.6076 avg training loss: 7.9155
batch: [8080/21305] batch time: 1.464 trainign loss: 7.0835 avg training loss: 7.9154
batch: [8090/21305] batch time: 0.748 trainign loss: 7.2269 avg training loss: 7.9153
batch: [8100/21305] batch time: 1.907 trainign loss: 6.4338 avg training loss: 7.9152
batch: [8110/21305] batch time: 1.611 trainign loss: 7.3625 avg training loss: 7.9150
batch: [8120/21305] batch time: 0.989 trainign loss: 6.3627 avg training loss: 7.9148
batch: [8130/21305] batch time: 0.983 trainign loss: 6.1214 avg training loss: 7.9146
batch: [8140/21305] batch time: 2.094 trainign loss: 6.3562 avg training loss: 7.9145
batch: [8150/21305] batch time: 0.462 trainign loss: 6.3736 avg training loss: 7.9143
batch: [8160/21305] batch time: 1.491 trainign loss: 6.5695 avg training loss: 7.9142
batch: [8170/21305] batch time: 1.336 trainign loss: 6.7955 avg training loss: 7.9140
batch: [8180/21305] batch time: 1.327 trainign loss: 6.4472 avg training loss: 7.9139
batch: [8190/21305] batch time: 0.695 trainign loss: 7.3940 avg training loss: 7.9138
batch: [8200/21305] batch time: 1.421 trainign loss: 6.8255 avg training loss: 7.9138
batch: [8210/21305] batch time: 1.143 trainign loss: 6.2037 avg training loss: 7.9136
batch: [8220/21305] batch time: 1.112 trainign loss: 6.4058 avg training loss: 7.9135
batch: [8230/21305] batch time: 0.900 trainign loss: 7.0343 avg training loss: 7.9133
batch: [8240/21305] batch time: 2.346 trainign loss: 7.3871 avg training loss: 7.9132
batch: [8250/21305] batch time: 0.831 trainign loss: 6.3650 avg training loss: 7.9131
batch: [8260/21305] batch time: 1.285 trainign loss: 6.8548 avg training loss: 7.9129
batch: [8270/21305] batch time: 0.595 trainign loss: 7.6749 avg training loss: 7.9127
batch: [8280/21305] batch time: 2.048 trainign loss: 6.5210 avg training loss: 7.9126
batch: [8290/21305] batch time: 0.054 trainign loss: 6.0024 avg training loss: 7.9124
batch: [8300/21305] batch time: 2.280 trainign loss: 6.3796 avg training loss: 7.9123
batch: [8310/21305] batch time: 0.060 trainign loss: 7.6007 avg training loss: 7.9122
batch: [8320/21305] batch time: 2.397 trainign loss: 7.1241 avg training loss: 7.9121
batch: [8330/21305] batch time: 0.061 trainign loss: 7.1974 avg training loss: 7.9120
batch: [8340/21305] batch time: 2.450 trainign loss: 6.0691 avg training loss: 7.9119
batch: [8350/21305] batch time: 0.056 trainign loss: 5.4586 avg training loss: 7.9116
batch: [8360/21305] batch time: 1.665 trainign loss: 5.7763 avg training loss: 7.9115
batch: [8370/21305] batch time: 0.055 trainign loss: 7.6952 avg training loss: 7.9113
batch: [8380/21305] batch time: 1.281 trainign loss: 5.1960 avg training loss: 7.9112
batch: [8390/21305] batch time: 0.056 trainign loss: 4.5599 avg training loss: 7.9108
batch: [8400/21305] batch time: 0.377 trainign loss: 8.3016 avg training loss: 7.9104
batch: [8410/21305] batch time: 0.062 trainign loss: 6.5809 avg training loss: 7.9103
batch: [8420/21305] batch time: 0.225 trainign loss: 7.0216 avg training loss: 7.9099
batch: [8430/21305] batch time: 0.057 trainign loss: 7.7467 avg training loss: 7.9098
batch: [8440/21305] batch time: 0.644 trainign loss: 8.0726 avg training loss: 7.9098
batch: [8450/21305] batch time: 0.056 trainign loss: 7.4754 avg training loss: 7.9097
batch: [8460/21305] batch time: 0.475 trainign loss: 7.5947 avg training loss: 7.9097
batch: [8470/21305] batch time: 0.057 trainign loss: 7.9466 avg training loss: 7.9096
batch: [8480/21305] batch time: 0.807 trainign loss: 7.1863 avg training loss: 7.9095
batch: [8490/21305] batch time: 0.056 trainign loss: 6.7438 avg training loss: 7.9093
batch: [8500/21305] batch time: 0.271 trainign loss: 5.7157 avg training loss: 7.9091
batch: [8510/21305] batch time: 0.061 trainign loss: 5.9542 avg training loss: 7.9089
batch: [8520/21305] batch time: 0.056 trainign loss: 7.0426 avg training loss: 7.9087
batch: [8530/21305] batch time: 0.546 trainign loss: 6.8682 avg training loss: 7.9086
batch: [8540/21305] batch time: 0.056 trainign loss: 7.1490 avg training loss: 7.9085
batch: [8550/21305] batch time: 0.487 trainign loss: 7.9759 avg training loss: 7.9084
batch: [8560/21305] batch time: 0.057 trainign loss: 7.0788 avg training loss: 7.9083
batch: [8570/21305] batch time: 0.574 trainign loss: 5.7708 avg training loss: 7.9081
batch: [8580/21305] batch time: 0.063 trainign loss: 6.7897 avg training loss: 7.9080
batch: [8590/21305] batch time: 0.062 trainign loss: 7.3343 avg training loss: 7.9078
batch: [8600/21305] batch time: 0.056 trainign loss: 6.6650 avg training loss: 7.9077
batch: [8610/21305] batch time: 0.096 trainign loss: 6.1541 avg training loss: 7.9075
batch: [8620/21305] batch time: 0.057 trainign loss: 7.2043 avg training loss: 7.9074
batch: [8630/21305] batch time: 0.054 trainign loss: 5.2072 avg training loss: 7.9072
batch: [8640/21305] batch time: 0.056 trainign loss: 6.8629 avg training loss: 7.9071
batch: [8650/21305] batch time: 0.056 trainign loss: 4.8195 avg training loss: 7.9069
batch: [8660/21305] batch time: 0.056 trainign loss: 4.3667 avg training loss: 7.9067
batch: [8670/21305] batch time: 0.056 trainign loss: 5.6221 avg training loss: 7.9065
batch: [8680/21305] batch time: 0.062 trainign loss: 7.3676 avg training loss: 7.9064
batch: [8690/21305] batch time: 0.051 trainign loss: 6.1268 avg training loss: 7.9063
batch: [8700/21305] batch time: 0.056 trainign loss: 5.4307 avg training loss: 7.9061
batch: [8710/21305] batch time: 0.051 trainign loss: 1.1889 avg training loss: 7.9058
batch: [8720/21305] batch time: 0.060 trainign loss: 0.0008 avg training loss: 7.9049
batch: [8730/21305] batch time: 0.054 trainign loss: 0.0001 avg training loss: 7.9041
batch: [8740/21305] batch time: 0.056 trainign loss: 9.0154 avg training loss: 7.9042
batch: [8750/21305] batch time: 0.052 trainign loss: 8.9057 avg training loss: 7.9043
batch: [8760/21305] batch time: 0.060 trainign loss: 7.7838 avg training loss: 7.9043
batch: [8770/21305] batch time: 0.058 trainign loss: 6.8610 avg training loss: 7.9041
batch: [8780/21305] batch time: 0.063 trainign loss: 5.7182 avg training loss: 7.9039
batch: [8790/21305] batch time: 0.054 trainign loss: 6.8337 avg training loss: 7.9039
batch: [8800/21305] batch time: 0.057 trainign loss: 6.1355 avg training loss: 7.9037
batch: [8810/21305] batch time: 0.057 trainign loss: 7.3338 avg training loss: 7.9036
batch: [8820/21305] batch time: 0.056 trainign loss: 6.6858 avg training loss: 7.9035
batch: [8830/21305] batch time: 0.051 trainign loss: 7.1248 avg training loss: 7.9034
batch: [8840/21305] batch time: 0.056 trainign loss: 6.5916 avg training loss: 7.9032
batch: [8850/21305] batch time: 0.050 trainign loss: 6.3731 avg training loss: 7.9030
batch: [8860/21305] batch time: 0.056 trainign loss: 7.3311 avg training loss: 7.9029
batch: [8870/21305] batch time: 0.056 trainign loss: 7.0825 avg training loss: 7.9029
batch: [8880/21305] batch time: 0.056 trainign loss: 5.4999 avg training loss: 7.9027
batch: [8890/21305] batch time: 0.054 trainign loss: 7.2300 avg training loss: 7.9026
batch: [8900/21305] batch time: 0.063 trainign loss: 6.0145 avg training loss: 7.9024
batch: [8910/21305] batch time: 0.061 trainign loss: 5.5223 avg training loss: 7.9022
batch: [8920/21305] batch time: 0.056 trainign loss: 2.5344 avg training loss: 7.9020
batch: [8930/21305] batch time: 0.050 trainign loss: 7.8410 avg training loss: 7.9018
batch: [8940/21305] batch time: 0.062 trainign loss: 6.6812 avg training loss: 7.9017
batch: [8950/21305] batch time: 0.059 trainign loss: 6.5287 avg training loss: 7.9016
batch: [8960/21305] batch time: 0.056 trainign loss: 6.7132 avg training loss: 7.9015
batch: [8970/21305] batch time: 0.056 trainign loss: 6.7154 avg training loss: 7.9014
batch: [8980/21305] batch time: 0.061 trainign loss: 5.7787 avg training loss: 7.9012
batch: [8990/21305] batch time: 0.132 trainign loss: 6.6643 avg training loss: 7.9011
batch: [9000/21305] batch time: 0.056 trainign loss: 1.1443 avg training loss: 7.9007
batch: [9010/21305] batch time: 0.532 trainign loss: 8.7406 avg training loss: 7.9005
batch: [9020/21305] batch time: 0.056 trainign loss: 6.6160 avg training loss: 7.9005
batch: [9030/21305] batch time: 0.908 trainign loss: 7.0543 avg training loss: 7.9003
batch: [9040/21305] batch time: 0.056 trainign loss: 7.1070 avg training loss: 7.9003
batch: [9050/21305] batch time: 0.806 trainign loss: 6.2986 avg training loss: 7.9001
batch: [9060/21305] batch time: 0.056 trainign loss: 5.9385 avg training loss: 7.9000
batch: [9070/21305] batch time: 0.708 trainign loss: 3.0043 avg training loss: 7.8997
batch: [9080/21305] batch time: 0.056 trainign loss: 8.3629 avg training loss: 7.8997
batch: [9090/21305] batch time: 0.693 trainign loss: 6.9582 avg training loss: 7.8996
batch: [9100/21305] batch time: 0.062 trainign loss: 7.0561 avg training loss: 7.8995
batch: [9110/21305] batch time: 0.056 trainign loss: 5.6029 avg training loss: 7.8994
batch: [9120/21305] batch time: 0.059 trainign loss: 7.3781 avg training loss: 7.8992
batch: [9130/21305] batch time: 0.056 trainign loss: 5.2383 avg training loss: 7.8989
batch: [9140/21305] batch time: 1.147 trainign loss: 7.1800 avg training loss: 7.8987
batch: [9150/21305] batch time: 0.062 trainign loss: 6.7622 avg training loss: 7.8986
batch: [9160/21305] batch time: 1.450 trainign loss: 7.2892 avg training loss: 7.8985
batch: [9170/21305] batch time: 0.056 trainign loss: 5.9385 avg training loss: 7.8983
batch: [9180/21305] batch time: 1.579 trainign loss: 7.4419 avg training loss: 7.8982
batch: [9190/21305] batch time: 0.060 trainign loss: 6.9065 avg training loss: 7.8981
batch: [9200/21305] batch time: 2.182 trainign loss: 7.0883 avg training loss: 7.8979
batch: [9210/21305] batch time: 0.060 trainign loss: 6.0413 avg training loss: 7.8978
batch: [9220/21305] batch time: 1.365 trainign loss: 5.4245 avg training loss: 7.8976
batch: [9230/21305] batch time: 0.056 trainign loss: 7.3148 avg training loss: 7.8975
batch: [9240/21305] batch time: 1.711 trainign loss: 5.3368 avg training loss: 7.8974
batch: [9250/21305] batch time: 0.056 trainign loss: 7.0511 avg training loss: 7.8972
batch: [9260/21305] batch time: 1.061 trainign loss: 6.7543 avg training loss: 7.8971
batch: [9270/21305] batch time: 0.108 trainign loss: 6.7206 avg training loss: 7.8969
batch: [9280/21305] batch time: 0.681 trainign loss: 6.4340 avg training loss: 7.8968
batch: [9290/21305] batch time: 1.469 trainign loss: 5.2448 avg training loss: 7.8965
batch: [9300/21305] batch time: 0.080 trainign loss: 7.5332 avg training loss: 7.8963
batch: [9310/21305] batch time: 1.120 trainign loss: 7.3123 avg training loss: 7.8962
batch: [9320/21305] batch time: 1.560 trainign loss: 5.2745 avg training loss: 7.8960
batch: [9330/21305] batch time: 0.606 trainign loss: 5.2147 avg training loss: 7.8958
batch: [9340/21305] batch time: 1.821 trainign loss: 7.5239 avg training loss: 7.8957
batch: [9350/21305] batch time: 1.127 trainign loss: 7.5678 avg training loss: 7.8957
batch: [9360/21305] batch time: 0.992 trainign loss: 5.4300 avg training loss: 7.8955
batch: [9370/21305] batch time: 1.319 trainign loss: 5.4268 avg training loss: 7.8954
batch: [9380/21305] batch time: 1.249 trainign loss: 6.0068 avg training loss: 7.8952
batch: [9390/21305] batch time: 1.195 trainign loss: 6.7623 avg training loss: 7.8951
batch: [9400/21305] batch time: 0.362 trainign loss: 6.8862 avg training loss: 7.8950
batch: [9410/21305] batch time: 2.102 trainign loss: 5.9861 avg training loss: 7.8948
batch: [9420/21305] batch time: 0.056 trainign loss: 3.6573 avg training loss: 7.8946
batch: [9430/21305] batch time: 1.952 trainign loss: 7.2786 avg training loss: 7.8944
batch: [9440/21305] batch time: 0.169 trainign loss: 7.4247 avg training loss: 7.8943
batch: [9450/21305] batch time: 1.502 trainign loss: 5.1939 avg training loss: 7.8940
batch: [9460/21305] batch time: 1.036 trainign loss: 7.4405 avg training loss: 7.8939
batch: [9470/21305] batch time: 0.637 trainign loss: 7.7893 avg training loss: 7.8938
batch: [9480/21305] batch time: 1.050 trainign loss: 5.9055 avg training loss: 7.8936
batch: [9490/21305] batch time: 0.739 trainign loss: 7.7644 avg training loss: 7.8934
batch: [9500/21305] batch time: 1.593 trainign loss: 7.7608 avg training loss: 7.8934
batch: [9510/21305] batch time: 0.712 trainign loss: 6.0775 avg training loss: 7.8932
batch: [9520/21305] batch time: 1.268 trainign loss: 7.3348 avg training loss: 7.8931
batch: [9530/21305] batch time: 0.715 trainign loss: 7.0127 avg training loss: 7.8930
batch: [9540/21305] batch time: 1.317 trainign loss: 6.2001 avg training loss: 7.8928
batch: [9550/21305] batch time: 0.826 trainign loss: 5.1046 avg training loss: 7.8927
batch: [9560/21305] batch time: 1.840 trainign loss: 5.2856 avg training loss: 7.8925
batch: [9570/21305] batch time: 0.056 trainign loss: 7.0005 avg training loss: 7.8923
batch: [9580/21305] batch time: 2.330 trainign loss: 8.2948 avg training loss: 7.8921
batch: [9590/21305] batch time: 0.060 trainign loss: 7.4222 avg training loss: 7.8920
batch: [9600/21305] batch time: 1.803 trainign loss: 7.2301 avg training loss: 7.8920
batch: [9610/21305] batch time: 0.056 trainign loss: 6.6804 avg training loss: 7.8918
batch: [9620/21305] batch time: 1.202 trainign loss: 6.9335 avg training loss: 7.8917
batch: [9630/21305] batch time: 0.056 trainign loss: 7.6300 avg training loss: 7.8915
batch: [9640/21305] batch time: 1.722 trainign loss: 7.7032 avg training loss: 7.8914
batch: [9650/21305] batch time: 0.056 trainign loss: 6.5016 avg training loss: 7.8913
batch: [9660/21305] batch time: 1.911 trainign loss: 4.5074 avg training loss: 7.8911
batch: [9670/21305] batch time: 0.059 trainign loss: 7.0739 avg training loss: 7.8910
batch: [9680/21305] batch time: 1.015 trainign loss: 7.2624 avg training loss: 7.8909
batch: [9690/21305] batch time: 0.063 trainign loss: 6.8693 avg training loss: 7.8908
batch: [9700/21305] batch time: 0.896 trainign loss: 6.5435 avg training loss: 7.8907
batch: [9710/21305] batch time: 0.063 trainign loss: 6.4135 avg training loss: 7.8906
batch: [9720/21305] batch time: 0.873 trainign loss: 7.1772 avg training loss: 7.8904
batch: [9730/21305] batch time: 0.060 trainign loss: 6.9015 avg training loss: 7.8904
batch: [9740/21305] batch time: 1.163 trainign loss: 6.5260 avg training loss: 7.8902
batch: [9750/21305] batch time: 0.056 trainign loss: 7.4999 avg training loss: 7.8900
batch: [9760/21305] batch time: 1.674 trainign loss: 6.7373 avg training loss: 7.8899
batch: [9770/21305] batch time: 0.060 trainign loss: 6.9162 avg training loss: 7.8898
batch: [9780/21305] batch time: 2.360 trainign loss: 7.6549 avg training loss: 7.8897
batch: [9790/21305] batch time: 0.063 trainign loss: 6.5988 avg training loss: 7.8895
batch: [9800/21305] batch time: 2.389 trainign loss: 6.4105 avg training loss: 7.8894
batch: [9810/21305] batch time: 0.059 trainign loss: 7.6566 avg training loss: 7.8892
batch: [9820/21305] batch time: 2.550 trainign loss: 5.6761 avg training loss: 7.8890
batch: [9830/21305] batch time: 0.051 trainign loss: 5.7618 avg training loss: 7.8888
batch: [9840/21305] batch time: 2.317 trainign loss: 4.3562 avg training loss: 7.8885
batch: [9850/21305] batch time: 0.056 trainign loss: 7.0755 avg training loss: 7.8884
batch: [9860/21305] batch time: 1.937 trainign loss: 3.6457 avg training loss: 7.8882
batch: [9870/21305] batch time: 0.056 trainign loss: 5.5727 avg training loss: 7.8881
batch: [9880/21305] batch time: 2.022 trainign loss: 7.4134 avg training loss: 7.8878
batch: [9890/21305] batch time: 0.285 trainign loss: 6.8823 avg training loss: 7.8877
batch: [9900/21305] batch time: 1.113 trainign loss: 6.4717 avg training loss: 7.8876
batch: [9910/21305] batch time: 0.056 trainign loss: 6.6422 avg training loss: 7.8875
batch: [9920/21305] batch time: 1.058 trainign loss: 6.6247 avg training loss: 7.8873
batch: [9930/21305] batch time: 0.063 trainign loss: 6.0231 avg training loss: 7.8872
batch: [9940/21305] batch time: 1.430 trainign loss: 7.5409 avg training loss: 7.8871
batch: [9950/21305] batch time: 0.696 trainign loss: 6.4073 avg training loss: 7.8869
batch: [9960/21305] batch time: 1.503 trainign loss: 7.2386 avg training loss: 7.8868
batch: [9970/21305] batch time: 0.483 trainign loss: 6.9944 avg training loss: 7.8867
batch: [9980/21305] batch time: 2.370 trainign loss: 6.4439 avg training loss: 7.8866
batch: [9990/21305] batch time: 0.056 trainign loss: 6.9490 avg training loss: 7.8864
batch: [10000/21305] batch time: 1.775 trainign loss: 5.6266 avg training loss: 7.8863
batch: [10010/21305] batch time: 0.930 trainign loss: 6.6485 avg training loss: 7.8861
batch: [10020/21305] batch time: 0.890 trainign loss: 7.5408 avg training loss: 7.8861
batch: [10030/21305] batch time: 0.659 trainign loss: 6.5805 avg training loss: 7.8860
batch: [10040/21305] batch time: 1.599 trainign loss: 6.3497 avg training loss: 7.8859
batch: [10050/21305] batch time: 0.261 trainign loss: 6.1168 avg training loss: 7.8857
batch: [10060/21305] batch time: 1.622 trainign loss: 4.9181 avg training loss: 7.8854
batch: [10070/21305] batch time: 0.544 trainign loss: 6.7748 avg training loss: 7.8853
batch: [10080/21305] batch time: 1.939 trainign loss: 5.8618 avg training loss: 7.8852
batch: [10090/21305] batch time: 0.795 trainign loss: 8.5580 avg training loss: 7.8849
batch: [10100/21305] batch time: 1.136 trainign loss: 8.0559 avg training loss: 7.8849
batch: [10110/21305] batch time: 0.378 trainign loss: 6.4468 avg training loss: 7.8848
batch: [10120/21305] batch time: 2.056 trainign loss: 7.3364 avg training loss: 7.8846
batch: [10130/21305] batch time: 0.063 trainign loss: 6.7191 avg training loss: 7.8846
batch: [10140/21305] batch time: 2.351 trainign loss: 7.4426 avg training loss: 7.8845
batch: [10150/21305] batch time: 0.056 trainign loss: 6.7639 avg training loss: 7.8843
batch: [10160/21305] batch time: 0.720 trainign loss: 6.1442 avg training loss: 7.8842
batch: [10170/21305] batch time: 0.056 trainign loss: 7.2870 avg training loss: 7.8841
batch: [10180/21305] batch time: 0.355 trainign loss: 6.9592 avg training loss: 7.8840
batch: [10190/21305] batch time: 0.286 trainign loss: 7.0188 avg training loss: 7.8839
batch: [10200/21305] batch time: 0.707 trainign loss: 5.6608 avg training loss: 7.8837
batch: [10210/21305] batch time: 0.522 trainign loss: 6.6094 avg training loss: 7.8836
batch: [10220/21305] batch time: 0.094 trainign loss: 6.1928 avg training loss: 7.8833
batch: [10230/21305] batch time: 0.447 trainign loss: 5.4214 avg training loss: 7.8832
batch: [10240/21305] batch time: 0.360 trainign loss: 7.2021 avg training loss: 7.8830
batch: [10250/21305] batch time: 0.686 trainign loss: 7.6455 avg training loss: 7.8828
batch: [10260/21305] batch time: 0.175 trainign loss: 7.6452 avg training loss: 7.8827
batch: [10270/21305] batch time: 0.285 trainign loss: 7.0889 avg training loss: 7.8826
batch: [10280/21305] batch time: 0.062 trainign loss: 7.0227 avg training loss: 7.8825
batch: [10290/21305] batch time: 0.559 trainign loss: 6.4828 avg training loss: 7.8823
batch: [10300/21305] batch time: 0.051 trainign loss: 6.1247 avg training loss: 7.8822
batch: [10310/21305] batch time: 0.908 trainign loss: 5.9696 avg training loss: 7.8819
batch: [10320/21305] batch time: 0.061 trainign loss: 2.5608 avg training loss: 7.8817
batch: [10330/21305] batch time: 1.132 trainign loss: 7.0979 avg training loss: 7.8816
batch: [10340/21305] batch time: 0.056 trainign loss: 7.5725 avg training loss: 7.8815
batch: [10350/21305] batch time: 1.274 trainign loss: 7.5259 avg training loss: 7.8814
batch: [10360/21305] batch time: 0.056 trainign loss: 6.6842 avg training loss: 7.8813
batch: [10370/21305] batch time: 1.083 trainign loss: 6.3140 avg training loss: 7.8812
batch: [10380/21305] batch time: 0.056 trainign loss: 7.0004 avg training loss: 7.8811
batch: [10390/21305] batch time: 2.527 trainign loss: 6.9978 avg training loss: 7.8809
batch: [10400/21305] batch time: 0.056 trainign loss: 6.0385 avg training loss: 7.8807
batch: [10410/21305] batch time: 2.062 trainign loss: 6.3792 avg training loss: 7.8805
batch: [10420/21305] batch time: 0.056 trainign loss: 7.8768 avg training loss: 7.8804
batch: [10430/21305] batch time: 2.337 trainign loss: 5.4158 avg training loss: 7.8803
batch: [10440/21305] batch time: 0.056 trainign loss: 6.4512 avg training loss: 7.8800
batch: [10450/21305] batch time: 2.302 trainign loss: 6.5995 avg training loss: 7.8799
batch: [10460/21305] batch time: 0.062 trainign loss: 7.0606 avg training loss: 7.8797
batch: [10470/21305] batch time: 1.882 trainign loss: 5.7314 avg training loss: 7.8796
batch: [10480/21305] batch time: 0.061 trainign loss: 7.7833 avg training loss: 7.8794
batch: [10490/21305] batch time: 2.222 trainign loss: 7.3549 avg training loss: 7.8793
batch: [10500/21305] batch time: 0.056 trainign loss: 5.6207 avg training loss: 7.8792
batch: [10510/21305] batch time: 2.352 trainign loss: 4.7790 avg training loss: 7.8790
batch: [10520/21305] batch time: 0.063 trainign loss: 7.1666 avg training loss: 7.8788
batch: [10530/21305] batch time: 2.189 trainign loss: 6.8849 avg training loss: 7.8787
batch: [10540/21305] batch time: 0.202 trainign loss: 5.6179 avg training loss: 7.8786
batch: [10550/21305] batch time: 1.887 trainign loss: 5.1605 avg training loss: 7.8784
batch: [10560/21305] batch time: 0.158 trainign loss: 8.7695 avg training loss: 7.8779
batch: [10570/21305] batch time: 1.731 trainign loss: 7.1777 avg training loss: 7.8779
batch: [10580/21305] batch time: 0.549 trainign loss: 7.8987 avg training loss: 7.8778
batch: [10590/21305] batch time: 1.524 trainign loss: 7.8594 avg training loss: 7.8777
batch: [10600/21305] batch time: 0.056 trainign loss: 6.8205 avg training loss: 7.8776
batch: [10610/21305] batch time: 1.625 trainign loss: 4.4885 avg training loss: 7.8774
batch: [10620/21305] batch time: 0.271 trainign loss: 6.9947 avg training loss: 7.8773
batch: [10630/21305] batch time: 1.203 trainign loss: 6.3728 avg training loss: 7.8772
batch: [10640/21305] batch time: 0.443 trainign loss: 5.8446 avg training loss: 7.8769
batch: [10650/21305] batch time: 1.696 trainign loss: 7.8931 avg training loss: 7.8768
batch: [10660/21305] batch time: 0.113 trainign loss: 6.9508 avg training loss: 7.8767
batch: [10670/21305] batch time: 1.212 trainign loss: 4.6890 avg training loss: 7.8765
batch: [10680/21305] batch time: 0.057 trainign loss: 6.9654 avg training loss: 7.8763
batch: [10690/21305] batch time: 1.896 trainign loss: 6.5781 avg training loss: 7.8761
batch: [10700/21305] batch time: 0.469 trainign loss: 7.2699 avg training loss: 7.8759
batch: [10710/21305] batch time: 1.953 trainign loss: 6.4809 avg training loss: 7.8757
batch: [10720/21305] batch time: 0.185 trainign loss: 7.5799 avg training loss: 7.8754
batch: [10730/21305] batch time: 1.958 trainign loss: 8.0563 avg training loss: 7.8753
batch: [10740/21305] batch time: 0.205 trainign loss: 7.6232 avg training loss: 7.8753
batch: [10750/21305] batch time: 2.283 trainign loss: 7.0083 avg training loss: 7.8752
batch: [10760/21305] batch time: 0.057 trainign loss: 4.4890 avg training loss: 7.8750
batch: [10770/21305] batch time: 2.439 trainign loss: 6.6650 avg training loss: 7.8748
batch: [10780/21305] batch time: 0.062 trainign loss: 7.6214 avg training loss: 7.8748
batch: [10790/21305] batch time: 2.557 trainign loss: 6.8540 avg training loss: 7.8746
batch: [10800/21305] batch time: 0.056 trainign loss: 6.1983 avg training loss: 7.8745
batch: [10810/21305] batch time: 2.101 trainign loss: 6.8718 avg training loss: 7.8744
batch: [10820/21305] batch time: 0.057 trainign loss: 4.3678 avg training loss: 7.8741
batch: [10830/21305] batch time: 2.298 trainign loss: 1.7701 avg training loss: 7.8738
batch: [10840/21305] batch time: 0.057 trainign loss: 7.3344 avg training loss: 7.8737
batch: [10850/21305] batch time: 2.466 trainign loss: 7.3700 avg training loss: 7.8736
batch: [10860/21305] batch time: 0.056 trainign loss: 6.7792 avg training loss: 7.8736
batch: [10870/21305] batch time: 2.154 trainign loss: 6.0155 avg training loss: 7.8734
batch: [10880/21305] batch time: 0.052 trainign loss: 5.1126 avg training loss: 7.8732
batch: [10890/21305] batch time: 2.335 trainign loss: 7.0883 avg training loss: 7.8730
batch: [10900/21305] batch time: 0.057 trainign loss: 7.8174 avg training loss: 7.8730
batch: [10910/21305] batch time: 2.404 trainign loss: 7.3082 avg training loss: 7.8728
batch: [10920/21305] batch time: 0.060 trainign loss: 6.8486 avg training loss: 7.8727
batch: [10930/21305] batch time: 2.456 trainign loss: 5.4307 avg training loss: 7.8725
batch: [10940/21305] batch time: 0.062 trainign loss: 7.0023 avg training loss: 7.8723
batch: [10950/21305] batch time: 2.244 trainign loss: 5.9442 avg training loss: 7.8721
batch: [10960/21305] batch time: 0.190 trainign loss: 6.9812 avg training loss: 7.8720
batch: [10970/21305] batch time: 2.205 trainign loss: 6.9024 avg training loss: 7.8720
batch: [10980/21305] batch time: 0.057 trainign loss: 5.2137 avg training loss: 7.8718
batch: [10990/21305] batch time: 2.343 trainign loss: 7.0785 avg training loss: 7.8716
batch: [11000/21305] batch time: 0.063 trainign loss: 6.3250 avg training loss: 7.8714
batch: [11010/21305] batch time: 1.845 trainign loss: 6.1416 avg training loss: 7.8712
batch: [11020/21305] batch time: 0.218 trainign loss: 7.3354 avg training loss: 7.8711
batch: [11030/21305] batch time: 2.575 trainign loss: 5.4913 avg training loss: 7.8709
batch: [11040/21305] batch time: 0.063 trainign loss: 7.3077 avg training loss: 7.8707
batch: [11050/21305] batch time: 2.275 trainign loss: 6.5873 avg training loss: 7.8706
batch: [11060/21305] batch time: 0.063 trainign loss: 6.9264 avg training loss: 7.8705
batch: [11070/21305] batch time: 2.032 trainign loss: 5.5526 avg training loss: 7.8703
batch: [11080/21305] batch time: 0.199 trainign loss: 7.4798 avg training loss: 7.8702
batch: [11090/21305] batch time: 2.028 trainign loss: 7.6533 avg training loss: 7.8701
batch: [11100/21305] batch time: 0.283 trainign loss: 6.3338 avg training loss: 7.8700
batch: [11110/21305] batch time: 2.313 trainign loss: 7.1698 avg training loss: 7.8699
batch: [11120/21305] batch time: 0.056 trainign loss: 7.1160 avg training loss: 7.8698
batch: [11130/21305] batch time: 2.381 trainign loss: 5.4146 avg training loss: 7.8697
batch: [11140/21305] batch time: 0.255 trainign loss: 4.1205 avg training loss: 7.8695
batch: [11150/21305] batch time: 2.378 trainign loss: 6.7693 avg training loss: 7.8693
batch: [11160/21305] batch time: 0.051 trainign loss: 6.7785 avg training loss: 7.8692
batch: [11170/21305] batch time: 2.556 trainign loss: 3.9809 avg training loss: 7.8689
batch: [11180/21305] batch time: 0.056 trainign loss: 8.5407 avg training loss: 7.8686
batch: [11190/21305] batch time: 2.461 trainign loss: 7.7547 avg training loss: 7.8685
batch: [11200/21305] batch time: 0.063 trainign loss: 7.7892 avg training loss: 7.8684
batch: [11210/21305] batch time: 2.050 trainign loss: 5.8138 avg training loss: 7.8683
batch: [11220/21305] batch time: 0.056 trainign loss: 6.6202 avg training loss: 7.8682
batch: [11230/21305] batch time: 2.151 trainign loss: 5.9705 avg training loss: 7.8681
batch: [11240/21305] batch time: 0.062 trainign loss: 6.1832 avg training loss: 7.8679
batch: [11250/21305] batch time: 1.959 trainign loss: 0.9022 avg training loss: 7.8675
batch: [11260/21305] batch time: 0.062 trainign loss: 7.9341 avg training loss: 7.8675
batch: [11270/21305] batch time: 2.436 trainign loss: 6.8578 avg training loss: 7.8674
batch: [11280/21305] batch time: 0.056 trainign loss: 5.6838 avg training loss: 7.8673
batch: [11290/21305] batch time: 2.271 trainign loss: 7.1916 avg training loss: 7.8672
batch: [11300/21305] batch time: 0.060 trainign loss: 2.4169 avg training loss: 7.8669
batch: [11310/21305] batch time: 2.414 trainign loss: 6.3293 avg training loss: 7.8669
batch: [11320/21305] batch time: 0.058 trainign loss: 7.2348 avg training loss: 7.8666
batch: [11330/21305] batch time: 2.296 trainign loss: 8.1539 avg training loss: 7.8665
batch: [11340/21305] batch time: 0.056 trainign loss: 8.1636 avg training loss: 7.8665
batch: [11350/21305] batch time: 1.983 trainign loss: 7.5072 avg training loss: 7.8665
batch: [11360/21305] batch time: 0.053 trainign loss: 6.5584 avg training loss: 7.8664
batch: [11370/21305] batch time: 2.286 trainign loss: 6.3164 avg training loss: 7.8662
batch: [11380/21305] batch time: 0.384 trainign loss: 4.7766 avg training loss: 7.8660
batch: [11390/21305] batch time: 2.153 trainign loss: 7.3875 avg training loss: 7.8654
batch: [11400/21305] batch time: 0.054 trainign loss: 8.8360 avg training loss: 7.8655
batch: [11410/21305] batch time: 2.285 trainign loss: 7.2874 avg training loss: 7.8655
batch: [11420/21305] batch time: 0.057 trainign loss: 6.6472 avg training loss: 7.8655
batch: [11430/21305] batch time: 2.599 trainign loss: 7.2775 avg training loss: 7.8654
batch: [11440/21305] batch time: 0.059 trainign loss: 7.0170 avg training loss: 7.8654
batch: [11450/21305] batch time: 2.379 trainign loss: 6.3606 avg training loss: 7.8652
batch: [11460/21305] batch time: 0.055 trainign loss: 6.6546 avg training loss: 7.8651
batch: [11470/21305] batch time: 2.557 trainign loss: 6.9530 avg training loss: 7.8649
batch: [11480/21305] batch time: 0.057 trainign loss: 6.8016 avg training loss: 7.8648
batch: [11490/21305] batch time: 2.240 trainign loss: 7.3115 avg training loss: 7.8645
batch: [11500/21305] batch time: 0.052 trainign loss: 6.5544 avg training loss: 7.8644
batch: [11510/21305] batch time: 2.546 trainign loss: 6.3201 avg training loss: 7.8642
batch: [11520/21305] batch time: 0.056 trainign loss: 7.3016 avg training loss: 7.8641
batch: [11530/21305] batch time: 2.080 trainign loss: 6.0865 avg training loss: 7.8639
batch: [11540/21305] batch time: 0.062 trainign loss: 8.2980 avg training loss: 7.8638
batch: [11550/21305] batch time: 2.302 trainign loss: 5.2006 avg training loss: 7.8637
batch: [11560/21305] batch time: 0.060 trainign loss: 7.3775 avg training loss: 7.8636
batch: [11570/21305] batch time: 2.357 trainign loss: 5.6358 avg training loss: 7.8635
batch: [11580/21305] batch time: 0.053 trainign loss: 7.7953 avg training loss: 7.8633
batch: [11590/21305] batch time: 2.230 trainign loss: 7.0813 avg training loss: 7.8632
batch: [11600/21305] batch time: 0.056 trainign loss: 4.9196 avg training loss: 7.8631
batch: [11610/21305] batch time: 2.187 trainign loss: 5.9613 avg training loss: 7.8629
batch: [11620/21305] batch time: 0.076 trainign loss: 7.1798 avg training loss: 7.8627
batch: [11630/21305] batch time: 2.246 trainign loss: 7.3468 avg training loss: 7.8626
batch: [11640/21305] batch time: 0.231 trainign loss: 6.5766 avg training loss: 7.8625
batch: [11650/21305] batch time: 1.932 trainign loss: 7.5808 avg training loss: 7.8623
batch: [11660/21305] batch time: 0.377 trainign loss: 5.8091 avg training loss: 7.8622
batch: [11670/21305] batch time: 2.318 trainign loss: 7.0140 avg training loss: 7.8621
batch: [11680/21305] batch time: 0.422 trainign loss: 5.8932 avg training loss: 7.8619
batch: [11690/21305] batch time: 2.037 trainign loss: 6.2307 avg training loss: 7.8617
batch: [11700/21305] batch time: 0.062 trainign loss: 6.8400 avg training loss: 7.8615
batch: [11710/21305] batch time: 2.319 trainign loss: 5.2352 avg training loss: 7.8614
batch: [11720/21305] batch time: 0.063 trainign loss: 7.3094 avg training loss: 7.8613
batch: [11730/21305] batch time: 2.549 trainign loss: 5.6607 avg training loss: 7.8611
batch: [11740/21305] batch time: 0.053 trainign loss: 5.9539 avg training loss: 7.8609
batch: [11750/21305] batch time: 2.420 trainign loss: 7.2785 avg training loss: 7.8608
batch: [11760/21305] batch time: 0.056 trainign loss: 7.0750 avg training loss: 7.8608
batch: [11770/21305] batch time: 2.364 trainign loss: 6.7218 avg training loss: 7.8607
batch: [11780/21305] batch time: 0.056 trainign loss: 4.9080 avg training loss: 7.8605
batch: [11790/21305] batch time: 1.996 trainign loss: 1.5304 avg training loss: 7.8601
batch: [11800/21305] batch time: 0.052 trainign loss: 0.0008 avg training loss: 7.8593
batch: [11810/21305] batch time: 2.373 trainign loss: 7.6582 avg training loss: 7.8592
batch: [11820/21305] batch time: 0.053 trainign loss: 7.7382 avg training loss: 7.8593
batch: [11830/21305] batch time: 2.230 trainign loss: 7.8945 avg training loss: 7.8592
batch: [11840/21305] batch time: 0.060 trainign loss: 7.5000 avg training loss: 7.8591
batch: [11850/21305] batch time: 2.824 trainign loss: 6.0536 avg training loss: 7.8589
batch: [11860/21305] batch time: 0.057 trainign loss: 4.5371 avg training loss: 7.8587
batch: [11870/21305] batch time: 2.505 trainign loss: 6.0024 avg training loss: 7.8585
batch: [11880/21305] batch time: 0.055 trainign loss: 7.9677 avg training loss: 7.8581
batch: [11890/21305] batch time: 2.159 trainign loss: 7.6125 avg training loss: 7.8581
batch: [11900/21305] batch time: 0.056 trainign loss: 5.6723 avg training loss: 7.8580
batch: [11910/21305] batch time: 2.267 trainign loss: 7.6447 avg training loss: 7.8578
batch: [11920/21305] batch time: 0.062 trainign loss: 6.2849 avg training loss: 7.8577
batch: [11930/21305] batch time: 2.092 trainign loss: 5.4391 avg training loss: 7.8574
batch: [11940/21305] batch time: 0.056 trainign loss: 7.4109 avg training loss: 7.8574
batch: [11950/21305] batch time: 2.224 trainign loss: 6.6025 avg training loss: 7.8573
batch: [11960/21305] batch time: 0.054 trainign loss: 7.2717 avg training loss: 7.8571
batch: [11970/21305] batch time: 2.680 trainign loss: 6.7507 avg training loss: 7.8570
batch: [11980/21305] batch time: 0.056 trainign loss: 6.8848 avg training loss: 7.8569
batch: [11990/21305] batch time: 2.298 trainign loss: 6.0623 avg training loss: 7.8568
batch: [12000/21305] batch time: 0.056 trainign loss: 6.7459 avg training loss: 7.8566
batch: [12010/21305] batch time: 2.342 trainign loss: 7.5478 avg training loss: 7.8565
batch: [12020/21305] batch time: 0.062 trainign loss: 6.9797 avg training loss: 7.8565
batch: [12030/21305] batch time: 2.392 trainign loss: 6.2194 avg training loss: 7.8564
batch: [12040/21305] batch time: 0.056 trainign loss: 5.9253 avg training loss: 7.8562
batch: [12050/21305] batch time: 2.412 trainign loss: 6.4253 avg training loss: 7.8561
batch: [12060/21305] batch time: 0.062 trainign loss: 7.0479 avg training loss: 7.8559
batch: [12070/21305] batch time: 2.527 trainign loss: 3.7727 avg training loss: 7.8557
batch: [12080/21305] batch time: 0.056 trainign loss: 6.1874 avg training loss: 7.8553
batch: [12090/21305] batch time: 2.398 trainign loss: 5.9880 avg training loss: 7.8552
batch: [12100/21305] batch time: 0.057 trainign loss: 6.9370 avg training loss: 7.8551
batch: [12110/21305] batch time: 2.394 trainign loss: 7.5822 avg training loss: 7.8550
batch: [12120/21305] batch time: 0.061 trainign loss: 4.8753 avg training loss: 7.8548
batch: [12130/21305] batch time: 2.729 trainign loss: 8.0855 avg training loss: 7.8546
batch: [12140/21305] batch time: 0.054 trainign loss: 5.9249 avg training loss: 7.8544
batch: [12150/21305] batch time: 2.541 trainign loss: 7.6259 avg training loss: 7.8544
batch: [12160/21305] batch time: 0.056 trainign loss: 7.7198 avg training loss: 7.8543
batch: [12170/21305] batch time: 2.281 trainign loss: 7.3332 avg training loss: 7.8543
batch: [12180/21305] batch time: 0.060 trainign loss: 7.2721 avg training loss: 7.8543
batch: [12190/21305] batch time: 2.074 trainign loss: 7.3480 avg training loss: 7.8542
batch: [12200/21305] batch time: 0.056 trainign loss: 6.4375 avg training loss: 7.8541
batch: [12210/21305] batch time: 2.334 trainign loss: 7.5196 avg training loss: 7.8540
batch: [12220/21305] batch time: 0.055 trainign loss: 7.1415 avg training loss: 7.8540
batch: [12230/21305] batch time: 2.273 trainign loss: 7.5301 avg training loss: 7.8539
batch: [12240/21305] batch time: 0.056 trainign loss: 6.6232 avg training loss: 7.8538
batch: [12250/21305] batch time: 2.339 trainign loss: 6.3318 avg training loss: 7.8537
batch: [12260/21305] batch time: 0.058 trainign loss: 6.9381 avg training loss: 7.8536
batch: [12270/21305] batch time: 2.470 trainign loss: 5.5481 avg training loss: 7.8534
batch: [12280/21305] batch time: 0.052 trainign loss: 4.8347 avg training loss: 7.8531
batch: [12290/21305] batch time: 2.154 trainign loss: 7.4605 avg training loss: 7.8531
batch: [12300/21305] batch time: 0.059 trainign loss: 7.7317 avg training loss: 7.8530
batch: [12310/21305] batch time: 2.411 trainign loss: 6.9178 avg training loss: 7.8530
batch: [12320/21305] batch time: 0.056 trainign loss: 6.6418 avg training loss: 7.8528
batch: [12330/21305] batch time: 2.351 trainign loss: 7.2333 avg training loss: 7.8527
batch: [12340/21305] batch time: 0.056 trainign loss: 6.7815 avg training loss: 7.8526
batch: [12350/21305] batch time: 2.246 trainign loss: 7.1016 avg training loss: 7.8525
batch: [12360/21305] batch time: 0.058 trainign loss: 5.2653 avg training loss: 7.8524
batch: [12370/21305] batch time: 1.810 trainign loss: 5.5554 avg training loss: 7.8522
batch: [12380/21305] batch time: 0.057 trainign loss: 6.9325 avg training loss: 7.8520
batch: [12390/21305] batch time: 0.622 trainign loss: 7.5602 avg training loss: 7.8520
batch: [12400/21305] batch time: 0.062 trainign loss: 6.1323 avg training loss: 7.8519
batch: [12410/21305] batch time: 0.579 trainign loss: 7.5055 avg training loss: 7.8517
batch: [12420/21305] batch time: 0.061 trainign loss: 5.7773 avg training loss: 7.8516
batch: [12430/21305] batch time: 1.369 trainign loss: 5.8013 avg training loss: 7.8515
batch: [12440/21305] batch time: 0.062 trainign loss: 7.2345 avg training loss: 7.8512
batch: [12450/21305] batch time: 1.566 trainign loss: 7.8814 avg training loss: 7.8512
batch: [12460/21305] batch time: 0.056 trainign loss: 7.2881 avg training loss: 7.8511
batch: [12470/21305] batch time: 0.544 trainign loss: 6.0397 avg training loss: 7.8510
batch: [12480/21305] batch time: 0.057 trainign loss: 5.7547 avg training loss: 7.8509
batch: [12490/21305] batch time: 1.139 trainign loss: 6.2130 avg training loss: 7.8507
batch: [12500/21305] batch time: 0.058 trainign loss: 6.9426 avg training loss: 7.8505
batch: [12510/21305] batch time: 0.192 trainign loss: 7.7291 avg training loss: 7.8504
batch: [12520/21305] batch time: 0.056 trainign loss: 7.1599 avg training loss: 7.8503
batch: [12530/21305] batch time: 0.141 trainign loss: 6.8592 avg training loss: 7.8502
batch: [12540/21305] batch time: 0.056 trainign loss: 5.1966 avg training loss: 7.8500
batch: [12550/21305] batch time: 0.371 trainign loss: 6.8161 avg training loss: 7.8498
batch: [12560/21305] batch time: 0.063 trainign loss: 4.5271 avg training loss: 7.8496
batch: [12570/21305] batch time: 0.235 trainign loss: 0.0063 avg training loss: 7.8489
batch: [12580/21305] batch time: 0.056 trainign loss: 8.0800 avg training loss: 7.8489
batch: [12590/21305] batch time: 0.643 trainign loss: 4.4105 avg training loss: 7.8488
batch: [12600/21305] batch time: 0.058 trainign loss: 8.3203 avg training loss: 7.8488
batch: [12610/21305] batch time: 1.091 trainign loss: 5.0468 avg training loss: 7.8487
batch: [12620/21305] batch time: 0.061 trainign loss: 6.9826 avg training loss: 7.8486
batch: [12630/21305] batch time: 0.594 trainign loss: 6.6119 avg training loss: 7.8486
batch: [12640/21305] batch time: 0.058 trainign loss: 7.1680 avg training loss: 7.8485
batch: [12650/21305] batch time: 0.301 trainign loss: 5.7051 avg training loss: 7.8483
batch: [12660/21305] batch time: 0.057 trainign loss: 6.6048 avg training loss: 7.8482
batch: [12670/21305] batch time: 0.685 trainign loss: 7.5482 avg training loss: 7.8481
batch: [12680/21305] batch time: 0.056 trainign loss: 5.1529 avg training loss: 7.8479
batch: [12690/21305] batch time: 0.587 trainign loss: 7.5534 avg training loss: 7.8477
batch: [12700/21305] batch time: 0.057 trainign loss: 6.9684 avg training loss: 7.8476
batch: [12710/21305] batch time: 0.125 trainign loss: 6.0912 avg training loss: 7.8475
batch: [12720/21305] batch time: 0.259 trainign loss: 6.1880 avg training loss: 7.8474
batch: [12730/21305] batch time: 0.579 trainign loss: 6.0838 avg training loss: 7.8472
batch: [12740/21305] batch time: 0.337 trainign loss: 5.3278 avg training loss: 7.8471
batch: [12750/21305] batch time: 0.104 trainign loss: 4.2046 avg training loss: 7.8468
batch: [12760/21305] batch time: 0.512 trainign loss: 6.4730 avg training loss: 7.8467
batch: [12770/21305] batch time: 0.056 trainign loss: 6.2468 avg training loss: 7.8466
batch: [12780/21305] batch time: 0.394 trainign loss: 7.7566 avg training loss: 7.8465
batch: [12790/21305] batch time: 0.062 trainign loss: 5.9670 avg training loss: 7.8464
batch: [12800/21305] batch time: 0.053 trainign loss: 7.4407 avg training loss: 7.8463
batch: [12810/21305] batch time: 0.058 trainign loss: 7.1646 avg training loss: 7.8462
batch: [12820/21305] batch time: 0.056 trainign loss: 6.6101 avg training loss: 7.8460
batch: [12830/21305] batch time: 0.060 trainign loss: 4.6703 avg training loss: 7.8459
batch: [12840/21305] batch time: 0.056 trainign loss: 6.9765 avg training loss: 7.8458
batch: [12850/21305] batch time: 0.054 trainign loss: 7.0801 avg training loss: 7.8457
batch: [12860/21305] batch time: 0.057 trainign loss: 7.0768 avg training loss: 7.8455
batch: [12870/21305] batch time: 0.056 trainign loss: 6.6374 avg training loss: 7.8454
batch: [12880/21305] batch time: 0.052 trainign loss: 6.0850 avg training loss: 7.8452
batch: [12890/21305] batch time: 0.051 trainign loss: 6.6898 avg training loss: 7.8452
batch: [12900/21305] batch time: 0.060 trainign loss: 7.4391 avg training loss: 7.8451
batch: [12910/21305] batch time: 0.062 trainign loss: 6.4317 avg training loss: 7.8450
batch: [12920/21305] batch time: 0.058 trainign loss: 6.6853 avg training loss: 7.8448
batch: [12930/21305] batch time: 0.061 trainign loss: 6.1036 avg training loss: 7.8447
batch: [12940/21305] batch time: 0.052 trainign loss: 6.4854 avg training loss: 7.8446
batch: [12950/21305] batch time: 0.055 trainign loss: 5.2467 avg training loss: 7.8443
batch: [12960/21305] batch time: 0.066 trainign loss: 6.6923 avg training loss: 7.8442
batch: [12970/21305] batch time: 0.056 trainign loss: 5.0702 avg training loss: 7.8440
batch: [12980/21305] batch time: 0.059 trainign loss: 7.1464 avg training loss: 7.8438
batch: [12990/21305] batch time: 0.056 trainign loss: 6.6084 avg training loss: 7.8438
batch: [13000/21305] batch time: 0.057 trainign loss: 7.1861 avg training loss: 7.8437
batch: [13010/21305] batch time: 0.056 trainign loss: 6.8859 avg training loss: 7.8434
batch: [13020/21305] batch time: 0.056 trainign loss: 7.8222 avg training loss: 7.8434
batch: [13030/21305] batch time: 0.056 trainign loss: 6.9959 avg training loss: 7.8433
batch: [13040/21305] batch time: 0.052 trainign loss: 7.0764 avg training loss: 7.8431
batch: [13050/21305] batch time: 0.056 trainign loss: 2.3679 avg training loss: 7.8429
batch: [13060/21305] batch time: 0.063 trainign loss: 6.9008 avg training loss: 7.8427
batch: [13070/21305] batch time: 0.056 trainign loss: 6.9085 avg training loss: 7.8426
batch: [13080/21305] batch time: 0.056 trainign loss: 6.5114 avg training loss: 7.8425
batch: [13090/21305] batch time: 0.056 trainign loss: 4.6896 avg training loss: 7.8423
batch: [13100/21305] batch time: 0.052 trainign loss: 7.9810 avg training loss: 7.8420
batch: [13110/21305] batch time: 0.056 trainign loss: 6.7745 avg training loss: 7.8418
batch: [13120/21305] batch time: 0.054 trainign loss: 6.8649 avg training loss: 7.8417
batch: [13130/21305] batch time: 0.062 trainign loss: 5.8666 avg training loss: 7.8416
batch: [13140/21305] batch time: 0.051 trainign loss: 2.4684 avg training loss: 7.8414
batch: [13150/21305] batch time: 0.056 trainign loss: 7.4560 avg training loss: 7.8412
batch: [13160/21305] batch time: 0.055 trainign loss: 6.2057 avg training loss: 7.8412
batch: [13170/21305] batch time: 0.063 trainign loss: 6.8800 avg training loss: 7.8411
batch: [13180/21305] batch time: 0.058 trainign loss: 5.4141 avg training loss: 7.8409
batch: [13190/21305] batch time: 0.061 trainign loss: 6.5431 avg training loss: 7.8407
batch: [13200/21305] batch time: 0.055 trainign loss: 3.7477 avg training loss: 7.8404
batch: [13210/21305] batch time: 0.058 trainign loss: 0.0033 avg training loss: 7.8397
batch: [13220/21305] batch time: 0.052 trainign loss: 0.0001 avg training loss: 7.8389
batch: [13230/21305] batch time: 0.705 trainign loss: 0.0000 avg training loss: 7.8381
batch: [13240/21305] batch time: 0.059 trainign loss: 8.3304 avg training loss: 7.8381
batch: [13250/21305] batch time: 0.537 trainign loss: 8.2118 avg training loss: 7.8381
batch: [13260/21305] batch time: 0.824 trainign loss: 7.9783 avg training loss: 7.8381
batch: [13270/21305] batch time: 0.728 trainign loss: 7.2773 avg training loss: 7.8381
batch: [13280/21305] batch time: 0.060 trainign loss: 7.6225 avg training loss: 7.8379
batch: [13290/21305] batch time: 0.696 trainign loss: 7.0153 avg training loss: 7.8378
batch: [13300/21305] batch time: 0.062 trainign loss: 7.1589 avg training loss: 7.8377
batch: [13310/21305] batch time: 0.592 trainign loss: 5.0857 avg training loss: 7.8375
batch: [13320/21305] batch time: 0.225 trainign loss: 5.7454 avg training loss: 7.8372
batch: [13330/21305] batch time: 1.813 trainign loss: 7.1370 avg training loss: 7.8371
batch: [13340/21305] batch time: 0.062 trainign loss: 7.6973 avg training loss: 7.8370
batch: [13350/21305] batch time: 2.151 trainign loss: 6.3154 avg training loss: 7.8369
batch: [13360/21305] batch time: 0.057 trainign loss: 8.2251 avg training loss: 7.8369
batch: [13370/21305] batch time: 1.370 trainign loss: 6.1945 avg training loss: 7.8368
batch: [13380/21305] batch time: 0.054 trainign loss: 4.6644 avg training loss: 7.8366
batch: [13390/21305] batch time: 0.588 trainign loss: 6.9265 avg training loss: 7.8364
batch: [13400/21305] batch time: 0.063 trainign loss: 4.7318 avg training loss: 7.8363
batch: [13410/21305] batch time: 1.112 trainign loss: 7.6530 avg training loss: 7.8363
batch: [13420/21305] batch time: 0.056 trainign loss: 6.8515 avg training loss: 7.8362
batch: [13430/21305] batch time: 0.574 trainign loss: 6.9047 avg training loss: 7.8361
batch: [13440/21305] batch time: 0.448 trainign loss: 6.2939 avg training loss: 7.8360
batch: [13450/21305] batch time: 0.056 trainign loss: 7.3165 avg training loss: 7.8359
batch: [13460/21305] batch time: 0.775 trainign loss: 6.7008 avg training loss: 7.8358
batch: [13470/21305] batch time: 0.348 trainign loss: 4.0875 avg training loss: 7.8356
batch: [13480/21305] batch time: 0.407 trainign loss: 4.6627 avg training loss: 7.8354
batch: [13490/21305] batch time: 0.057 trainign loss: 6.9314 avg training loss: 7.8353
batch: [13500/21305] batch time: 0.055 trainign loss: 6.6643 avg training loss: 7.8351
batch: [13510/21305] batch time: 0.056 trainign loss: 5.8837 avg training loss: 7.8350
batch: [13520/21305] batch time: 0.050 trainign loss: 6.3981 avg training loss: 7.8349
batch: [13530/21305] batch time: 0.056 trainign loss: 6.0074 avg training loss: 7.8347
batch: [13540/21305] batch time: 0.056 trainign loss: 3.7238 avg training loss: 7.8345
batch: [13550/21305] batch time: 0.056 trainign loss: 7.3736 avg training loss: 7.8344
batch: [13560/21305] batch time: 1.144 trainign loss: 6.6474 avg training loss: 7.8343
batch: [13570/21305] batch time: 0.800 trainign loss: 6.8171 avg training loss: 7.8342
batch: [13580/21305] batch time: 1.285 trainign loss: 7.6455 avg training loss: 7.8340
batch: [13590/21305] batch time: 0.675 trainign loss: 5.3991 avg training loss: 7.8339
batch: [13600/21305] batch time: 2.269 trainign loss: 6.4265 avg training loss: 7.8338
batch: [13610/21305] batch time: 0.052 trainign loss: 8.0944 avg training loss: 7.8336
batch: [13620/21305] batch time: 2.459 trainign loss: 6.9163 avg training loss: 7.8335
batch: [13630/21305] batch time: 0.166 trainign loss: 6.7314 avg training loss: 7.8334
batch: [13640/21305] batch time: 2.106 trainign loss: 6.8949 avg training loss: 7.8333
batch: [13650/21305] batch time: 0.486 trainign loss: 6.2446 avg training loss: 7.8332
batch: [13660/21305] batch time: 1.470 trainign loss: 6.0854 avg training loss: 7.8330
batch: [13670/21305] batch time: 1.373 trainign loss: 6.9081 avg training loss: 7.8330
batch: [13680/21305] batch time: 1.384 trainign loss: 5.6339 avg training loss: 7.8328
batch: [13690/21305] batch time: 1.053 trainign loss: 3.9145 avg training loss: 7.8326
batch: [13700/21305] batch time: 1.593 trainign loss: 8.1608 avg training loss: 7.8325
batch: [13710/21305] batch time: 1.343 trainign loss: 7.0635 avg training loss: 7.8325
batch: [13720/21305] batch time: 1.740 trainign loss: 6.4304 avg training loss: 7.8324
batch: [13730/21305] batch time: 0.885 trainign loss: 7.3634 avg training loss: 7.8323
batch: [13740/21305] batch time: 1.555 trainign loss: 5.8101 avg training loss: 7.8321
batch: [13750/21305] batch time: 0.259 trainign loss: 7.3075 avg training loss: 7.8320
batch: [13760/21305] batch time: 2.574 trainign loss: 7.4878 avg training loss: 7.8319
batch: [13770/21305] batch time: 0.056 trainign loss: 7.0558 avg training loss: 7.8318
batch: [13780/21305] batch time: 2.382 trainign loss: 5.8657 avg training loss: 7.8316
batch: [13790/21305] batch time: 0.060 trainign loss: 7.4717 avg training loss: 7.8316
batch: [13800/21305] batch time: 2.307 trainign loss: 6.4096 avg training loss: 7.8315
batch: [13810/21305] batch time: 0.056 trainign loss: 6.6850 avg training loss: 7.8313
batch: [13820/21305] batch time: 2.355 trainign loss: 7.6131 avg training loss: 7.8312
batch: [13830/21305] batch time: 0.060 trainign loss: 6.4620 avg training loss: 7.8311
batch: [13840/21305] batch time: 2.347 trainign loss: 6.4581 avg training loss: 7.8310
batch: [13850/21305] batch time: 0.056 trainign loss: 1.4172 avg training loss: 7.8306
batch: [13860/21305] batch time: 2.258 trainign loss: 6.5018 avg training loss: 7.8305
batch: [13870/21305] batch time: 0.061 trainign loss: 7.8487 avg training loss: 7.8304
batch: [13880/21305] batch time: 2.274 trainign loss: 6.3861 avg training loss: 7.8303
batch: [13890/21305] batch time: 0.056 trainign loss: 7.1759 avg training loss: 7.8302
batch: [13900/21305] batch time: 2.244 trainign loss: 6.2385 avg training loss: 7.8301
batch: [13910/21305] batch time: 0.052 trainign loss: 7.3078 avg training loss: 7.8299
batch: [13920/21305] batch time: 2.063 trainign loss: 4.7114 avg training loss: 7.8297
batch: [13930/21305] batch time: 0.180 trainign loss: 6.0755 avg training loss: 7.8294
batch: [13940/21305] batch time: 2.201 trainign loss: 7.8796 avg training loss: 7.8293
batch: [13950/21305] batch time: 0.459 trainign loss: 5.3418 avg training loss: 7.8292
batch: [13960/21305] batch time: 2.464 trainign loss: 6.1847 avg training loss: 7.8291
batch: [13970/21305] batch time: 0.056 trainign loss: 7.3856 avg training loss: 7.8291
batch: [13980/21305] batch time: 2.465 trainign loss: 6.8654 avg training loss: 7.8289
batch: [13990/21305] batch time: 0.063 trainign loss: 6.8231 avg training loss: 7.8288
batch: [14000/21305] batch time: 2.178 trainign loss: 6.5307 avg training loss: 7.8286
batch: [14010/21305] batch time: 0.057 trainign loss: 5.1917 avg training loss: 7.8284
batch: [14020/21305] batch time: 2.481 trainign loss: 7.6000 avg training loss: 7.8281
batch: [14030/21305] batch time: 0.056 trainign loss: 6.7854 avg training loss: 7.8281
batch: [14040/21305] batch time: 2.804 trainign loss: 7.3942 avg training loss: 7.8279
batch: [14050/21305] batch time: 0.056 trainign loss: 7.4508 avg training loss: 7.8279
batch: [14060/21305] batch time: 2.451 trainign loss: 6.5999 avg training loss: 7.8278
batch: [14070/21305] batch time: 0.063 trainign loss: 6.9936 avg training loss: 7.8277
batch: [14080/21305] batch time: 2.095 trainign loss: 6.7551 avg training loss: 7.8276
batch: [14090/21305] batch time: 0.056 trainign loss: 6.9870 avg training loss: 7.8275
batch: [14100/21305] batch time: 1.999 trainign loss: 6.5650 avg training loss: 7.8274
batch: [14110/21305] batch time: 0.056 trainign loss: 6.5052 avg training loss: 7.8273
batch: [14120/21305] batch time: 2.293 trainign loss: 7.4831 avg training loss: 7.8271
batch: [14130/21305] batch time: 0.056 trainign loss: 6.1288 avg training loss: 7.8268
batch: [14140/21305] batch time: 2.108 trainign loss: 6.8084 avg training loss: 7.8267
batch: [14150/21305] batch time: 0.056 trainign loss: 6.8564 avg training loss: 7.8265
batch: [14160/21305] batch time: 2.286 trainign loss: 6.9825 avg training loss: 7.8264
batch: [14170/21305] batch time: 0.054 trainign loss: 6.8201 avg training loss: 7.8263
batch: [14180/21305] batch time: 2.081 trainign loss: 6.2988 avg training loss: 7.8262
batch: [14190/21305] batch time: 0.056 trainign loss: 2.6062 avg training loss: 7.8259
batch: [14200/21305] batch time: 2.671 trainign loss: 7.4049 avg training loss: 7.8258
batch: [14210/21305] batch time: 0.054 trainign loss: 6.1521 avg training loss: 7.8257
batch: [14220/21305] batch time: 2.647 trainign loss: 7.7904 avg training loss: 7.8254
batch: [14230/21305] batch time: 0.053 trainign loss: 6.3526 avg training loss: 7.8253
batch: [14240/21305] batch time: 2.217 trainign loss: 7.1922 avg training loss: 7.8252
batch: [14250/21305] batch time: 0.055 trainign loss: 6.9600 avg training loss: 7.8251
batch: [14260/21305] batch time: 2.169 trainign loss: 5.6196 avg training loss: 7.8250
batch: [14270/21305] batch time: 0.056 trainign loss: 4.8287 avg training loss: 7.8248
batch: [14280/21305] batch time: 2.043 trainign loss: 5.2102 avg training loss: 7.8246
batch: [14290/21305] batch time: 0.063 trainign loss: 6.4314 avg training loss: 7.8245
batch: [14300/21305] batch time: 2.264 trainign loss: 6.9837 avg training loss: 7.8243
batch: [14310/21305] batch time: 0.053 trainign loss: 6.4591 avg training loss: 7.8242
batch: [14320/21305] batch time: 2.045 trainign loss: 2.4007 avg training loss: 7.8239
batch: [14330/21305] batch time: 0.061 trainign loss: 7.0204 avg training loss: 7.8238
batch: [14340/21305] batch time: 2.463 trainign loss: 6.5482 avg training loss: 7.8237
batch: [14350/21305] batch time: 0.055 trainign loss: 5.6488 avg training loss: 7.8236
batch: [14360/21305] batch time: 2.444 trainign loss: 5.9884 avg training loss: 7.8235
batch: [14370/21305] batch time: 0.052 trainign loss: 5.9939 avg training loss: 7.8233
batch: [14380/21305] batch time: 2.262 trainign loss: 6.6841 avg training loss: 7.8233
batch: [14390/21305] batch time: 0.054 trainign loss: 6.8642 avg training loss: 7.8232
batch: [14400/21305] batch time: 2.051 trainign loss: 5.9790 avg training loss: 7.8230
batch: [14410/21305] batch time: 0.056 trainign loss: 6.4446 avg training loss: 7.8228
batch: [14420/21305] batch time: 2.326 trainign loss: 6.5980 avg training loss: 7.8227
batch: [14430/21305] batch time: 0.056 trainign loss: 6.8244 avg training loss: 7.8225
batch: [14440/21305] batch time: 2.299 trainign loss: 7.0984 avg training loss: 7.8225
batch: [14450/21305] batch time: 0.055 trainign loss: 5.5883 avg training loss: 7.8224
batch: [14460/21305] batch time: 2.414 trainign loss: 6.8606 avg training loss: 7.8222
batch: [14470/21305] batch time: 0.056 trainign loss: 5.9454 avg training loss: 7.8220
batch: [14480/21305] batch time: 2.475 trainign loss: 4.3611 avg training loss: 7.8219
batch: [14490/21305] batch time: 0.062 trainign loss: 5.5940 avg training loss: 7.8217
batch: [14500/21305] batch time: 1.765 trainign loss: 2.0903 avg training loss: 7.8214
batch: [14510/21305] batch time: 0.063 trainign loss: 0.0015 avg training loss: 7.8206
batch: [14520/21305] batch time: 1.567 trainign loss: 0.0003 avg training loss: 7.8199
batch: [14530/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.8191
batch: [14540/21305] batch time: 1.890 trainign loss: 0.0001 avg training loss: 7.8183
batch: [14550/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 7.8175
batch: [14560/21305] batch time: 1.821 trainign loss: 0.0001 avg training loss: 7.8167
batch: [14570/21305] batch time: 0.060 trainign loss: 0.0000 avg training loss: 7.8159
batch: [14580/21305] batch time: 1.852 trainign loss: 0.0000 avg training loss: 7.8152
batch: [14590/21305] batch time: 0.054 trainign loss: 0.0000 avg training loss: 7.8144
batch: [14600/21305] batch time: 2.534 trainign loss: 7.0057 avg training loss: 7.8141
batch: [14610/21305] batch time: 0.063 trainign loss: 8.4906 avg training loss: 7.8142
batch: [14620/21305] batch time: 2.242 trainign loss: 7.4564 avg training loss: 7.8142
batch: [14630/21305] batch time: 0.062 trainign loss: 6.4360 avg training loss: 7.8141
batch: [14640/21305] batch time: 2.740 trainign loss: 5.7426 avg training loss: 7.8139
batch: [14650/21305] batch time: 0.056 trainign loss: 7.1051 avg training loss: 7.8139
batch: [14660/21305] batch time: 1.883 trainign loss: 6.3563 avg training loss: 7.8137
batch: [14670/21305] batch time: 0.056 trainign loss: 5.1742 avg training loss: 7.8136
batch: [14680/21305] batch time: 2.558 trainign loss: 2.3001 avg training loss: 7.8133
batch: [14690/21305] batch time: 0.056 trainign loss: 6.9260 avg training loss: 7.8132
batch: [14700/21305] batch time: 2.302 trainign loss: 4.4661 avg training loss: 7.8130
batch: [14710/21305] batch time: 0.061 trainign loss: 0.0053 avg training loss: 7.8123
batch: [14720/21305] batch time: 2.143 trainign loss: 8.0799 avg training loss: 7.8122
batch: [14730/21305] batch time: 0.061 trainign loss: 6.2435 avg training loss: 7.8122
batch: [14740/21305] batch time: 2.624 trainign loss: 5.3313 avg training loss: 7.8121
batch: [14750/21305] batch time: 0.056 trainign loss: 7.8075 avg training loss: 7.8120
batch: [14760/21305] batch time: 1.959 trainign loss: 7.2949 avg training loss: 7.8119
batch: [14770/21305] batch time: 0.057 trainign loss: 6.9367 avg training loss: 7.8119
batch: [14780/21305] batch time: 2.440 trainign loss: 6.5405 avg training loss: 7.8118
batch: [14790/21305] batch time: 0.056 trainign loss: 7.2511 avg training loss: 7.8117
batch: [14800/21305] batch time: 2.478 trainign loss: 6.5986 avg training loss: 7.8116
batch: [14810/21305] batch time: 0.056 trainign loss: 6.1896 avg training loss: 7.8114
batch: [14820/21305] batch time: 2.198 trainign loss: 6.7705 avg training loss: 7.8113
batch: [14830/21305] batch time: 0.053 trainign loss: 6.4620 avg training loss: 7.8112
batch: [14840/21305] batch time: 2.212 trainign loss: 5.5705 avg training loss: 7.8111
batch: [14850/21305] batch time: 0.056 trainign loss: 7.4546 avg training loss: 7.8110
batch: [14860/21305] batch time: 1.930 trainign loss: 7.3680 avg training loss: 7.8109
batch: [14870/21305] batch time: 0.063 trainign loss: 6.6344 avg training loss: 7.8108
batch: [14880/21305] batch time: 2.223 trainign loss: 6.9873 avg training loss: 7.8108
batch: [14890/21305] batch time: 0.057 trainign loss: 6.2809 avg training loss: 7.8106
batch: [14900/21305] batch time: 1.964 trainign loss: 6.5587 avg training loss: 7.8105
batch: [14910/21305] batch time: 0.056 trainign loss: 3.6315 avg training loss: 7.8103
batch: [14920/21305] batch time: 2.202 trainign loss: 6.3051 avg training loss: 7.8102
batch: [14930/21305] batch time: 0.057 trainign loss: 6.1638 avg training loss: 7.8100
batch: [14940/21305] batch time: 1.615 trainign loss: 6.7100 avg training loss: 7.8099
batch: [14950/21305] batch time: 0.056 trainign loss: 7.0711 avg training loss: 7.8097
batch: [14960/21305] batch time: 2.162 trainign loss: 7.4469 avg training loss: 7.8095
batch: [14970/21305] batch time: 0.062 trainign loss: 7.7641 avg training loss: 7.8094
batch: [14980/21305] batch time: 1.996 trainign loss: 7.5775 avg training loss: 7.8093
batch: [14990/21305] batch time: 0.063 trainign loss: 5.1624 avg training loss: 7.8091
batch: [15000/21305] batch time: 1.832 trainign loss: 6.1976 avg training loss: 7.8090
batch: [15010/21305] batch time: 0.056 trainign loss: 7.8384 avg training loss: 7.8089
batch: [15020/21305] batch time: 1.941 trainign loss: 6.1646 avg training loss: 7.8088
batch: [15030/21305] batch time: 0.054 trainign loss: 7.2035 avg training loss: 7.8087
batch: [15040/21305] batch time: 2.025 trainign loss: 2.7242 avg training loss: 7.8084
batch: [15050/21305] batch time: 0.063 trainign loss: 7.1898 avg training loss: 7.8084
batch: [15060/21305] batch time: 1.934 trainign loss: 6.5291 avg training loss: 7.8083
batch: [15070/21305] batch time: 0.058 trainign loss: 6.1204 avg training loss: 7.8081
batch: [15080/21305] batch time: 2.275 trainign loss: 3.9698 avg training loss: 7.8076
batch: [15090/21305] batch time: 0.063 trainign loss: 7.8693 avg training loss: 7.8077
batch: [15100/21305] batch time: 1.930 trainign loss: 5.6790 avg training loss: 7.8076
batch: [15110/21305] batch time: 0.061 trainign loss: 3.0491 avg training loss: 7.8074
batch: [15120/21305] batch time: 1.841 trainign loss: 7.6263 avg training loss: 7.8073
batch: [15130/21305] batch time: 0.062 trainign loss: 6.6654 avg training loss: 7.8072
batch: [15140/21305] batch time: 1.686 trainign loss: 7.7782 avg training loss: 7.8071
batch: [15150/21305] batch time: 0.058 trainign loss: 6.9437 avg training loss: 7.8070
batch: [15160/21305] batch time: 1.385 trainign loss: 6.8861 avg training loss: 7.8070
batch: [15170/21305] batch time: 0.056 trainign loss: 6.6498 avg training loss: 7.8068
batch: [15180/21305] batch time: 2.308 trainign loss: 5.5999 avg training loss: 7.8066
batch: [15190/21305] batch time: 0.054 trainign loss: 7.0655 avg training loss: 7.8065
batch: [15200/21305] batch time: 2.358 trainign loss: 7.3721 avg training loss: 7.8065
batch: [15210/21305] batch time: 0.056 trainign loss: 5.9203 avg training loss: 7.8063
batch: [15220/21305] batch time: 2.006 trainign loss: 6.6045 avg training loss: 7.8062
batch: [15230/21305] batch time: 0.062 trainign loss: 7.1786 avg training loss: 7.8061
batch: [15240/21305] batch time: 2.133 trainign loss: 6.6378 avg training loss: 7.8060
batch: [15250/21305] batch time: 0.353 trainign loss: 7.2816 avg training loss: 7.8059
batch: [15260/21305] batch time: 2.467 trainign loss: 5.7971 avg training loss: 7.8057
batch: [15270/21305] batch time: 0.056 trainign loss: 7.7692 avg training loss: 7.8055
batch: [15280/21305] batch time: 2.361 trainign loss: 6.1741 avg training loss: 7.8055
batch: [15290/21305] batch time: 0.054 trainign loss: 7.5854 avg training loss: 7.8053
batch: [15300/21305] batch time: 1.978 trainign loss: 6.7430 avg training loss: 7.8052
batch: [15310/21305] batch time: 0.056 trainign loss: 6.1922 avg training loss: 7.8050
batch: [15320/21305] batch time: 2.070 trainign loss: 7.6106 avg training loss: 7.8049
batch: [15330/21305] batch time: 0.051 trainign loss: 6.8471 avg training loss: 7.8049
batch: [15340/21305] batch time: 2.529 trainign loss: 5.8394 avg training loss: 7.8048
batch: [15350/21305] batch time: 0.056 trainign loss: 6.5638 avg training loss: 7.8046
batch: [15360/21305] batch time: 2.297 trainign loss: 7.4628 avg training loss: 7.8045
batch: [15370/21305] batch time: 0.062 trainign loss: 5.4855 avg training loss: 7.8043
batch: [15380/21305] batch time: 2.137 trainign loss: 6.2867 avg training loss: 7.8042
batch: [15390/21305] batch time: 0.054 trainign loss: 6.8609 avg training loss: 7.8041
batch: [15400/21305] batch time: 1.988 trainign loss: 7.1283 avg training loss: 7.8041
batch: [15410/21305] batch time: 0.061 trainign loss: 7.1065 avg training loss: 7.8040
batch: [15420/21305] batch time: 2.402 trainign loss: 6.7117 avg training loss: 7.8039
batch: [15430/21305] batch time: 0.060 trainign loss: 6.1197 avg training loss: 7.8038
batch: [15440/21305] batch time: 2.238 trainign loss: 7.3801 avg training loss: 7.8037
batch: [15450/21305] batch time: 0.062 trainign loss: 6.8350 avg training loss: 7.8035
batch: [15460/21305] batch time: 2.045 trainign loss: 6.9462 avg training loss: 7.8034
batch: [15470/21305] batch time: 0.052 trainign loss: 6.4379 avg training loss: 7.8033
batch: [15480/21305] batch time: 2.543 trainign loss: 6.9781 avg training loss: 7.8032
batch: [15490/21305] batch time: 0.057 trainign loss: 6.0976 avg training loss: 7.8031
batch: [15500/21305] batch time: 2.331 trainign loss: 5.2547 avg training loss: 7.8029
batch: [15510/21305] batch time: 0.053 trainign loss: 4.0257 avg training loss: 7.8026
batch: [15520/21305] batch time: 1.866 trainign loss: 8.2039 avg training loss: 7.8024
batch: [15530/21305] batch time: 0.056 trainign loss: 8.0357 avg training loss: 7.8023
batch: [15540/21305] batch time: 2.416 trainign loss: 7.5474 avg training loss: 7.8023
batch: [15550/21305] batch time: 0.063 trainign loss: 5.5369 avg training loss: 7.8022
batch: [15560/21305] batch time: 2.523 trainign loss: 8.2229 avg training loss: 7.8021
batch: [15570/21305] batch time: 0.055 trainign loss: 6.9694 avg training loss: 7.8020
batch: [15580/21305] batch time: 2.346 trainign loss: 6.9446 avg training loss: 7.8020
batch: [15590/21305] batch time: 0.183 trainign loss: 6.1755 avg training loss: 7.8019
batch: [15600/21305] batch time: 1.978 trainign loss: 6.6163 avg training loss: 7.8017
batch: [15610/21305] batch time: 0.831 trainign loss: 6.4689 avg training loss: 7.8016
batch: [15620/21305] batch time: 1.473 trainign loss: 6.3079 avg training loss: 7.8015
batch: [15630/21305] batch time: 1.036 trainign loss: 6.1231 avg training loss: 7.8014
batch: [15640/21305] batch time: 1.433 trainign loss: 7.0730 avg training loss: 7.8013
batch: [15650/21305] batch time: 0.633 trainign loss: 5.4934 avg training loss: 7.8011
batch: [15660/21305] batch time: 1.679 trainign loss: 5.5576 avg training loss: 7.8010
batch: [15670/21305] batch time: 0.056 trainign loss: 6.6541 avg training loss: 7.8008
batch: [15680/21305] batch time: 2.089 trainign loss: 7.2389 avg training loss: 7.8007
batch: [15690/21305] batch time: 0.061 trainign loss: 7.1262 avg training loss: 7.8006
batch: [15700/21305] batch time: 2.334 trainign loss: 6.6431 avg training loss: 7.8005
batch: [15710/21305] batch time: 0.056 trainign loss: 6.4935 avg training loss: 7.8003
batch: [15720/21305] batch time: 2.276 trainign loss: 4.9884 avg training loss: 7.8002
batch: [15730/21305] batch time: 0.063 trainign loss: 0.4277 avg training loss: 7.7998
batch: [15740/21305] batch time: 2.526 trainign loss: 12.4871 avg training loss: 7.7992
batch: [15750/21305] batch time: 0.054 trainign loss: 7.9172 avg training loss: 7.7993
batch: [15760/21305] batch time: 2.175 trainign loss: 7.4575 avg training loss: 7.7994
batch: [15770/21305] batch time: 0.061 trainign loss: 6.5637 avg training loss: 7.7993
batch: [15780/21305] batch time: 2.322 trainign loss: 6.6224 avg training loss: 7.7993
batch: [15790/21305] batch time: 0.056 trainign loss: 6.4274 avg training loss: 7.7992
batch: [15800/21305] batch time: 2.165 trainign loss: 4.9450 avg training loss: 7.7990
batch: [15810/21305] batch time: 0.061 trainign loss: 8.0789 avg training loss: 7.7988
batch: [15820/21305] batch time: 2.674 trainign loss: 6.6797 avg training loss: 7.7988
batch: [15830/21305] batch time: 0.055 trainign loss: 6.2727 avg training loss: 7.7987
batch: [15840/21305] batch time: 2.216 trainign loss: 6.2262 avg training loss: 7.7986
batch: [15850/21305] batch time: 0.056 trainign loss: 5.3289 avg training loss: 7.7985
batch: [15860/21305] batch time: 2.100 trainign loss: 7.9016 avg training loss: 7.7984
batch: [15870/21305] batch time: 0.061 trainign loss: 6.1482 avg training loss: 7.7983
batch: [15880/21305] batch time: 2.374 trainign loss: 7.3221 avg training loss: 7.7983
batch: [15890/21305] batch time: 0.056 trainign loss: 7.3774 avg training loss: 7.7982
batch: [15900/21305] batch time: 2.346 trainign loss: 5.6463 avg training loss: 7.7981
batch: [15910/21305] batch time: 0.056 trainign loss: 6.6408 avg training loss: 7.7980
batch: [15920/21305] batch time: 2.450 trainign loss: 6.9006 avg training loss: 7.7979
batch: [15930/21305] batch time: 0.057 trainign loss: 7.3377 avg training loss: 7.7977
batch: [15940/21305] batch time: 2.234 trainign loss: 6.3968 avg training loss: 7.7975
batch: [15950/21305] batch time: 0.052 trainign loss: 7.3793 avg training loss: 7.7974
batch: [15960/21305] batch time: 2.135 trainign loss: 5.2524 avg training loss: 7.7973
batch: [15970/21305] batch time: 0.054 trainign loss: 5.1750 avg training loss: 7.7971
batch: [15980/21305] batch time: 2.476 trainign loss: 6.2140 avg training loss: 7.7970
batch: [15990/21305] batch time: 0.051 trainign loss: 7.2578 avg training loss: 7.7970
batch: [16000/21305] batch time: 2.303 trainign loss: 6.8039 avg training loss: 7.7968
batch: [16010/21305] batch time: 0.063 trainign loss: 1.1224 avg training loss: 7.7965
batch: [16020/21305] batch time: 2.562 trainign loss: 7.0184 avg training loss: 7.7964
batch: [16030/21305] batch time: 0.056 trainign loss: 7.6133 avg training loss: 7.7963
batch: [16040/21305] batch time: 2.287 trainign loss: 4.6007 avg training loss: 7.7962
batch: [16050/21305] batch time: 0.056 trainign loss: 6.9037 avg training loss: 7.7961
batch: [16060/21305] batch time: 2.258 trainign loss: 6.0461 avg training loss: 7.7959
batch: [16070/21305] batch time: 0.056 trainign loss: 7.5392 avg training loss: 7.7959
batch: [16080/21305] batch time: 2.345 trainign loss: 6.9405 avg training loss: 7.7957
batch: [16090/21305] batch time: 0.051 trainign loss: 6.3666 avg training loss: 7.7956
batch: [16100/21305] batch time: 2.374 trainign loss: 5.9969 avg training loss: 7.7955
batch: [16110/21305] batch time: 0.057 trainign loss: 5.9251 avg training loss: 7.7954
batch: [16120/21305] batch time: 1.652 trainign loss: 4.5843 avg training loss: 7.7952
batch: [16130/21305] batch time: 0.056 trainign loss: 6.5568 avg training loss: 7.7951
batch: [16140/21305] batch time: 1.237 trainign loss: 7.3053 avg training loss: 7.7950
batch: [16150/21305] batch time: 0.056 trainign loss: 6.9175 avg training loss: 7.7949
batch: [16160/21305] batch time: 1.435 trainign loss: 4.4806 avg training loss: 7.7947
batch: [16170/21305] batch time: 0.056 trainign loss: 7.7199 avg training loss: 7.7947
batch: [16180/21305] batch time: 0.955 trainign loss: 7.4183 avg training loss: 7.7946
batch: [16190/21305] batch time: 0.056 trainign loss: 7.0674 avg training loss: 7.7946
batch: [16200/21305] batch time: 0.846 trainign loss: 5.4855 avg training loss: 7.7944
batch: [16210/21305] batch time: 0.059 trainign loss: 7.0552 avg training loss: 7.7943
batch: [16220/21305] batch time: 1.661 trainign loss: 6.8331 avg training loss: 7.7942
batch: [16230/21305] batch time: 0.062 trainign loss: 7.0618 avg training loss: 7.7941
batch: [16240/21305] batch time: 1.457 trainign loss: 7.4112 avg training loss: 7.7940
batch: [16250/21305] batch time: 0.056 trainign loss: 6.7728 avg training loss: 7.7939
batch: [16260/21305] batch time: 2.399 trainign loss: 6.9435 avg training loss: 7.7938
batch: [16270/21305] batch time: 0.056 trainign loss: 7.4610 avg training loss: 7.7937
batch: [16280/21305] batch time: 1.565 trainign loss: 7.3710 avg training loss: 7.7936
batch: [16290/21305] batch time: 0.055 trainign loss: 6.7283 avg training loss: 7.7935
batch: [16300/21305] batch time: 1.857 trainign loss: 6.8152 avg training loss: 7.7934
batch: [16310/21305] batch time: 0.062 trainign loss: 6.9311 avg training loss: 7.7932
batch: [16320/21305] batch time: 2.210 trainign loss: 7.2231 avg training loss: 7.7931
batch: [16330/21305] batch time: 0.056 trainign loss: 5.4952 avg training loss: 7.7930
batch: [16340/21305] batch time: 2.327 trainign loss: 5.2076 avg training loss: 7.7927
batch: [16350/21305] batch time: 0.062 trainign loss: 7.6936 avg training loss: 7.7926
batch: [16360/21305] batch time: 2.283 trainign loss: 6.2709 avg training loss: 7.7925
batch: [16370/21305] batch time: 0.056 trainign loss: 6.2979 avg training loss: 7.7923
batch: [16380/21305] batch time: 2.092 trainign loss: 7.0747 avg training loss: 7.7922
batch: [16390/21305] batch time: 0.053 trainign loss: 6.2875 avg training loss: 7.7921
batch: [16400/21305] batch time: 2.334 trainign loss: 7.7672 avg training loss: 7.7920
batch: [16410/21305] batch time: 0.051 trainign loss: 7.6440 avg training loss: 7.7919
batch: [16420/21305] batch time: 2.131 trainign loss: 7.0067 avg training loss: 7.7919
batch: [16430/21305] batch time: 0.054 trainign loss: 5.9559 avg training loss: 7.7917
batch: [16440/21305] batch time: 2.507 trainign loss: 7.3839 avg training loss: 7.7916
batch: [16450/21305] batch time: 0.056 trainign loss: 7.7736 avg training loss: 7.7915
batch: [16460/21305] batch time: 2.511 trainign loss: 7.1025 avg training loss: 7.7914
batch: [16470/21305] batch time: 0.055 trainign loss: 7.0189 avg training loss: 7.7914
batch: [16480/21305] batch time: 2.607 trainign loss: 6.0350 avg training loss: 7.7912
batch: [16490/21305] batch time: 0.056 trainign loss: 5.8144 avg training loss: 7.7911
batch: [16500/21305] batch time: 2.240 trainign loss: 6.2196 avg training loss: 7.7910
batch: [16510/21305] batch time: 0.060 trainign loss: 6.6137 avg training loss: 7.7908
batch: [16520/21305] batch time: 2.184 trainign loss: 6.5145 avg training loss: 7.7907
batch: [16530/21305] batch time: 0.057 trainign loss: 6.7580 avg training loss: 7.7906
batch: [16540/21305] batch time: 2.378 trainign loss: 6.1625 avg training loss: 7.7903
batch: [16550/21305] batch time: 0.056 trainign loss: 7.3108 avg training loss: 7.7902
batch: [16560/21305] batch time: 1.752 trainign loss: 7.1165 avg training loss: 7.7901
batch: [16570/21305] batch time: 0.056 trainign loss: 6.0029 avg training loss: 7.7900
batch: [16580/21305] batch time: 1.720 trainign loss: 7.1523 avg training loss: 7.7899
batch: [16590/21305] batch time: 0.056 trainign loss: 7.0135 avg training loss: 7.7897
batch: [16600/21305] batch time: 1.226 trainign loss: 6.5835 avg training loss: 7.7896
batch: [16610/21305] batch time: 0.061 trainign loss: 5.2806 avg training loss: 7.7893
batch: [16620/21305] batch time: 0.451 trainign loss: 6.2808 avg training loss: 7.7891
batch: [16630/21305] batch time: 0.052 trainign loss: 6.0711 avg training loss: 7.7889
batch: [16640/21305] batch time: 0.696 trainign loss: 7.8231 avg training loss: 7.7888
batch: [16650/21305] batch time: 0.053 trainign loss: 7.3546 avg training loss: 7.7888
batch: [16660/21305] batch time: 0.827 trainign loss: 6.9695 avg training loss: 7.7887
batch: [16670/21305] batch time: 0.052 trainign loss: 6.8523 avg training loss: 7.7885
batch: [16680/21305] batch time: 0.664 trainign loss: 7.5910 avg training loss: 7.7885
batch: [16690/21305] batch time: 0.674 trainign loss: 7.4017 avg training loss: 7.7884
batch: [16700/21305] batch time: 1.036 trainign loss: 7.0079 avg training loss: 7.7883
batch: [16710/21305] batch time: 0.054 trainign loss: 6.9214 avg training loss: 7.7882
batch: [16720/21305] batch time: 1.950 trainign loss: 7.1453 avg training loss: 7.7882
batch: [16730/21305] batch time: 0.061 trainign loss: 5.3861 avg training loss: 7.7880
batch: [16740/21305] batch time: 2.325 trainign loss: 6.5211 avg training loss: 7.7879
batch: [16750/21305] batch time: 0.060 trainign loss: 6.8184 avg training loss: 7.7878
batch: [16760/21305] batch time: 1.878 trainign loss: 6.3884 avg training loss: 7.7877
batch: [16770/21305] batch time: 0.061 trainign loss: 7.0712 avg training loss: 7.7876
batch: [16780/21305] batch time: 2.175 trainign loss: 6.3874 avg training loss: 7.7875
batch: [16790/21305] batch time: 0.060 trainign loss: 6.5223 avg training loss: 7.7873
batch: [16800/21305] batch time: 1.837 trainign loss: 2.7766 avg training loss: 7.7870
batch: [16810/21305] batch time: 0.054 trainign loss: 7.8765 avg training loss: 7.7870
batch: [16820/21305] batch time: 1.296 trainign loss: 6.3174 avg training loss: 7.7869
batch: [16830/21305] batch time: 0.056 trainign loss: 6.6930 avg training loss: 7.7868
batch: [16840/21305] batch time: 0.966 trainign loss: 7.4693 avg training loss: 7.7866
batch: [16850/21305] batch time: 0.056 trainign loss: 6.8989 avg training loss: 7.7865
batch: [16860/21305] batch time: 0.805 trainign loss: 7.0129 avg training loss: 7.7864
batch: [16870/21305] batch time: 0.057 trainign loss: 6.9587 avg training loss: 7.7862
batch: [16880/21305] batch time: 0.057 trainign loss: 6.8526 avg training loss: 7.7861
batch: [16890/21305] batch time: 0.061 trainign loss: 6.9030 avg training loss: 7.7860
batch: [16900/21305] batch time: 0.602 trainign loss: 4.2844 avg training loss: 7.7858
batch: [16910/21305] batch time: 0.054 trainign loss: 0.0058 avg training loss: 7.7851
batch: [16920/21305] batch time: 1.006 trainign loss: 8.3728 avg training loss: 7.7851
batch: [16930/21305] batch time: 0.058 trainign loss: 4.8276 avg training loss: 7.7850
batch: [16940/21305] batch time: 0.942 trainign loss: 7.2890 avg training loss: 7.7850
batch: [16950/21305] batch time: 0.053 trainign loss: 7.2746 avg training loss: 7.7849
batch: [16960/21305] batch time: 0.603 trainign loss: 5.0378 avg training loss: 7.7848
batch: [16970/21305] batch time: 0.062 trainign loss: 5.7791 avg training loss: 7.7846
batch: [16980/21305] batch time: 0.827 trainign loss: 3.7817 avg training loss: 7.7844
batch: [16990/21305] batch time: 0.056 trainign loss: 3.9038 avg training loss: 7.7841
batch: [17000/21305] batch time: 1.037 trainign loss: 2.3604 avg training loss: 7.7838
batch: [17010/21305] batch time: 0.056 trainign loss: 7.5684 avg training loss: 7.7838
batch: [17020/21305] batch time: 1.430 trainign loss: 0.7628 avg training loss: 7.7835
batch: [17030/21305] batch time: 0.056 trainign loss: 8.0157 avg training loss: 7.7833
batch: [17040/21305] batch time: 2.260 trainign loss: 7.7549 avg training loss: 7.7833
batch: [17050/21305] batch time: 0.061 trainign loss: 6.9864 avg training loss: 7.7832
batch: [17060/21305] batch time: 1.334 trainign loss: 7.2427 avg training loss: 7.7830
batch: [17070/21305] batch time: 0.056 trainign loss: 7.6509 avg training loss: 7.7828
batch: [17080/21305] batch time: 0.580 trainign loss: 7.4555 avg training loss: 7.7828
batch: [17090/21305] batch time: 0.054 trainign loss: 6.5344 avg training loss: 7.7827
batch: [17100/21305] batch time: 0.062 trainign loss: 1.8814 avg training loss: 7.7824
batch: [17110/21305] batch time: 0.051 trainign loss: 7.9781 avg training loss: 7.7824
batch: [17120/21305] batch time: 0.056 trainign loss: 7.3119 avg training loss: 7.7824
batch: [17130/21305] batch time: 0.061 trainign loss: 6.0703 avg training loss: 7.7823
batch: [17140/21305] batch time: 0.750 trainign loss: 5.8187 avg training loss: 7.7821
batch: [17150/21305] batch time: 0.056 trainign loss: 6.8494 avg training loss: 7.7820
batch: [17160/21305] batch time: 0.056 trainign loss: 6.7585 avg training loss: 7.7818
batch: [17170/21305] batch time: 0.052 trainign loss: 5.3202 avg training loss: 7.7815
batch: [17180/21305] batch time: 0.285 trainign loss: 7.5418 avg training loss: 7.7815
batch: [17190/21305] batch time: 0.056 trainign loss: 5.6475 avg training loss: 7.7814
batch: [17200/21305] batch time: 0.692 trainign loss: 6.4494 avg training loss: 7.7812
batch: [17210/21305] batch time: 0.053 trainign loss: 6.8277 avg training loss: 7.7811
batch: [17220/21305] batch time: 0.056 trainign loss: 6.7991 avg training loss: 7.7810
batch: [17230/21305] batch time: 0.057 trainign loss: 5.0295 avg training loss: 7.7808
batch: [17240/21305] batch time: 0.580 trainign loss: 6.3552 avg training loss: 7.7806
batch: [17250/21305] batch time: 0.056 trainign loss: 8.4505 avg training loss: 7.7804
batch: [17260/21305] batch time: 1.013 trainign loss: 7.7012 avg training loss: 7.7804
batch: [17270/21305] batch time: 0.063 trainign loss: 6.4471 avg training loss: 7.7803
batch: [17280/21305] batch time: 0.802 trainign loss: 7.0875 avg training loss: 7.7802
batch: [17290/21305] batch time: 0.061 trainign loss: 4.9533 avg training loss: 7.7801
batch: [17300/21305] batch time: 0.057 trainign loss: 6.3354 avg training loss: 7.7800
batch: [17310/21305] batch time: 0.051 trainign loss: 3.4827 avg training loss: 7.7797
batch: [17320/21305] batch time: 0.057 trainign loss: 11.7091 avg training loss: 7.7791
batch: [17330/21305] batch time: 0.054 trainign loss: 8.7745 avg training loss: 7.7793
batch: [17340/21305] batch time: 0.056 trainign loss: 7.8804 avg training loss: 7.7793
batch: [17350/21305] batch time: 0.057 trainign loss: 7.8209 avg training loss: 7.7790
batch: [17360/21305] batch time: 0.059 trainign loss: 6.5309 avg training loss: 7.7789
batch: [17370/21305] batch time: 0.051 trainign loss: 6.3471 avg training loss: 7.7788
batch: [17380/21305] batch time: 0.056 trainign loss: 7.0247 avg training loss: 7.7788
batch: [17390/21305] batch time: 0.051 trainign loss: 7.6529 avg training loss: 7.7787
batch: [17400/21305] batch time: 0.062 trainign loss: 7.1930 avg training loss: 7.7787
batch: [17410/21305] batch time: 0.056 trainign loss: 7.1399 avg training loss: 7.7786
batch: [17420/21305] batch time: 0.059 trainign loss: 6.2687 avg training loss: 7.7785
batch: [17430/21305] batch time: 0.053 trainign loss: 5.7329 avg training loss: 7.7784
batch: [17440/21305] batch time: 0.058 trainign loss: 6.8971 avg training loss: 7.7782
batch: [17450/21305] batch time: 0.061 trainign loss: 6.5178 avg training loss: 7.7781
batch: [17460/21305] batch time: 0.056 trainign loss: 5.0078 avg training loss: 7.7780
batch: [17470/21305] batch time: 0.051 trainign loss: 7.2374 avg training loss: 7.7778
batch: [17480/21305] batch time: 0.062 trainign loss: 5.9831 avg training loss: 7.7777
batch: [17490/21305] batch time: 0.050 trainign loss: 7.1677 avg training loss: 7.7776
batch: [17500/21305] batch time: 0.189 trainign loss: 6.7115 avg training loss: 7.7775
batch: [17510/21305] batch time: 0.051 trainign loss: 6.5114 avg training loss: 7.7774
batch: [17520/21305] batch time: 0.060 trainign loss: 5.3572 avg training loss: 7.7772
batch: [17530/21305] batch time: 0.056 trainign loss: 5.3830 avg training loss: 7.7771
batch: [17540/21305] batch time: 0.056 trainign loss: 6.5477 avg training loss: 7.7770
batch: [17550/21305] batch time: 0.055 trainign loss: 6.7343 avg training loss: 7.7769
batch: [17560/21305] batch time: 0.057 trainign loss: 5.9003 avg training loss: 7.7767
batch: [17570/21305] batch time: 0.057 trainign loss: 7.0170 avg training loss: 7.7765
batch: [17580/21305] batch time: 0.057 trainign loss: 1.5739 avg training loss: 7.7762
batch: [17590/21305] batch time: 0.056 trainign loss: 10.9912 avg training loss: 7.7756
batch: [17600/21305] batch time: 0.056 trainign loss: 8.4622 avg training loss: 7.7754
batch: [17610/21305] batch time: 0.055 trainign loss: 8.8958 avg training loss: 7.7755
batch: [17620/21305] batch time: 0.062 trainign loss: 6.4465 avg training loss: 7.7755
batch: [17630/21305] batch time: 0.052 trainign loss: 6.8902 avg training loss: 7.7754
batch: [17640/21305] batch time: 0.055 trainign loss: 5.5744 avg training loss: 7.7753
batch: [17650/21305] batch time: 0.056 trainign loss: 5.9347 avg training loss: 7.7752
batch: [17660/21305] batch time: 0.059 trainign loss: 6.4583 avg training loss: 7.7751
batch: [17670/21305] batch time: 0.054 trainign loss: 5.9972 avg training loss: 7.7750
batch: [17680/21305] batch time: 0.056 trainign loss: 6.9510 avg training loss: 7.7748
batch: [17690/21305] batch time: 0.056 trainign loss: 5.8466 avg training loss: 7.7747
batch: [17700/21305] batch time: 0.063 trainign loss: 6.4336 avg training loss: 7.7745
batch: [17710/21305] batch time: 0.051 trainign loss: 6.8144 avg training loss: 7.7745
batch: [17720/21305] batch time: 0.057 trainign loss: 6.4668 avg training loss: 7.7744
batch: [17730/21305] batch time: 0.051 trainign loss: 6.6250 avg training loss: 7.7742
batch: [17740/21305] batch time: 0.057 trainign loss: 5.6722 avg training loss: 7.7741
batch: [17750/21305] batch time: 0.053 trainign loss: 6.7995 avg training loss: 7.7740
batch: [17760/21305] batch time: 0.055 trainign loss: 6.6671 avg training loss: 7.7740
batch: [17770/21305] batch time: 0.057 trainign loss: 6.9036 avg training loss: 7.7739
batch: [17780/21305] batch time: 0.057 trainign loss: 6.1634 avg training loss: 7.7737
batch: [17790/21305] batch time: 0.057 trainign loss: 7.2676 avg training loss: 7.7736
batch: [17800/21305] batch time: 0.057 trainign loss: 6.2547 avg training loss: 7.7735
batch: [17810/21305] batch time: 0.055 trainign loss: 7.1245 avg training loss: 7.7734
batch: [17820/21305] batch time: 0.056 trainign loss: 6.5376 avg training loss: 7.7733
batch: [17830/21305] batch time: 0.051 trainign loss: 5.5786 avg training loss: 7.7732
batch: [17840/21305] batch time: 0.057 trainign loss: 7.4498 avg training loss: 7.7731
batch: [17850/21305] batch time: 0.056 trainign loss: 6.8380 avg training loss: 7.7730
batch: [17860/21305] batch time: 0.057 trainign loss: 6.7274 avg training loss: 7.7729
batch: [17870/21305] batch time: 0.054 trainign loss: 5.9033 avg training loss: 7.7727
batch: [17880/21305] batch time: 0.056 trainign loss: 6.9785 avg training loss: 7.7727
batch: [17890/21305] batch time: 0.053 trainign loss: 6.7735 avg training loss: 7.7725
batch: [17900/21305] batch time: 0.056 trainign loss: 6.4380 avg training loss: 7.7724
batch: [17910/21305] batch time: 0.057 trainign loss: 3.4689 avg training loss: 7.7721
batch: [17920/21305] batch time: 0.053 trainign loss: 7.7692 avg training loss: 7.7721
batch: [17930/21305] batch time: 0.061 trainign loss: 6.0716 avg training loss: 7.7720
batch: [17940/21305] batch time: 0.056 trainign loss: 6.5765 avg training loss: 7.7719
batch: [17950/21305] batch time: 0.051 trainign loss: 6.6774 avg training loss: 7.7717
batch: [17960/21305] batch time: 0.061 trainign loss: 6.8209 avg training loss: 7.7716
batch: [17970/21305] batch time: 0.056 trainign loss: 5.8171 avg training loss: 7.7714
batch: [17980/21305] batch time: 0.056 trainign loss: 1.6366 avg training loss: 7.7711
batch: [17990/21305] batch time: 0.056 trainign loss: 5.0818 avg training loss: 7.7710
batch: [18000/21305] batch time: 0.057 trainign loss: 7.4225 avg training loss: 7.7709
batch: [18010/21305] batch time: 0.056 trainign loss: 8.5126 avg training loss: 7.7708
batch: [18020/21305] batch time: 0.056 trainign loss: 4.9907 avg training loss: 7.7707
batch: [18030/21305] batch time: 0.054 trainign loss: 6.8714 avg training loss: 7.7706
batch: [18040/21305] batch time: 0.056 trainign loss: 6.3696 avg training loss: 7.7705
batch: [18050/21305] batch time: 0.051 trainign loss: 6.6677 avg training loss: 7.7704
batch: [18060/21305] batch time: 0.056 trainign loss: 7.3548 avg training loss: 7.7703
batch: [18070/21305] batch time: 0.055 trainign loss: 7.5180 avg training loss: 7.7702
batch: [18080/21305] batch time: 0.056 trainign loss: 6.6363 avg training loss: 7.7701
batch: [18090/21305] batch time: 0.052 trainign loss: 5.4579 avg training loss: 7.7700
batch: [18100/21305] batch time: 0.057 trainign loss: 6.1886 avg training loss: 7.7698
batch: [18110/21305] batch time: 0.056 trainign loss: 5.6556 avg training loss: 7.7697
batch: [18120/21305] batch time: 0.057 trainign loss: 7.0239 avg training loss: 7.7694
batch: [18130/21305] batch time: 0.052 trainign loss: 7.5690 avg training loss: 7.7694
batch: [18140/21305] batch time: 0.061 trainign loss: 7.2960 avg training loss: 7.7693
batch: [18150/21305] batch time: 0.056 trainign loss: 7.2652 avg training loss: 7.7692
batch: [18160/21305] batch time: 0.056 trainign loss: 5.7352 avg training loss: 7.7691
batch: [18170/21305] batch time: 0.063 trainign loss: 5.9333 avg training loss: 7.7690
batch: [18180/21305] batch time: 0.057 trainign loss: 0.1391 avg training loss: 7.7685
batch: [18190/21305] batch time: 0.057 trainign loss: 7.4479 avg training loss: 7.7684
batch: [18200/21305] batch time: 0.057 trainign loss: 7.3552 avg training loss: 7.7684
batch: [18210/21305] batch time: 0.056 trainign loss: 6.9933 avg training loss: 7.7683
batch: [18220/21305] batch time: 0.056 trainign loss: 4.6287 avg training loss: 7.7681
batch: [18230/21305] batch time: 0.056 trainign loss: 7.9440 avg training loss: 7.7681
batch: [18240/21305] batch time: 0.056 trainign loss: 6.3161 avg training loss: 7.7680
batch: [18250/21305] batch time: 0.056 trainign loss: 5.7399 avg training loss: 7.7679
batch: [18260/21305] batch time: 0.271 trainign loss: 2.4994 avg training loss: 7.7676
batch: [18270/21305] batch time: 0.056 trainign loss: 6.4210 avg training loss: 7.7675
batch: [18280/21305] batch time: 0.553 trainign loss: 6.6060 avg training loss: 7.7674
batch: [18290/21305] batch time: 0.062 trainign loss: 7.3662 avg training loss: 7.7673
batch: [18300/21305] batch time: 0.831 trainign loss: 6.6603 avg training loss: 7.7673
batch: [18310/21305] batch time: 0.063 trainign loss: 6.7246 avg training loss: 7.7672
batch: [18320/21305] batch time: 0.463 trainign loss: 5.1760 avg training loss: 7.7670
batch: [18330/21305] batch time: 0.063 trainign loss: 6.3083 avg training loss: 7.7669
batch: [18340/21305] batch time: 0.232 trainign loss: 7.4825 avg training loss: 7.7668
batch: [18350/21305] batch time: 0.056 trainign loss: 7.4123 avg training loss: 7.7667
batch: [18360/21305] batch time: 0.056 trainign loss: 5.6467 avg training loss: 7.7666
batch: [18370/21305] batch time: 0.061 trainign loss: 7.3362 avg training loss: 7.7665
batch: [18380/21305] batch time: 0.056 trainign loss: 6.3326 avg training loss: 7.7664
batch: [18390/21305] batch time: 0.056 trainign loss: 6.4187 avg training loss: 7.7662
batch: [18400/21305] batch time: 0.053 trainign loss: 7.3154 avg training loss: 7.7661
batch: [18410/21305] batch time: 0.056 trainign loss: 5.1103 avg training loss: 7.7660
batch: [18420/21305] batch time: 0.058 trainign loss: 6.4524 avg training loss: 7.7659
batch: [18430/21305] batch time: 0.057 trainign loss: 6.6634 avg training loss: 7.7657
batch: [18440/21305] batch time: 0.054 trainign loss: 0.0646 avg training loss: 7.7652
batch: [18450/21305] batch time: 0.056 trainign loss: 9.7070 avg training loss: 7.7651
batch: [18460/21305] batch time: 0.055 trainign loss: 7.2096 avg training loss: 7.7651
batch: [18470/21305] batch time: 0.063 trainign loss: 7.6867 avg training loss: 7.7651
batch: [18480/21305] batch time: 0.051 trainign loss: 6.3015 avg training loss: 7.7649
batch: [18490/21305] batch time: 0.057 trainign loss: 4.0253 avg training loss: 7.7647
batch: [18500/21305] batch time: 0.052 trainign loss: 0.0044 avg training loss: 7.7640
batch: [18510/21305] batch time: 0.056 trainign loss: 8.6006 avg training loss: 7.7640
batch: [18520/21305] batch time: 0.056 trainign loss: 8.4668 avg training loss: 7.7641
batch: [18530/21305] batch time: 0.062 trainign loss: 6.3884 avg training loss: 7.7640
batch: [18540/21305] batch time: 0.056 trainign loss: 6.3533 avg training loss: 7.7639
batch: [18550/21305] batch time: 0.056 trainign loss: 4.1965 avg training loss: 7.7637
batch: [18560/21305] batch time: 0.054 trainign loss: 7.5866 avg training loss: 7.7636
batch: [18570/21305] batch time: 0.056 trainign loss: 7.4887 avg training loss: 7.7635
batch: [18580/21305] batch time: 0.056 trainign loss: 6.8446 avg training loss: 7.7634
batch: [18590/21305] batch time: 0.056 trainign loss: 7.0245 avg training loss: 7.7633
batch: [18600/21305] batch time: 0.053 trainign loss: 5.7418 avg training loss: 7.7632
batch: [18610/21305] batch time: 0.063 trainign loss: 6.4571 avg training loss: 7.7629
batch: [18620/21305] batch time: 0.057 trainign loss: 7.8642 avg training loss: 7.7628
batch: [18630/21305] batch time: 0.056 trainign loss: 6.4929 avg training loss: 7.7628
batch: [18640/21305] batch time: 0.051 trainign loss: 5.1671 avg training loss: 7.7626
batch: [18650/21305] batch time: 0.063 trainign loss: 3.8622 avg training loss: 7.7624
batch: [18660/21305] batch time: 0.056 trainign loss: 0.0028 avg training loss: 7.7617
batch: [18670/21305] batch time: 0.056 trainign loss: 9.4911 avg training loss: 7.7616
batch: [18680/21305] batch time: 0.057 trainign loss: 8.0069 avg training loss: 7.7617
batch: [18690/21305] batch time: 0.063 trainign loss: 7.9977 avg training loss: 7.7617
batch: [18700/21305] batch time: 0.050 trainign loss: 7.4658 avg training loss: 7.7617
batch: [18710/21305] batch time: 0.062 trainign loss: 7.1665 avg training loss: 7.7617
batch: [18720/21305] batch time: 0.055 trainign loss: 6.3712 avg training loss: 7.7616
batch: [18730/21305] batch time: 0.057 trainign loss: 5.6457 avg training loss: 7.7614
batch: [18740/21305] batch time: 0.056 trainign loss: 7.6307 avg training loss: 7.7613
batch: [18750/21305] batch time: 0.056 trainign loss: 5.8063 avg training loss: 7.7612
batch: [18760/21305] batch time: 0.056 trainign loss: 5.2639 avg training loss: 7.7611
batch: [18770/21305] batch time: 0.056 trainign loss: 7.9817 avg training loss: 7.7609
batch: [18780/21305] batch time: 0.051 trainign loss: 7.1883 avg training loss: 7.7608
batch: [18790/21305] batch time: 0.056 trainign loss: 5.8541 avg training loss: 7.7607
batch: [18800/21305] batch time: 0.056 trainign loss: 7.5516 avg training loss: 7.7606
batch: [18810/21305] batch time: 0.056 trainign loss: 7.1308 avg training loss: 7.7606
batch: [18820/21305] batch time: 0.051 trainign loss: 4.9935 avg training loss: 7.7605
batch: [18830/21305] batch time: 0.056 trainign loss: 7.5911 avg training loss: 7.7603
batch: [18840/21305] batch time: 0.057 trainign loss: 6.5151 avg training loss: 7.7602
batch: [18850/21305] batch time: 0.056 trainign loss: 7.2189 avg training loss: 7.7601
batch: [18860/21305] batch time: 0.056 trainign loss: 6.8123 avg training loss: 7.7599
batch: [18870/21305] batch time: 0.056 trainign loss: 6.5436 avg training loss: 7.7598
batch: [18880/21305] batch time: 0.056 trainign loss: 4.5615 avg training loss: 7.7595
batch: [18890/21305] batch time: 0.056 trainign loss: 7.6201 avg training loss: 7.7595
batch: [18900/21305] batch time: 0.052 trainign loss: 6.2790 avg training loss: 7.7595
batch: [18910/21305] batch time: 0.056 trainign loss: 6.8353 avg training loss: 7.7594
batch: [18920/21305] batch time: 0.056 trainign loss: 7.0822 avg training loss: 7.7593
batch: [18930/21305] batch time: 0.057 trainign loss: 7.5100 avg training loss: 7.7592
batch: [18940/21305] batch time: 0.056 trainign loss: 6.6005 avg training loss: 7.7591
batch: [18950/21305] batch time: 0.056 trainign loss: 7.0189 avg training loss: 7.7590
batch: [18960/21305] batch time: 0.051 trainign loss: 7.0409 avg training loss: 7.7588
batch: [18970/21305] batch time: 0.061 trainign loss: 6.4763 avg training loss: 7.7587
batch: [18980/21305] batch time: 0.056 trainign loss: 8.4590 avg training loss: 7.7584
batch: [18990/21305] batch time: 0.056 trainign loss: 7.6337 avg training loss: 7.7583
batch: [19000/21305] batch time: 0.052 trainign loss: 6.0293 avg training loss: 7.7582
batch: [19010/21305] batch time: 0.056 trainign loss: 7.5773 avg training loss: 7.7581
batch: [19020/21305] batch time: 0.060 trainign loss: 6.8523 avg training loss: 7.7580
batch: [19030/21305] batch time: 0.063 trainign loss: 7.6028 avg training loss: 7.7579
batch: [19040/21305] batch time: 0.056 trainign loss: 7.1729 avg training loss: 7.7578
batch: [19050/21305] batch time: 0.058 trainign loss: 7.4196 avg training loss: 7.7577
batch: [19060/21305] batch time: 0.063 trainign loss: 5.6547 avg training loss: 7.7574
batch: [19070/21305] batch time: 0.063 trainign loss: 7.1417 avg training loss: 7.7573
batch: [19080/21305] batch time: 0.051 trainign loss: 7.2770 avg training loss: 7.7573
batch: [19090/21305] batch time: 0.062 trainign loss: 8.0360 avg training loss: 7.7572
batch: [19100/21305] batch time: 0.058 trainign loss: 7.4878 avg training loss: 7.7572
batch: [19110/21305] batch time: 0.062 trainign loss: 3.9142 avg training loss: 7.7570
batch: [19120/21305] batch time: 0.056 trainign loss: 6.8733 avg training loss: 7.7569
batch: [19130/21305] batch time: 0.380 trainign loss: 7.5526 avg training loss: 7.7569
batch: [19140/21305] batch time: 0.052 trainign loss: 6.2331 avg training loss: 7.7567
batch: [19150/21305] batch time: 0.056 trainign loss: 6.2031 avg training loss: 7.7566
batch: [19160/21305] batch time: 0.062 trainign loss: 7.0635 avg training loss: 7.7566
batch: [19170/21305] batch time: 0.057 trainign loss: 6.8098 avg training loss: 7.7565
batch: [19180/21305] batch time: 0.056 trainign loss: 4.8467 avg training loss: 7.7563
batch: [19190/21305] batch time: 0.063 trainign loss: 6.8703 avg training loss: 7.7561
batch: [19200/21305] batch time: 0.056 trainign loss: 4.4890 avg training loss: 7.7559
batch: [19210/21305] batch time: 0.216 trainign loss: 6.9640 avg training loss: 7.7559
batch: [19220/21305] batch time: 0.057 trainign loss: 7.2393 avg training loss: 7.7557
batch: [19230/21305] batch time: 1.095 trainign loss: 6.6612 avg training loss: 7.7556
batch: [19240/21305] batch time: 0.058 trainign loss: 6.0654 avg training loss: 7.7556
batch: [19250/21305] batch time: 1.420 trainign loss: 3.4261 avg training loss: 7.7553
batch: [19260/21305] batch time: 0.061 trainign loss: 5.2957 avg training loss: 7.7551
batch: [19270/21305] batch time: 0.868 trainign loss: 7.4103 avg training loss: 7.7550
batch: [19280/21305] batch time: 0.057 trainign loss: 8.2856 avg training loss: 7.7550
batch: [19290/21305] batch time: 0.851 trainign loss: 7.3656 avg training loss: 7.7549
batch: [19300/21305] batch time: 0.056 trainign loss: 7.4605 avg training loss: 7.7549
batch: [19310/21305] batch time: 0.236 trainign loss: 7.7682 avg training loss: 7.7548
batch: [19320/21305] batch time: 0.056 trainign loss: 7.6607 avg training loss: 7.7547
batch: [19330/21305] batch time: 0.560 trainign loss: 6.9947 avg training loss: 7.7547
batch: [19340/21305] batch time: 0.062 trainign loss: 5.5997 avg training loss: 7.7546
batch: [19350/21305] batch time: 0.814 trainign loss: 7.1893 avg training loss: 7.7545
batch: [19360/21305] batch time: 0.056 trainign loss: 6.3353 avg training loss: 7.7544
batch: [19370/21305] batch time: 0.625 trainign loss: 6.8495 avg training loss: 7.7543
batch: [19380/21305] batch time: 0.056 trainign loss: 6.3362 avg training loss: 7.7541
batch: [19390/21305] batch time: 1.191 trainign loss: 7.9870 avg training loss: 7.7541
batch: [19400/21305] batch time: 0.062 trainign loss: 7.2668 avg training loss: 7.7541
batch: [19410/21305] batch time: 0.599 trainign loss: 5.5733 avg training loss: 7.7539
batch: [19420/21305] batch time: 0.056 trainign loss: 6.8032 avg training loss: 7.7538
batch: [19430/21305] batch time: 0.056 trainign loss: 7.2079 avg training loss: 7.7537
batch: [19440/21305] batch time: 0.052 trainign loss: 7.5554 avg training loss: 7.7537
batch: [19450/21305] batch time: 0.349 trainign loss: 7.0927 avg training loss: 7.7536
batch: [19460/21305] batch time: 0.063 trainign loss: 6.5722 avg training loss: 7.7535
batch: [19470/21305] batch time: 0.058 trainign loss: 7.3198 avg training loss: 7.7535
batch: [19480/21305] batch time: 0.055 trainign loss: 6.1034 avg training loss: 7.7534
batch: [19490/21305] batch time: 0.057 trainign loss: 5.5132 avg training loss: 7.7532
batch: [19500/21305] batch time: 0.057 trainign loss: 7.1072 avg training loss: 7.7532
batch: [19510/21305] batch time: 0.056 trainign loss: 6.6058 avg training loss: 7.7531
batch: [19520/21305] batch time: 0.057 trainign loss: 6.8515 avg training loss: 7.7530
batch: [19530/21305] batch time: 0.056 trainign loss: 7.0665 avg training loss: 7.7529
batch: [19540/21305] batch time: 0.056 trainign loss: 6.5401 avg training loss: 7.7527
batch: [19550/21305] batch time: 0.056 trainign loss: 6.5648 avg training loss: 7.7526
batch: [19560/21305] batch time: 0.061 trainign loss: 7.4475 avg training loss: 7.7525
batch: [19570/21305] batch time: 0.060 trainign loss: 6.8748 avg training loss: 7.7524
batch: [19580/21305] batch time: 0.055 trainign loss: 5.7183 avg training loss: 7.7523
batch: [19590/21305] batch time: 0.062 trainign loss: 5.6045 avg training loss: 7.7520
batch: [19600/21305] batch time: 0.053 trainign loss: 6.9688 avg training loss: 7.7519
batch: [19610/21305] batch time: 0.056 trainign loss: 6.7367 avg training loss: 7.7519
batch: [19620/21305] batch time: 0.051 trainign loss: 6.8789 avg training loss: 7.7518
batch: [19630/21305] batch time: 0.056 trainign loss: 7.3407 avg training loss: 7.7517
batch: [19640/21305] batch time: 0.057 trainign loss: 7.4245 avg training loss: 7.7516
batch: [19650/21305] batch time: 0.057 trainign loss: 6.6463 avg training loss: 7.7515
batch: [19660/21305] batch time: 0.057 trainign loss: 6.5751 avg training loss: 7.7514
batch: [19670/21305] batch time: 0.058 trainign loss: 6.6522 avg training loss: 7.7512
batch: [19680/21305] batch time: 0.051 trainign loss: 0.0185 avg training loss: 7.7507
batch: [19690/21305] batch time: 0.056 trainign loss: 7.7965 avg training loss: 7.7507
batch: [19700/21305] batch time: 0.062 trainign loss: 8.0043 avg training loss: 7.7507
batch: [19710/21305] batch time: 0.056 trainign loss: 6.9189 avg training loss: 7.7506
batch: [19720/21305] batch time: 0.056 trainign loss: 6.3237 avg training loss: 7.7505
batch: [19730/21305] batch time: 0.056 trainign loss: 5.6161 avg training loss: 7.7504
batch: [19740/21305] batch time: 0.056 trainign loss: 5.7694 avg training loss: 7.7502
batch: [19750/21305] batch time: 0.056 trainign loss: 7.4366 avg training loss: 7.7501
batch: [19760/21305] batch time: 0.056 trainign loss: 6.7647 avg training loss: 7.7500
batch: [19770/21305] batch time: 0.057 trainign loss: 6.3065 avg training loss: 7.7500
batch: [19780/21305] batch time: 0.053 trainign loss: 5.1127 avg training loss: 7.7498
batch: [19790/21305] batch time: 0.051 trainign loss: 6.3015 avg training loss: 7.7496
batch: [19800/21305] batch time: 0.056 trainign loss: 6.4772 avg training loss: 7.7495
batch: [19810/21305] batch time: 0.050 trainign loss: 5.3124 avg training loss: 7.7493
batch: [19820/21305] batch time: 0.062 trainign loss: 7.0338 avg training loss: 7.7492
batch: [19830/21305] batch time: 0.056 trainign loss: 7.3019 avg training loss: 7.7492
batch: [19840/21305] batch time: 0.062 trainign loss: 6.6540 avg training loss: 7.7491
batch: [19850/21305] batch time: 0.052 trainign loss: 6.8410 avg training loss: 7.7489
batch: [19860/21305] batch time: 0.063 trainign loss: 7.7724 avg training loss: 7.7489
batch: [19870/21305] batch time: 0.063 trainign loss: 7.1986 avg training loss: 7.7488
batch: [19880/21305] batch time: 0.061 trainign loss: 5.3501 avg training loss: 7.7487
batch: [19890/21305] batch time: 0.057 trainign loss: 5.3662 avg training loss: 7.7485
batch: [19900/21305] batch time: 0.062 trainign loss: 9.1253 avg training loss: 7.7482
batch: [19910/21305] batch time: 0.053 trainign loss: 1.6797 avg training loss: 7.7478
batch: [19920/21305] batch time: 0.056 trainign loss: 7.6431 avg training loss: 7.7478
batch: [19930/21305] batch time: 0.057 trainign loss: 8.0993 avg training loss: 7.7479
batch: [19940/21305] batch time: 0.056 trainign loss: 6.3242 avg training loss: 7.7478
batch: [19950/21305] batch time: 0.055 trainign loss: 6.8884 avg training loss: 7.7476
batch: [19960/21305] batch time: 0.056 trainign loss: 7.0462 avg training loss: 7.7474
batch: [19970/21305] batch time: 0.056 trainign loss: 7.1428 avg training loss: 7.7473
batch: [19980/21305] batch time: 0.056 trainign loss: 7.6886 avg training loss: 7.7473
batch: [19990/21305] batch time: 0.055 trainign loss: 6.2315 avg training loss: 7.7472
batch: [20000/21305] batch time: 0.056 trainign loss: 7.0438 avg training loss: 7.7471
batch: [20010/21305] batch time: 0.052 trainign loss: 6.3015 avg training loss: 7.7470
batch: [20020/21305] batch time: 0.057 trainign loss: 1.4005 avg training loss: 7.7467
batch: [20030/21305] batch time: 0.053 trainign loss: 7.0503 avg training loss: 7.7465
batch: [20040/21305] batch time: 0.057 trainign loss: 5.8917 avg training loss: 7.7463
batch: [20050/21305] batch time: 0.056 trainign loss: 3.0300 avg training loss: 7.7461
batch: [20060/21305] batch time: 0.056 trainign loss: 4.6381 avg training loss: 7.7459
batch: [20070/21305] batch time: 0.057 trainign loss: 7.5130 avg training loss: 7.7458
batch: [20080/21305] batch time: 0.051 trainign loss: 7.1388 avg training loss: 7.7458
batch: [20090/21305] batch time: 0.058 trainign loss: 4.4665 avg training loss: 7.7456
batch: [20100/21305] batch time: 0.052 trainign loss: 10.9063 avg training loss: 7.7452
batch: [20110/21305] batch time: 0.057 trainign loss: 7.2683 avg training loss: 7.7453
batch: [20120/21305] batch time: 0.127 trainign loss: 7.8829 avg training loss: 7.7453
batch: [20130/21305] batch time: 0.059 trainign loss: 7.0806 avg training loss: 7.7452
batch: [20140/21305] batch time: 0.059 trainign loss: 6.9635 avg training loss: 7.7451
batch: [20150/21305] batch time: 0.058 trainign loss: 6.4810 avg training loss: 7.7451
batch: [20160/21305] batch time: 0.114 trainign loss: 7.2003 avg training loss: 7.7450
batch: [20170/21305] batch time: 0.056 trainign loss: 7.4669 avg training loss: 7.7449
batch: [20180/21305] batch time: 0.372 trainign loss: 6.4953 avg training loss: 7.7448
batch: [20190/21305] batch time: 0.056 trainign loss: 3.7663 avg training loss: 7.7446
batch: [20200/21305] batch time: 0.646 trainign loss: 5.2147 avg training loss: 7.7445
batch: [20210/21305] batch time: 0.658 trainign loss: 7.6825 avg training loss: 7.7443
batch: [20220/21305] batch time: 0.666 trainign loss: 7.3686 avg training loss: 7.7443
batch: [20230/21305] batch time: 0.415 trainign loss: 6.6610 avg training loss: 7.7442
batch: [20240/21305] batch time: 0.778 trainign loss: 5.2959 avg training loss: 7.7440
batch: [20250/21305] batch time: 0.058 trainign loss: 7.6120 avg training loss: 7.7438
batch: [20260/21305] batch time: 0.405 trainign loss: 7.1978 avg training loss: 7.7438
batch: [20270/21305] batch time: 0.062 trainign loss: 1.4995 avg training loss: 7.7435
batch: [20280/21305] batch time: 0.423 trainign loss: 8.2699 avg training loss: 7.7434
batch: [20290/21305] batch time: 0.056 trainign loss: 7.5349 avg training loss: 7.7434
batch: [20300/21305] batch time: 1.179 trainign loss: 7.4592 avg training loss: 7.7432
batch: [20310/21305] batch time: 0.057 trainign loss: 7.1295 avg training loss: 7.7432
batch: [20320/21305] batch time: 1.581 trainign loss: 6.6343 avg training loss: 7.7432
batch: [20330/21305] batch time: 0.063 trainign loss: 5.3664 avg training loss: 7.7430
batch: [20340/21305] batch time: 1.948 trainign loss: 6.7707 avg training loss: 7.7428
batch: [20350/21305] batch time: 0.056 trainign loss: 7.5466 avg training loss: 7.7428
batch: [20360/21305] batch time: 1.238 trainign loss: 7.1346 avg training loss: 7.7427
batch: [20370/21305] batch time: 0.057 trainign loss: 7.0275 avg training loss: 7.7425
batch: [20380/21305] batch time: 1.163 trainign loss: 6.2685 avg training loss: 7.7424
batch: [20390/21305] batch time: 0.063 trainign loss: 7.3643 avg training loss: 7.7423
batch: [20400/21305] batch time: 0.613 trainign loss: 6.8848 avg training loss: 7.7422
batch: [20410/21305] batch time: 0.346 trainign loss: 6.7198 avg training loss: 7.7421
batch: [20420/21305] batch time: 0.063 trainign loss: 6.7601 avg training loss: 7.7420
batch: [20430/21305] batch time: 0.058 trainign loss: 6.8518 avg training loss: 7.7418
batch: [20440/21305] batch time: 0.051 trainign loss: 6.7058 avg training loss: 7.7416
batch: [20450/21305] batch time: 0.056 trainign loss: 6.0802 avg training loss: 7.7416
batch: [20460/21305] batch time: 0.070 trainign loss: 7.3584 avg training loss: 7.7415
batch: [20470/21305] batch time: 0.056 trainign loss: 5.0537 avg training loss: 7.7413
batch: [20480/21305] batch time: 0.053 trainign loss: 0.0762 avg training loss: 7.7408
batch: [20490/21305] batch time: 0.059 trainign loss: 14.4339 avg training loss: 7.7404
batch: [20500/21305] batch time: 0.056 trainign loss: 8.8839 avg training loss: 7.7406
batch: [20510/21305] batch time: 0.056 trainign loss: 8.3991 avg training loss: 7.7407
batch: [20520/21305] batch time: 0.051 trainign loss: 7.4528 avg training loss: 7.7407
batch: [20530/21305] batch time: 0.060 trainign loss: 6.7117 avg training loss: 7.7405
batch: [20540/21305] batch time: 0.062 trainign loss: 8.2491 avg training loss: 7.7403
batch: [20550/21305] batch time: 0.060 trainign loss: 7.1449 avg training loss: 7.7402
batch: [20560/21305] batch time: 0.059 trainign loss: 6.6888 avg training loss: 7.7401
batch: [20570/21305] batch time: 0.261 trainign loss: 7.8580 avg training loss: 7.7400
batch: [20580/21305] batch time: 0.059 trainign loss: 6.9285 avg training loss: 7.7400
batch: [20590/21305] batch time: 0.955 trainign loss: 6.1177 avg training loss: 7.7398
batch: [20600/21305] batch time: 0.069 trainign loss: 6.4868 avg training loss: 7.7397
batch: [20610/21305] batch time: 0.611 trainign loss: 7.3908 avg training loss: 7.7397
batch: [20620/21305] batch time: 0.531 trainign loss: 7.0091 avg training loss: 7.7396
batch: [20630/21305] batch time: 0.056 trainign loss: 6.6213 avg training loss: 7.7395
batch: [20640/21305] batch time: 0.173 trainign loss: 6.9007 avg training loss: 7.7394
batch: [20650/21305] batch time: 0.401 trainign loss: 7.1101 avg training loss: 7.7393
batch: [20660/21305] batch time: 0.329 trainign loss: 6.0219 avg training loss: 7.7392
batch: [20670/21305] batch time: 0.513 trainign loss: 7.6701 avg training loss: 7.7391
batch: [20680/21305] batch time: 0.504 trainign loss: 5.7991 avg training loss: 7.7390
batch: [20690/21305] batch time: 0.360 trainign loss: 6.8461 avg training loss: 7.7389
batch: [20700/21305] batch time: 0.317 trainign loss: 7.2152 avg training loss: 7.7388
batch: [20710/21305] batch time: 0.111 trainign loss: 6.8007 avg training loss: 7.7387
batch: [20720/21305] batch time: 0.401 trainign loss: 7.0108 avg training loss: 7.7387
batch: [20730/21305] batch time: 0.061 trainign loss: 6.3838 avg training loss: 7.7386
batch: [20740/21305] batch time: 0.056 trainign loss: 6.6180 avg training loss: 7.7385
batch: [20750/21305] batch time: 0.061 trainign loss: 5.5822 avg training loss: 7.7383
batch: [20760/21305] batch time: 0.051 trainign loss: 7.5573 avg training loss: 7.7383
batch: [20770/21305] batch time: 0.058 trainign loss: 6.8862 avg training loss: 7.7382
batch: [20780/21305] batch time: 0.908 trainign loss: 6.7135 avg training loss: 7.7382
batch: [20790/21305] batch time: 0.060 trainign loss: 5.9264 avg training loss: 7.7380
batch: [20800/21305] batch time: 2.147 trainign loss: 6.3099 avg training loss: 7.7379
batch: [20810/21305] batch time: 0.056 trainign loss: 7.3991 avg training loss: 7.7378
batch: [20820/21305] batch time: 1.984 trainign loss: 6.4298 avg training loss: 7.7377
batch: [20830/21305] batch time: 0.056 trainign loss: 6.6674 avg training loss: 7.7377
batch: [20840/21305] batch time: 1.480 trainign loss: 5.8772 avg training loss: 7.7375
batch: [20850/21305] batch time: 0.052 trainign loss: 5.9620 avg training loss: 7.7373
batch: [20860/21305] batch time: 2.333 trainign loss: 7.0366 avg training loss: 7.7372
batch: [20870/21305] batch time: 0.053 trainign loss: 6.2103 avg training loss: 7.7371
batch: [20880/21305] batch time: 1.623 trainign loss: 6.0772 avg training loss: 7.7370
batch: [20890/21305] batch time: 1.094 trainign loss: 7.6024 avg training loss: 7.7369
batch: [20900/21305] batch time: 0.458 trainign loss: 4.7611 avg training loss: 7.7368
batch: [20910/21305] batch time: 1.806 trainign loss: 6.2129 avg training loss: 7.7366
batch: [20920/21305] batch time: 0.324 trainign loss: 5.0395 avg training loss: 7.7364
batch: [20930/21305] batch time: 1.815 trainign loss: 8.2530 avg training loss: 7.7362
batch: [20940/21305] batch time: 1.368 trainign loss: 3.1287 avg training loss: 7.7359
batch: [20950/21305] batch time: 1.114 trainign loss: 7.2777 avg training loss: 7.7358
batch: [20960/21305] batch time: 0.927 trainign loss: 7.6298 avg training loss: 7.7359
batch: [20970/21305] batch time: 1.122 trainign loss: 6.1280 avg training loss: 7.7359
batch: [20980/21305] batch time: 0.274 trainign loss: 7.4608 avg training loss: 7.7358
batch: [20990/21305] batch time: 2.274 trainign loss: 7.5911 avg training loss: 7.7358
batch: [21000/21305] batch time: 0.124 trainign loss: 6.6921 avg training loss: 7.7357
batch: [21010/21305] batch time: 2.400 trainign loss: 6.9104 avg training loss: 7.7356
batch: [21020/21305] batch time: 0.056 trainign loss: 6.5843 avg training loss: 7.7355
batch: [21030/21305] batch time: 2.414 trainign loss: 7.1424 avg training loss: 7.7355
batch: [21040/21305] batch time: 0.061 trainign loss: 7.4783 avg training loss: 7.7354
batch: [21050/21305] batch time: 2.288 trainign loss: 6.5932 avg training loss: 7.7353
batch: [21060/21305] batch time: 0.060 trainign loss: 6.4226 avg training loss: 7.7352
batch: [21070/21305] batch time: 1.720 trainign loss: 6.3131 avg training loss: 7.7350
batch: [21080/21305] batch time: 0.410 trainign loss: 6.5766 avg training loss: 7.7348
batch: [21090/21305] batch time: 2.090 trainign loss: 7.6810 avg training loss: 7.7347
batch: [21100/21305] batch time: 0.055 trainign loss: 4.4586 avg training loss: 7.7345
batch: [21110/21305] batch time: 2.028 trainign loss: 8.0816 avg training loss: 7.7344
batch: [21120/21305] batch time: 0.060 trainign loss: 7.6793 avg training loss: 7.7344
batch: [21130/21305] batch time: 2.382 trainign loss: 7.4201 avg training loss: 7.7343
batch: [21140/21305] batch time: 0.062 trainign loss: 7.0993 avg training loss: 7.7342
batch: [21150/21305] batch time: 2.251 trainign loss: 7.1149 avg training loss: 7.7341
batch: [21160/21305] batch time: 0.528 trainign loss: 6.7579 avg training loss: 7.7339
batch: [21170/21305] batch time: 2.670 trainign loss: 4.8690 avg training loss: 7.7336
batch: [21180/21305] batch time: 0.344 trainign loss: 7.8270 avg training loss: 7.7336
batch: [21190/21305] batch time: 1.729 trainign loss: 7.5257 avg training loss: 7.7336
batch: [21200/21305] batch time: 0.736 trainign loss: 6.9158 avg training loss: 7.7335
batch: [21210/21305] batch time: 1.942 trainign loss: 6.7778 avg training loss: 7.7334
batch: [21220/21305] batch time: 0.062 trainign loss: 6.2237 avg training loss: 7.7334
batch: [21230/21305] batch time: 2.149 trainign loss: 6.7665 avg training loss: 7.7332
batch: [21240/21305] batch time: 0.058 trainign loss: 5.6756 avg training loss: 7.7331
batch: [21250/21305] batch time: 2.150 trainign loss: 6.9809 avg training loss: 7.7330
batch: [21260/21305] batch time: 0.063 trainign loss: 7.2479 avg training loss: 7.7329
batch: [21270/21305] batch time: 2.041 trainign loss: 3.7431 avg training loss: 7.7328
batch: [21280/21305] batch time: 0.056 trainign loss: 6.1693 avg training loss: 7.7326
batch: [21290/21305] batch time: 2.024 trainign loss: 5.1415 avg training loss: 7.7324
batch: [21300/21305] batch time: 0.057 trainign loss: 7.3366 avg training loss: 7.7323
Epoch: 6
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.440 trainign loss: 6.8705 avg training loss: 7.7322
batch: [10/21305] batch time: 0.056 trainign loss: 8.9490 avg training loss: 7.7319
batch: [20/21305] batch time: 1.579 trainign loss: 8.0605 avg training loss: 7.7319
batch: [30/21305] batch time: 0.710 trainign loss: 7.2002 avg training loss: 7.7319
batch: [40/21305] batch time: 1.700 trainign loss: 7.3777 avg training loss: 7.7318
batch: [50/21305] batch time: 0.185 trainign loss: 4.2313 avg training loss: 7.7316
batch: [60/21305] batch time: 1.714 trainign loss: 7.5609 avg training loss: 7.7315
batch: [70/21305] batch time: 0.525 trainign loss: 7.4727 avg training loss: 7.7314
batch: [80/21305] batch time: 1.363 trainign loss: 6.9905 avg training loss: 7.7314
batch: [90/21305] batch time: 0.386 trainign loss: 6.9303 avg training loss: 7.7313
batch: [100/21305] batch time: 1.809 trainign loss: 7.1088 avg training loss: 7.7312
batch: [110/21305] batch time: 0.057 trainign loss: 6.6165 avg training loss: 7.7311
batch: [120/21305] batch time: 1.155 trainign loss: 6.7648 avg training loss: 7.7309
batch: [130/21305] batch time: 0.061 trainign loss: 7.3025 avg training loss: 7.7308
batch: [140/21305] batch time: 0.237 trainign loss: 5.1140 avg training loss: 7.7307
batch: [150/21305] batch time: 0.062 trainign loss: 7.6139 avg training loss: 7.7305
batch: [160/21305] batch time: 0.644 trainign loss: 6.8285 avg training loss: 7.7305
batch: [170/21305] batch time: 0.056 trainign loss: 6.9724 avg training loss: 7.7304
batch: [180/21305] batch time: 1.800 trainign loss: 6.4504 avg training loss: 7.7303
batch: [190/21305] batch time: 0.059 trainign loss: 5.2733 avg training loss: 7.7301
batch: [200/21305] batch time: 0.055 trainign loss: 0.0086 avg training loss: 7.7295
batch: [210/21305] batch time: 0.056 trainign loss: 8.5868 avg training loss: 7.7294
batch: [220/21305] batch time: 0.121 trainign loss: 7.7631 avg training loss: 7.7295
batch: [230/21305] batch time: 0.057 trainign loss: 6.4923 avg training loss: 7.7294
batch: [240/21305] batch time: 0.056 trainign loss: 7.7338 avg training loss: 7.7294
batch: [250/21305] batch time: 0.269 trainign loss: 7.7270 avg training loss: 7.7293
batch: [260/21305] batch time: 0.055 trainign loss: 7.1798 avg training loss: 7.7293
batch: [270/21305] batch time: 0.121 trainign loss: 5.0737 avg training loss: 7.7291
batch: [280/21305] batch time: 0.060 trainign loss: 6.3354 avg training loss: 7.7290
batch: [290/21305] batch time: 0.330 trainign loss: 7.1804 avg training loss: 7.7288
batch: [300/21305] batch time: 0.062 trainign loss: 7.2867 avg training loss: 7.7287
batch: [310/21305] batch time: 1.025 trainign loss: 6.0278 avg training loss: 7.7287
batch: [320/21305] batch time: 0.056 trainign loss: 7.2305 avg training loss: 7.7286
batch: [330/21305] batch time: 1.321 trainign loss: 6.7926 avg training loss: 7.7285
batch: [340/21305] batch time: 0.056 trainign loss: 8.0404 avg training loss: 7.7284
batch: [350/21305] batch time: 1.616 trainign loss: 8.0629 avg training loss: 7.7284
batch: [360/21305] batch time: 0.052 trainign loss: 7.5234 avg training loss: 7.7283
batch: [370/21305] batch time: 2.461 trainign loss: 6.1107 avg training loss: 7.7282
batch: [380/21305] batch time: 0.058 trainign loss: 6.5282 avg training loss: 7.7282
batch: [390/21305] batch time: 2.293 trainign loss: 4.2697 avg training loss: 7.7279
batch: [400/21305] batch time: 0.054 trainign loss: 6.7309 avg training loss: 7.7278
batch: [410/21305] batch time: 2.330 trainign loss: 8.1540 avg training loss: 7.7277
batch: [420/21305] batch time: 0.060 trainign loss: 7.7567 avg training loss: 7.7277
batch: [430/21305] batch time: 1.852 trainign loss: 7.4035 avg training loss: 7.7277
batch: [440/21305] batch time: 0.056 trainign loss: 6.3182 avg training loss: 7.7275
batch: [450/21305] batch time: 2.153 trainign loss: 5.3497 avg training loss: 7.7274
batch: [460/21305] batch time: 0.054 trainign loss: 7.3611 avg training loss: 7.7272
batch: [470/21305] batch time: 1.571 trainign loss: 6.7225 avg training loss: 7.7272
batch: [480/21305] batch time: 0.054 trainign loss: 7.7866 avg training loss: 7.7272
batch: [490/21305] batch time: 0.749 trainign loss: 7.1172 avg training loss: 7.7271
batch: [500/21305] batch time: 0.061 trainign loss: 4.8719 avg training loss: 7.7270
batch: [510/21305] batch time: 0.774 trainign loss: 6.0493 avg training loss: 7.7269
batch: [520/21305] batch time: 0.055 trainign loss: 4.4786 avg training loss: 7.7268
batch: [530/21305] batch time: 0.062 trainign loss: 6.3752 avg training loss: 7.7267
batch: [540/21305] batch time: 0.062 trainign loss: 6.8778 avg training loss: 7.7266
batch: [550/21305] batch time: 0.056 trainign loss: 4.7138 avg training loss: 7.7263
batch: [560/21305] batch time: 0.056 trainign loss: 6.4486 avg training loss: 7.7263
batch: [570/21305] batch time: 0.537 trainign loss: 6.3225 avg training loss: 7.7261
batch: [580/21305] batch time: 0.055 trainign loss: 6.9100 avg training loss: 7.7260
batch: [590/21305] batch time: 0.056 trainign loss: 7.2955 avg training loss: 7.7258
batch: [600/21305] batch time: 0.056 trainign loss: 6.9376 avg training loss: 7.7258
batch: [610/21305] batch time: 0.058 trainign loss: 7.8741 avg training loss: 7.7258
batch: [620/21305] batch time: 0.052 trainign loss: 7.3020 avg training loss: 7.7258
batch: [630/21305] batch time: 0.063 trainign loss: 6.4899 avg training loss: 7.7257
batch: [640/21305] batch time: 0.051 trainign loss: 7.2456 avg training loss: 7.7256
batch: [650/21305] batch time: 0.063 trainign loss: 6.2897 avg training loss: 7.7255
batch: [660/21305] batch time: 0.054 trainign loss: 5.9933 avg training loss: 7.7253
batch: [670/21305] batch time: 0.058 trainign loss: 8.7438 avg training loss: 7.7251
batch: [680/21305] batch time: 0.051 trainign loss: 8.3653 avg training loss: 7.7251
batch: [690/21305] batch time: 0.057 trainign loss: 6.7262 avg training loss: 7.7251
batch: [700/21305] batch time: 0.055 trainign loss: 7.8985 avg training loss: 7.7250
batch: [710/21305] batch time: 0.056 trainign loss: 6.8587 avg training loss: 7.7249
batch: [720/21305] batch time: 0.709 trainign loss: 6.9244 avg training loss: 7.7248
batch: [730/21305] batch time: 0.061 trainign loss: 7.7803 avg training loss: 7.7247
batch: [740/21305] batch time: 0.343 trainign loss: 7.6704 avg training loss: 7.7247
batch: [750/21305] batch time: 0.062 trainign loss: 6.9466 avg training loss: 7.7246
batch: [760/21305] batch time: 0.056 trainign loss: 6.2524 avg training loss: 7.7245
batch: [770/21305] batch time: 0.061 trainign loss: 5.3520 avg training loss: 7.7243
batch: [780/21305] batch time: 0.056 trainign loss: 7.4890 avg training loss: 7.7242
batch: [790/21305] batch time: 0.062 trainign loss: 7.2628 avg training loss: 7.7242
batch: [800/21305] batch time: 0.056 trainign loss: 7.1554 avg training loss: 7.7241
batch: [810/21305] batch time: 0.061 trainign loss: 6.5370 avg training loss: 7.7241
batch: [820/21305] batch time: 0.053 trainign loss: 6.8145 avg training loss: 7.7239
batch: [830/21305] batch time: 0.057 trainign loss: 6.4563 avg training loss: 7.7238
batch: [840/21305] batch time: 0.940 trainign loss: 6.7176 avg training loss: 7.7237
batch: [850/21305] batch time: 0.056 trainign loss: 5.8207 avg training loss: 7.7236
batch: [860/21305] batch time: 0.161 trainign loss: 5.6491 avg training loss: 7.7234
batch: [870/21305] batch time: 0.056 trainign loss: 5.3511 avg training loss: 7.7233
batch: [880/21305] batch time: 1.034 trainign loss: 7.1355 avg training loss: 7.7230
batch: [890/21305] batch time: 0.056 trainign loss: 8.5951 avg training loss: 7.7230
batch: [900/21305] batch time: 1.593 trainign loss: 6.5867 avg training loss: 7.7230
batch: [910/21305] batch time: 0.056 trainign loss: 6.9101 avg training loss: 7.7229
batch: [920/21305] batch time: 1.456 trainign loss: 4.5467 avg training loss: 7.7227
batch: [930/21305] batch time: 0.060 trainign loss: 6.8099 avg training loss: 7.7226
batch: [940/21305] batch time: 0.660 trainign loss: 4.0135 avg training loss: 7.7224
batch: [950/21305] batch time: 0.056 trainign loss: 0.0034 avg training loss: 7.7218
batch: [960/21305] batch time: 0.315 trainign loss: 8.3824 avg training loss: 7.7218
batch: [970/21305] batch time: 0.061 trainign loss: 8.0927 avg training loss: 7.7219
batch: [980/21305] batch time: 0.051 trainign loss: 7.3052 avg training loss: 7.7219
batch: [990/21305] batch time: 0.062 trainign loss: 6.9355 avg training loss: 7.7218
batch: [1000/21305] batch time: 0.055 trainign loss: 5.0102 avg training loss: 7.7217
batch: [1010/21305] batch time: 0.056 trainign loss: 7.1624 avg training loss: 7.7215
batch: [1020/21305] batch time: 0.058 trainign loss: 7.8760 avg training loss: 7.7214
batch: [1030/21305] batch time: 0.063 trainign loss: 6.9611 avg training loss: 7.7213
batch: [1040/21305] batch time: 0.055 trainign loss: 6.9654 avg training loss: 7.7213
batch: [1050/21305] batch time: 0.062 trainign loss: 6.4869 avg training loss: 7.7211
batch: [1060/21305] batch time: 0.051 trainign loss: 5.9468 avg training loss: 7.7210
batch: [1070/21305] batch time: 0.057 trainign loss: 5.1353 avg training loss: 7.7208
batch: [1080/21305] batch time: 0.056 trainign loss: 0.0794 avg training loss: 7.7204
batch: [1090/21305] batch time: 0.062 trainign loss: 7.9018 avg training loss: 7.7202
batch: [1100/21305] batch time: 0.058 trainign loss: 6.7091 avg training loss: 7.7201
batch: [1110/21305] batch time: 0.056 trainign loss: 7.0918 avg training loss: 7.7201
batch: [1120/21305] batch time: 0.051 trainign loss: 7.8412 avg training loss: 7.7201
batch: [1130/21305] batch time: 0.056 trainign loss: 6.2582 avg training loss: 7.7200
batch: [1140/21305] batch time: 0.052 trainign loss: 7.1753 avg training loss: 7.7199
batch: [1150/21305] batch time: 0.062 trainign loss: 7.0414 avg training loss: 7.7198
batch: [1160/21305] batch time: 0.056 trainign loss: 7.2206 avg training loss: 7.7197
batch: [1170/21305] batch time: 0.061 trainign loss: 6.8092 avg training loss: 7.7196
batch: [1180/21305] batch time: 0.056 trainign loss: 6.6308 avg training loss: 7.7195
batch: [1190/21305] batch time: 0.057 trainign loss: 7.2915 avg training loss: 7.7194
batch: [1200/21305] batch time: 0.056 trainign loss: 5.4014 avg training loss: 7.7193
batch: [1210/21305] batch time: 0.059 trainign loss: 7.3527 avg training loss: 7.7192
batch: [1220/21305] batch time: 0.055 trainign loss: 7.8224 avg training loss: 7.7191
batch: [1230/21305] batch time: 0.056 trainign loss: 6.4398 avg training loss: 7.7190
batch: [1240/21305] batch time: 0.050 trainign loss: 6.6543 avg training loss: 7.7189
batch: [1250/21305] batch time: 0.062 trainign loss: 7.2429 avg training loss: 7.7188
batch: [1260/21305] batch time: 0.050 trainign loss: 5.9816 avg training loss: 7.7187
batch: [1270/21305] batch time: 0.062 trainign loss: 6.9412 avg training loss: 7.7186
batch: [1280/21305] batch time: 0.063 trainign loss: 7.2515 avg training loss: 7.7185
batch: [1290/21305] batch time: 0.211 trainign loss: 6.3687 avg training loss: 7.7184
batch: [1300/21305] batch time: 0.051 trainign loss: 6.5779 avg training loss: 7.7183
batch: [1310/21305] batch time: 0.320 trainign loss: 7.1446 avg training loss: 7.7182
batch: [1320/21305] batch time: 0.062 trainign loss: 5.7100 avg training loss: 7.7181
batch: [1330/21305] batch time: 0.062 trainign loss: 7.2126 avg training loss: 7.7180
batch: [1340/21305] batch time: 0.052 trainign loss: 7.2510 avg training loss: 7.7179
batch: [1350/21305] batch time: 0.056 trainign loss: 7.2581 avg training loss: 7.7179
batch: [1360/21305] batch time: 0.056 trainign loss: 4.8324 avg training loss: 7.7177
batch: [1370/21305] batch time: 0.055 trainign loss: 7.6256 avg training loss: 7.7175
batch: [1380/21305] batch time: 0.062 trainign loss: 5.8343 avg training loss: 7.7174
batch: [1390/21305] batch time: 0.062 trainign loss: 6.9593 avg training loss: 7.7173
batch: [1400/21305] batch time: 0.051 trainign loss: 6.3371 avg training loss: 7.7172
batch: [1410/21305] batch time: 0.056 trainign loss: 7.2912 avg training loss: 7.7170
batch: [1420/21305] batch time: 0.051 trainign loss: 6.6269 avg training loss: 7.7168
batch: [1430/21305] batch time: 0.056 trainign loss: 6.5283 avg training loss: 7.7167
batch: [1440/21305] batch time: 0.051 trainign loss: 5.9467 avg training loss: 7.7167
batch: [1450/21305] batch time: 0.056 trainign loss: 6.8041 avg training loss: 7.7165
batch: [1460/21305] batch time: 0.054 trainign loss: 6.5857 avg training loss: 7.7165
batch: [1470/21305] batch time: 0.056 trainign loss: 5.9192 avg training loss: 7.7163
batch: [1480/21305] batch time: 0.056 trainign loss: 6.9397 avg training loss: 7.7162
batch: [1490/21305] batch time: 0.056 trainign loss: 6.9901 avg training loss: 7.7161
batch: [1500/21305] batch time: 0.057 trainign loss: 6.7619 avg training loss: 7.7160
batch: [1510/21305] batch time: 0.057 trainign loss: 6.8527 avg training loss: 7.7158
batch: [1520/21305] batch time: 0.050 trainign loss: 2.6335 avg training loss: 7.7156
batch: [1530/21305] batch time: 0.056 trainign loss: 11.7043 avg training loss: 7.7151
batch: [1540/21305] batch time: 0.052 trainign loss: 7.6376 avg training loss: 7.7152
batch: [1550/21305] batch time: 0.057 trainign loss: 5.3163 avg training loss: 7.7151
batch: [1560/21305] batch time: 0.054 trainign loss: 6.7886 avg training loss: 7.7151
batch: [1570/21305] batch time: 0.056 trainign loss: 5.8554 avg training loss: 7.7150
batch: [1580/21305] batch time: 0.056 trainign loss: 6.1749 avg training loss: 7.7148
batch: [1590/21305] batch time: 0.062 trainign loss: 8.5734 avg training loss: 7.7144
batch: [1600/21305] batch time: 0.056 trainign loss: 7.6766 avg training loss: 7.7145
batch: [1610/21305] batch time: 0.055 trainign loss: 7.2761 avg training loss: 7.7145
batch: [1620/21305] batch time: 0.056 trainign loss: 6.4970 avg training loss: 7.7144
batch: [1630/21305] batch time: 0.136 trainign loss: 7.8084 avg training loss: 7.7143
batch: [1640/21305] batch time: 0.055 trainign loss: 6.9650 avg training loss: 7.7142
batch: [1650/21305] batch time: 0.063 trainign loss: 6.0167 avg training loss: 7.7141
batch: [1660/21305] batch time: 0.056 trainign loss: 6.2824 avg training loss: 7.7140
batch: [1670/21305] batch time: 0.056 trainign loss: 6.7185 avg training loss: 7.7139
batch: [1680/21305] batch time: 0.056 trainign loss: 5.8437 avg training loss: 7.7138
batch: [1690/21305] batch time: 0.052 trainign loss: 6.7072 avg training loss: 7.7137
batch: [1700/21305] batch time: 0.058 trainign loss: 6.5409 avg training loss: 7.7136
batch: [1710/21305] batch time: 0.050 trainign loss: 5.0370 avg training loss: 7.7135
batch: [1720/21305] batch time: 0.056 trainign loss: 7.4051 avg training loss: 7.7133
batch: [1730/21305] batch time: 0.051 trainign loss: 6.5959 avg training loss: 7.7132
batch: [1740/21305] batch time: 0.057 trainign loss: 6.0816 avg training loss: 7.7131
batch: [1750/21305] batch time: 0.054 trainign loss: 6.6989 avg training loss: 7.7130
batch: [1760/21305] batch time: 0.056 trainign loss: 7.1294 avg training loss: 7.7130
batch: [1770/21305] batch time: 0.062 trainign loss: 6.0891 avg training loss: 7.7128
batch: [1780/21305] batch time: 0.061 trainign loss: 6.2509 avg training loss: 7.7127
batch: [1790/21305] batch time: 0.056 trainign loss: 6.6397 avg training loss: 7.7125
batch: [1800/21305] batch time: 0.061 trainign loss: 6.2442 avg training loss: 7.7125
batch: [1810/21305] batch time: 0.061 trainign loss: 6.5504 avg training loss: 7.7124
batch: [1820/21305] batch time: 0.057 trainign loss: 4.5374 avg training loss: 7.7122
batch: [1830/21305] batch time: 0.056 trainign loss: 7.6233 avg training loss: 7.7121
batch: [1840/21305] batch time: 0.056 trainign loss: 7.0024 avg training loss: 7.7120
batch: [1850/21305] batch time: 0.051 trainign loss: 6.6024 avg training loss: 7.7119
batch: [1860/21305] batch time: 0.055 trainign loss: 6.1123 avg training loss: 7.7118
batch: [1870/21305] batch time: 0.056 trainign loss: 6.4268 avg training loss: 7.7117
batch: [1880/21305] batch time: 0.056 trainign loss: 4.8478 avg training loss: 7.7116
batch: [1890/21305] batch time: 0.056 trainign loss: 4.4075 avg training loss: 7.7112
batch: [1900/21305] batch time: 0.056 trainign loss: 7.4471 avg training loss: 7.7112
batch: [1910/21305] batch time: 0.060 trainign loss: 7.0386 avg training loss: 7.7111
batch: [1920/21305] batch time: 0.056 trainign loss: 6.3300 avg training loss: 7.7110
batch: [1930/21305] batch time: 0.061 trainign loss: 3.0770 avg training loss: 7.7108
batch: [1940/21305] batch time: 0.058 trainign loss: 6.5994 avg training loss: 7.7107
batch: [1950/21305] batch time: 0.055 trainign loss: 6.5639 avg training loss: 7.7106
batch: [1960/21305] batch time: 0.057 trainign loss: 5.0649 avg training loss: 7.7105
batch: [1970/21305] batch time: 0.052 trainign loss: 6.9823 avg training loss: 7.7104
batch: [1980/21305] batch time: 0.056 trainign loss: 4.8633 avg training loss: 7.7102
batch: [1990/21305] batch time: 0.060 trainign loss: 6.9029 avg training loss: 7.7102
batch: [2000/21305] batch time: 0.061 trainign loss: 5.4681 avg training loss: 7.7100
batch: [2010/21305] batch time: 0.056 trainign loss: 6.9229 avg training loss: 7.7099
batch: [2020/21305] batch time: 0.062 trainign loss: 3.6029 avg training loss: 7.7097
batch: [2030/21305] batch time: 0.052 trainign loss: 6.0042 avg training loss: 7.7095
batch: [2040/21305] batch time: 0.057 trainign loss: 6.9290 avg training loss: 7.7094
batch: [2050/21305] batch time: 0.051 trainign loss: 5.8753 avg training loss: 7.7094
batch: [2060/21305] batch time: 0.057 trainign loss: 2.7268 avg training loss: 7.7091
batch: [2070/21305] batch time: 0.060 trainign loss: 1.3710 avg training loss: 7.7088
batch: [2080/21305] batch time: 0.058 trainign loss: 0.0379 avg training loss: 7.7083
batch: [2090/21305] batch time: 0.165 trainign loss: 7.9740 avg training loss: 7.7080
batch: [2100/21305] batch time: 0.062 trainign loss: 8.1700 avg training loss: 7.7081
batch: [2110/21305] batch time: 0.056 trainign loss: 6.2797 avg training loss: 7.7081
batch: [2120/21305] batch time: 0.060 trainign loss: 0.3626 avg training loss: 7.7077
batch: [2130/21305] batch time: 0.051 trainign loss: 0.0004 avg training loss: 7.7070
batch: [2140/21305] batch time: 0.062 trainign loss: 0.0001 avg training loss: 7.7062
batch: [2150/21305] batch time: 0.058 trainign loss: 7.8444 avg training loss: 7.7065
batch: [2160/21305] batch time: 0.056 trainign loss: 7.8552 avg training loss: 7.7065
batch: [2170/21305] batch time: 0.052 trainign loss: 7.7710 avg training loss: 7.7065
batch: [2180/21305] batch time: 0.063 trainign loss: 7.3878 avg training loss: 7.7064
batch: [2190/21305] batch time: 0.056 trainign loss: 6.3685 avg training loss: 7.7063
batch: [2200/21305] batch time: 1.092 trainign loss: 6.7732 avg training loss: 7.7062
batch: [2210/21305] batch time: 0.057 trainign loss: 6.0666 avg training loss: 7.7061
batch: [2220/21305] batch time: 1.235 trainign loss: 7.0248 avg training loss: 7.7061
batch: [2230/21305] batch time: 0.053 trainign loss: 6.5124 avg training loss: 7.7059
batch: [2240/21305] batch time: 0.309 trainign loss: 6.6404 avg training loss: 7.7057
batch: [2250/21305] batch time: 0.058 trainign loss: 6.4232 avg training loss: 7.7056
batch: [2260/21305] batch time: 0.056 trainign loss: 5.9409 avg training loss: 7.7055
batch: [2270/21305] batch time: 0.056 trainign loss: 4.2128 avg training loss: 7.7052
batch: [2280/21305] batch time: 0.056 trainign loss: 6.4885 avg training loss: 7.7049
batch: [2290/21305] batch time: 0.052 trainign loss: 3.7670 avg training loss: 7.7047
batch: [2300/21305] batch time: 0.061 trainign loss: 5.9928 avg training loss: 7.7047
batch: [2310/21305] batch time: 0.057 trainign loss: 7.0783 avg training loss: 7.7045
batch: [2320/21305] batch time: 0.057 trainign loss: 7.9513 avg training loss: 7.7045
batch: [2330/21305] batch time: 0.056 trainign loss: 7.4454 avg training loss: 7.7044
batch: [2340/21305] batch time: 0.056 trainign loss: 5.3482 avg training loss: 7.7043
batch: [2350/21305] batch time: 0.062 trainign loss: 5.5275 avg training loss: 7.7042
batch: [2360/21305] batch time: 0.056 trainign loss: 7.4925 avg training loss: 7.7041
batch: [2370/21305] batch time: 0.052 trainign loss: 5.2215 avg training loss: 7.7040
batch: [2380/21305] batch time: 0.057 trainign loss: 6.5760 avg training loss: 7.7039
batch: [2390/21305] batch time: 0.056 trainign loss: 6.8993 avg training loss: 7.7038
batch: [2400/21305] batch time: 0.058 trainign loss: 5.7616 avg training loss: 7.7037
batch: [2410/21305] batch time: 0.057 trainign loss: 7.6770 avg training loss: 7.7036
batch: [2420/21305] batch time: 0.062 trainign loss: 6.7973 avg training loss: 7.7035
batch: [2430/21305] batch time: 0.056 trainign loss: 5.1462 avg training loss: 7.7034
batch: [2440/21305] batch time: 0.055 trainign loss: 6.8036 avg training loss: 7.7033
batch: [2450/21305] batch time: 0.063 trainign loss: 6.9974 avg training loss: 7.7032
batch: [2460/21305] batch time: 0.460 trainign loss: 6.5952 avg training loss: 7.7031
batch: [2470/21305] batch time: 0.056 trainign loss: 6.6576 avg training loss: 7.7029
batch: [2480/21305] batch time: 0.419 trainign loss: 7.2997 avg training loss: 7.7028
batch: [2490/21305] batch time: 0.057 trainign loss: 6.8153 avg training loss: 7.7026
batch: [2500/21305] batch time: 1.235 trainign loss: 7.7285 avg training loss: 7.7026
batch: [2510/21305] batch time: 0.056 trainign loss: 5.8887 avg training loss: 7.7025
batch: [2520/21305] batch time: 0.057 trainign loss: 5.5642 avg training loss: 7.7024
batch: [2530/21305] batch time: 0.056 trainign loss: 5.3833 avg training loss: 7.7023
batch: [2540/21305] batch time: 0.974 trainign loss: 6.7035 avg training loss: 7.7021
batch: [2550/21305] batch time: 0.052 trainign loss: 5.9407 avg training loss: 7.7020
batch: [2560/21305] batch time: 0.303 trainign loss: 6.1330 avg training loss: 7.7019
batch: [2570/21305] batch time: 0.056 trainign loss: 6.9455 avg training loss: 7.7019
batch: [2580/21305] batch time: 0.056 trainign loss: 5.2977 avg training loss: 7.7017
batch: [2590/21305] batch time: 0.052 trainign loss: 6.1344 avg training loss: 7.7016
batch: [2600/21305] batch time: 0.936 trainign loss: 0.3433 avg training loss: 7.7012
batch: [2610/21305] batch time: 0.055 trainign loss: 7.7592 avg training loss: 7.7010
batch: [2620/21305] batch time: 1.245 trainign loss: 8.2015 avg training loss: 7.7011
batch: [2630/21305] batch time: 0.062 trainign loss: 6.8155 avg training loss: 7.7011
batch: [2640/21305] batch time: 1.039 trainign loss: 3.7331 avg training loss: 7.7009
batch: [2650/21305] batch time: 0.056 trainign loss: 7.5806 avg training loss: 7.7007
batch: [2660/21305] batch time: 0.904 trainign loss: 7.4494 avg training loss: 7.7006
batch: [2670/21305] batch time: 0.061 trainign loss: 6.6927 avg training loss: 7.7006
batch: [2680/21305] batch time: 1.660 trainign loss: 4.6692 avg training loss: 7.7004
batch: [2690/21305] batch time: 0.056 trainign loss: 7.4238 avg training loss: 7.7003
batch: [2700/21305] batch time: 1.816 trainign loss: 6.8737 avg training loss: 7.7002
batch: [2710/21305] batch time: 0.056 trainign loss: 5.2137 avg training loss: 7.7000
batch: [2720/21305] batch time: 2.118 trainign loss: 2.0416 avg training loss: 7.6997
batch: [2730/21305] batch time: 0.056 trainign loss: 7.7103 avg training loss: 7.6996
batch: [2740/21305] batch time: 2.453 trainign loss: 7.6608 avg training loss: 7.6996
batch: [2750/21305] batch time: 0.056 trainign loss: 7.0157 avg training loss: 7.6995
batch: [2760/21305] batch time: 2.208 trainign loss: 6.8542 avg training loss: 7.6994
batch: [2770/21305] batch time: 0.052 trainign loss: 7.1009 avg training loss: 7.6993
batch: [2780/21305] batch time: 2.212 trainign loss: 7.4464 avg training loss: 7.6992
batch: [2790/21305] batch time: 0.058 trainign loss: 6.8066 avg training loss: 7.6991
batch: [2800/21305] batch time: 2.301 trainign loss: 6.8943 avg training loss: 7.6990
batch: [2810/21305] batch time: 0.056 trainign loss: 6.1505 avg training loss: 7.6989
batch: [2820/21305] batch time: 2.026 trainign loss: 6.9838 avg training loss: 7.6987
batch: [2830/21305] batch time: 0.056 trainign loss: 7.4762 avg training loss: 7.6986
batch: [2840/21305] batch time: 2.276 trainign loss: 1.7423 avg training loss: 7.6984
batch: [2850/21305] batch time: 0.062 trainign loss: 5.0617 avg training loss: 7.6982
batch: [2860/21305] batch time: 2.286 trainign loss: 7.1127 avg training loss: 7.6981
batch: [2870/21305] batch time: 0.056 trainign loss: 7.9509 avg training loss: 7.6979
batch: [2880/21305] batch time: 2.578 trainign loss: 6.7073 avg training loss: 7.6978
batch: [2890/21305] batch time: 0.056 trainign loss: 6.8532 avg training loss: 7.6977
batch: [2900/21305] batch time: 2.152 trainign loss: 6.7450 avg training loss: 7.6976
batch: [2910/21305] batch time: 0.051 trainign loss: 7.2485 avg training loss: 7.6976
batch: [2920/21305] batch time: 2.415 trainign loss: 6.4860 avg training loss: 7.6975
batch: [2930/21305] batch time: 0.062 trainign loss: 5.4026 avg training loss: 7.6973
batch: [2940/21305] batch time: 2.621 trainign loss: 5.2646 avg training loss: 7.6972
batch: [2950/21305] batch time: 0.056 trainign loss: 0.5528 avg training loss: 7.6968
batch: [2960/21305] batch time: 2.374 trainign loss: 6.8396 avg training loss: 7.6967
batch: [2970/21305] batch time: 0.056 trainign loss: 6.0818 avg training loss: 7.6966
batch: [2980/21305] batch time: 2.234 trainign loss: 7.0494 avg training loss: 7.6966
batch: [2990/21305] batch time: 0.057 trainign loss: 6.6804 avg training loss: 7.6965
batch: [3000/21305] batch time: 2.535 trainign loss: 5.1045 avg training loss: 7.6964
batch: [3010/21305] batch time: 0.058 trainign loss: 7.9102 avg training loss: 7.6962
batch: [3020/21305] batch time: 2.377 trainign loss: 5.3202 avg training loss: 7.6961
batch: [3030/21305] batch time: 0.058 trainign loss: 5.9919 avg training loss: 7.6960
batch: [3040/21305] batch time: 2.311 trainign loss: 6.3512 avg training loss: 7.6959
batch: [3050/21305] batch time: 0.054 trainign loss: 7.0583 avg training loss: 7.6958
batch: [3060/21305] batch time: 2.242 trainign loss: 6.5342 avg training loss: 7.6957
batch: [3070/21305] batch time: 0.060 trainign loss: 6.6010 avg training loss: 7.6956
batch: [3080/21305] batch time: 2.100 trainign loss: 6.9510 avg training loss: 7.6955
batch: [3090/21305] batch time: 0.062 trainign loss: 6.6736 avg training loss: 7.6953
batch: [3100/21305] batch time: 2.309 trainign loss: 6.9452 avg training loss: 7.6952
batch: [3110/21305] batch time: 0.060 trainign loss: 7.5859 avg training loss: 7.6952
batch: [3120/21305] batch time: 2.283 trainign loss: 6.1185 avg training loss: 7.6950
batch: [3130/21305] batch time: 0.053 trainign loss: 0.7595 avg training loss: 7.6947
batch: [3140/21305] batch time: 2.095 trainign loss: 7.6950 avg training loss: 7.6946
batch: [3150/21305] batch time: 0.056 trainign loss: 7.0060 avg training loss: 7.6946
batch: [3160/21305] batch time: 2.182 trainign loss: 7.2844 avg training loss: 7.6945
batch: [3170/21305] batch time: 0.056 trainign loss: 6.6266 avg training loss: 7.6944
batch: [3180/21305] batch time: 2.334 trainign loss: 5.9901 avg training loss: 7.6944
batch: [3190/21305] batch time: 0.061 trainign loss: 6.5176 avg training loss: 7.6943
batch: [3200/21305] batch time: 2.481 trainign loss: 7.0390 avg training loss: 7.6941
batch: [3210/21305] batch time: 0.057 trainign loss: 7.0541 avg training loss: 7.6941
batch: [3220/21305] batch time: 2.079 trainign loss: 6.8602 avg training loss: 7.6939
batch: [3230/21305] batch time: 0.056 trainign loss: 6.9815 avg training loss: 7.6938
batch: [3240/21305] batch time: 2.600 trainign loss: 6.0209 avg training loss: 7.6938
batch: [3250/21305] batch time: 0.056 trainign loss: 6.4054 avg training loss: 7.6936
batch: [3260/21305] batch time: 2.449 trainign loss: 7.1227 avg training loss: 7.6935
batch: [3270/21305] batch time: 0.056 trainign loss: 5.7745 avg training loss: 7.6934
batch: [3280/21305] batch time: 2.371 trainign loss: 3.4366 avg training loss: 7.6932
batch: [3290/21305] batch time: 0.056 trainign loss: 5.9742 avg training loss: 7.6929
batch: [3300/21305] batch time: 2.630 trainign loss: 5.6737 avg training loss: 7.6928
batch: [3310/21305] batch time: 0.056 trainign loss: 0.8049 avg training loss: 7.6924
batch: [3320/21305] batch time: 2.243 trainign loss: 7.5662 avg training loss: 7.6923
batch: [3330/21305] batch time: 0.052 trainign loss: 6.5777 avg training loss: 7.6923
batch: [3340/21305] batch time: 2.181 trainign loss: 4.0679 avg training loss: 7.6922
batch: [3350/21305] batch time: 0.052 trainign loss: 5.2480 avg training loss: 7.6920
batch: [3360/21305] batch time: 2.114 trainign loss: 7.5184 avg training loss: 7.6918
batch: [3370/21305] batch time: 0.056 trainign loss: 3.7041 avg training loss: 7.6915
batch: [3380/21305] batch time: 2.165 trainign loss: 8.1719 avg training loss: 7.6915
batch: [3390/21305] batch time: 0.056 trainign loss: 7.0467 avg training loss: 7.6914
batch: [3400/21305] batch time: 2.468 trainign loss: 6.3727 avg training loss: 7.6913
batch: [3410/21305] batch time: 0.056 trainign loss: 6.1012 avg training loss: 7.6912
batch: [3420/21305] batch time: 2.220 trainign loss: 6.8329 avg training loss: 7.6910
batch: [3430/21305] batch time: 0.056 trainign loss: 6.6683 avg training loss: 7.6909
batch: [3440/21305] batch time: 2.158 trainign loss: 7.2160 avg training loss: 7.6908
batch: [3450/21305] batch time: 0.698 trainign loss: 5.7917 avg training loss: 7.6907
batch: [3460/21305] batch time: 1.677 trainign loss: 6.2213 avg training loss: 7.6905
batch: [3470/21305] batch time: 0.595 trainign loss: 7.4850 avg training loss: 7.6905
batch: [3480/21305] batch time: 2.110 trainign loss: 7.4399 avg training loss: 7.6904
batch: [3490/21305] batch time: 0.851 trainign loss: 6.8855 avg training loss: 7.6903
batch: [3500/21305] batch time: 2.126 trainign loss: 6.2968 avg training loss: 7.6902
batch: [3510/21305] batch time: 0.059 trainign loss: 6.8128 avg training loss: 7.6902
batch: [3520/21305] batch time: 1.835 trainign loss: 6.5310 avg training loss: 7.6901
batch: [3530/21305] batch time: 0.256 trainign loss: 6.8673 avg training loss: 7.6900
batch: [3540/21305] batch time: 1.646 trainign loss: 5.4964 avg training loss: 7.6898
batch: [3550/21305] batch time: 0.523 trainign loss: 6.9327 avg training loss: 7.6896
batch: [3560/21305] batch time: 1.522 trainign loss: 6.9951 avg training loss: 7.6894
batch: [3570/21305] batch time: 1.136 trainign loss: 7.9075 avg training loss: 7.6894
batch: [3580/21305] batch time: 0.977 trainign loss: 6.5006 avg training loss: 7.6893
batch: [3590/21305] batch time: 1.161 trainign loss: 5.1523 avg training loss: 7.6892
batch: [3600/21305] batch time: 1.050 trainign loss: 7.9710 avg training loss: 7.6891
batch: [3610/21305] batch time: 1.032 trainign loss: 8.0159 avg training loss: 7.6890
batch: [3620/21305] batch time: 1.704 trainign loss: 7.4996 avg training loss: 7.6890
batch: [3630/21305] batch time: 0.379 trainign loss: 6.9652 avg training loss: 7.6889
batch: [3640/21305] batch time: 1.797 trainign loss: 6.9016 avg training loss: 7.6888
batch: [3650/21305] batch time: 0.676 trainign loss: 6.1658 avg training loss: 7.6887
batch: [3660/21305] batch time: 1.500 trainign loss: 6.0064 avg training loss: 7.6886
batch: [3670/21305] batch time: 0.357 trainign loss: 5.4720 avg training loss: 7.6885
batch: [3680/21305] batch time: 1.940 trainign loss: 7.3614 avg training loss: 7.6884
batch: [3690/21305] batch time: 0.881 trainign loss: 6.6930 avg training loss: 7.6883
batch: [3700/21305] batch time: 2.184 trainign loss: 5.8939 avg training loss: 7.6882
batch: [3710/21305] batch time: 0.690 trainign loss: 5.2768 avg training loss: 7.6881
batch: [3720/21305] batch time: 1.828 trainign loss: 6.5497 avg training loss: 7.6879
batch: [3730/21305] batch time: 0.474 trainign loss: 6.0489 avg training loss: 7.6879
batch: [3740/21305] batch time: 2.316 trainign loss: 7.4979 avg training loss: 7.6878
batch: [3750/21305] batch time: 0.056 trainign loss: 7.0408 avg training loss: 7.6877
batch: [3760/21305] batch time: 1.525 trainign loss: 6.5164 avg training loss: 7.6875
batch: [3770/21305] batch time: 1.279 trainign loss: 4.7294 avg training loss: 7.6874
batch: [3780/21305] batch time: 0.647 trainign loss: 7.3516 avg training loss: 7.6873
batch: [3790/21305] batch time: 1.487 trainign loss: 4.8361 avg training loss: 7.6872
batch: [3800/21305] batch time: 0.230 trainign loss: 6.2540 avg training loss: 7.6871
batch: [3810/21305] batch time: 1.374 trainign loss: 7.1522 avg training loss: 7.6870
batch: [3820/21305] batch time: 1.000 trainign loss: 4.8355 avg training loss: 7.6869
batch: [3830/21305] batch time: 1.219 trainign loss: 6.5129 avg training loss: 7.6868
batch: [3840/21305] batch time: 0.940 trainign loss: 6.5386 avg training loss: 7.6867
batch: [3850/21305] batch time: 0.992 trainign loss: 6.2018 avg training loss: 7.6866
batch: [3860/21305] batch time: 1.109 trainign loss: 6.5764 avg training loss: 7.6865
batch: [3870/21305] batch time: 0.324 trainign loss: 5.3124 avg training loss: 7.6864
batch: [3880/21305] batch time: 1.669 trainign loss: 0.1700 avg training loss: 7.6859
batch: [3890/21305] batch time: 0.187 trainign loss: 8.6101 avg training loss: 7.6857
batch: [3900/21305] batch time: 1.895 trainign loss: 8.0325 avg training loss: 7.6857
batch: [3910/21305] batch time: 1.176 trainign loss: 7.3730 avg training loss: 7.6857
batch: [3920/21305] batch time: 1.398 trainign loss: 5.2281 avg training loss: 7.6856
batch: [3930/21305] batch time: 0.848 trainign loss: 6.9395 avg training loss: 7.6855
batch: [3940/21305] batch time: 2.023 trainign loss: 6.0877 avg training loss: 7.6854
batch: [3950/21305] batch time: 0.476 trainign loss: 6.4445 avg training loss: 7.6853
batch: [3960/21305] batch time: 1.854 trainign loss: 7.3022 avg training loss: 7.6852
batch: [3970/21305] batch time: 0.721 trainign loss: 6.9996 avg training loss: 7.6851
batch: [3980/21305] batch time: 1.688 trainign loss: 5.7249 avg training loss: 7.6850
batch: [3990/21305] batch time: 0.469 trainign loss: 5.3691 avg training loss: 7.6848
batch: [4000/21305] batch time: 2.363 trainign loss: 7.4415 avg training loss: 7.6846
batch: [4010/21305] batch time: 0.057 trainign loss: 6.7299 avg training loss: 7.6846
batch: [4020/21305] batch time: 2.429 trainign loss: 4.4128 avg training loss: 7.6844
batch: [4030/21305] batch time: 0.052 trainign loss: 6.9789 avg training loss: 7.6843
batch: [4040/21305] batch time: 2.308 trainign loss: 6.6871 avg training loss: 7.6842
batch: [4050/21305] batch time: 0.055 trainign loss: 4.3120 avg training loss: 7.6840
batch: [4060/21305] batch time: 2.352 trainign loss: 10.2535 avg training loss: 7.6835
batch: [4070/21305] batch time: 0.056 trainign loss: 8.6170 avg training loss: 7.6834
batch: [4080/21305] batch time: 2.358 trainign loss: 8.0788 avg training loss: 7.6834
batch: [4090/21305] batch time: 0.058 trainign loss: 7.9005 avg training loss: 7.6833
batch: [4100/21305] batch time: 2.366 trainign loss: 6.4330 avg training loss: 7.6833
batch: [4110/21305] batch time: 0.056 trainign loss: 5.6542 avg training loss: 7.6832
batch: [4120/21305] batch time: 2.110 trainign loss: 5.4156 avg training loss: 7.6830
batch: [4130/21305] batch time: 0.061 trainign loss: 6.9367 avg training loss: 7.6829
batch: [4140/21305] batch time: 2.486 trainign loss: 5.5379 avg training loss: 7.6827
batch: [4150/21305] batch time: 0.062 trainign loss: 7.9986 avg training loss: 7.6827
batch: [4160/21305] batch time: 2.418 trainign loss: 7.2094 avg training loss: 7.6826
batch: [4170/21305] batch time: 0.061 trainign loss: 5.7497 avg training loss: 7.6825
batch: [4180/21305] batch time: 2.420 trainign loss: 7.2823 avg training loss: 7.6823
batch: [4190/21305] batch time: 0.056 trainign loss: 7.1109 avg training loss: 7.6822
batch: [4200/21305] batch time: 2.001 trainign loss: 6.2442 avg training loss: 7.6821
batch: [4210/21305] batch time: 0.057 trainign loss: 2.2221 avg training loss: 7.6819
batch: [4220/21305] batch time: 2.516 trainign loss: 6.7953 avg training loss: 7.6817
batch: [4230/21305] batch time: 0.056 trainign loss: 6.6302 avg training loss: 7.6816
batch: [4240/21305] batch time: 2.619 trainign loss: 5.2611 avg training loss: 7.6814
batch: [4250/21305] batch time: 0.056 trainign loss: 0.4970 avg training loss: 7.6809
batch: [4260/21305] batch time: 2.699 trainign loss: 7.9113 avg training loss: 7.6810
batch: [4270/21305] batch time: 0.056 trainign loss: 8.6027 avg training loss: 7.6810
batch: [4280/21305] batch time: 2.296 trainign loss: 7.5053 avg training loss: 7.6810
batch: [4290/21305] batch time: 0.056 trainign loss: 6.9900 avg training loss: 7.6809
batch: [4300/21305] batch time: 2.123 trainign loss: 6.5077 avg training loss: 7.6809
batch: [4310/21305] batch time: 0.056 trainign loss: 6.9337 avg training loss: 7.6808
batch: [4320/21305] batch time: 2.390 trainign loss: 5.9071 avg training loss: 7.6806
batch: [4330/21305] batch time: 0.056 trainign loss: 4.7263 avg training loss: 7.6805
batch: [4340/21305] batch time: 2.430 trainign loss: 5.3896 avg training loss: 7.6804
batch: [4350/21305] batch time: 0.056 trainign loss: 6.1081 avg training loss: 7.6802
batch: [4360/21305] batch time: 2.522 trainign loss: 5.2666 avg training loss: 7.6801
batch: [4370/21305] batch time: 0.053 trainign loss: 7.3120 avg training loss: 7.6799
batch: [4380/21305] batch time: 2.685 trainign loss: 6.5030 avg training loss: 7.6798
batch: [4390/21305] batch time: 0.056 trainign loss: 6.3849 avg training loss: 7.6796
batch: [4400/21305] batch time: 2.141 trainign loss: 7.4058 avg training loss: 7.6796
batch: [4410/21305] batch time: 0.056 trainign loss: 6.7218 avg training loss: 7.6795
batch: [4420/21305] batch time: 2.301 trainign loss: 5.6085 avg training loss: 7.6794
batch: [4430/21305] batch time: 0.056 trainign loss: 1.6557 avg training loss: 7.6791
batch: [4440/21305] batch time: 2.196 trainign loss: 9.1563 avg training loss: 7.6788
batch: [4450/21305] batch time: 0.055 trainign loss: 7.4855 avg training loss: 7.6787
batch: [4460/21305] batch time: 2.149 trainign loss: 3.7133 avg training loss: 7.6785
batch: [4470/21305] batch time: 0.053 trainign loss: 9.5174 avg training loss: 7.6783
batch: [4480/21305] batch time: 2.098 trainign loss: 7.2732 avg training loss: 7.6783
batch: [4490/21305] batch time: 0.076 trainign loss: 4.3224 avg training loss: 7.6781
batch: [4500/21305] batch time: 2.160 trainign loss: 7.8041 avg training loss: 7.6780
batch: [4510/21305] batch time: 0.056 trainign loss: 7.3978 avg training loss: 7.6778
batch: [4520/21305] batch time: 1.548 trainign loss: 7.9931 avg training loss: 7.6778
batch: [4530/21305] batch time: 0.058 trainign loss: 7.4875 avg training loss: 7.6778
batch: [4540/21305] batch time: 1.785 trainign loss: 6.8012 avg training loss: 7.6777
batch: [4550/21305] batch time: 0.057 trainign loss: 6.3294 avg training loss: 7.6776
batch: [4560/21305] batch time: 1.540 trainign loss: 7.2468 avg training loss: 7.6775
batch: [4570/21305] batch time: 0.056 trainign loss: 7.0340 avg training loss: 7.6774
batch: [4580/21305] batch time: 0.882 trainign loss: 7.2213 avg training loss: 7.6773
batch: [4590/21305] batch time: 0.062 trainign loss: 5.4251 avg training loss: 7.6772
batch: [4600/21305] batch time: 0.322 trainign loss: 6.4538 avg training loss: 7.6771
batch: [4610/21305] batch time: 0.056 trainign loss: 6.0384 avg training loss: 7.6770
batch: [4620/21305] batch time: 0.591 trainign loss: 6.5234 avg training loss: 7.6768
batch: [4630/21305] batch time: 0.060 trainign loss: 6.1915 avg training loss: 7.6767
batch: [4640/21305] batch time: 0.911 trainign loss: 5.6403 avg training loss: 7.6766
batch: [4650/21305] batch time: 0.062 trainign loss: 6.5908 avg training loss: 7.6765
batch: [4660/21305] batch time: 1.172 trainign loss: 6.9506 avg training loss: 7.6764
batch: [4670/21305] batch time: 0.057 trainign loss: 6.9650 avg training loss: 7.6762
batch: [4680/21305] batch time: 1.163 trainign loss: 7.6919 avg training loss: 7.6762
batch: [4690/21305] batch time: 0.056 trainign loss: 5.4891 avg training loss: 7.6761
batch: [4700/21305] batch time: 2.286 trainign loss: 6.5221 avg training loss: 7.6759
batch: [4710/21305] batch time: 0.062 trainign loss: 7.6296 avg training loss: 7.6758
batch: [4720/21305] batch time: 2.325 trainign loss: 6.7399 avg training loss: 7.6758
batch: [4730/21305] batch time: 0.056 trainign loss: 6.3873 avg training loss: 7.6757
batch: [4740/21305] batch time: 2.151 trainign loss: 7.6553 avg training loss: 7.6755
batch: [4750/21305] batch time: 0.149 trainign loss: 6.9568 avg training loss: 7.6754
batch: [4760/21305] batch time: 2.059 trainign loss: 4.5054 avg training loss: 7.6753
batch: [4770/21305] batch time: 0.221 trainign loss: 6.3703 avg training loss: 7.6751
batch: [4780/21305] batch time: 2.121 trainign loss: 4.0803 avg training loss: 7.6750
batch: [4790/21305] batch time: 0.056 trainign loss: 6.2518 avg training loss: 7.6748
batch: [4800/21305] batch time: 1.902 trainign loss: 7.3230 avg training loss: 7.6747
batch: [4810/21305] batch time: 0.056 trainign loss: 5.5479 avg training loss: 7.6746
batch: [4820/21305] batch time: 1.804 trainign loss: 6.8434 avg training loss: 7.6745
batch: [4830/21305] batch time: 0.056 trainign loss: 5.8894 avg training loss: 7.6744
batch: [4840/21305] batch time: 2.776 trainign loss: 1.2224 avg training loss: 7.6741
batch: [4850/21305] batch time: 0.056 trainign loss: 10.0208 avg training loss: 7.6738
batch: [4860/21305] batch time: 2.166 trainign loss: 6.3938 avg training loss: 7.6738
batch: [4870/21305] batch time: 0.056 trainign loss: 8.4420 avg training loss: 7.6738
batch: [4880/21305] batch time: 2.244 trainign loss: 7.4056 avg training loss: 7.6737
batch: [4890/21305] batch time: 0.056 trainign loss: 6.6350 avg training loss: 7.6736
batch: [4900/21305] batch time: 2.362 trainign loss: 6.1090 avg training loss: 7.6735
batch: [4910/21305] batch time: 0.063 trainign loss: 4.6750 avg training loss: 7.6732
batch: [4920/21305] batch time: 2.083 trainign loss: 8.0462 avg training loss: 7.6732
batch: [4930/21305] batch time: 0.056 trainign loss: 5.9240 avg training loss: 7.6731
batch: [4940/21305] batch time: 2.314 trainign loss: 7.0809 avg training loss: 7.6730
batch: [4950/21305] batch time: 0.053 trainign loss: 5.3703 avg training loss: 7.6728
batch: [4960/21305] batch time: 2.136 trainign loss: 6.9148 avg training loss: 7.6727
batch: [4970/21305] batch time: 0.056 trainign loss: 6.2240 avg training loss: 7.6725
batch: [4980/21305] batch time: 2.499 trainign loss: 4.8282 avg training loss: 7.6724
batch: [4990/21305] batch time: 0.062 trainign loss: 0.0109 avg training loss: 7.6718
batch: [5000/21305] batch time: 2.190 trainign loss: 7.1072 avg training loss: 7.6718
batch: [5010/21305] batch time: 0.056 trainign loss: 8.1089 avg training loss: 7.6718
batch: [5020/21305] batch time: 2.358 trainign loss: 6.7426 avg training loss: 7.6717
batch: [5030/21305] batch time: 0.057 trainign loss: 6.1770 avg training loss: 7.6716
batch: [5040/21305] batch time: 2.193 trainign loss: 3.6330 avg training loss: 7.6714
batch: [5050/21305] batch time: 0.056 trainign loss: 6.5288 avg training loss: 7.6712
batch: [5060/21305] batch time: 2.414 trainign loss: 7.6668 avg training loss: 7.6711
batch: [5070/21305] batch time: 0.056 trainign loss: 4.9999 avg training loss: 7.6710
batch: [5080/21305] batch time: 2.486 trainign loss: 0.7637 avg training loss: 7.6704
batch: [5090/21305] batch time: 0.061 trainign loss: 7.0749 avg training loss: 7.6705
batch: [5100/21305] batch time: 2.579 trainign loss: 7.4505 avg training loss: 7.6705
batch: [5110/21305] batch time: 0.056 trainign loss: 6.5365 avg training loss: 7.6704
batch: [5120/21305] batch time: 2.329 trainign loss: 6.6653 avg training loss: 7.6703
batch: [5130/21305] batch time: 0.061 trainign loss: 6.6696 avg training loss: 7.6702
batch: [5140/21305] batch time: 2.535 trainign loss: 6.7983 avg training loss: 7.6700
batch: [5150/21305] batch time: 0.062 trainign loss: 7.5387 avg training loss: 7.6699
batch: [5160/21305] batch time: 2.058 trainign loss: 7.5819 avg training loss: 7.6699
batch: [5170/21305] batch time: 0.061 trainign loss: 7.1254 avg training loss: 7.6698
batch: [5180/21305] batch time: 2.094 trainign loss: 6.4640 avg training loss: 7.6697
batch: [5190/21305] batch time: 0.056 trainign loss: 6.5608 avg training loss: 7.6696
batch: [5200/21305] batch time: 2.440 trainign loss: 5.9832 avg training loss: 7.6695
batch: [5210/21305] batch time: 0.056 trainign loss: 6.0570 avg training loss: 7.6695
batch: [5220/21305] batch time: 1.860 trainign loss: 6.4631 avg training loss: 7.6693
batch: [5230/21305] batch time: 0.056 trainign loss: 6.3830 avg training loss: 7.6692
batch: [5240/21305] batch time: 1.652 trainign loss: 5.1440 avg training loss: 7.6691
batch: [5250/21305] batch time: 0.056 trainign loss: 5.8350 avg training loss: 7.6689
batch: [5260/21305] batch time: 0.543 trainign loss: 7.2650 avg training loss: 7.6689
batch: [5270/21305] batch time: 0.056 trainign loss: 7.3568 avg training loss: 7.6689
batch: [5280/21305] batch time: 0.937 trainign loss: 7.0997 avg training loss: 7.6688
batch: [5290/21305] batch time: 0.056 trainign loss: 5.6130 avg training loss: 7.6687
batch: [5300/21305] batch time: 0.966 trainign loss: 6.6466 avg training loss: 7.6685
batch: [5310/21305] batch time: 0.057 trainign loss: 6.9759 avg training loss: 7.6684
batch: [5320/21305] batch time: 0.456 trainign loss: 7.0869 avg training loss: 7.6684
batch: [5330/21305] batch time: 0.057 trainign loss: 6.5684 avg training loss: 7.6683
batch: [5340/21305] batch time: 1.017 trainign loss: 6.2065 avg training loss: 7.6681
batch: [5350/21305] batch time: 0.062 trainign loss: 5.7231 avg training loss: 7.6679
batch: [5360/21305] batch time: 1.721 trainign loss: 8.0182 avg training loss: 7.6678
batch: [5370/21305] batch time: 0.057 trainign loss: 7.4480 avg training loss: 7.6677
batch: [5380/21305] batch time: 1.609 trainign loss: 7.2126 avg training loss: 7.6676
batch: [5390/21305] batch time: 0.061 trainign loss: 4.9960 avg training loss: 7.6675
batch: [5400/21305] batch time: 2.198 trainign loss: 7.0851 avg training loss: 7.6674
batch: [5410/21305] batch time: 0.057 trainign loss: 6.7661 avg training loss: 7.6674
batch: [5420/21305] batch time: 1.649 trainign loss: 7.4921 avg training loss: 7.6672
batch: [5430/21305] batch time: 0.052 trainign loss: 6.3875 avg training loss: 7.6671
batch: [5440/21305] batch time: 1.798 trainign loss: 6.4316 avg training loss: 7.6670
batch: [5450/21305] batch time: 0.061 trainign loss: 5.2376 avg training loss: 7.6668
batch: [5460/21305] batch time: 1.172 trainign loss: 7.5358 avg training loss: 7.6667
batch: [5470/21305] batch time: 0.308 trainign loss: 5.7984 avg training loss: 7.6666
batch: [5480/21305] batch time: 1.608 trainign loss: 7.4804 avg training loss: 7.6665
batch: [5490/21305] batch time: 0.062 trainign loss: 6.0941 avg training loss: 7.6664
batch: [5500/21305] batch time: 1.179 trainign loss: 5.6749 avg training loss: 7.6663
batch: [5510/21305] batch time: 0.058 trainign loss: 1.9827 avg training loss: 7.6660
batch: [5520/21305] batch time: 1.391 trainign loss: 7.6191 avg training loss: 7.6659
batch: [5530/21305] batch time: 1.156 trainign loss: 7.0597 avg training loss: 7.6658
batch: [5540/21305] batch time: 1.659 trainign loss: 6.7391 avg training loss: 7.6657
batch: [5550/21305] batch time: 1.542 trainign loss: 7.7895 avg training loss: 7.6657
batch: [5560/21305] batch time: 0.384 trainign loss: 5.1867 avg training loss: 7.6655
batch: [5570/21305] batch time: 2.326 trainign loss: 2.8415 avg training loss: 7.6653
batch: [5580/21305] batch time: 0.061 trainign loss: 8.7473 avg training loss: 7.6651
batch: [5590/21305] batch time: 2.357 trainign loss: 7.3910 avg training loss: 7.6651
batch: [5600/21305] batch time: 0.063 trainign loss: 6.9263 avg training loss: 7.6651
batch: [5610/21305] batch time: 2.267 trainign loss: 7.2511 avg training loss: 7.6650
batch: [5620/21305] batch time: 0.376 trainign loss: 7.4334 avg training loss: 7.6649
batch: [5630/21305] batch time: 1.001 trainign loss: 6.8301 avg training loss: 7.6648
batch: [5640/21305] batch time: 0.650 trainign loss: 5.9777 avg training loss: 7.6647
batch: [5650/21305] batch time: 0.986 trainign loss: 7.6862 avg training loss: 7.6646
batch: [5660/21305] batch time: 1.482 trainign loss: 4.8375 avg training loss: 7.6645
batch: [5670/21305] batch time: 0.612 trainign loss: 7.2260 avg training loss: 7.6644
batch: [5680/21305] batch time: 1.789 trainign loss: 6.0595 avg training loss: 7.6643
batch: [5690/21305] batch time: 0.427 trainign loss: 6.1177 avg training loss: 7.6643
batch: [5700/21305] batch time: 1.435 trainign loss: 6.2729 avg training loss: 7.6641
batch: [5710/21305] batch time: 1.009 trainign loss: 5.1486 avg training loss: 7.6640
batch: [5720/21305] batch time: 1.061 trainign loss: 6.9644 avg training loss: 7.6637
batch: [5730/21305] batch time: 1.670 trainign loss: 7.8567 avg training loss: 7.6637
batch: [5740/21305] batch time: 0.598 trainign loss: 7.5489 avg training loss: 7.6637
batch: [5750/21305] batch time: 2.102 trainign loss: 6.7473 avg training loss: 7.6637
batch: [5760/21305] batch time: 0.054 trainign loss: 6.0690 avg training loss: 7.6636
batch: [5770/21305] batch time: 2.015 trainign loss: 6.6066 avg training loss: 7.6635
batch: [5780/21305] batch time: 0.591 trainign loss: 6.3536 avg training loss: 7.6634
batch: [5790/21305] batch time: 1.035 trainign loss: 7.6185 avg training loss: 7.6633
batch: [5800/21305] batch time: 1.482 trainign loss: 6.1452 avg training loss: 7.6633
batch: [5810/21305] batch time: 0.535 trainign loss: 7.7811 avg training loss: 7.6632
batch: [5820/21305] batch time: 1.762 trainign loss: 7.5031 avg training loss: 7.6631
batch: [5830/21305] batch time: 0.586 trainign loss: 6.2644 avg training loss: 7.6630
batch: [5840/21305] batch time: 1.446 trainign loss: 6.7232 avg training loss: 7.6629
batch: [5850/21305] batch time: 0.662 trainign loss: 6.7559 avg training loss: 7.6629
batch: [5860/21305] batch time: 1.720 trainign loss: 7.3911 avg training loss: 7.6628
batch: [5870/21305] batch time: 1.005 trainign loss: 6.9572 avg training loss: 7.6628
batch: [5880/21305] batch time: 1.256 trainign loss: 6.3920 avg training loss: 7.6627
batch: [5890/21305] batch time: 0.609 trainign loss: 6.2878 avg training loss: 7.6626
batch: [5900/21305] batch time: 2.060 trainign loss: 5.1159 avg training loss: 7.6624
batch: [5910/21305] batch time: 0.957 trainign loss: 5.6529 avg training loss: 7.6622
batch: [5920/21305] batch time: 1.674 trainign loss: 5.2994 avg training loss: 7.6620
batch: [5930/21305] batch time: 0.287 trainign loss: 7.0020 avg training loss: 7.6619
batch: [5940/21305] batch time: 2.327 trainign loss: 5.9928 avg training loss: 7.6619
batch: [5950/21305] batch time: 0.117 trainign loss: 5.2936 avg training loss: 7.6617
batch: [5960/21305] batch time: 2.539 trainign loss: 4.8447 avg training loss: 7.6615
batch: [5970/21305] batch time: 0.449 trainign loss: 7.8990 avg training loss: 7.6615
batch: [5980/21305] batch time: 1.744 trainign loss: 6.2386 avg training loss: 7.6614
batch: [5990/21305] batch time: 0.769 trainign loss: 7.4225 avg training loss: 7.6614
batch: [6000/21305] batch time: 1.569 trainign loss: 7.1810 avg training loss: 7.6614
batch: [6010/21305] batch time: 1.761 trainign loss: 5.7002 avg training loss: 7.6613
batch: [6020/21305] batch time: 0.724 trainign loss: 6.8134 avg training loss: 7.6612
batch: [6030/21305] batch time: 1.703 trainign loss: 5.9124 avg training loss: 7.6610
batch: [6040/21305] batch time: 0.537 trainign loss: 2.0339 avg training loss: 7.6607
batch: [6050/21305] batch time: 2.334 trainign loss: 7.3121 avg training loss: 7.6606
batch: [6060/21305] batch time: 0.062 trainign loss: 7.5162 avg training loss: 7.6605
batch: [6070/21305] batch time: 1.868 trainign loss: 2.9817 avg training loss: 7.6603
batch: [6080/21305] batch time: 0.056 trainign loss: 5.7408 avg training loss: 7.6601
batch: [6090/21305] batch time: 1.466 trainign loss: 6.7509 avg training loss: 7.6600
batch: [6100/21305] batch time: 0.056 trainign loss: 7.8874 avg training loss: 7.6600
batch: [6110/21305] batch time: 2.076 trainign loss: 5.8687 avg training loss: 7.6598
batch: [6120/21305] batch time: 0.057 trainign loss: 6.5582 avg training loss: 7.6597
batch: [6130/21305] batch time: 2.325 trainign loss: 7.0519 avg training loss: 7.6595
batch: [6140/21305] batch time: 0.305 trainign loss: 5.6850 avg training loss: 7.6594
batch: [6150/21305] batch time: 2.266 trainign loss: 6.3358 avg training loss: 7.6594
batch: [6160/21305] batch time: 0.225 trainign loss: 5.8749 avg training loss: 7.6593
batch: [6170/21305] batch time: 2.598 trainign loss: 4.8933 avg training loss: 7.6592
batch: [6180/21305] batch time: 0.759 trainign loss: 7.1373 avg training loss: 7.6590
batch: [6190/21305] batch time: 0.800 trainign loss: 7.8709 avg training loss: 7.6590
batch: [6200/21305] batch time: 1.815 trainign loss: 6.9422 avg training loss: 7.6590
batch: [6210/21305] batch time: 0.449 trainign loss: 6.6075 avg training loss: 7.6589
batch: [6220/21305] batch time: 1.455 trainign loss: 6.9238 avg training loss: 7.6588
batch: [6230/21305] batch time: 0.462 trainign loss: 6.6982 avg training loss: 7.6587
batch: [6240/21305] batch time: 1.982 trainign loss: 7.5119 avg training loss: 7.6586
batch: [6250/21305] batch time: 0.052 trainign loss: 6.9088 avg training loss: 7.6585
batch: [6260/21305] batch time: 2.297 trainign loss: 7.5161 avg training loss: 7.6584
batch: [6270/21305] batch time: 0.057 trainign loss: 6.8027 avg training loss: 7.6583
batch: [6280/21305] batch time: 2.167 trainign loss: 6.8934 avg training loss: 7.6583
batch: [6290/21305] batch time: 0.493 trainign loss: 4.2529 avg training loss: 7.6581
batch: [6300/21305] batch time: 2.204 trainign loss: 7.4097 avg training loss: 7.6580
batch: [6310/21305] batch time: 0.058 trainign loss: 6.7812 avg training loss: 7.6579
batch: [6320/21305] batch time: 1.785 trainign loss: 7.0683 avg training loss: 7.6578
batch: [6330/21305] batch time: 0.905 trainign loss: 6.7101 avg training loss: 7.6577
batch: [6340/21305] batch time: 1.456 trainign loss: 4.4196 avg training loss: 7.6576
batch: [6350/21305] batch time: 0.565 trainign loss: 7.0335 avg training loss: 7.6574
batch: [6360/21305] batch time: 1.865 trainign loss: 7.7189 avg training loss: 7.6574
batch: [6370/21305] batch time: 0.361 trainign loss: 6.0803 avg training loss: 7.6574
batch: [6380/21305] batch time: 1.511 trainign loss: 6.7993 avg training loss: 7.6572
batch: [6390/21305] batch time: 0.196 trainign loss: 5.0717 avg training loss: 7.6571
batch: [6400/21305] batch time: 1.603 trainign loss: 6.7533 avg training loss: 7.6570
batch: [6410/21305] batch time: 0.358 trainign loss: 7.5920 avg training loss: 7.6569
batch: [6420/21305] batch time: 1.818 trainign loss: 6.4545 avg training loss: 7.6567
batch: [6430/21305] batch time: 0.072 trainign loss: 5.1255 avg training loss: 7.6566
batch: [6440/21305] batch time: 1.691 trainign loss: 6.3824 avg training loss: 7.6563
batch: [6450/21305] batch time: 0.872 trainign loss: 7.9818 avg training loss: 7.6561
batch: [6460/21305] batch time: 1.349 trainign loss: 7.3921 avg training loss: 7.6562
batch: [6470/21305] batch time: 0.543 trainign loss: 7.2358 avg training loss: 7.6561
batch: [6480/21305] batch time: 1.019 trainign loss: 6.4783 avg training loss: 7.6560
batch: [6490/21305] batch time: 0.592 trainign loss: 6.2536 avg training loss: 7.6559
batch: [6500/21305] batch time: 2.265 trainign loss: 5.5007 avg training loss: 7.6558
batch: [6510/21305] batch time: 0.141 trainign loss: 6.6324 avg training loss: 7.6557
batch: [6520/21305] batch time: 2.107 trainign loss: 6.4679 avg training loss: 7.6556
batch: [6530/21305] batch time: 0.739 trainign loss: 5.1672 avg training loss: 7.6554
batch: [6540/21305] batch time: 1.910 trainign loss: 7.1655 avg training loss: 7.6553
batch: [6550/21305] batch time: 0.624 trainign loss: 6.0339 avg training loss: 7.6552
batch: [6560/21305] batch time: 1.288 trainign loss: 6.6164 avg training loss: 7.6551
batch: [6570/21305] batch time: 0.756 trainign loss: 6.2347 avg training loss: 7.6550
batch: [6580/21305] batch time: 1.651 trainign loss: 4.6757 avg training loss: 7.6549
batch: [6590/21305] batch time: 0.757 trainign loss: 6.2593 avg training loss: 7.6548
batch: [6600/21305] batch time: 1.616 trainign loss: 7.2373 avg training loss: 7.6547
batch: [6610/21305] batch time: 0.860 trainign loss: 7.5328 avg training loss: 7.6546
batch: [6620/21305] batch time: 1.890 trainign loss: 6.6171 avg training loss: 7.6546
batch: [6630/21305] batch time: 0.056 trainign loss: 6.4282 avg training loss: 7.6544
batch: [6640/21305] batch time: 1.028 trainign loss: 4.8879 avg training loss: 7.6543
batch: [6650/21305] batch time: 0.590 trainign loss: 5.8451 avg training loss: 7.6542
batch: [6660/21305] batch time: 0.732 trainign loss: 6.0373 avg training loss: 7.6540
batch: [6670/21305] batch time: 0.168 trainign loss: 6.6796 avg training loss: 7.6539
batch: [6680/21305] batch time: 0.083 trainign loss: 7.5758 avg training loss: 7.6539
batch: [6690/21305] batch time: 0.951 trainign loss: 7.1864 avg training loss: 7.6538
batch: [6700/21305] batch time: 0.063 trainign loss: 4.9920 avg training loss: 7.6537
batch: [6710/21305] batch time: 1.958 trainign loss: 5.6658 avg training loss: 7.6536
batch: [6720/21305] batch time: 0.056 trainign loss: 6.9432 avg training loss: 7.6535
batch: [6730/21305] batch time: 1.853 trainign loss: 5.8483 avg training loss: 7.6533
batch: [6740/21305] batch time: 0.056 trainign loss: 7.6561 avg training loss: 7.6533
batch: [6750/21305] batch time: 1.445 trainign loss: 7.5517 avg training loss: 7.6532
batch: [6760/21305] batch time: 0.056 trainign loss: 6.7504 avg training loss: 7.6532
batch: [6770/21305] batch time: 1.007 trainign loss: 5.5336 avg training loss: 7.6530
batch: [6780/21305] batch time: 0.413 trainign loss: 5.7701 avg training loss: 7.6529
batch: [6790/21305] batch time: 1.550 trainign loss: 6.8963 avg training loss: 7.6528
batch: [6800/21305] batch time: 0.085 trainign loss: 5.4655 avg training loss: 7.6527
batch: [6810/21305] batch time: 0.771 trainign loss: 6.4174 avg training loss: 7.6525
batch: [6820/21305] batch time: 0.056 trainign loss: 5.7389 avg training loss: 7.6524
batch: [6830/21305] batch time: 1.063 trainign loss: 6.6155 avg training loss: 7.6523
batch: [6840/21305] batch time: 0.664 trainign loss: 7.7719 avg training loss: 7.6520
batch: [6850/21305] batch time: 0.426 trainign loss: 6.2419 avg training loss: 7.6519
batch: [6860/21305] batch time: 1.741 trainign loss: 6.2042 avg training loss: 7.6519
batch: [6870/21305] batch time: 0.056 trainign loss: 6.5047 avg training loss: 7.6517
batch: [6880/21305] batch time: 1.160 trainign loss: 6.0638 avg training loss: 7.6516
batch: [6890/21305] batch time: 1.144 trainign loss: 7.2562 avg training loss: 7.6514
batch: [6900/21305] batch time: 0.719 trainign loss: 6.7337 avg training loss: 7.6513
batch: [6910/21305] batch time: 1.477 trainign loss: 6.0277 avg training loss: 7.6512
batch: [6920/21305] batch time: 0.063 trainign loss: 6.8714 avg training loss: 7.6511
batch: [6930/21305] batch time: 1.260 trainign loss: 5.5578 avg training loss: 7.6510
batch: [6940/21305] batch time: 0.057 trainign loss: 5.1776 avg training loss: 7.6508
batch: [6950/21305] batch time: 1.437 trainign loss: 6.3555 avg training loss: 7.6506
batch: [6960/21305] batch time: 0.063 trainign loss: 4.2707 avg training loss: 7.6505
batch: [6970/21305] batch time: 2.241 trainign loss: 3.6419 avg training loss: 7.6503
batch: [6980/21305] batch time: 0.062 trainign loss: 6.6794 avg training loss: 7.6502
batch: [6990/21305] batch time: 1.565 trainign loss: 7.1402 avg training loss: 7.6502
batch: [7000/21305] batch time: 0.268 trainign loss: 5.4106 avg training loss: 7.6501
batch: [7010/21305] batch time: 0.436 trainign loss: 6.4250 avg training loss: 7.6499
batch: [7020/21305] batch time: 1.276 trainign loss: 7.6573 avg training loss: 7.6499
batch: [7030/21305] batch time: 0.362 trainign loss: 6.5032 avg training loss: 7.6498
batch: [7040/21305] batch time: 0.677 trainign loss: 7.2266 avg training loss: 7.6497
batch: [7050/21305] batch time: 0.342 trainign loss: 5.9126 avg training loss: 7.6495
batch: [7060/21305] batch time: 0.627 trainign loss: 6.0154 avg training loss: 7.6494
batch: [7070/21305] batch time: 0.063 trainign loss: 6.9752 avg training loss: 7.6494
batch: [7080/21305] batch time: 0.558 trainign loss: 6.4969 avg training loss: 7.6493
batch: [7090/21305] batch time: 0.058 trainign loss: 6.0726 avg training loss: 7.6492
batch: [7100/21305] batch time: 0.222 trainign loss: 6.1050 avg training loss: 7.6491
batch: [7110/21305] batch time: 0.062 trainign loss: 6.3196 avg training loss: 7.6489
batch: [7120/21305] batch time: 0.063 trainign loss: 4.1972 avg training loss: 7.6486
batch: [7130/21305] batch time: 0.832 trainign loss: 7.4287 avg training loss: 7.6485
batch: [7140/21305] batch time: 0.055 trainign loss: 7.5101 avg training loss: 7.6484
batch: [7150/21305] batch time: 0.990 trainign loss: 5.0937 avg training loss: 7.6483
batch: [7160/21305] batch time: 0.054 trainign loss: 7.6459 avg training loss: 7.6481
batch: [7170/21305] batch time: 0.788 trainign loss: 5.9982 avg training loss: 7.6480
batch: [7180/21305] batch time: 0.061 trainign loss: 6.4189 avg training loss: 7.6479
batch: [7190/21305] batch time: 0.296 trainign loss: 4.9801 avg training loss: 7.6478
batch: [7200/21305] batch time: 0.056 trainign loss: 5.6005 avg training loss: 7.6476
batch: [7210/21305] batch time: 0.058 trainign loss: 6.4246 avg training loss: 7.6475
batch: [7220/21305] batch time: 0.233 trainign loss: 5.0446 avg training loss: 7.6473
batch: [7230/21305] batch time: 0.056 trainign loss: 5.1168 avg training loss: 7.6472
batch: [7240/21305] batch time: 0.055 trainign loss: 7.5413 avg training loss: 7.6471
batch: [7250/21305] batch time: 0.062 trainign loss: 6.0736 avg training loss: 7.6471
batch: [7260/21305] batch time: 0.056 trainign loss: 5.4617 avg training loss: 7.6469
batch: [7270/21305] batch time: 0.056 trainign loss: 7.0346 avg training loss: 7.6469
batch: [7280/21305] batch time: 0.051 trainign loss: 6.5668 avg training loss: 7.6468
batch: [7290/21305] batch time: 0.056 trainign loss: 5.7659 avg training loss: 7.6467
batch: [7300/21305] batch time: 0.055 trainign loss: 6.7182 avg training loss: 7.6466
batch: [7310/21305] batch time: 0.062 trainign loss: 7.2034 avg training loss: 7.6465
batch: [7320/21305] batch time: 0.051 trainign loss: 6.9272 avg training loss: 7.6464
batch: [7330/21305] batch time: 0.060 trainign loss: 5.4681 avg training loss: 7.6463
batch: [7340/21305] batch time: 0.056 trainign loss: 7.1687 avg training loss: 7.6462
batch: [7350/21305] batch time: 0.056 trainign loss: 7.2047 avg training loss: 7.6461
batch: [7360/21305] batch time: 0.057 trainign loss: 6.7964 avg training loss: 7.6459
batch: [7370/21305] batch time: 0.056 trainign loss: 2.8350 avg training loss: 7.6458
batch: [7380/21305] batch time: 0.052 trainign loss: 6.7430 avg training loss: 7.6456
batch: [7390/21305] batch time: 0.056 trainign loss: 7.1368 avg training loss: 7.6455
batch: [7400/21305] batch time: 0.214 trainign loss: 4.7156 avg training loss: 7.6454
batch: [7410/21305] batch time: 0.061 trainign loss: 7.0426 avg training loss: 7.6451
batch: [7420/21305] batch time: 0.738 trainign loss: 7.4455 avg training loss: 7.6451
batch: [7430/21305] batch time: 0.057 trainign loss: 6.8949 avg training loss: 7.6451
batch: [7440/21305] batch time: 1.703 trainign loss: 5.9033 avg training loss: 7.6450
batch: [7450/21305] batch time: 0.063 trainign loss: 6.7952 avg training loss: 7.6449
batch: [7460/21305] batch time: 0.767 trainign loss: 3.1459 avg training loss: 7.6446
batch: [7470/21305] batch time: 0.057 trainign loss: 4.8124 avg training loss: 7.6445
batch: [7480/21305] batch time: 0.337 trainign loss: 6.3385 avg training loss: 7.6444
batch: [7490/21305] batch time: 0.057 trainign loss: 6.3089 avg training loss: 7.6443
batch: [7500/21305] batch time: 0.833 trainign loss: 7.4235 avg training loss: 7.6442
batch: [7510/21305] batch time: 0.056 trainign loss: 5.9358 avg training loss: 7.6441
batch: [7520/21305] batch time: 1.273 trainign loss: 7.3550 avg training loss: 7.6440
batch: [7530/21305] batch time: 0.058 trainign loss: 5.6147 avg training loss: 7.6439
batch: [7540/21305] batch time: 1.803 trainign loss: 7.1553 avg training loss: 7.6438
batch: [7550/21305] batch time: 0.056 trainign loss: 7.3664 avg training loss: 7.6437
batch: [7560/21305] batch time: 2.419 trainign loss: 6.4104 avg training loss: 7.6436
batch: [7570/21305] batch time: 0.057 trainign loss: 6.9646 avg training loss: 7.6435
batch: [7580/21305] batch time: 1.910 trainign loss: 6.5315 avg training loss: 7.6435
batch: [7590/21305] batch time: 0.056 trainign loss: 7.4833 avg training loss: 7.6433
batch: [7600/21305] batch time: 1.522 trainign loss: 6.4690 avg training loss: 7.6433
batch: [7610/21305] batch time: 0.063 trainign loss: 6.0362 avg training loss: 7.6432
batch: [7620/21305] batch time: 1.684 trainign loss: 4.5191 avg training loss: 7.6429
batch: [7630/21305] batch time: 0.056 trainign loss: 7.5739 avg training loss: 7.6428
batch: [7640/21305] batch time: 2.179 trainign loss: 6.7062 avg training loss: 7.6428
batch: [7650/21305] batch time: 0.060 trainign loss: 5.2203 avg training loss: 7.6426
batch: [7660/21305] batch time: 2.477 trainign loss: 7.2571 avg training loss: 7.6425
batch: [7670/21305] batch time: 0.056 trainign loss: 7.3956 avg training loss: 7.6424
batch: [7680/21305] batch time: 2.541 trainign loss: 5.6758 avg training loss: 7.6423
batch: [7690/21305] batch time: 0.056 trainign loss: 5.1699 avg training loss: 7.6422
batch: [7700/21305] batch time: 2.006 trainign loss: 7.1222 avg training loss: 7.6420
batch: [7710/21305] batch time: 0.056 trainign loss: 5.6907 avg training loss: 7.6418
batch: [7720/21305] batch time: 1.560 trainign loss: 6.2274 avg training loss: 7.6417
batch: [7730/21305] batch time: 0.058 trainign loss: 4.3358 avg training loss: 7.6413
batch: [7740/21305] batch time: 2.364 trainign loss: 7.3209 avg training loss: 7.6413
batch: [7750/21305] batch time: 0.057 trainign loss: 8.0085 avg training loss: 7.6413
batch: [7760/21305] batch time: 1.549 trainign loss: 7.1911 avg training loss: 7.6413
batch: [7770/21305] batch time: 0.057 trainign loss: 5.0816 avg training loss: 7.6412
batch: [7780/21305] batch time: 1.452 trainign loss: 7.0811 avg training loss: 7.6411
batch: [7790/21305] batch time: 0.056 trainign loss: 7.2693 avg training loss: 7.6410
batch: [7800/21305] batch time: 2.063 trainign loss: 7.0030 avg training loss: 7.6409
batch: [7810/21305] batch time: 0.056 trainign loss: 6.0921 avg training loss: 7.6408
batch: [7820/21305] batch time: 1.834 trainign loss: 6.6695 avg training loss: 7.6407
batch: [7830/21305] batch time: 0.058 trainign loss: 6.0994 avg training loss: 7.6406
batch: [7840/21305] batch time: 0.785 trainign loss: 7.0103 avg training loss: 7.6406
batch: [7850/21305] batch time: 0.056 trainign loss: 5.2037 avg training loss: 7.6404
batch: [7860/21305] batch time: 1.240 trainign loss: 5.3773 avg training loss: 7.6403
batch: [7870/21305] batch time: 0.057 trainign loss: 6.3161 avg training loss: 7.6402
batch: [7880/21305] batch time: 1.374 trainign loss: 4.5814 avg training loss: 7.6401
batch: [7890/21305] batch time: 0.058 trainign loss: 6.0583 avg training loss: 7.6399
batch: [7900/21305] batch time: 1.634 trainign loss: 5.5934 avg training loss: 7.6397
batch: [7910/21305] batch time: 0.057 trainign loss: 7.5737 avg training loss: 7.6396
batch: [7920/21305] batch time: 1.355 trainign loss: 6.0779 avg training loss: 7.6396
batch: [7930/21305] batch time: 0.056 trainign loss: 6.3536 avg training loss: 7.6395
batch: [7940/21305] batch time: 0.536 trainign loss: 7.4833 avg training loss: 7.6394
batch: [7950/21305] batch time: 0.063 trainign loss: 7.2085 avg training loss: 7.6394
batch: [7960/21305] batch time: 0.447 trainign loss: 6.0762 avg training loss: 7.6393
batch: [7970/21305] batch time: 0.059 trainign loss: 6.3546 avg training loss: 7.6392
batch: [7980/21305] batch time: 0.432 trainign loss: 7.0660 avg training loss: 7.6390
batch: [7990/21305] batch time: 0.057 trainign loss: 7.3702 avg training loss: 7.6390
batch: [8000/21305] batch time: 1.089 trainign loss: 6.8083 avg training loss: 7.6389
batch: [8010/21305] batch time: 0.056 trainign loss: 5.4570 avg training loss: 7.6387
batch: [8020/21305] batch time: 0.605 trainign loss: 2.4688 avg training loss: 7.6385
batch: [8030/21305] batch time: 0.057 trainign loss: 5.5760 avg training loss: 7.6382
batch: [8040/21305] batch time: 0.851 trainign loss: 7.3745 avg training loss: 7.6381
batch: [8050/21305] batch time: 0.057 trainign loss: 8.2170 avg training loss: 7.6381
batch: [8060/21305] batch time: 0.512 trainign loss: 7.2618 avg training loss: 7.6381
batch: [8070/21305] batch time: 0.056 trainign loss: 6.5013 avg training loss: 7.6380
batch: [8080/21305] batch time: 0.496 trainign loss: 7.1430 avg training loss: 7.6380
batch: [8090/21305] batch time: 0.056 trainign loss: 7.1299 avg training loss: 7.6379
batch: [8100/21305] batch time: 0.055 trainign loss: 6.4234 avg training loss: 7.6378
batch: [8110/21305] batch time: 0.057 trainign loss: 7.3847 avg training loss: 7.6377
batch: [8120/21305] batch time: 0.143 trainign loss: 6.5362 avg training loss: 7.6376
batch: [8130/21305] batch time: 0.057 trainign loss: 6.0969 avg training loss: 7.6375
batch: [8140/21305] batch time: 0.592 trainign loss: 6.5677 avg training loss: 7.6374
batch: [8150/21305] batch time: 0.056 trainign loss: 6.1976 avg training loss: 7.6373
batch: [8160/21305] batch time: 0.998 trainign loss: 6.4223 avg training loss: 7.6372
batch: [8170/21305] batch time: 0.060 trainign loss: 6.4197 avg training loss: 7.6371
batch: [8180/21305] batch time: 0.880 trainign loss: 5.9661 avg training loss: 7.6369
batch: [8190/21305] batch time: 0.062 trainign loss: 6.9926 avg training loss: 7.6369
batch: [8200/21305] batch time: 0.373 trainign loss: 6.8548 avg training loss: 7.6369
batch: [8210/21305] batch time: 0.056 trainign loss: 6.1697 avg training loss: 7.6367
batch: [8220/21305] batch time: 0.504 trainign loss: 6.3603 avg training loss: 7.6366
batch: [8230/21305] batch time: 0.056 trainign loss: 6.9934 avg training loss: 7.6365
batch: [8240/21305] batch time: 0.765 trainign loss: 7.1779 avg training loss: 7.6364
batch: [8250/21305] batch time: 0.056 trainign loss: 6.3085 avg training loss: 7.6364
batch: [8260/21305] batch time: 0.456 trainign loss: 6.7528 avg training loss: 7.6362
batch: [8270/21305] batch time: 0.062 trainign loss: 7.7658 avg training loss: 7.6361
batch: [8280/21305] batch time: 0.718 trainign loss: 6.6387 avg training loss: 7.6361
batch: [8290/21305] batch time: 0.056 trainign loss: 5.8803 avg training loss: 7.6360
batch: [8300/21305] batch time: 1.416 trainign loss: 6.0247 avg training loss: 7.6359
batch: [8310/21305] batch time: 0.057 trainign loss: 7.4634 avg training loss: 7.6358
batch: [8320/21305] batch time: 1.760 trainign loss: 6.9959 avg training loss: 7.6357
batch: [8330/21305] batch time: 0.057 trainign loss: 7.2401 avg training loss: 7.6357
batch: [8340/21305] batch time: 2.250 trainign loss: 6.1967 avg training loss: 7.6356
batch: [8350/21305] batch time: 0.056 trainign loss: 5.0201 avg training loss: 7.6354
batch: [8360/21305] batch time: 2.122 trainign loss: 5.6594 avg training loss: 7.6353
batch: [8370/21305] batch time: 0.056 trainign loss: 7.5097 avg training loss: 7.6352
batch: [8380/21305] batch time: 2.343 trainign loss: 5.2682 avg training loss: 7.6351
batch: [8390/21305] batch time: 0.056 trainign loss: 4.1255 avg training loss: 7.6348
batch: [8400/21305] batch time: 0.824 trainign loss: 6.7577 avg training loss: 7.6344
batch: [8410/21305] batch time: 0.056 trainign loss: 6.2478 avg training loss: 7.6343
batch: [8420/21305] batch time: 0.443 trainign loss: 7.1962 avg training loss: 7.6340
batch: [8430/21305] batch time: 0.056 trainign loss: 7.7816 avg training loss: 7.6340
batch: [8440/21305] batch time: 0.807 trainign loss: 8.1232 avg training loss: 7.6340
batch: [8450/21305] batch time: 0.056 trainign loss: 7.4337 avg training loss: 7.6339
batch: [8460/21305] batch time: 0.905 trainign loss: 7.5848 avg training loss: 7.6339
batch: [8470/21305] batch time: 0.060 trainign loss: 7.7244 avg training loss: 7.6338
batch: [8480/21305] batch time: 0.876 trainign loss: 6.9307 avg training loss: 7.6338
batch: [8490/21305] batch time: 0.056 trainign loss: 6.3937 avg training loss: 7.6337
batch: [8500/21305] batch time: 1.516 trainign loss: 5.8585 avg training loss: 7.6335
batch: [8510/21305] batch time: 0.478 trainign loss: 6.0377 avg training loss: 7.6334
batch: [8520/21305] batch time: 1.065 trainign loss: 6.8423 avg training loss: 7.6332
batch: [8530/21305] batch time: 0.758 trainign loss: 6.9543 avg training loss: 7.6331
batch: [8540/21305] batch time: 1.854 trainign loss: 7.1403 avg training loss: 7.6330
batch: [8550/21305] batch time: 0.062 trainign loss: 7.5576 avg training loss: 7.6330
batch: [8560/21305] batch time: 1.700 trainign loss: 6.8968 avg training loss: 7.6329
batch: [8570/21305] batch time: 0.110 trainign loss: 5.6699 avg training loss: 7.6327
batch: [8580/21305] batch time: 1.406 trainign loss: 6.4816 avg training loss: 7.6327
batch: [8590/21305] batch time: 0.709 trainign loss: 7.2571 avg training loss: 7.6326
batch: [8600/21305] batch time: 2.006 trainign loss: 6.6616 avg training loss: 7.6325
batch: [8610/21305] batch time: 0.622 trainign loss: 5.6262 avg training loss: 7.6323
batch: [8620/21305] batch time: 1.136 trainign loss: 7.0026 avg training loss: 7.6322
batch: [8630/21305] batch time: 0.126 trainign loss: 5.1098 avg training loss: 7.6321
batch: [8640/21305] batch time: 1.503 trainign loss: 6.6463 avg training loss: 7.6320
batch: [8650/21305] batch time: 0.368 trainign loss: 4.6060 avg training loss: 7.6319
batch: [8660/21305] batch time: 1.677 trainign loss: 4.2732 avg training loss: 7.6317
batch: [8670/21305] batch time: 0.544 trainign loss: 5.5323 avg training loss: 7.6316
batch: [8680/21305] batch time: 1.503 trainign loss: 7.4665 avg training loss: 7.6315
batch: [8690/21305] batch time: 0.062 trainign loss: 5.9338 avg training loss: 7.6314
batch: [8700/21305] batch time: 2.395 trainign loss: 5.0490 avg training loss: 7.6313
batch: [8710/21305] batch time: 0.061 trainign loss: 1.2971 avg training loss: 7.6310
batch: [8720/21305] batch time: 2.533 trainign loss: 0.0017 avg training loss: 7.6303
batch: [8730/21305] batch time: 0.056 trainign loss: 0.0002 avg training loss: 7.6297
batch: [8740/21305] batch time: 2.174 trainign loss: 8.3976 avg training loss: 7.6297
batch: [8750/21305] batch time: 0.056 trainign loss: 8.3389 avg training loss: 7.6298
batch: [8760/21305] batch time: 2.116 trainign loss: 7.5600 avg training loss: 7.6297
batch: [8770/21305] batch time: 0.054 trainign loss: 6.8532 avg training loss: 7.6296
batch: [8780/21305] batch time: 2.034 trainign loss: 6.0405 avg training loss: 7.6295
batch: [8790/21305] batch time: 0.062 trainign loss: 6.3023 avg training loss: 7.6295
batch: [8800/21305] batch time: 2.212 trainign loss: 6.3690 avg training loss: 7.6294
batch: [8810/21305] batch time: 0.052 trainign loss: 7.5897 avg training loss: 7.6293
batch: [8820/21305] batch time: 2.230 trainign loss: 6.8356 avg training loss: 7.6293
batch: [8830/21305] batch time: 0.062 trainign loss: 7.0969 avg training loss: 7.6292
batch: [8840/21305] batch time: 2.311 trainign loss: 6.4175 avg training loss: 7.6291
batch: [8850/21305] batch time: 0.057 trainign loss: 6.3770 avg training loss: 7.6290
batch: [8860/21305] batch time: 2.329 trainign loss: 7.2986 avg training loss: 7.6289
batch: [8870/21305] batch time: 0.053 trainign loss: 7.1372 avg training loss: 7.6289
batch: [8880/21305] batch time: 2.373 trainign loss: 5.5576 avg training loss: 7.6288
batch: [8890/21305] batch time: 0.062 trainign loss: 7.1292 avg training loss: 7.6287
batch: [8900/21305] batch time: 2.546 trainign loss: 5.9357 avg training loss: 7.6286
batch: [8910/21305] batch time: 0.056 trainign loss: 5.4085 avg training loss: 7.6284
batch: [8920/21305] batch time: 2.327 trainign loss: 2.4899 avg training loss: 7.6282
batch: [8930/21305] batch time: 0.062 trainign loss: 7.9492 avg training loss: 7.6281
batch: [8940/21305] batch time: 2.176 trainign loss: 6.6403 avg training loss: 7.6281
batch: [8950/21305] batch time: 0.056 trainign loss: 5.9706 avg training loss: 7.6280
batch: [8960/21305] batch time: 2.378 trainign loss: 6.4958 avg training loss: 7.6279
batch: [8970/21305] batch time: 0.056 trainign loss: 6.3742 avg training loss: 7.6278
batch: [8980/21305] batch time: 2.566 trainign loss: 5.5659 avg training loss: 7.6277
batch: [8990/21305] batch time: 0.057 trainign loss: 6.1254 avg training loss: 7.6275
batch: [9000/21305] batch time: 2.557 trainign loss: 1.0950 avg training loss: 7.6272
batch: [9010/21305] batch time: 0.056 trainign loss: 7.5775 avg training loss: 7.6271
batch: [9020/21305] batch time: 2.353 trainign loss: 6.2816 avg training loss: 7.6270
batch: [9030/21305] batch time: 0.056 trainign loss: 6.3100 avg training loss: 7.6269
batch: [9040/21305] batch time: 2.191 trainign loss: 6.7271 avg training loss: 7.6269
batch: [9050/21305] batch time: 0.056 trainign loss: 6.0524 avg training loss: 7.6268
batch: [9060/21305] batch time: 2.272 trainign loss: 5.4559 avg training loss: 7.6266
batch: [9070/21305] batch time: 0.056 trainign loss: 3.4906 avg training loss: 7.6264
batch: [9080/21305] batch time: 1.819 trainign loss: 8.1417 avg training loss: 7.6264
batch: [9090/21305] batch time: 0.056 trainign loss: 6.7961 avg training loss: 7.6263
batch: [9100/21305] batch time: 0.551 trainign loss: 6.9525 avg training loss: 7.6262
batch: [9110/21305] batch time: 0.057 trainign loss: 5.5350 avg training loss: 7.6261
batch: [9120/21305] batch time: 0.225 trainign loss: 7.0886 avg training loss: 7.6260
batch: [9130/21305] batch time: 0.060 trainign loss: 5.0219 avg training loss: 7.6257
batch: [9140/21305] batch time: 0.064 trainign loss: 6.8362 avg training loss: 7.6256
batch: [9150/21305] batch time: 0.063 trainign loss: 6.7062 avg training loss: 7.6255
batch: [9160/21305] batch time: 0.310 trainign loss: 6.8865 avg training loss: 7.6254
batch: [9170/21305] batch time: 0.062 trainign loss: 5.5724 avg training loss: 7.6252
batch: [9180/21305] batch time: 0.233 trainign loss: 7.2726 avg training loss: 7.6252
batch: [9190/21305] batch time: 0.057 trainign loss: 6.7398 avg training loss: 7.6251
batch: [9200/21305] batch time: 0.057 trainign loss: 6.8512 avg training loss: 7.6250
batch: [9210/21305] batch time: 0.058 trainign loss: 5.8854 avg training loss: 7.6248
batch: [9220/21305] batch time: 0.063 trainign loss: 5.2273 avg training loss: 7.6247
batch: [9230/21305] batch time: 1.019 trainign loss: 6.8778 avg training loss: 7.6247
batch: [9240/21305] batch time: 0.052 trainign loss: 5.2475 avg training loss: 7.6246
batch: [9250/21305] batch time: 1.696 trainign loss: 7.1655 avg training loss: 7.6245
batch: [9260/21305] batch time: 0.056 trainign loss: 6.8062 avg training loss: 7.6244
batch: [9270/21305] batch time: 1.732 trainign loss: 6.6376 avg training loss: 7.6243
batch: [9280/21305] batch time: 0.056 trainign loss: 6.2185 avg training loss: 7.6241
batch: [9290/21305] batch time: 2.359 trainign loss: 5.0777 avg training loss: 7.6240
batch: [9300/21305] batch time: 0.062 trainign loss: 7.7598 avg training loss: 7.6238
batch: [9310/21305] batch time: 1.985 trainign loss: 7.3471 avg training loss: 7.6238
batch: [9320/21305] batch time: 0.054 trainign loss: 5.2886 avg training loss: 7.6236
batch: [9330/21305] batch time: 1.838 trainign loss: 5.4079 avg training loss: 7.6235
batch: [9340/21305] batch time: 0.056 trainign loss: 7.3255 avg training loss: 7.6234
batch: [9350/21305] batch time: 1.963 trainign loss: 7.3957 avg training loss: 7.6234
batch: [9360/21305] batch time: 0.053 trainign loss: 5.5105 avg training loss: 7.6233
batch: [9370/21305] batch time: 1.915 trainign loss: 5.2104 avg training loss: 7.6232
batch: [9380/21305] batch time: 0.063 trainign loss: 5.7515 avg training loss: 7.6231
batch: [9390/21305] batch time: 1.213 trainign loss: 6.5141 avg training loss: 7.6230
batch: [9400/21305] batch time: 0.061 trainign loss: 6.8990 avg training loss: 7.6229
batch: [9410/21305] batch time: 1.415 trainign loss: 6.0136 avg training loss: 7.6228
batch: [9420/21305] batch time: 0.056 trainign loss: 4.2331 avg training loss: 7.6226
batch: [9430/21305] batch time: 1.486 trainign loss: 6.8215 avg training loss: 7.6225
batch: [9440/21305] batch time: 0.056 trainign loss: 7.1174 avg training loss: 7.6224
batch: [9450/21305] batch time: 0.619 trainign loss: 5.0872 avg training loss: 7.6221
batch: [9460/21305] batch time: 0.056 trainign loss: 7.2519 avg training loss: 7.6221
batch: [9470/21305] batch time: 1.467 trainign loss: 7.3423 avg training loss: 7.6220
batch: [9480/21305] batch time: 0.055 trainign loss: 5.9552 avg training loss: 7.6219
batch: [9490/21305] batch time: 1.095 trainign loss: 7.0435 avg training loss: 7.6217
batch: [9500/21305] batch time: 0.056 trainign loss: 7.6097 avg training loss: 7.6217
batch: [9510/21305] batch time: 2.501 trainign loss: 6.1608 avg training loss: 7.6215
batch: [9520/21305] batch time: 0.062 trainign loss: 7.1287 avg training loss: 7.6215
batch: [9530/21305] batch time: 1.680 trainign loss: 6.9512 avg training loss: 7.6214
batch: [9540/21305] batch time: 0.062 trainign loss: 6.2583 avg training loss: 7.6213
batch: [9550/21305] batch time: 0.397 trainign loss: 4.7976 avg training loss: 7.6212
batch: [9560/21305] batch time: 0.056 trainign loss: 5.4801 avg training loss: 7.6210
batch: [9570/21305] batch time: 0.906 trainign loss: 6.8549 avg training loss: 7.6209
batch: [9580/21305] batch time: 0.056 trainign loss: 8.7584 avg training loss: 7.6207
batch: [9590/21305] batch time: 0.486 trainign loss: 7.2468 avg training loss: 7.6207
batch: [9600/21305] batch time: 0.056 trainign loss: 6.9042 avg training loss: 7.6207
batch: [9610/21305] batch time: 0.056 trainign loss: 6.4104 avg training loss: 7.6205
batch: [9620/21305] batch time: 0.050 trainign loss: 6.8141 avg training loss: 7.6204
batch: [9630/21305] batch time: 0.357 trainign loss: 7.6131 avg training loss: 7.6203
batch: [9640/21305] batch time: 0.052 trainign loss: 7.5388 avg training loss: 7.6202
batch: [9650/21305] batch time: 0.063 trainign loss: 6.4806 avg training loss: 7.6202
batch: [9660/21305] batch time: 0.055 trainign loss: 4.3951 avg training loss: 7.6200
batch: [9670/21305] batch time: 0.124 trainign loss: 6.6451 avg training loss: 7.6199
batch: [9680/21305] batch time: 0.055 trainign loss: 7.0228 avg training loss: 7.6198
batch: [9690/21305] batch time: 0.056 trainign loss: 6.9616 avg training loss: 7.6198
batch: [9700/21305] batch time: 0.058 trainign loss: 6.5755 avg training loss: 7.6197
batch: [9710/21305] batch time: 0.056 trainign loss: 6.2017 avg training loss: 7.6196
batch: [9720/21305] batch time: 0.056 trainign loss: 6.8707 avg training loss: 7.6195
batch: [9730/21305] batch time: 0.056 trainign loss: 6.6633 avg training loss: 7.6195
batch: [9740/21305] batch time: 0.051 trainign loss: 6.3252 avg training loss: 7.6193
batch: [9750/21305] batch time: 0.056 trainign loss: 7.1042 avg training loss: 7.6192
batch: [9760/21305] batch time: 0.056 trainign loss: 6.4402 avg training loss: 7.6191
batch: [9770/21305] batch time: 0.167 trainign loss: 6.6792 avg training loss: 7.6190
batch: [9780/21305] batch time: 0.056 trainign loss: 7.6443 avg training loss: 7.6189
batch: [9790/21305] batch time: 0.063 trainign loss: 6.6388 avg training loss: 7.6188
batch: [9800/21305] batch time: 0.340 trainign loss: 6.1502 avg training loss: 7.6187
batch: [9810/21305] batch time: 0.058 trainign loss: 7.5940 avg training loss: 7.6185
batch: [9820/21305] batch time: 0.053 trainign loss: 5.6732 avg training loss: 7.6184
batch: [9830/21305] batch time: 0.062 trainign loss: 5.7581 avg training loss: 7.6183
batch: [9840/21305] batch time: 0.146 trainign loss: 4.3692 avg training loss: 7.6181
batch: [9850/21305] batch time: 0.056 trainign loss: 7.2395 avg training loss: 7.6180
batch: [9860/21305] batch time: 0.051 trainign loss: 4.0792 avg training loss: 7.6179
batch: [9870/21305] batch time: 0.056 trainign loss: 4.8756 avg training loss: 7.6178
batch: [9880/21305] batch time: 0.056 trainign loss: 7.2501 avg training loss: 7.6176
batch: [9890/21305] batch time: 0.056 trainign loss: 6.6720 avg training loss: 7.6175
batch: [9900/21305] batch time: 1.129 trainign loss: 6.3978 avg training loss: 7.6174
batch: [9910/21305] batch time: 0.056 trainign loss: 6.6605 avg training loss: 7.6173
batch: [9920/21305] batch time: 0.059 trainign loss: 6.2354 avg training loss: 7.6172
batch: [9930/21305] batch time: 0.056 trainign loss: 5.7147 avg training loss: 7.6171
batch: [9940/21305] batch time: 0.315 trainign loss: 7.4721 avg training loss: 7.6170
batch: [9950/21305] batch time: 0.056 trainign loss: 6.4851 avg training loss: 7.6169
batch: [9960/21305] batch time: 0.173 trainign loss: 7.3000 avg training loss: 7.6168
batch: [9970/21305] batch time: 0.057 trainign loss: 6.8444 avg training loss: 7.6168
batch: [9980/21305] batch time: 0.381 trainign loss: 6.1964 avg training loss: 7.6167
batch: [9990/21305] batch time: 0.056 trainign loss: 6.6928 avg training loss: 7.6166
batch: [10000/21305] batch time: 0.205 trainign loss: 5.6872 avg training loss: 7.6165
batch: [10010/21305] batch time: 0.057 trainign loss: 6.5834 avg training loss: 7.6164
batch: [10020/21305] batch time: 0.559 trainign loss: 7.2004 avg training loss: 7.6163
batch: [10030/21305] batch time: 0.437 trainign loss: 6.0717 avg training loss: 7.6162
batch: [10040/21305] batch time: 0.063 trainign loss: 5.9674 avg training loss: 7.6161
batch: [10050/21305] batch time: 0.865 trainign loss: 6.0140 avg training loss: 7.6160
batch: [10060/21305] batch time: 0.615 trainign loss: 4.7043 avg training loss: 7.6158
batch: [10070/21305] batch time: 1.096 trainign loss: 6.6701 avg training loss: 7.6156
batch: [10080/21305] batch time: 1.243 trainign loss: 5.7661 avg training loss: 7.6156
batch: [10090/21305] batch time: 0.868 trainign loss: 8.1288 avg training loss: 7.6154
batch: [10100/21305] batch time: 1.861 trainign loss: 7.9390 avg training loss: 7.6153
batch: [10110/21305] batch time: 1.164 trainign loss: 6.4782 avg training loss: 7.6153
batch: [10120/21305] batch time: 1.780 trainign loss: 7.1491 avg training loss: 7.6151
batch: [10130/21305] batch time: 0.421 trainign loss: 6.4707 avg training loss: 7.6151
batch: [10140/21305] batch time: 1.915 trainign loss: 7.3698 avg training loss: 7.6150
batch: [10150/21305] batch time: 0.368 trainign loss: 6.7360 avg training loss: 7.6150
batch: [10160/21305] batch time: 1.386 trainign loss: 6.0971 avg training loss: 7.6149
batch: [10170/21305] batch time: 0.594 trainign loss: 7.0611 avg training loss: 7.6148
batch: [10180/21305] batch time: 2.179 trainign loss: 6.7717 avg training loss: 7.6147
batch: [10190/21305] batch time: 0.063 trainign loss: 6.8362 avg training loss: 7.6146
batch: [10200/21305] batch time: 2.251 trainign loss: 5.7396 avg training loss: 7.6145
batch: [10210/21305] batch time: 0.105 trainign loss: 6.5856 avg training loss: 7.6144
batch: [10220/21305] batch time: 2.168 trainign loss: 5.9044 avg training loss: 7.6142
batch: [10230/21305] batch time: 0.063 trainign loss: 5.3390 avg training loss: 7.6141
batch: [10240/21305] batch time: 1.078 trainign loss: 6.9610 avg training loss: 7.6140
batch: [10250/21305] batch time: 0.056 trainign loss: 7.2304 avg training loss: 7.6139
batch: [10260/21305] batch time: 1.844 trainign loss: 7.6002 avg training loss: 7.6138
batch: [10270/21305] batch time: 0.062 trainign loss: 7.3985 avg training loss: 7.6137
batch: [10280/21305] batch time: 1.801 trainign loss: 7.0288 avg training loss: 7.6137
batch: [10290/21305] batch time: 0.062 trainign loss: 6.6821 avg training loss: 7.6135
batch: [10300/21305] batch time: 1.620 trainign loss: 5.9825 avg training loss: 7.6134
batch: [10310/21305] batch time: 0.060 trainign loss: 5.7566 avg training loss: 7.6133
batch: [10320/21305] batch time: 1.186 trainign loss: 2.7664 avg training loss: 7.6131
batch: [10330/21305] batch time: 0.056 trainign loss: 7.1955 avg training loss: 7.6130
batch: [10340/21305] batch time: 1.478 trainign loss: 7.4031 avg training loss: 7.6130
batch: [10350/21305] batch time: 0.055 trainign loss: 7.5861 avg training loss: 7.6129
batch: [10360/21305] batch time: 2.098 trainign loss: 6.8680 avg training loss: 7.6129
batch: [10370/21305] batch time: 0.057 trainign loss: 6.4549 avg training loss: 7.6128
batch: [10380/21305] batch time: 1.968 trainign loss: 7.0581 avg training loss: 7.6127
batch: [10390/21305] batch time: 0.056 trainign loss: 7.0073 avg training loss: 7.6126
batch: [10400/21305] batch time: 2.048 trainign loss: 6.0615 avg training loss: 7.6125
batch: [10410/21305] batch time: 0.060 trainign loss: 6.6522 avg training loss: 7.6123
batch: [10420/21305] batch time: 1.819 trainign loss: 7.8228 avg training loss: 7.6123
batch: [10430/21305] batch time: 0.055 trainign loss: 5.7275 avg training loss: 7.6122
batch: [10440/21305] batch time: 1.728 trainign loss: 6.3151 avg training loss: 7.6120
batch: [10450/21305] batch time: 0.056 trainign loss: 6.7393 avg training loss: 7.6120
batch: [10460/21305] batch time: 1.208 trainign loss: 6.9806 avg training loss: 7.6119
batch: [10470/21305] batch time: 0.057 trainign loss: 5.8291 avg training loss: 7.6118
batch: [10480/21305] batch time: 1.139 trainign loss: 7.5774 avg training loss: 7.6116
batch: [10490/21305] batch time: 0.056 trainign loss: 7.4502 avg training loss: 7.6116
batch: [10500/21305] batch time: 1.104 trainign loss: 5.5511 avg training loss: 7.6115
batch: [10510/21305] batch time: 0.056 trainign loss: 4.5339 avg training loss: 7.6113
batch: [10520/21305] batch time: 1.544 trainign loss: 7.0247 avg training loss: 7.6112
batch: [10530/21305] batch time: 0.058 trainign loss: 6.8976 avg training loss: 7.6112
batch: [10540/21305] batch time: 1.190 trainign loss: 5.6679 avg training loss: 7.6111
batch: [10550/21305] batch time: 0.056 trainign loss: 5.1276 avg training loss: 7.6110
batch: [10560/21305] batch time: 0.130 trainign loss: 8.7642 avg training loss: 7.6106
batch: [10570/21305] batch time: 0.056 trainign loss: 7.2083 avg training loss: 7.6106
batch: [10580/21305] batch time: 0.057 trainign loss: 7.7438 avg training loss: 7.6106
batch: [10590/21305] batch time: 0.061 trainign loss: 7.4686 avg training loss: 7.6105
batch: [10600/21305] batch time: 0.052 trainign loss: 7.1478 avg training loss: 7.6105
batch: [10610/21305] batch time: 0.056 trainign loss: 4.4782 avg training loss: 7.6103
batch: [10620/21305] batch time: 0.051 trainign loss: 6.7987 avg training loss: 7.6102
batch: [10630/21305] batch time: 0.058 trainign loss: 6.5900 avg training loss: 7.6101
batch: [10640/21305] batch time: 0.056 trainign loss: 5.8248 avg training loss: 7.6100
batch: [10650/21305] batch time: 0.056 trainign loss: 7.9168 avg training loss: 7.6099
batch: [10660/21305] batch time: 0.051 trainign loss: 7.2837 avg training loss: 7.6098
batch: [10670/21305] batch time: 0.057 trainign loss: 4.8591 avg training loss: 7.6097
batch: [10680/21305] batch time: 0.052 trainign loss: 7.0106 avg training loss: 7.6096
batch: [10690/21305] batch time: 0.057 trainign loss: 6.4713 avg training loss: 7.6094
batch: [10700/21305] batch time: 0.052 trainign loss: 7.2727 avg training loss: 7.6093
batch: [10710/21305] batch time: 0.058 trainign loss: 6.4959 avg training loss: 7.6091
batch: [10720/21305] batch time: 0.053 trainign loss: 6.9891 avg training loss: 7.6089
batch: [10730/21305] batch time: 0.056 trainign loss: 7.7451 avg training loss: 7.6088
batch: [10740/21305] batch time: 0.051 trainign loss: 7.3590 avg training loss: 7.6088
batch: [10750/21305] batch time: 0.058 trainign loss: 6.7961 avg training loss: 7.6087
batch: [10760/21305] batch time: 0.056 trainign loss: 4.5116 avg training loss: 7.6086
batch: [10770/21305] batch time: 0.056 trainign loss: 6.7733 avg training loss: 7.6084
batch: [10780/21305] batch time: 0.051 trainign loss: 7.4204 avg training loss: 7.6084
batch: [10790/21305] batch time: 0.058 trainign loss: 6.7297 avg training loss: 7.6083
batch: [10800/21305] batch time: 0.057 trainign loss: 6.2472 avg training loss: 7.6083
batch: [10810/21305] batch time: 0.057 trainign loss: 6.7681 avg training loss: 7.6082
batch: [10820/21305] batch time: 0.051 trainign loss: 4.3207 avg training loss: 7.6080
batch: [10830/21305] batch time: 0.060 trainign loss: 1.7565 avg training loss: 7.6077
batch: [10840/21305] batch time: 0.050 trainign loss: 7.1597 avg training loss: 7.6077
batch: [10850/21305] batch time: 0.056 trainign loss: 7.0278 avg training loss: 7.6076
batch: [10860/21305] batch time: 0.051 trainign loss: 6.4962 avg training loss: 7.6076
batch: [10870/21305] batch time: 0.056 trainign loss: 5.7335 avg training loss: 7.6075
batch: [10880/21305] batch time: 0.056 trainign loss: 5.0710 avg training loss: 7.6073
batch: [10890/21305] batch time: 0.051 trainign loss: 6.8607 avg training loss: 7.6072
batch: [10900/21305] batch time: 0.056 trainign loss: 7.4643 avg training loss: 7.6071
batch: [10910/21305] batch time: 0.063 trainign loss: 7.0894 avg training loss: 7.6070
batch: [10920/21305] batch time: 0.062 trainign loss: 7.0082 avg training loss: 7.6069
batch: [10930/21305] batch time: 0.056 trainign loss: 5.1848 avg training loss: 7.6068
batch: [10940/21305] batch time: 0.062 trainign loss: 6.5559 avg training loss: 7.6066
batch: [10950/21305] batch time: 0.062 trainign loss: 5.8185 avg training loss: 7.6064
batch: [10960/21305] batch time: 0.050 trainign loss: 6.6203 avg training loss: 7.6064
batch: [10970/21305] batch time: 0.056 trainign loss: 6.7074 avg training loss: 7.6063
batch: [10980/21305] batch time: 0.056 trainign loss: 5.2940 avg training loss: 7.6061
batch: [10990/21305] batch time: 0.057 trainign loss: 7.2241 avg training loss: 7.6060
batch: [11000/21305] batch time: 0.058 trainign loss: 6.3501 avg training loss: 7.6059
batch: [11010/21305] batch time: 0.055 trainign loss: 5.8361 avg training loss: 7.6057
batch: [11020/21305] batch time: 0.056 trainign loss: 7.1686 avg training loss: 7.6057
batch: [11030/21305] batch time: 0.290 trainign loss: 5.9142 avg training loss: 7.6056
batch: [11040/21305] batch time: 0.056 trainign loss: 6.9587 avg training loss: 7.6054
batch: [11050/21305] batch time: 0.594 trainign loss: 6.5362 avg training loss: 7.6053
batch: [11060/21305] batch time: 0.056 trainign loss: 6.8324 avg training loss: 7.6052
batch: [11070/21305] batch time: 1.247 trainign loss: 5.4771 avg training loss: 7.6051
batch: [11080/21305] batch time: 0.155 trainign loss: 7.4081 avg training loss: 7.6050
batch: [11090/21305] batch time: 1.285 trainign loss: 7.4418 avg training loss: 7.6050
batch: [11100/21305] batch time: 0.063 trainign loss: 5.8390 avg training loss: 7.6048
batch: [11110/21305] batch time: 1.448 trainign loss: 6.9058 avg training loss: 7.6048
batch: [11120/21305] batch time: 0.187 trainign loss: 6.8961 avg training loss: 7.6047
batch: [11130/21305] batch time: 1.629 trainign loss: 5.0051 avg training loss: 7.6046
batch: [11140/21305] batch time: 0.342 trainign loss: 3.7729 avg training loss: 7.6044
batch: [11150/21305] batch time: 1.946 trainign loss: 6.9719 avg training loss: 7.6043
batch: [11160/21305] batch time: 0.374 trainign loss: 6.8750 avg training loss: 7.6042
batch: [11170/21305] batch time: 2.214 trainign loss: 3.3680 avg training loss: 7.6041
batch: [11180/21305] batch time: 0.061 trainign loss: 8.9054 avg training loss: 7.6038
batch: [11190/21305] batch time: 2.276 trainign loss: 7.5674 avg training loss: 7.6037
batch: [11200/21305] batch time: 0.056 trainign loss: 7.5909 avg training loss: 7.6037
batch: [11210/21305] batch time: 2.058 trainign loss: 5.4375 avg training loss: 7.6036
batch: [11220/21305] batch time: 0.056 trainign loss: 6.4737 avg training loss: 7.6035
batch: [11230/21305] batch time: 1.538 trainign loss: 5.8300 avg training loss: 7.6034
batch: [11240/21305] batch time: 0.056 trainign loss: 6.2142 avg training loss: 7.6033
batch: [11250/21305] batch time: 1.299 trainign loss: 1.1096 avg training loss: 7.6030
batch: [11260/21305] batch time: 0.063 trainign loss: 7.8626 avg training loss: 7.6030
batch: [11270/21305] batch time: 1.770 trainign loss: 6.6889 avg training loss: 7.6029
batch: [11280/21305] batch time: 0.062 trainign loss: 5.6492 avg training loss: 7.6028
batch: [11290/21305] batch time: 2.024 trainign loss: 6.9117 avg training loss: 7.6028
batch: [11300/21305] batch time: 0.057 trainign loss: 1.5965 avg training loss: 7.6025
batch: [11310/21305] batch time: 2.411 trainign loss: 6.6923 avg training loss: 7.6025
batch: [11320/21305] batch time: 0.056 trainign loss: 7.0066 avg training loss: 7.6023
batch: [11330/21305] batch time: 2.126 trainign loss: 7.6944 avg training loss: 7.6022
batch: [11340/21305] batch time: 0.062 trainign loss: 7.7563 avg training loss: 7.6022
batch: [11350/21305] batch time: 1.859 trainign loss: 7.3684 avg training loss: 7.6022
batch: [11360/21305] batch time: 0.247 trainign loss: 6.3639 avg training loss: 7.6021
batch: [11370/21305] batch time: 2.080 trainign loss: 6.1684 avg training loss: 7.6019
batch: [11380/21305] batch time: 1.136 trainign loss: 4.5111 avg training loss: 7.6018
batch: [11390/21305] batch time: 0.408 trainign loss: 7.4999 avg training loss: 7.6013
batch: [11400/21305] batch time: 1.820 trainign loss: 8.4715 avg training loss: 7.6014
batch: [11410/21305] batch time: 0.700 trainign loss: 7.2138 avg training loss: 7.6014
batch: [11420/21305] batch time: 2.073 trainign loss: 6.2306 avg training loss: 7.6013
batch: [11430/21305] batch time: 0.599 trainign loss: 6.8654 avg training loss: 7.6013
batch: [11440/21305] batch time: 2.155 trainign loss: 6.4776 avg training loss: 7.6012
batch: [11450/21305] batch time: 0.611 trainign loss: 6.2559 avg training loss: 7.6011
batch: [11460/21305] batch time: 2.046 trainign loss: 6.3775 avg training loss: 7.6010
batch: [11470/21305] batch time: 0.054 trainign loss: 6.9085 avg training loss: 7.6008
batch: [11480/21305] batch time: 2.526 trainign loss: 6.5734 avg training loss: 7.6007
batch: [11490/21305] batch time: 0.056 trainign loss: 7.3285 avg training loss: 7.6005
batch: [11500/21305] batch time: 2.150 trainign loss: 6.4386 avg training loss: 7.6005
batch: [11510/21305] batch time: 0.053 trainign loss: 6.0409 avg training loss: 7.6003
batch: [11520/21305] batch time: 1.971 trainign loss: 7.1819 avg training loss: 7.6002
batch: [11530/21305] batch time: 0.108 trainign loss: 5.7874 avg training loss: 7.6001
batch: [11540/21305] batch time: 1.642 trainign loss: 7.9249 avg training loss: 7.6000
batch: [11550/21305] batch time: 0.843 trainign loss: 4.7991 avg training loss: 7.5999
batch: [11560/21305] batch time: 2.052 trainign loss: 7.4775 avg training loss: 7.5999
batch: [11570/21305] batch time: 0.148 trainign loss: 5.6461 avg training loss: 7.5997
batch: [11580/21305] batch time: 2.034 trainign loss: 7.7329 avg training loss: 7.5997
batch: [11590/21305] batch time: 0.142 trainign loss: 6.9174 avg training loss: 7.5996
batch: [11600/21305] batch time: 2.493 trainign loss: 4.5796 avg training loss: 7.5995
batch: [11610/21305] batch time: 0.056 trainign loss: 6.0065 avg training loss: 7.5994
batch: [11620/21305] batch time: 2.161 trainign loss: 6.9947 avg training loss: 7.5992
batch: [11630/21305] batch time: 0.056 trainign loss: 7.6001 avg training loss: 7.5992
batch: [11640/21305] batch time: 2.257 trainign loss: 6.7935 avg training loss: 7.5991
batch: [11650/21305] batch time: 0.061 trainign loss: 7.4206 avg training loss: 7.5990
batch: [11660/21305] batch time: 2.322 trainign loss: 5.7684 avg training loss: 7.5989
batch: [11670/21305] batch time: 0.057 trainign loss: 6.9449 avg training loss: 7.5988
batch: [11680/21305] batch time: 2.270 trainign loss: 6.1099 avg training loss: 7.5987
batch: [11690/21305] batch time: 0.056 trainign loss: 6.1562 avg training loss: 7.5986
batch: [11700/21305] batch time: 2.161 trainign loss: 6.8589 avg training loss: 7.5984
batch: [11710/21305] batch time: 0.054 trainign loss: 5.3972 avg training loss: 7.5984
batch: [11720/21305] batch time: 2.400 trainign loss: 7.2633 avg training loss: 7.5983
batch: [11730/21305] batch time: 0.056 trainign loss: 5.6839 avg training loss: 7.5982
batch: [11740/21305] batch time: 2.121 trainign loss: 6.0229 avg training loss: 7.5981
batch: [11750/21305] batch time: 0.056 trainign loss: 7.0068 avg training loss: 7.5980
batch: [11760/21305] batch time: 2.122 trainign loss: 6.8672 avg training loss: 7.5979
batch: [11770/21305] batch time: 0.055 trainign loss: 6.7356 avg training loss: 7.5979
batch: [11780/21305] batch time: 2.124 trainign loss: 5.2158 avg training loss: 7.5977
batch: [11790/21305] batch time: 0.056 trainign loss: 1.1867 avg training loss: 7.5974
batch: [11800/21305] batch time: 2.408 trainign loss: 0.0009 avg training loss: 7.5968
batch: [11810/21305] batch time: 0.056 trainign loss: 7.3728 avg training loss: 7.5967
batch: [11820/21305] batch time: 2.084 trainign loss: 7.3754 avg training loss: 7.5967
batch: [11830/21305] batch time: 0.057 trainign loss: 7.7701 avg training loss: 7.5966
batch: [11840/21305] batch time: 2.143 trainign loss: 7.2813 avg training loss: 7.5966
batch: [11850/21305] batch time: 0.056 trainign loss: 6.1058 avg training loss: 7.5964
batch: [11860/21305] batch time: 2.431 trainign loss: 4.3566 avg training loss: 7.5963
batch: [11870/21305] batch time: 0.053 trainign loss: 6.1293 avg training loss: 7.5961
batch: [11880/21305] batch time: 2.142 trainign loss: 8.3085 avg training loss: 7.5958
batch: [11890/21305] batch time: 0.056 trainign loss: 7.8621 avg training loss: 7.5958
batch: [11900/21305] batch time: 2.079 trainign loss: 6.0203 avg training loss: 7.5958
batch: [11910/21305] batch time: 0.063 trainign loss: 7.5725 avg training loss: 7.5957
batch: [11920/21305] batch time: 2.219 trainign loss: 6.4996 avg training loss: 7.5956
batch: [11930/21305] batch time: 0.056 trainign loss: 5.4739 avg training loss: 7.5954
batch: [11940/21305] batch time: 2.545 trainign loss: 7.6208 avg training loss: 7.5954
batch: [11950/21305] batch time: 0.055 trainign loss: 6.8899 avg training loss: 7.5954
batch: [11960/21305] batch time: 2.163 trainign loss: 7.0331 avg training loss: 7.5953
batch: [11970/21305] batch time: 0.062 trainign loss: 6.4506 avg training loss: 7.5952
batch: [11980/21305] batch time: 2.102 trainign loss: 6.7404 avg training loss: 7.5951
batch: [11990/21305] batch time: 0.055 trainign loss: 5.9495 avg training loss: 7.5950
batch: [12000/21305] batch time: 2.275 trainign loss: 6.8051 avg training loss: 7.5949
batch: [12010/21305] batch time: 0.062 trainign loss: 7.6875 avg training loss: 7.5949
batch: [12020/21305] batch time: 2.353 trainign loss: 6.9534 avg training loss: 7.5948
batch: [12030/21305] batch time: 0.056 trainign loss: 6.0788 avg training loss: 7.5948
batch: [12040/21305] batch time: 2.016 trainign loss: 5.8346 avg training loss: 7.5946
batch: [12050/21305] batch time: 0.057 trainign loss: 6.3883 avg training loss: 7.5945
batch: [12060/21305] batch time: 2.150 trainign loss: 6.9498 avg training loss: 7.5944
batch: [12070/21305] batch time: 0.056 trainign loss: 3.4109 avg training loss: 7.5943
batch: [12080/21305] batch time: 2.117 trainign loss: 6.0393 avg training loss: 7.5939
batch: [12090/21305] batch time: 0.056 trainign loss: 6.0390 avg training loss: 7.5939
batch: [12100/21305] batch time: 2.405 trainign loss: 6.5912 avg training loss: 7.5938
batch: [12110/21305] batch time: 0.057 trainign loss: 7.0635 avg training loss: 7.5938
batch: [12120/21305] batch time: 2.567 trainign loss: 5.2741 avg training loss: 7.5936
batch: [12130/21305] batch time: 0.056 trainign loss: 7.0030 avg training loss: 7.5934
batch: [12140/21305] batch time: 2.331 trainign loss: 5.5758 avg training loss: 7.5933
batch: [12150/21305] batch time: 0.053 trainign loss: 7.3090 avg training loss: 7.5932
batch: [12160/21305] batch time: 2.144 trainign loss: 7.3694 avg training loss: 7.5932
batch: [12170/21305] batch time: 0.061 trainign loss: 6.8877 avg training loss: 7.5931
batch: [12180/21305] batch time: 2.460 trainign loss: 6.8845 avg training loss: 7.5931
batch: [12190/21305] batch time: 0.054 trainign loss: 7.2080 avg training loss: 7.5931
batch: [12200/21305] batch time: 2.490 trainign loss: 6.2142 avg training loss: 7.5930
batch: [12210/21305] batch time: 0.062 trainign loss: 7.1936 avg training loss: 7.5929
batch: [12220/21305] batch time: 2.047 trainign loss: 6.7470 avg training loss: 7.5928
batch: [12230/21305] batch time: 0.056 trainign loss: 7.1613 avg training loss: 7.5928
batch: [12240/21305] batch time: 2.244 trainign loss: 6.4215 avg training loss: 7.5927
batch: [12250/21305] batch time: 0.062 trainign loss: 5.9872 avg training loss: 7.5926
batch: [12260/21305] batch time: 2.396 trainign loss: 6.8569 avg training loss: 7.5925
batch: [12270/21305] batch time: 0.053 trainign loss: 5.5302 avg training loss: 7.5924
batch: [12280/21305] batch time: 2.148 trainign loss: 4.4092 avg training loss: 7.5922
batch: [12290/21305] batch time: 0.063 trainign loss: 6.9753 avg training loss: 7.5921
batch: [12300/21305] batch time: 2.617 trainign loss: 7.4878 avg training loss: 7.5921
batch: [12310/21305] batch time: 0.056 trainign loss: 6.9044 avg training loss: 7.5920
batch: [12320/21305] batch time: 2.047 trainign loss: 6.4278 avg training loss: 7.5919
batch: [12330/21305] batch time: 0.055 trainign loss: 7.1077 avg training loss: 7.5919
batch: [12340/21305] batch time: 2.025 trainign loss: 6.8829 avg training loss: 7.5918
batch: [12350/21305] batch time: 0.063 trainign loss: 7.1437 avg training loss: 7.5917
batch: [12360/21305] batch time: 2.344 trainign loss: 5.6367 avg training loss: 7.5917
batch: [12370/21305] batch time: 0.057 trainign loss: 5.2929 avg training loss: 7.5915
batch: [12380/21305] batch time: 2.242 trainign loss: 6.7441 avg training loss: 7.5914
batch: [12390/21305] batch time: 0.057 trainign loss: 7.7362 avg training loss: 7.5914
batch: [12400/21305] batch time: 2.007 trainign loss: 6.2725 avg training loss: 7.5913
batch: [12410/21305] batch time: 0.056 trainign loss: 7.2447 avg training loss: 7.5912
batch: [12420/21305] batch time: 1.875 trainign loss: 5.6358 avg training loss: 7.5911
batch: [12430/21305] batch time: 0.062 trainign loss: 5.3905 avg training loss: 7.5910
batch: [12440/21305] batch time: 2.335 trainign loss: 7.3493 avg training loss: 7.5908
batch: [12450/21305] batch time: 0.056 trainign loss: 7.9238 avg training loss: 7.5908
batch: [12460/21305] batch time: 2.205 trainign loss: 7.4530 avg training loss: 7.5908
batch: [12470/21305] batch time: 0.055 trainign loss: 5.8948 avg training loss: 7.5907
batch: [12480/21305] batch time: 2.655 trainign loss: 5.8948 avg training loss: 7.5906
batch: [12490/21305] batch time: 0.060 trainign loss: 6.2510 avg training loss: 7.5905
batch: [12500/21305] batch time: 2.071 trainign loss: 7.1632 avg training loss: 7.5903
batch: [12510/21305] batch time: 0.056 trainign loss: 7.6124 avg training loss: 7.5903
batch: [12520/21305] batch time: 1.615 trainign loss: 7.2017 avg training loss: 7.5903
batch: [12530/21305] batch time: 0.107 trainign loss: 6.7284 avg training loss: 7.5902
batch: [12540/21305] batch time: 2.154 trainign loss: 5.0939 avg training loss: 7.5900
batch: [12550/21305] batch time: 0.061 trainign loss: 7.0524 avg training loss: 7.5899
batch: [12560/21305] batch time: 1.906 trainign loss: 3.4229 avg training loss: 7.5897
batch: [12570/21305] batch time: 0.053 trainign loss: 0.0035 avg training loss: 7.5891
batch: [12580/21305] batch time: 2.381 trainign loss: 8.0960 avg training loss: 7.5892
batch: [12590/21305] batch time: 0.056 trainign loss: 3.5587 avg training loss: 7.5891
batch: [12600/21305] batch time: 2.125 trainign loss: 7.9258 avg training loss: 7.5890
batch: [12610/21305] batch time: 0.056 trainign loss: 4.4318 avg training loss: 7.5890
batch: [12620/21305] batch time: 2.165 trainign loss: 6.8337 avg training loss: 7.5889
batch: [12630/21305] batch time: 0.240 trainign loss: 6.6480 avg training loss: 7.5889
batch: [12640/21305] batch time: 2.162 trainign loss: 7.0649 avg training loss: 7.5888
batch: [12650/21305] batch time: 0.056 trainign loss: 5.5760 avg training loss: 7.5887
batch: [12660/21305] batch time: 2.323 trainign loss: 6.5271 avg training loss: 7.5886
batch: [12670/21305] batch time: 0.075 trainign loss: 7.4464 avg training loss: 7.5886
batch: [12680/21305] batch time: 2.087 trainign loss: 4.6105 avg training loss: 7.5885
batch: [12690/21305] batch time: 0.384 trainign loss: 7.4930 avg training loss: 7.5883
batch: [12700/21305] batch time: 2.124 trainign loss: 6.8966 avg training loss: 7.5882
batch: [12710/21305] batch time: 0.276 trainign loss: 5.8287 avg training loss: 7.5881
batch: [12720/21305] batch time: 2.200 trainign loss: 5.6734 avg training loss: 7.5880
batch: [12730/21305] batch time: 0.228 trainign loss: 5.6356 avg training loss: 7.5878
batch: [12740/21305] batch time: 2.092 trainign loss: 5.2207 avg training loss: 7.5877
batch: [12750/21305] batch time: 0.056 trainign loss: 3.6147 avg training loss: 7.5875
batch: [12760/21305] batch time: 1.794 trainign loss: 6.6254 avg training loss: 7.5874
batch: [12770/21305] batch time: 0.056 trainign loss: 6.0301 avg training loss: 7.5873
batch: [12780/21305] batch time: 2.147 trainign loss: 7.5686 avg training loss: 7.5873
batch: [12790/21305] batch time: 0.056 trainign loss: 5.4084 avg training loss: 7.5872
batch: [12800/21305] batch time: 2.502 trainign loss: 7.2150 avg training loss: 7.5871
batch: [12810/21305] batch time: 0.057 trainign loss: 7.1709 avg training loss: 7.5871
batch: [12820/21305] batch time: 2.639 trainign loss: 6.2617 avg training loss: 7.5869
batch: [12830/21305] batch time: 0.061 trainign loss: 4.3729 avg training loss: 7.5868
batch: [12840/21305] batch time: 2.152 trainign loss: 6.4009 avg training loss: 7.5867
batch: [12850/21305] batch time: 0.056 trainign loss: 7.0341 avg training loss: 7.5866
batch: [12860/21305] batch time: 2.109 trainign loss: 6.7912 avg training loss: 7.5865
batch: [12870/21305] batch time: 0.056 trainign loss: 6.5456 avg training loss: 7.5864
batch: [12880/21305] batch time: 2.402 trainign loss: 5.9285 avg training loss: 7.5863
batch: [12890/21305] batch time: 0.060 trainign loss: 6.1725 avg training loss: 7.5862
batch: [12900/21305] batch time: 1.936 trainign loss: 7.2122 avg training loss: 7.5862
batch: [12910/21305] batch time: 0.056 trainign loss: 6.2807 avg training loss: 7.5861
batch: [12920/21305] batch time: 1.933 trainign loss: 6.3612 avg training loss: 7.5860
batch: [12930/21305] batch time: 0.058 trainign loss: 6.0951 avg training loss: 7.5859
batch: [12940/21305] batch time: 0.914 trainign loss: 6.3987 avg training loss: 7.5858
batch: [12950/21305] batch time: 0.060 trainign loss: 5.0857 avg training loss: 7.5856
batch: [12960/21305] batch time: 0.599 trainign loss: 6.5316 avg training loss: 7.5855
batch: [12970/21305] batch time: 0.057 trainign loss: 4.9974 avg training loss: 7.5854
batch: [12980/21305] batch time: 1.507 trainign loss: 7.0509 avg training loss: 7.5852
batch: [12990/21305] batch time: 0.057 trainign loss: 6.3676 avg training loss: 7.5851
batch: [13000/21305] batch time: 1.136 trainign loss: 7.0183 avg training loss: 7.5851
batch: [13010/21305] batch time: 0.063 trainign loss: 6.4722 avg training loss: 7.5849
batch: [13020/21305] batch time: 1.416 trainign loss: 7.6479 avg training loss: 7.5848
batch: [13030/21305] batch time: 0.056 trainign loss: 6.9939 avg training loss: 7.5848
batch: [13040/21305] batch time: 1.267 trainign loss: 7.0294 avg training loss: 7.5847
batch: [13050/21305] batch time: 0.062 trainign loss: 2.2179 avg training loss: 7.5845
batch: [13060/21305] batch time: 1.491 trainign loss: 6.8032 avg training loss: 7.5843
batch: [13070/21305] batch time: 0.056 trainign loss: 6.9194 avg training loss: 7.5843
batch: [13080/21305] batch time: 0.779 trainign loss: 6.0589 avg training loss: 7.5842
batch: [13090/21305] batch time: 0.062 trainign loss: 4.8915 avg training loss: 7.5840
batch: [13100/21305] batch time: 1.335 trainign loss: 7.5971 avg training loss: 7.5838
batch: [13110/21305] batch time: 0.063 trainign loss: 6.8014 avg training loss: 7.5836
batch: [13120/21305] batch time: 1.735 trainign loss: 6.8786 avg training loss: 7.5836
batch: [13130/21305] batch time: 0.056 trainign loss: 5.7535 avg training loss: 7.5836
batch: [13140/21305] batch time: 2.508 trainign loss: 1.9411 avg training loss: 7.5834
batch: [13150/21305] batch time: 0.056 trainign loss: 7.4289 avg training loss: 7.5833
batch: [13160/21305] batch time: 2.234 trainign loss: 5.7266 avg training loss: 7.5832
batch: [13170/21305] batch time: 0.051 trainign loss: 7.0217 avg training loss: 7.5831
batch: [13180/21305] batch time: 1.983 trainign loss: 5.2947 avg training loss: 7.5830
batch: [13190/21305] batch time: 0.056 trainign loss: 6.4568 avg training loss: 7.5829
batch: [13200/21305] batch time: 1.929 trainign loss: 3.4548 avg training loss: 7.5827
batch: [13210/21305] batch time: 0.056 trainign loss: 0.0016 avg training loss: 7.5821
batch: [13220/21305] batch time: 1.551 trainign loss: 0.0001 avg training loss: 7.5815
batch: [13230/21305] batch time: 0.056 trainign loss: 0.0000 avg training loss: 7.5808
batch: [13240/21305] batch time: 1.333 trainign loss: 8.3292 avg training loss: 7.5809
batch: [13250/21305] batch time: 0.056 trainign loss: 7.9863 avg training loss: 7.5810
batch: [13260/21305] batch time: 2.187 trainign loss: 7.8291 avg training loss: 7.5810
batch: [13270/21305] batch time: 0.056 trainign loss: 6.7996 avg training loss: 7.5809
batch: [13280/21305] batch time: 1.833 trainign loss: 7.6110 avg training loss: 7.5808
batch: [13290/21305] batch time: 0.056 trainign loss: 6.6506 avg training loss: 7.5808
batch: [13300/21305] batch time: 0.590 trainign loss: 7.2033 avg training loss: 7.5807
batch: [13310/21305] batch time: 0.056 trainign loss: 5.1869 avg training loss: 7.5805
batch: [13320/21305] batch time: 1.859 trainign loss: 5.1362 avg training loss: 7.5803
batch: [13330/21305] batch time: 0.948 trainign loss: 7.1194 avg training loss: 7.5802
batch: [13340/21305] batch time: 1.700 trainign loss: 7.3617 avg training loss: 7.5801
batch: [13350/21305] batch time: 0.058 trainign loss: 6.1657 avg training loss: 7.5801
batch: [13360/21305] batch time: 1.763 trainign loss: 7.8891 avg training loss: 7.5800
batch: [13370/21305] batch time: 0.062 trainign loss: 5.8779 avg training loss: 7.5799
batch: [13380/21305] batch time: 1.349 trainign loss: 4.9892 avg training loss: 7.5798
batch: [13390/21305] batch time: 0.061 trainign loss: 6.1962 avg training loss: 7.5796
batch: [13400/21305] batch time: 1.405 trainign loss: 5.0227 avg training loss: 7.5795
batch: [13410/21305] batch time: 0.056 trainign loss: 7.4094 avg training loss: 7.5795
batch: [13420/21305] batch time: 0.056 trainign loss: 6.7401 avg training loss: 7.5795
batch: [13430/21305] batch time: 0.063 trainign loss: 6.7606 avg training loss: 7.5794
batch: [13440/21305] batch time: 0.192 trainign loss: 5.8947 avg training loss: 7.5793
batch: [13450/21305] batch time: 0.056 trainign loss: 7.0550 avg training loss: 7.5793
batch: [13460/21305] batch time: 0.051 trainign loss: 6.4230 avg training loss: 7.5792
batch: [13470/21305] batch time: 0.062 trainign loss: 3.6995 avg training loss: 7.5790
batch: [13480/21305] batch time: 0.053 trainign loss: 4.3256 avg training loss: 7.5789
batch: [13490/21305] batch time: 0.062 trainign loss: 6.8705 avg training loss: 7.5788
batch: [13500/21305] batch time: 0.052 trainign loss: 6.4881 avg training loss: 7.5787
batch: [13510/21305] batch time: 0.063 trainign loss: 5.4488 avg training loss: 7.5785
batch: [13520/21305] batch time: 0.056 trainign loss: 6.4095 avg training loss: 7.5784
batch: [13530/21305] batch time: 0.056 trainign loss: 6.2137 avg training loss: 7.5783
batch: [13540/21305] batch time: 0.057 trainign loss: 3.5577 avg training loss: 7.5782
batch: [13550/21305] batch time: 0.060 trainign loss: 7.1458 avg training loss: 7.5781
batch: [13560/21305] batch time: 0.052 trainign loss: 6.5790 avg training loss: 7.5781
batch: [13570/21305] batch time: 0.058 trainign loss: 6.6497 avg training loss: 7.5779
batch: [13580/21305] batch time: 0.058 trainign loss: 7.4306 avg training loss: 7.5778
batch: [13590/21305] batch time: 0.056 trainign loss: 5.3948 avg training loss: 7.5778
batch: [13600/21305] batch time: 0.057 trainign loss: 6.3234 avg training loss: 7.5777
batch: [13610/21305] batch time: 0.058 trainign loss: 7.7730 avg training loss: 7.5775
batch: [13620/21305] batch time: 0.056 trainign loss: 6.8517 avg training loss: 7.5775
batch: [13630/21305] batch time: 0.056 trainign loss: 6.7893 avg training loss: 7.5774
batch: [13640/21305] batch time: 0.056 trainign loss: 6.9181 avg training loss: 7.5773
batch: [13650/21305] batch time: 0.505 trainign loss: 6.1066 avg training loss: 7.5773
batch: [13660/21305] batch time: 0.056 trainign loss: 5.8109 avg training loss: 7.5771
batch: [13670/21305] batch time: 0.056 trainign loss: 6.5900 avg training loss: 7.5770
batch: [13680/21305] batch time: 0.056 trainign loss: 5.6170 avg training loss: 7.5769
batch: [13690/21305] batch time: 0.532 trainign loss: 3.9898 avg training loss: 7.5767
batch: [13700/21305] batch time: 0.061 trainign loss: 7.9614 avg training loss: 7.5767
batch: [13710/21305] batch time: 1.426 trainign loss: 6.7780 avg training loss: 7.5767
batch: [13720/21305] batch time: 0.060 trainign loss: 6.5689 avg training loss: 7.5766
batch: [13730/21305] batch time: 1.062 trainign loss: 7.2640 avg training loss: 7.5765
batch: [13740/21305] batch time: 0.057 trainign loss: 5.3532 avg training loss: 7.5764
batch: [13750/21305] batch time: 1.169 trainign loss: 7.4565 avg training loss: 7.5763
batch: [13760/21305] batch time: 0.059 trainign loss: 7.3504 avg training loss: 7.5763
batch: [13770/21305] batch time: 1.431 trainign loss: 6.8964 avg training loss: 7.5762
batch: [13780/21305] batch time: 0.055 trainign loss: 5.7251 avg training loss: 7.5760
batch: [13790/21305] batch time: 2.018 trainign loss: 7.1799 avg training loss: 7.5760
batch: [13800/21305] batch time: 0.056 trainign loss: 6.0844 avg training loss: 7.5759
batch: [13810/21305] batch time: 1.949 trainign loss: 6.6225 avg training loss: 7.5758
batch: [13820/21305] batch time: 0.056 trainign loss: 7.2694 avg training loss: 7.5757
batch: [13830/21305] batch time: 1.791 trainign loss: 5.9680 avg training loss: 7.5756
batch: [13840/21305] batch time: 0.054 trainign loss: 6.0470 avg training loss: 7.5755
batch: [13850/21305] batch time: 1.783 trainign loss: 1.1057 avg training loss: 7.5752
batch: [13860/21305] batch time: 0.052 trainign loss: 6.3453 avg training loss: 7.5751
batch: [13870/21305] batch time: 1.674 trainign loss: 7.8142 avg training loss: 7.5750
batch: [13880/21305] batch time: 0.056 trainign loss: 6.2609 avg training loss: 7.5749
batch: [13890/21305] batch time: 1.994 trainign loss: 7.1158 avg training loss: 7.5748
batch: [13900/21305] batch time: 0.056 trainign loss: 6.2773 avg training loss: 7.5748
batch: [13910/21305] batch time: 2.559 trainign loss: 6.9483 avg training loss: 7.5746
batch: [13920/21305] batch time: 0.056 trainign loss: 4.6154 avg training loss: 7.5745
batch: [13930/21305] batch time: 2.089 trainign loss: 6.1696 avg training loss: 7.5742
batch: [13940/21305] batch time: 0.056 trainign loss: 7.8779 avg training loss: 7.5742
batch: [13950/21305] batch time: 2.317 trainign loss: 5.1778 avg training loss: 7.5742
batch: [13960/21305] batch time: 0.063 trainign loss: 6.1651 avg training loss: 7.5741
batch: [13970/21305] batch time: 2.451 trainign loss: 7.3153 avg training loss: 7.5741
batch: [13980/21305] batch time: 0.056 trainign loss: 6.9545 avg training loss: 7.5740
batch: [13990/21305] batch time: 2.317 trainign loss: 6.7509 avg training loss: 7.5739
batch: [14000/21305] batch time: 0.055 trainign loss: 6.4230 avg training loss: 7.5737
batch: [14010/21305] batch time: 2.361 trainign loss: 4.8655 avg training loss: 7.5735
batch: [14020/21305] batch time: 0.063 trainign loss: 7.7051 avg training loss: 7.5733
batch: [14030/21305] batch time: 2.364 trainign loss: 7.0999 avg training loss: 7.5733
batch: [14040/21305] batch time: 0.061 trainign loss: 7.4849 avg training loss: 7.5733
batch: [14050/21305] batch time: 2.702 trainign loss: 7.0741 avg training loss: 7.5732
batch: [14060/21305] batch time: 0.057 trainign loss: 6.3445 avg training loss: 7.5732
batch: [14070/21305] batch time: 2.217 trainign loss: 6.9203 avg training loss: 7.5731
batch: [14080/21305] batch time: 0.056 trainign loss: 6.8228 avg training loss: 7.5730
batch: [14090/21305] batch time: 2.057 trainign loss: 7.0484 avg training loss: 7.5730
batch: [14100/21305] batch time: 0.054 trainign loss: 6.6319 avg training loss: 7.5729
batch: [14110/21305] batch time: 2.074 trainign loss: 6.3931 avg training loss: 7.5728
batch: [14120/21305] batch time: 0.062 trainign loss: 7.5230 avg training loss: 7.5727
batch: [14130/21305] batch time: 2.283 trainign loss: 5.9452 avg training loss: 7.5725
batch: [14140/21305] batch time: 0.060 trainign loss: 6.3931 avg training loss: 7.5723
batch: [14150/21305] batch time: 1.932 trainign loss: 6.5671 avg training loss: 7.5723
batch: [14160/21305] batch time: 0.056 trainign loss: 6.6839 avg training loss: 7.5721
batch: [14170/21305] batch time: 2.216 trainign loss: 6.5648 avg training loss: 7.5721
batch: [14180/21305] batch time: 0.057 trainign loss: 6.1204 avg training loss: 7.5720
batch: [14190/21305] batch time: 2.245 trainign loss: 2.2746 avg training loss: 7.5717
batch: [14200/21305] batch time: 0.056 trainign loss: 7.3275 avg training loss: 7.5717
batch: [14210/21305] batch time: 1.079 trainign loss: 6.1989 avg training loss: 7.5716
batch: [14220/21305] batch time: 0.056 trainign loss: 7.8975 avg training loss: 7.5714
batch: [14230/21305] batch time: 1.051 trainign loss: 6.3004 avg training loss: 7.5714
batch: [14240/21305] batch time: 0.063 trainign loss: 7.4325 avg training loss: 7.5713
batch: [14250/21305] batch time: 0.584 trainign loss: 7.0867 avg training loss: 7.5713
batch: [14260/21305] batch time: 0.056 trainign loss: 5.1509 avg training loss: 7.5712
batch: [14270/21305] batch time: 0.051 trainign loss: 5.2952 avg training loss: 7.5710
batch: [14280/21305] batch time: 0.056 trainign loss: 5.3935 avg training loss: 7.5710
batch: [14290/21305] batch time: 0.739 trainign loss: 6.5265 avg training loss: 7.5709
batch: [14300/21305] batch time: 0.058 trainign loss: 7.0108 avg training loss: 7.5708
batch: [14310/21305] batch time: 1.653 trainign loss: 6.6386 avg training loss: 7.5707
batch: [14320/21305] batch time: 0.061 trainign loss: 2.4322 avg training loss: 7.5705
batch: [14330/21305] batch time: 1.646 trainign loss: 7.0034 avg training loss: 7.5704
batch: [14340/21305] batch time: 0.057 trainign loss: 6.2765 avg training loss: 7.5704
batch: [14350/21305] batch time: 2.517 trainign loss: 5.5747 avg training loss: 7.5703
batch: [14360/21305] batch time: 0.063 trainign loss: 6.0063 avg training loss: 7.5702
batch: [14370/21305] batch time: 2.647 trainign loss: 6.0324 avg training loss: 7.5701
batch: [14380/21305] batch time: 0.061 trainign loss: 6.3698 avg training loss: 7.5701
batch: [14390/21305] batch time: 2.227 trainign loss: 6.8831 avg training loss: 7.5700
batch: [14400/21305] batch time: 0.056 trainign loss: 6.2592 avg training loss: 7.5699
batch: [14410/21305] batch time: 2.586 trainign loss: 6.4305 avg training loss: 7.5698
batch: [14420/21305] batch time: 0.054 trainign loss: 6.3373 avg training loss: 7.5696
batch: [14430/21305] batch time: 2.599 trainign loss: 6.6579 avg training loss: 7.5696
batch: [14440/21305] batch time: 0.055 trainign loss: 6.9542 avg training loss: 7.5695
batch: [14450/21305] batch time: 2.659 trainign loss: 5.4754 avg training loss: 7.5694
batch: [14460/21305] batch time: 0.059 trainign loss: 6.7829 avg training loss: 7.5693
batch: [14470/21305] batch time: 2.433 trainign loss: 5.8194 avg training loss: 7.5692
batch: [14480/21305] batch time: 0.056 trainign loss: 4.7752 avg training loss: 7.5691
batch: [14490/21305] batch time: 2.397 trainign loss: 5.5111 avg training loss: 7.5689
batch: [14500/21305] batch time: 0.055 trainign loss: 1.6087 avg training loss: 7.5687
batch: [14510/21305] batch time: 1.856 trainign loss: 0.0013 avg training loss: 7.5681
batch: [14520/21305] batch time: 0.060 trainign loss: 0.0002 avg training loss: 7.5674
batch: [14530/21305] batch time: 1.995 trainign loss: 0.0001 avg training loss: 7.5668
batch: [14540/21305] batch time: 0.051 trainign loss: 0.0001 avg training loss: 7.5662
batch: [14550/21305] batch time: 1.980 trainign loss: 0.0000 avg training loss: 7.5656
batch: [14560/21305] batch time: 0.057 trainign loss: 0.0000 avg training loss: 7.5649
batch: [14570/21305] batch time: 1.929 trainign loss: 0.0001 avg training loss: 7.5643
batch: [14580/21305] batch time: 0.057 trainign loss: 0.0000 avg training loss: 7.5637
batch: [14590/21305] batch time: 2.332 trainign loss: 0.0000 avg training loss: 7.5631
batch: [14600/21305] batch time: 0.058 trainign loss: 6.9821 avg training loss: 7.5629
batch: [14610/21305] batch time: 2.190 trainign loss: 8.1936 avg training loss: 7.5630
batch: [14620/21305] batch time: 0.057 trainign loss: 7.1357 avg training loss: 7.5630
batch: [14630/21305] batch time: 2.227 trainign loss: 6.1883 avg training loss: 7.5628
batch: [14640/21305] batch time: 0.055 trainign loss: 5.5277 avg training loss: 7.5628
batch: [14650/21305] batch time: 2.276 trainign loss: 7.0006 avg training loss: 7.5627
batch: [14660/21305] batch time: 0.056 trainign loss: 5.9652 avg training loss: 7.5626
batch: [14670/21305] batch time: 2.476 trainign loss: 4.9680 avg training loss: 7.5625
batch: [14680/21305] batch time: 0.056 trainign loss: 2.0383 avg training loss: 7.5622
batch: [14690/21305] batch time: 2.521 trainign loss: 6.7910 avg training loss: 7.5621
batch: [14700/21305] batch time: 0.056 trainign loss: 4.7651 avg training loss: 7.5620
batch: [14710/21305] batch time: 2.054 trainign loss: 0.0072 avg training loss: 7.5615
batch: [14720/21305] batch time: 0.056 trainign loss: 7.2846 avg training loss: 7.5614
batch: [14730/21305] batch time: 2.289 trainign loss: 5.5358 avg training loss: 7.5613
batch: [14740/21305] batch time: 0.055 trainign loss: 5.5144 avg training loss: 7.5612
batch: [14750/21305] batch time: 1.977 trainign loss: 7.8818 avg training loss: 7.5612
batch: [14760/21305] batch time: 0.062 trainign loss: 7.4996 avg training loss: 7.5612
batch: [14770/21305] batch time: 2.572 trainign loss: 6.8452 avg training loss: 7.5611
batch: [14780/21305] batch time: 0.058 trainign loss: 6.4072 avg training loss: 7.5611
batch: [14790/21305] batch time: 2.602 trainign loss: 7.0115 avg training loss: 7.5610
batch: [14800/21305] batch time: 0.056 trainign loss: 6.6451 avg training loss: 7.5609
batch: [14810/21305] batch time: 2.258 trainign loss: 5.9641 avg training loss: 7.5608
batch: [14820/21305] batch time: 0.053 trainign loss: 6.9879 avg training loss: 7.5607
batch: [14830/21305] batch time: 2.279 trainign loss: 6.4798 avg training loss: 7.5607
batch: [14840/21305] batch time: 0.062 trainign loss: 5.6545 avg training loss: 7.5606
batch: [14850/21305] batch time: 2.341 trainign loss: 7.0431 avg training loss: 7.5605
batch: [14860/21305] batch time: 0.060 trainign loss: 7.1508 avg training loss: 7.5604
batch: [14870/21305] batch time: 1.974 trainign loss: 6.2473 avg training loss: 7.5604
batch: [14880/21305] batch time: 0.176 trainign loss: 6.6660 avg training loss: 7.5603
batch: [14890/21305] batch time: 1.147 trainign loss: 6.4331 avg training loss: 7.5602
batch: [14900/21305] batch time: 2.266 trainign loss: 6.2805 avg training loss: 7.5601
batch: [14910/21305] batch time: 0.346 trainign loss: 2.9218 avg training loss: 7.5599
batch: [14920/21305] batch time: 1.948 trainign loss: 6.0864 avg training loss: 7.5599
batch: [14930/21305] batch time: 0.608 trainign loss: 6.1592 avg training loss: 7.5598
batch: [14940/21305] batch time: 1.895 trainign loss: 6.6584 avg training loss: 7.5596
batch: [14950/21305] batch time: 0.296 trainign loss: 6.9048 avg training loss: 7.5596
batch: [14960/21305] batch time: 2.199 trainign loss: 7.2479 avg training loss: 7.5594
batch: [14970/21305] batch time: 0.056 trainign loss: 7.7840 avg training loss: 7.5593
batch: [14980/21305] batch time: 2.216 trainign loss: 7.7915 avg training loss: 7.5593
batch: [14990/21305] batch time: 0.056 trainign loss: 5.3759 avg training loss: 7.5591
batch: [15000/21305] batch time: 1.868 trainign loss: 5.8633 avg training loss: 7.5590
batch: [15010/21305] batch time: 0.053 trainign loss: 7.8144 avg training loss: 7.5590
batch: [15020/21305] batch time: 2.019 trainign loss: 6.1250 avg training loss: 7.5589
batch: [15030/21305] batch time: 0.063 trainign loss: 7.0928 avg training loss: 7.5588
batch: [15040/21305] batch time: 2.214 trainign loss: 2.6577 avg training loss: 7.5586
batch: [15050/21305] batch time: 0.055 trainign loss: 7.4204 avg training loss: 7.5586
batch: [15060/21305] batch time: 3.002 trainign loss: 6.2602 avg training loss: 7.5585
batch: [15070/21305] batch time: 0.052 trainign loss: 6.0020 avg training loss: 7.5584
batch: [15080/21305] batch time: 2.136 trainign loss: 3.6731 avg training loss: 7.5580
batch: [15090/21305] batch time: 0.056 trainign loss: 7.6495 avg training loss: 7.5581
batch: [15100/21305] batch time: 1.951 trainign loss: 5.7599 avg training loss: 7.5581
batch: [15110/21305] batch time: 0.056 trainign loss: 3.4207 avg training loss: 7.5579
batch: [15120/21305] batch time: 2.601 trainign loss: 7.4883 avg training loss: 7.5579
batch: [15130/21305] batch time: 0.054 trainign loss: 6.7532 avg training loss: 7.5577
batch: [15140/21305] batch time: 2.193 trainign loss: 7.6542 avg training loss: 7.5577
batch: [15150/21305] batch time: 0.062 trainign loss: 6.8785 avg training loss: 7.5577
batch: [15160/21305] batch time: 1.475 trainign loss: 6.8371 avg training loss: 7.5576
batch: [15170/21305] batch time: 0.056 trainign loss: 6.5604 avg training loss: 7.5575
batch: [15180/21305] batch time: 0.821 trainign loss: 5.2543 avg training loss: 7.5574
batch: [15190/21305] batch time: 0.057 trainign loss: 6.9112 avg training loss: 7.5573
batch: [15200/21305] batch time: 1.484 trainign loss: 7.3970 avg training loss: 7.5573
batch: [15210/21305] batch time: 0.062 trainign loss: 5.8528 avg training loss: 7.5571
batch: [15220/21305] batch time: 1.592 trainign loss: 6.8074 avg training loss: 7.5570
batch: [15230/21305] batch time: 0.376 trainign loss: 7.1749 avg training loss: 7.5570
batch: [15240/21305] batch time: 1.558 trainign loss: 6.6628 avg training loss: 7.5569
batch: [15250/21305] batch time: 0.349 trainign loss: 7.0078 avg training loss: 7.5568
batch: [15260/21305] batch time: 2.479 trainign loss: 5.8811 avg training loss: 7.5567
batch: [15270/21305] batch time: 0.062 trainign loss: 7.4296 avg training loss: 7.5566
batch: [15280/21305] batch time: 2.505 trainign loss: 5.9515 avg training loss: 7.5565
batch: [15290/21305] batch time: 0.056 trainign loss: 7.4201 avg training loss: 7.5564
batch: [15300/21305] batch time: 2.106 trainign loss: 6.6913 avg training loss: 7.5563
batch: [15310/21305] batch time: 0.061 trainign loss: 6.2457 avg training loss: 7.5562
batch: [15320/21305] batch time: 2.355 trainign loss: 7.5097 avg training loss: 7.5561
batch: [15330/21305] batch time: 0.057 trainign loss: 6.9887 avg training loss: 7.5561
batch: [15340/21305] batch time: 2.428 trainign loss: 5.5601 avg training loss: 7.5560
batch: [15350/21305] batch time: 0.057 trainign loss: 6.5065 avg training loss: 7.5559
batch: [15360/21305] batch time: 2.065 trainign loss: 7.5485 avg training loss: 7.5559
batch: [15370/21305] batch time: 0.056 trainign loss: 5.4165 avg training loss: 7.5557
batch: [15380/21305] batch time: 2.564 trainign loss: 6.2976 avg training loss: 7.5557
batch: [15390/21305] batch time: 0.061 trainign loss: 6.9612 avg training loss: 7.5556
batch: [15400/21305] batch time: 2.466 trainign loss: 7.1369 avg training loss: 7.5556
batch: [15410/21305] batch time: 0.056 trainign loss: 6.8043 avg training loss: 7.5555
batch: [15420/21305] batch time: 2.236 trainign loss: 6.3817 avg training loss: 7.5555
batch: [15430/21305] batch time: 0.052 trainign loss: 6.0043 avg training loss: 7.5553
batch: [15440/21305] batch time: 2.299 trainign loss: 7.3431 avg training loss: 7.5552
batch: [15450/21305] batch time: 0.063 trainign loss: 6.7762 avg training loss: 7.5551
batch: [15460/21305] batch time: 2.331 trainign loss: 6.8971 avg training loss: 7.5551
batch: [15470/21305] batch time: 0.056 trainign loss: 6.4110 avg training loss: 7.5550
batch: [15480/21305] batch time: 2.112 trainign loss: 7.1108 avg training loss: 7.5549
batch: [15490/21305] batch time: 0.243 trainign loss: 6.1885 avg training loss: 7.5549
batch: [15500/21305] batch time: 2.062 trainign loss: 4.8712 avg training loss: 7.5547
batch: [15510/21305] batch time: 0.057 trainign loss: 4.0744 avg training loss: 7.5546
batch: [15520/21305] batch time: 1.240 trainign loss: 7.8637 avg training loss: 7.5543
batch: [15530/21305] batch time: 0.056 trainign loss: 7.6711 avg training loss: 7.5543
batch: [15540/21305] batch time: 0.843 trainign loss: 7.4746 avg training loss: 7.5543
batch: [15550/21305] batch time: 0.057 trainign loss: 5.7391 avg training loss: 7.5542
batch: [15560/21305] batch time: 1.582 trainign loss: 8.1427 avg training loss: 7.5541
batch: [15570/21305] batch time: 0.056 trainign loss: 7.2219 avg training loss: 7.5541
batch: [15580/21305] batch time: 1.074 trainign loss: 6.7087 avg training loss: 7.5541
batch: [15590/21305] batch time: 0.057 trainign loss: 5.8787 avg training loss: 7.5540
batch: [15600/21305] batch time: 0.343 trainign loss: 6.3305 avg training loss: 7.5539
batch: [15610/21305] batch time: 0.058 trainign loss: 6.4757 avg training loss: 7.5538
batch: [15620/21305] batch time: 0.051 trainign loss: 6.3142 avg training loss: 7.5537
batch: [15630/21305] batch time: 0.062 trainign loss: 6.0945 avg training loss: 7.5537
batch: [15640/21305] batch time: 0.056 trainign loss: 7.0867 avg training loss: 7.5536
batch: [15650/21305] batch time: 0.056 trainign loss: 5.5151 avg training loss: 7.5535
batch: [15660/21305] batch time: 0.052 trainign loss: 5.5419 avg training loss: 7.5534
batch: [15670/21305] batch time: 0.056 trainign loss: 6.3603 avg training loss: 7.5533
batch: [15680/21305] batch time: 0.058 trainign loss: 7.1176 avg training loss: 7.5532
batch: [15690/21305] batch time: 0.057 trainign loss: 7.0349 avg training loss: 7.5531
batch: [15700/21305] batch time: 0.055 trainign loss: 6.4905 avg training loss: 7.5530
batch: [15710/21305] batch time: 0.428 trainign loss: 6.3832 avg training loss: 7.5529
batch: [15720/21305] batch time: 0.102 trainign loss: 4.9793 avg training loss: 7.5528
batch: [15730/21305] batch time: 1.323 trainign loss: 0.2450 avg training loss: 7.5524
batch: [15740/21305] batch time: 0.062 trainign loss: 11.9052 avg training loss: 7.5520
batch: [15750/21305] batch time: 2.081 trainign loss: 7.5649 avg training loss: 7.5521
batch: [15760/21305] batch time: 0.513 trainign loss: 7.3231 avg training loss: 7.5521
batch: [15770/21305] batch time: 0.539 trainign loss: 6.2120 avg training loss: 7.5521
batch: [15780/21305] batch time: 1.030 trainign loss: 6.0824 avg training loss: 7.5520
batch: [15790/21305] batch time: 0.056 trainign loss: 6.5166 avg training loss: 7.5519
batch: [15800/21305] batch time: 0.830 trainign loss: 5.1618 avg training loss: 7.5518
batch: [15810/21305] batch time: 0.058 trainign loss: 7.6513 avg training loss: 7.5517
batch: [15820/21305] batch time: 1.486 trainign loss: 6.6885 avg training loss: 7.5516
batch: [15830/21305] batch time: 0.062 trainign loss: 6.1150 avg training loss: 7.5516
batch: [15840/21305] batch time: 1.769 trainign loss: 6.1339 avg training loss: 7.5515
batch: [15850/21305] batch time: 0.063 trainign loss: 4.8083 avg training loss: 7.5514
batch: [15860/21305] batch time: 1.956 trainign loss: 7.6954 avg training loss: 7.5513
batch: [15870/21305] batch time: 0.056 trainign loss: 6.2563 avg training loss: 7.5513
batch: [15880/21305] batch time: 2.334 trainign loss: 7.2092 avg training loss: 7.5513
batch: [15890/21305] batch time: 0.051 trainign loss: 7.2091 avg training loss: 7.5512
batch: [15900/21305] batch time: 2.295 trainign loss: 5.6705 avg training loss: 7.5511
batch: [15910/21305] batch time: 0.052 trainign loss: 6.4661 avg training loss: 7.5510
batch: [15920/21305] batch time: 2.338 trainign loss: 6.4807 avg training loss: 7.5510
batch: [15930/21305] batch time: 0.056 trainign loss: 7.1683 avg training loss: 7.5508
batch: [15940/21305] batch time: 2.544 trainign loss: 6.4768 avg training loss: 7.5506
batch: [15950/21305] batch time: 0.056 trainign loss: 7.1270 avg training loss: 7.5506
batch: [15960/21305] batch time: 2.038 trainign loss: 5.1322 avg training loss: 7.5505
batch: [15970/21305] batch time: 0.057 trainign loss: 5.1762 avg training loss: 7.5504
batch: [15980/21305] batch time: 2.435 trainign loss: 6.1163 avg training loss: 7.5503
batch: [15990/21305] batch time: 0.057 trainign loss: 6.8122 avg training loss: 7.5502
batch: [16000/21305] batch time: 2.433 trainign loss: 6.5839 avg training loss: 7.5501
batch: [16010/21305] batch time: 0.060 trainign loss: 1.0514 avg training loss: 7.5499
batch: [16020/21305] batch time: 2.488 trainign loss: 6.6429 avg training loss: 7.5498
batch: [16030/21305] batch time: 0.056 trainign loss: 7.6927 avg training loss: 7.5497
batch: [16040/21305] batch time: 2.500 trainign loss: 4.9451 avg training loss: 7.5496
batch: [16050/21305] batch time: 0.056 trainign loss: 6.6418 avg training loss: 7.5496
batch: [16060/21305] batch time: 2.003 trainign loss: 6.3192 avg training loss: 7.5495
batch: [16070/21305] batch time: 0.052 trainign loss: 7.6740 avg training loss: 7.5495
batch: [16080/21305] batch time: 2.072 trainign loss: 6.9179 avg training loss: 7.5494
batch: [16090/21305] batch time: 0.063 trainign loss: 6.1229 avg training loss: 7.5493
batch: [16100/21305] batch time: 1.824 trainign loss: 5.7818 avg training loss: 7.5492
batch: [16110/21305] batch time: 0.058 trainign loss: 5.6148 avg training loss: 7.5491
batch: [16120/21305] batch time: 1.337 trainign loss: 5.0678 avg training loss: 7.5490
batch: [16130/21305] batch time: 0.056 trainign loss: 6.5128 avg training loss: 7.5489
batch: [16140/21305] batch time: 1.593 trainign loss: 6.9458 avg training loss: 7.5488
batch: [16150/21305] batch time: 0.061 trainign loss: 6.7355 avg training loss: 7.5488
batch: [16160/21305] batch time: 1.722 trainign loss: 4.4435 avg training loss: 7.5486
batch: [16170/21305] batch time: 0.057 trainign loss: 7.6410 avg training loss: 7.5486
batch: [16180/21305] batch time: 0.655 trainign loss: 7.4595 avg training loss: 7.5486
batch: [16190/21305] batch time: 0.057 trainign loss: 7.0211 avg training loss: 7.5485
batch: [16200/21305] batch time: 0.144 trainign loss: 5.4997 avg training loss: 7.5485
batch: [16210/21305] batch time: 0.056 trainign loss: 6.7701 avg training loss: 7.5484
batch: [16220/21305] batch time: 1.227 trainign loss: 6.4966 avg training loss: 7.5483
batch: [16230/21305] batch time: 0.057 trainign loss: 6.7860 avg training loss: 7.5482
batch: [16240/21305] batch time: 0.859 trainign loss: 7.2010 avg training loss: 7.5481
batch: [16250/21305] batch time: 0.056 trainign loss: 6.8715 avg training loss: 7.5480
batch: [16260/21305] batch time: 0.986 trainign loss: 6.8659 avg training loss: 7.5480
batch: [16270/21305] batch time: 0.063 trainign loss: 7.2694 avg training loss: 7.5479
batch: [16280/21305] batch time: 1.577 trainign loss: 7.2088 avg training loss: 7.5478
batch: [16290/21305] batch time: 0.056 trainign loss: 6.4144 avg training loss: 7.5477
batch: [16300/21305] batch time: 2.179 trainign loss: 6.8449 avg training loss: 7.5476
batch: [16310/21305] batch time: 0.051 trainign loss: 6.7216 avg training loss: 7.5475
batch: [16320/21305] batch time: 1.835 trainign loss: 6.9950 avg training loss: 7.5475
batch: [16330/21305] batch time: 0.055 trainign loss: 5.5308 avg training loss: 7.5473
batch: [16340/21305] batch time: 2.288 trainign loss: 4.7481 avg training loss: 7.5472
batch: [16350/21305] batch time: 0.057 trainign loss: 7.3804 avg training loss: 7.5471
batch: [16360/21305] batch time: 2.408 trainign loss: 6.0801 avg training loss: 7.5469
batch: [16370/21305] batch time: 0.056 trainign loss: 6.1327 avg training loss: 7.5468
batch: [16380/21305] batch time: 2.128 trainign loss: 6.9999 avg training loss: 7.5467
batch: [16390/21305] batch time: 0.062 trainign loss: 6.0908 avg training loss: 7.5466
batch: [16400/21305] batch time: 2.306 trainign loss: 7.2907 avg training loss: 7.5465
batch: [16410/21305] batch time: 0.062 trainign loss: 7.5197 avg training loss: 7.5465
batch: [16420/21305] batch time: 2.158 trainign loss: 6.7286 avg training loss: 7.5464
batch: [16430/21305] batch time: 0.056 trainign loss: 5.7764 avg training loss: 7.5463
batch: [16440/21305] batch time: 2.465 trainign loss: 6.9953 avg training loss: 7.5462
batch: [16450/21305] batch time: 0.057 trainign loss: 7.4192 avg training loss: 7.5461
batch: [16460/21305] batch time: 2.299 trainign loss: 6.8687 avg training loss: 7.5461
batch: [16470/21305] batch time: 0.063 trainign loss: 6.8286 avg training loss: 7.5460
batch: [16480/21305] batch time: 2.572 trainign loss: 5.8309 avg training loss: 7.5459
batch: [16490/21305] batch time: 0.056 trainign loss: 5.5494 avg training loss: 7.5458
batch: [16500/21305] batch time: 2.025 trainign loss: 6.4218 avg training loss: 7.5457
batch: [16510/21305] batch time: 0.061 trainign loss: 6.7692 avg training loss: 7.5456
batch: [16520/21305] batch time: 2.595 trainign loss: 6.5551 avg training loss: 7.5455
batch: [16530/21305] batch time: 0.056 trainign loss: 6.7944 avg training loss: 7.5454
batch: [16540/21305] batch time: 2.344 trainign loss: 6.0528 avg training loss: 7.5452
batch: [16550/21305] batch time: 0.055 trainign loss: 7.2522 avg training loss: 7.5452
batch: [16560/21305] batch time: 2.000 trainign loss: 7.3155 avg training loss: 7.5451
batch: [16570/21305] batch time: 0.056 trainign loss: 6.1891 avg training loss: 7.5451
batch: [16580/21305] batch time: 2.333 trainign loss: 7.2779 avg training loss: 7.5450
batch: [16590/21305] batch time: 0.056 trainign loss: 7.2644 avg training loss: 7.5449
batch: [16600/21305] batch time: 2.296 trainign loss: 6.7532 avg training loss: 7.5448
batch: [16610/21305] batch time: 0.056 trainign loss: 5.0626 avg training loss: 7.5446
batch: [16620/21305] batch time: 2.069 trainign loss: 6.5415 avg training loss: 7.5444
batch: [16630/21305] batch time: 0.056 trainign loss: 5.8555 avg training loss: 7.5443
batch: [16640/21305] batch time: 2.253 trainign loss: 7.6688 avg training loss: 7.5442
batch: [16650/21305] batch time: 0.061 trainign loss: 6.9874 avg training loss: 7.5442
batch: [16660/21305] batch time: 2.243 trainign loss: 6.6325 avg training loss: 7.5441
batch: [16670/21305] batch time: 0.052 trainign loss: 6.6181 avg training loss: 7.5440
batch: [16680/21305] batch time: 2.132 trainign loss: 7.2839 avg training loss: 7.5439
batch: [16690/21305] batch time: 0.056 trainign loss: 7.1853 avg training loss: 7.5439
batch: [16700/21305] batch time: 2.263 trainign loss: 6.8375 avg training loss: 7.5438
batch: [16710/21305] batch time: 0.056 trainign loss: 6.9899 avg training loss: 7.5437
batch: [16720/21305] batch time: 2.213 trainign loss: 7.1004 avg training loss: 7.5437
batch: [16730/21305] batch time: 0.062 trainign loss: 5.2055 avg training loss: 7.5436
batch: [16740/21305] batch time: 2.524 trainign loss: 6.6829 avg training loss: 7.5435
batch: [16750/21305] batch time: 0.056 trainign loss: 7.0636 avg training loss: 7.5435
batch: [16760/21305] batch time: 2.111 trainign loss: 6.4897 avg training loss: 7.5434
batch: [16770/21305] batch time: 0.056 trainign loss: 7.0454 avg training loss: 7.5433
batch: [16780/21305] batch time: 2.289 trainign loss: 6.2740 avg training loss: 7.5433
batch: [16790/21305] batch time: 0.059 trainign loss: 6.5005 avg training loss: 7.5432
batch: [16800/21305] batch time: 2.232 trainign loss: 2.6572 avg training loss: 7.5429
batch: [16810/21305] batch time: 0.054 trainign loss: 7.6175 avg training loss: 7.5429
batch: [16820/21305] batch time: 2.790 trainign loss: 6.0329 avg training loss: 7.5428
batch: [16830/21305] batch time: 0.056 trainign loss: 6.5326 avg training loss: 7.5427
batch: [16840/21305] batch time: 2.352 trainign loss: 7.2596 avg training loss: 7.5426
batch: [16850/21305] batch time: 0.056 trainign loss: 6.9139 avg training loss: 7.5425
batch: [16860/21305] batch time: 2.205 trainign loss: 6.9198 avg training loss: 7.5424
batch: [16870/21305] batch time: 0.052 trainign loss: 6.6913 avg training loss: 7.5423
batch: [16880/21305] batch time: 2.133 trainign loss: 6.5397 avg training loss: 7.5422
batch: [16890/21305] batch time: 0.056 trainign loss: 6.8507 avg training loss: 7.5421
batch: [16900/21305] batch time: 2.202 trainign loss: 4.5257 avg training loss: 7.5420
batch: [16910/21305] batch time: 0.056 trainign loss: 0.0084 avg training loss: 7.5415
batch: [16920/21305] batch time: 2.350 trainign loss: 9.1374 avg training loss: 7.5415
batch: [16930/21305] batch time: 0.056 trainign loss: 5.0514 avg training loss: 7.5415
batch: [16940/21305] batch time: 2.518 trainign loss: 7.3230 avg training loss: 7.5414
batch: [16950/21305] batch time: 0.057 trainign loss: 7.4889 avg training loss: 7.5414
batch: [16960/21305] batch time: 2.168 trainign loss: 5.0038 avg training loss: 7.5413
batch: [16970/21305] batch time: 0.056 trainign loss: 5.7501 avg training loss: 7.5412
batch: [16980/21305] batch time: 2.039 trainign loss: 3.6323 avg training loss: 7.5410
batch: [16990/21305] batch time: 0.061 trainign loss: 3.4476 avg training loss: 7.5408
batch: [17000/21305] batch time: 2.261 trainign loss: 2.7475 avg training loss: 7.5406
batch: [17010/21305] batch time: 0.053 trainign loss: 7.5944 avg training loss: 7.5406
batch: [17020/21305] batch time: 1.988 trainign loss: 0.8458 avg training loss: 7.5404
batch: [17030/21305] batch time: 0.056 trainign loss: 7.6324 avg training loss: 7.5402
batch: [17040/21305] batch time: 2.465 trainign loss: 7.6770 avg training loss: 7.5402
batch: [17050/21305] batch time: 0.056 trainign loss: 6.7712 avg training loss: 7.5401
batch: [17060/21305] batch time: 2.466 trainign loss: 7.2042 avg training loss: 7.5400
batch: [17070/21305] batch time: 0.057 trainign loss: 7.5259 avg training loss: 7.5398
batch: [17080/21305] batch time: 2.303 trainign loss: 7.7613 avg training loss: 7.5398
batch: [17090/21305] batch time: 0.057 trainign loss: 6.5810 avg training loss: 7.5398
batch: [17100/21305] batch time: 2.286 trainign loss: 1.5171 avg training loss: 7.5396
batch: [17110/21305] batch time: 0.063 trainign loss: 8.1604 avg training loss: 7.5396
batch: [17120/21305] batch time: 2.221 trainign loss: 7.3416 avg training loss: 7.5396
batch: [17130/21305] batch time: 0.056 trainign loss: 6.1396 avg training loss: 7.5395
batch: [17140/21305] batch time: 2.323 trainign loss: 5.8150 avg training loss: 7.5394
batch: [17150/21305] batch time: 0.056 trainign loss: 6.8102 avg training loss: 7.5393
batch: [17160/21305] batch time: 2.330 trainign loss: 6.7803 avg training loss: 7.5392
batch: [17170/21305] batch time: 0.053 trainign loss: 5.2850 avg training loss: 7.5390
batch: [17180/21305] batch time: 2.270 trainign loss: 7.8666 avg training loss: 7.5390
batch: [17190/21305] batch time: 0.057 trainign loss: 6.1211 avg training loss: 7.5390
batch: [17200/21305] batch time: 2.448 trainign loss: 6.3447 avg training loss: 7.5389
batch: [17210/21305] batch time: 0.056 trainign loss: 6.8116 avg training loss: 7.5388
batch: [17220/21305] batch time: 2.059 trainign loss: 6.8793 avg training loss: 7.5387
batch: [17230/21305] batch time: 0.050 trainign loss: 4.7789 avg training loss: 7.5386
batch: [17240/21305] batch time: 2.501 trainign loss: 6.6033 avg training loss: 7.5384
batch: [17250/21305] batch time: 0.056 trainign loss: 7.9999 avg training loss: 7.5383
batch: [17260/21305] batch time: 2.237 trainign loss: 7.8431 avg training loss: 7.5383
batch: [17270/21305] batch time: 0.056 trainign loss: 6.7505 avg training loss: 7.5383
batch: [17280/21305] batch time: 2.126 trainign loss: 7.2202 avg training loss: 7.5382
batch: [17290/21305] batch time: 0.053 trainign loss: 5.1153 avg training loss: 7.5382
batch: [17300/21305] batch time: 2.084 trainign loss: 6.3952 avg training loss: 7.5381
batch: [17310/21305] batch time: 0.058 trainign loss: 3.6473 avg training loss: 7.5379
batch: [17320/21305] batch time: 1.269 trainign loss: 11.5044 avg training loss: 7.5374
batch: [17330/21305] batch time: 0.057 trainign loss: 8.6246 avg training loss: 7.5375
batch: [17340/21305] batch time: 1.245 trainign loss: 7.6944 avg training loss: 7.5375
batch: [17350/21305] batch time: 0.057 trainign loss: 7.6473 avg training loss: 7.5373
batch: [17360/21305] batch time: 0.510 trainign loss: 6.4681 avg training loss: 7.5373
batch: [17370/21305] batch time: 0.051 trainign loss: 6.2113 avg training loss: 7.5372
batch: [17380/21305] batch time: 0.062 trainign loss: 7.2681 avg training loss: 7.5372
batch: [17390/21305] batch time: 0.062 trainign loss: 7.3964 avg training loss: 7.5371
batch: [17400/21305] batch time: 0.054 trainign loss: 6.9449 avg training loss: 7.5371
batch: [17410/21305] batch time: 0.428 trainign loss: 6.7916 avg training loss: 7.5370
batch: [17420/21305] batch time: 0.056 trainign loss: 6.0533 avg training loss: 7.5370
batch: [17430/21305] batch time: 0.052 trainign loss: 5.5136 avg training loss: 7.5369
batch: [17440/21305] batch time: 0.056 trainign loss: 6.7039 avg training loss: 7.5368
batch: [17450/21305] batch time: 0.612 trainign loss: 6.5509 avg training loss: 7.5367
batch: [17460/21305] batch time: 0.062 trainign loss: 5.0393 avg training loss: 7.5366
batch: [17470/21305] batch time: 0.364 trainign loss: 7.0385 avg training loss: 7.5365
batch: [17480/21305] batch time: 0.051 trainign loss: 5.9240 avg training loss: 7.5363
batch: [17490/21305] batch time: 0.774 trainign loss: 6.9334 avg training loss: 7.5363
batch: [17500/21305] batch time: 0.056 trainign loss: 6.2048 avg training loss: 7.5362
batch: [17510/21305] batch time: 0.889 trainign loss: 6.3507 avg training loss: 7.5361
batch: [17520/21305] batch time: 0.056 trainign loss: 5.5270 avg training loss: 7.5360
batch: [17530/21305] batch time: 0.492 trainign loss: 5.3292 avg training loss: 7.5359
batch: [17540/21305] batch time: 0.056 trainign loss: 6.6914 avg training loss: 7.5358
batch: [17550/21305] batch time: 0.645 trainign loss: 6.8903 avg training loss: 7.5358
batch: [17560/21305] batch time: 0.056 trainign loss: 6.0231 avg training loss: 7.5357
batch: [17570/21305] batch time: 0.057 trainign loss: 6.9753 avg training loss: 7.5355
batch: [17580/21305] batch time: 0.053 trainign loss: 1.6418 avg training loss: 7.5353
batch: [17590/21305] batch time: 0.062 trainign loss: 10.8348 avg training loss: 7.5348
batch: [17600/21305] batch time: 0.058 trainign loss: 8.4840 avg training loss: 7.5347
batch: [17610/21305] batch time: 0.062 trainign loss: 8.6222 avg training loss: 7.5347
batch: [17620/21305] batch time: 0.056 trainign loss: 6.3136 avg training loss: 7.5347
batch: [17630/21305] batch time: 0.057 trainign loss: 6.6786 avg training loss: 7.5347
batch: [17640/21305] batch time: 0.056 trainign loss: 5.4286 avg training loss: 7.5346
batch: [17650/21305] batch time: 0.051 trainign loss: 5.7973 avg training loss: 7.5345
batch: [17660/21305] batch time: 0.062 trainign loss: 6.3549 avg training loss: 7.5345
batch: [17670/21305] batch time: 1.125 trainign loss: 5.9935 avg training loss: 7.5344
batch: [17680/21305] batch time: 0.061 trainign loss: 6.4856 avg training loss: 7.5342
batch: [17690/21305] batch time: 1.305 trainign loss: 5.4599 avg training loss: 7.5341
batch: [17700/21305] batch time: 0.063 trainign loss: 6.0774 avg training loss: 7.5340
batch: [17710/21305] batch time: 0.563 trainign loss: 6.8492 avg training loss: 7.5339
batch: [17720/21305] batch time: 0.056 trainign loss: 6.4839 avg training loss: 7.5339
batch: [17730/21305] batch time: 0.919 trainign loss: 6.6616 avg training loss: 7.5338
batch: [17740/21305] batch time: 0.056 trainign loss: 5.7606 avg training loss: 7.5337
batch: [17750/21305] batch time: 1.121 trainign loss: 6.5266 avg training loss: 7.5336
batch: [17760/21305] batch time: 0.062 trainign loss: 6.4601 avg training loss: 7.5336
batch: [17770/21305] batch time: 1.217 trainign loss: 6.7077 avg training loss: 7.5335
batch: [17780/21305] batch time: 0.056 trainign loss: 6.1536 avg training loss: 7.5334
batch: [17790/21305] batch time: 2.076 trainign loss: 7.1082 avg training loss: 7.5333
batch: [17800/21305] batch time: 0.057 trainign loss: 6.0139 avg training loss: 7.5332
batch: [17810/21305] batch time: 2.356 trainign loss: 6.9451 avg training loss: 7.5332
batch: [17820/21305] batch time: 0.057 trainign loss: 6.4508 avg training loss: 7.5331
batch: [17830/21305] batch time: 2.419 trainign loss: 5.4395 avg training loss: 7.5330
batch: [17840/21305] batch time: 0.056 trainign loss: 7.1233 avg training loss: 7.5329
batch: [17850/21305] batch time: 2.139 trainign loss: 6.9330 avg training loss: 7.5329
batch: [17860/21305] batch time: 0.056 trainign loss: 6.8153 avg training loss: 7.5328
batch: [17870/21305] batch time: 1.402 trainign loss: 6.0587 avg training loss: 7.5327
batch: [17880/21305] batch time: 0.058 trainign loss: 6.8903 avg training loss: 7.5327
batch: [17890/21305] batch time: 1.917 trainign loss: 6.7283 avg training loss: 7.5326
batch: [17900/21305] batch time: 0.058 trainign loss: 6.3436 avg training loss: 7.5324
batch: [17910/21305] batch time: 2.290 trainign loss: 3.2209 avg training loss: 7.5322
batch: [17920/21305] batch time: 0.063 trainign loss: 7.7954 avg training loss: 7.5322
batch: [17930/21305] batch time: 1.001 trainign loss: 6.1149 avg training loss: 7.5322
batch: [17940/21305] batch time: 0.056 trainign loss: 6.2012 avg training loss: 7.5321
batch: [17950/21305] batch time: 1.472 trainign loss: 6.6627 avg training loss: 7.5319
batch: [17960/21305] batch time: 0.058 trainign loss: 7.0340 avg training loss: 7.5318
batch: [17970/21305] batch time: 0.896 trainign loss: 5.9048 avg training loss: 7.5317
batch: [17980/21305] batch time: 0.056 trainign loss: 1.7122 avg training loss: 7.5315
batch: [17990/21305] batch time: 0.713 trainign loss: 5.6482 avg training loss: 7.5314
batch: [18000/21305] batch time: 0.056 trainign loss: 7.1797 avg training loss: 7.5314
batch: [18010/21305] batch time: 0.056 trainign loss: 8.0588 avg training loss: 7.5313
batch: [18020/21305] batch time: 0.056 trainign loss: 4.4619 avg training loss: 7.5312
batch: [18030/21305] batch time: 0.052 trainign loss: 6.5815 avg training loss: 7.5311
batch: [18040/21305] batch time: 0.056 trainign loss: 6.1623 avg training loss: 7.5310
batch: [18050/21305] batch time: 1.173 trainign loss: 6.4522 avg training loss: 7.5309
batch: [18060/21305] batch time: 0.284 trainign loss: 7.2336 avg training loss: 7.5308
batch: [18070/21305] batch time: 0.558 trainign loss: 7.3557 avg training loss: 7.5308
batch: [18080/21305] batch time: 0.655 trainign loss: 6.4462 avg training loss: 7.5307
batch: [18090/21305] batch time: 1.256 trainign loss: 5.1965 avg training loss: 7.5306
batch: [18100/21305] batch time: 0.263 trainign loss: 5.9333 avg training loss: 7.5304
batch: [18110/21305] batch time: 1.084 trainign loss: 5.3974 avg training loss: 7.5304
batch: [18120/21305] batch time: 0.057 trainign loss: 6.7270 avg training loss: 7.5301
batch: [18130/21305] batch time: 1.510 trainign loss: 7.4002 avg training loss: 7.5301
batch: [18140/21305] batch time: 0.056 trainign loss: 7.4801 avg training loss: 7.5300
batch: [18150/21305] batch time: 1.455 trainign loss: 7.3277 avg training loss: 7.5300
batch: [18160/21305] batch time: 0.063 trainign loss: 5.4734 avg training loss: 7.5299
batch: [18170/21305] batch time: 1.111 trainign loss: 5.7868 avg training loss: 7.5298
batch: [18180/21305] batch time: 0.062 trainign loss: 0.1035 avg training loss: 7.5294
batch: [18190/21305] batch time: 1.001 trainign loss: 7.3641 avg training loss: 7.5294
batch: [18200/21305] batch time: 0.056 trainign loss: 7.4006 avg training loss: 7.5294
batch: [18210/21305] batch time: 0.337 trainign loss: 6.7609 avg training loss: 7.5293
batch: [18220/21305] batch time: 0.056 trainign loss: 4.4325 avg training loss: 7.5291
batch: [18230/21305] batch time: 0.051 trainign loss: 7.8456 avg training loss: 7.5291
batch: [18240/21305] batch time: 0.056 trainign loss: 6.2333 avg training loss: 7.5290
batch: [18250/21305] batch time: 0.051 trainign loss: 5.7984 avg training loss: 7.5290
batch: [18260/21305] batch time: 0.056 trainign loss: 2.7927 avg training loss: 7.5287
batch: [18270/21305] batch time: 0.050 trainign loss: 6.0403 avg training loss: 7.5287
batch: [18280/21305] batch time: 0.056 trainign loss: 6.6226 avg training loss: 7.5285
batch: [18290/21305] batch time: 0.056 trainign loss: 7.2360 avg training loss: 7.5285
batch: [18300/21305] batch time: 0.056 trainign loss: 6.5548 avg training loss: 7.5285
batch: [18310/21305] batch time: 0.056 trainign loss: 6.5760 avg training loss: 7.5284
batch: [18320/21305] batch time: 0.056 trainign loss: 5.1395 avg training loss: 7.5283
batch: [18330/21305] batch time: 0.051 trainign loss: 5.8273 avg training loss: 7.5282
batch: [18340/21305] batch time: 0.058 trainign loss: 7.2520 avg training loss: 7.5281
batch: [18350/21305] batch time: 0.051 trainign loss: 7.1256 avg training loss: 7.5281
batch: [18360/21305] batch time: 0.056 trainign loss: 5.9040 avg training loss: 7.5280
batch: [18370/21305] batch time: 0.056 trainign loss: 6.9498 avg training loss: 7.5279
batch: [18380/21305] batch time: 0.430 trainign loss: 6.4369 avg training loss: 7.5278
batch: [18390/21305] batch time: 0.052 trainign loss: 5.9518 avg training loss: 7.5276
batch: [18400/21305] batch time: 0.891 trainign loss: 7.2233 avg training loss: 7.5276
batch: [18410/21305] batch time: 0.059 trainign loss: 5.3250 avg training loss: 7.5275
batch: [18420/21305] batch time: 1.472 trainign loss: 6.6514 avg training loss: 7.5274
batch: [18430/21305] batch time: 0.056 trainign loss: 6.3277 avg training loss: 7.5273
batch: [18440/21305] batch time: 2.258 trainign loss: 0.1502 avg training loss: 7.5270
batch: [18450/21305] batch time: 0.056 trainign loss: 8.0988 avg training loss: 7.5268
batch: [18460/21305] batch time: 2.306 trainign loss: 6.7937 avg training loss: 7.5268
batch: [18470/21305] batch time: 0.061 trainign loss: 7.1835 avg training loss: 7.5268
batch: [18480/21305] batch time: 2.356 trainign loss: 6.2856 avg training loss: 7.5267
batch: [18490/21305] batch time: 0.063 trainign loss: 3.8830 avg training loss: 7.5265
batch: [18500/21305] batch time: 2.168 trainign loss: 0.0047 avg training loss: 7.5259
batch: [18510/21305] batch time: 0.060 trainign loss: 8.5141 avg training loss: 7.5259
batch: [18520/21305] batch time: 2.301 trainign loss: 8.3399 avg training loss: 7.5260
batch: [18530/21305] batch time: 0.056 trainign loss: 6.5188 avg training loss: 7.5260
batch: [18540/21305] batch time: 2.270 trainign loss: 6.1418 avg training loss: 7.5259
batch: [18550/21305] batch time: 0.052 trainign loss: 4.3219 avg training loss: 7.5257
batch: [18560/21305] batch time: 2.312 trainign loss: 7.6619 avg training loss: 7.5256
batch: [18570/21305] batch time: 0.062 trainign loss: 7.2606 avg training loss: 7.5256
batch: [18580/21305] batch time: 2.454 trainign loss: 6.9310 avg training loss: 7.5255
batch: [18590/21305] batch time: 0.062 trainign loss: 6.9285 avg training loss: 7.5254
batch: [18600/21305] batch time: 2.354 trainign loss: 5.6605 avg training loss: 7.5253
batch: [18610/21305] batch time: 0.377 trainign loss: 6.2688 avg training loss: 7.5251
batch: [18620/21305] batch time: 1.164 trainign loss: 7.8959 avg training loss: 7.5251
batch: [18630/21305] batch time: 1.273 trainign loss: 6.5243 avg training loss: 7.5250
batch: [18640/21305] batch time: 1.504 trainign loss: 5.3909 avg training loss: 7.5249
batch: [18650/21305] batch time: 0.367 trainign loss: 3.7371 avg training loss: 7.5248
batch: [18660/21305] batch time: 1.807 trainign loss: 0.0033 avg training loss: 7.5242
batch: [18670/21305] batch time: 0.439 trainign loss: 7.2590 avg training loss: 7.5241
batch: [18680/21305] batch time: 1.539 trainign loss: 7.6821 avg training loss: 7.5241
batch: [18690/21305] batch time: 0.996 trainign loss: 7.8384 avg training loss: 7.5242
batch: [18700/21305] batch time: 1.501 trainign loss: 7.1685 avg training loss: 7.5241
batch: [18710/21305] batch time: 0.863 trainign loss: 7.0801 avg training loss: 7.5241
batch: [18720/21305] batch time: 1.432 trainign loss: 6.2702 avg training loss: 7.5240
batch: [18730/21305] batch time: 0.882 trainign loss: 5.2349 avg training loss: 7.5239
batch: [18740/21305] batch time: 1.696 trainign loss: 7.3780 avg training loss: 7.5238
batch: [18750/21305] batch time: 0.684 trainign loss: 5.8608 avg training loss: 7.5238
batch: [18760/21305] batch time: 1.430 trainign loss: 5.0859 avg training loss: 7.5236
batch: [18770/21305] batch time: 0.810 trainign loss: 7.4655 avg training loss: 7.5235
batch: [18780/21305] batch time: 1.443 trainign loss: 7.3078 avg training loss: 7.5234
batch: [18790/21305] batch time: 0.515 trainign loss: 6.1428 avg training loss: 7.5234
batch: [18800/21305] batch time: 1.834 trainign loss: 7.3219 avg training loss: 7.5233
batch: [18810/21305] batch time: 0.697 trainign loss: 7.0247 avg training loss: 7.5233
batch: [18820/21305] batch time: 2.039 trainign loss: 4.5580 avg training loss: 7.5232
batch: [18830/21305] batch time: 1.044 trainign loss: 7.4391 avg training loss: 7.5231
batch: [18840/21305] batch time: 1.241 trainign loss: 6.5771 avg training loss: 7.5230
batch: [18850/21305] batch time: 1.638 trainign loss: 7.4739 avg training loss: 7.5228
batch: [18860/21305] batch time: 1.446 trainign loss: 6.8010 avg training loss: 7.5228
batch: [18870/21305] batch time: 0.921 trainign loss: 6.2713 avg training loss: 7.5227
batch: [18880/21305] batch time: 1.023 trainign loss: 4.3102 avg training loss: 7.5224
batch: [18890/21305] batch time: 0.434 trainign loss: 7.4266 avg training loss: 7.5224
batch: [18900/21305] batch time: 0.918 trainign loss: 5.8935 avg training loss: 7.5224
batch: [18910/21305] batch time: 0.617 trainign loss: 6.7360 avg training loss: 7.5223
batch: [18920/21305] batch time: 0.257 trainign loss: 7.1117 avg training loss: 7.5222
batch: [18930/21305] batch time: 0.696 trainign loss: 7.6157 avg training loss: 7.5222
batch: [18940/21305] batch time: 0.175 trainign loss: 6.4904 avg training loss: 7.5222
batch: [18950/21305] batch time: 0.701 trainign loss: 7.0052 avg training loss: 7.5221
batch: [18960/21305] batch time: 0.916 trainign loss: 6.7854 avg training loss: 7.5219
batch: [18970/21305] batch time: 0.177 trainign loss: 6.5321 avg training loss: 7.5218
batch: [18980/21305] batch time: 0.606 trainign loss: 8.4943 avg training loss: 7.5216
batch: [18990/21305] batch time: 0.243 trainign loss: 7.5322 avg training loss: 7.5216
batch: [19000/21305] batch time: 0.353 trainign loss: 5.6166 avg training loss: 7.5215
batch: [19010/21305] batch time: 0.056 trainign loss: 7.4774 avg training loss: 7.5214
batch: [19020/21305] batch time: 0.452 trainign loss: 6.8913 avg training loss: 7.5214
batch: [19030/21305] batch time: 0.276 trainign loss: 7.2369 avg training loss: 7.5213
batch: [19040/21305] batch time: 0.516 trainign loss: 6.7036 avg training loss: 7.5212
batch: [19050/21305] batch time: 0.061 trainign loss: 7.0115 avg training loss: 7.5211
batch: [19060/21305] batch time: 0.614 trainign loss: 5.2692 avg training loss: 7.5209
batch: [19070/21305] batch time: 0.057 trainign loss: 6.7468 avg training loss: 7.5207
batch: [19080/21305] batch time: 0.063 trainign loss: 7.0183 avg training loss: 7.5207
batch: [19090/21305] batch time: 0.057 trainign loss: 7.8023 avg training loss: 7.5207
batch: [19100/21305] batch time: 0.052 trainign loss: 7.3004 avg training loss: 7.5206
batch: [19110/21305] batch time: 0.057 trainign loss: 4.0053 avg training loss: 7.5205
batch: [19120/21305] batch time: 0.056 trainign loss: 6.5863 avg training loss: 7.5204
batch: [19130/21305] batch time: 0.062 trainign loss: 7.3295 avg training loss: 7.5204
batch: [19140/21305] batch time: 0.056 trainign loss: 6.4695 avg training loss: 7.5203
batch: [19150/21305] batch time: 0.056 trainign loss: 6.2914 avg training loss: 7.5203
batch: [19160/21305] batch time: 0.056 trainign loss: 6.7378 avg training loss: 7.5202
batch: [19170/21305] batch time: 0.058 trainign loss: 6.6011 avg training loss: 7.5201
batch: [19180/21305] batch time: 0.051 trainign loss: 5.0349 avg training loss: 7.5200
batch: [19190/21305] batch time: 0.056 trainign loss: 6.6354 avg training loss: 7.5199
batch: [19200/21305] batch time: 0.056 trainign loss: 4.2561 avg training loss: 7.5197
batch: [19210/21305] batch time: 0.058 trainign loss: 6.7851 avg training loss: 7.5197
batch: [19220/21305] batch time: 0.055 trainign loss: 6.7460 avg training loss: 7.5195
batch: [19230/21305] batch time: 0.056 trainign loss: 6.2299 avg training loss: 7.5195
batch: [19240/21305] batch time: 0.052 trainign loss: 5.9698 avg training loss: 7.5194
batch: [19250/21305] batch time: 0.059 trainign loss: 3.2387 avg training loss: 7.5192
batch: [19260/21305] batch time: 0.053 trainign loss: 5.5023 avg training loss: 7.5191
batch: [19270/21305] batch time: 0.056 trainign loss: 7.3238 avg training loss: 7.5190
batch: [19280/21305] batch time: 0.062 trainign loss: 8.4209 avg training loss: 7.5190
batch: [19290/21305] batch time: 0.061 trainign loss: 7.2851 avg training loss: 7.5190
batch: [19300/21305] batch time: 0.054 trainign loss: 7.3211 avg training loss: 7.5189
batch: [19310/21305] batch time: 0.053 trainign loss: 7.5604 avg training loss: 7.5189
batch: [19320/21305] batch time: 0.051 trainign loss: 7.4072 avg training loss: 7.5188
batch: [19330/21305] batch time: 0.054 trainign loss: 6.6767 avg training loss: 7.5188
batch: [19340/21305] batch time: 0.051 trainign loss: 5.4038 avg training loss: 7.5187
batch: [19350/21305] batch time: 0.062 trainign loss: 6.8571 avg training loss: 7.5186
batch: [19360/21305] batch time: 0.056 trainign loss: 6.0920 avg training loss: 7.5185
batch: [19370/21305] batch time: 0.056 trainign loss: 6.5202 avg training loss: 7.5185
batch: [19380/21305] batch time: 0.051 trainign loss: 6.2230 avg training loss: 7.5183
batch: [19390/21305] batch time: 0.062 trainign loss: 7.8255 avg training loss: 7.5182
batch: [19400/21305] batch time: 0.056 trainign loss: 7.0683 avg training loss: 7.5182
batch: [19410/21305] batch time: 0.058 trainign loss: 5.3546 avg training loss: 7.5181
batch: [19420/21305] batch time: 0.056 trainign loss: 6.5252 avg training loss: 7.5180
batch: [19430/21305] batch time: 0.056 trainign loss: 6.9715 avg training loss: 7.5179
batch: [19440/21305] batch time: 0.051 trainign loss: 7.3206 avg training loss: 7.5179
batch: [19450/21305] batch time: 0.058 trainign loss: 6.9641 avg training loss: 7.5178
batch: [19460/21305] batch time: 0.056 trainign loss: 6.3956 avg training loss: 7.5178
batch: [19470/21305] batch time: 0.061 trainign loss: 7.1411 avg training loss: 7.5178
batch: [19480/21305] batch time: 0.052 trainign loss: 6.0141 avg training loss: 7.5177
batch: [19490/21305] batch time: 0.056 trainign loss: 5.2610 avg training loss: 7.5176
batch: [19500/21305] batch time: 0.056 trainign loss: 6.8742 avg training loss: 7.5175
batch: [19510/21305] batch time: 0.062 trainign loss: 6.3100 avg training loss: 7.5175
batch: [19520/21305] batch time: 0.063 trainign loss: 6.7677 avg training loss: 7.5174
batch: [19530/21305] batch time: 0.058 trainign loss: 6.9513 avg training loss: 7.5173
batch: [19540/21305] batch time: 0.052 trainign loss: 6.3067 avg training loss: 7.5172
batch: [19550/21305] batch time: 0.056 trainign loss: 6.1954 avg training loss: 7.5170
batch: [19560/21305] batch time: 0.150 trainign loss: 7.3041 avg training loss: 7.5170
batch: [19570/21305] batch time: 0.057 trainign loss: 6.8337 avg training loss: 7.5169
batch: [19580/21305] batch time: 1.200 trainign loss: 5.4043 avg training loss: 7.5168
batch: [19590/21305] batch time: 0.056 trainign loss: 5.6427 avg training loss: 7.5166
batch: [19600/21305] batch time: 0.437 trainign loss: 6.6250 avg training loss: 7.5166
batch: [19610/21305] batch time: 0.056 trainign loss: 6.8432 avg training loss: 7.5165
batch: [19620/21305] batch time: 0.632 trainign loss: 6.6808 avg training loss: 7.5164
batch: [19630/21305] batch time: 0.058 trainign loss: 7.1164 avg training loss: 7.5164
batch: [19640/21305] batch time: 0.953 trainign loss: 7.2090 avg training loss: 7.5163
batch: [19650/21305] batch time: 0.056 trainign loss: 6.6968 avg training loss: 7.5162
batch: [19660/21305] batch time: 0.734 trainign loss: 6.4720 avg training loss: 7.5161
batch: [19670/21305] batch time: 0.058 trainign loss: 6.4432 avg training loss: 7.5160
batch: [19680/21305] batch time: 0.052 trainign loss: 0.0321 avg training loss: 7.5156
batch: [19690/21305] batch time: 0.057 trainign loss: 7.9557 avg training loss: 7.5156
batch: [19700/21305] batch time: 0.062 trainign loss: 8.0126 avg training loss: 7.5157
batch: [19710/21305] batch time: 0.056 trainign loss: 6.5875 avg training loss: 7.5156
batch: [19720/21305] batch time: 0.054 trainign loss: 6.3754 avg training loss: 7.5155
batch: [19730/21305] batch time: 0.062 trainign loss: 6.1208 avg training loss: 7.5154
batch: [19740/21305] batch time: 0.053 trainign loss: 5.7604 avg training loss: 7.5154
batch: [19750/21305] batch time: 0.057 trainign loss: 7.2637 avg training loss: 7.5153
batch: [19760/21305] batch time: 0.056 trainign loss: 6.5296 avg training loss: 7.5152
batch: [19770/21305] batch time: 0.056 trainign loss: 5.9442 avg training loss: 7.5152
batch: [19780/21305] batch time: 0.053 trainign loss: 5.3365 avg training loss: 7.5151
batch: [19790/21305] batch time: 0.339 trainign loss: 6.1680 avg training loss: 7.5149
batch: [19800/21305] batch time: 0.058 trainign loss: 6.2688 avg training loss: 7.5149
batch: [19810/21305] batch time: 0.055 trainign loss: 5.3696 avg training loss: 7.5147
batch: [19820/21305] batch time: 0.060 trainign loss: 6.7973 avg training loss: 7.5146
batch: [19830/21305] batch time: 0.570 trainign loss: 7.1050 avg training loss: 7.5146
batch: [19840/21305] batch time: 0.056 trainign loss: 6.2134 avg training loss: 7.5145
batch: [19850/21305] batch time: 0.812 trainign loss: 6.5816 avg training loss: 7.5144
batch: [19860/21305] batch time: 0.058 trainign loss: 7.5062 avg training loss: 7.5143
batch: [19870/21305] batch time: 0.313 trainign loss: 7.1704 avg training loss: 7.5143
batch: [19880/21305] batch time: 0.057 trainign loss: 5.3366 avg training loss: 7.5141
batch: [19890/21305] batch time: 0.080 trainign loss: 4.8377 avg training loss: 7.5140
batch: [19900/21305] batch time: 0.059 trainign loss: 9.2098 avg training loss: 7.5138
batch: [19910/21305] batch time: 0.053 trainign loss: 1.5805 avg training loss: 7.5134
batch: [19920/21305] batch time: 0.053 trainign loss: 7.4580 avg training loss: 7.5135
batch: [19930/21305] batch time: 0.063 trainign loss: 7.9375 avg training loss: 7.5135
batch: [19940/21305] batch time: 0.061 trainign loss: 6.1796 avg training loss: 7.5134
batch: [19950/21305] batch time: 0.381 trainign loss: 6.6731 avg training loss: 7.5133
batch: [19960/21305] batch time: 0.063 trainign loss: 6.4365 avg training loss: 7.5131
batch: [19970/21305] batch time: 0.909 trainign loss: 7.2612 avg training loss: 7.5130
batch: [19980/21305] batch time: 0.056 trainign loss: 7.5954 avg training loss: 7.5130
batch: [19990/21305] batch time: 0.284 trainign loss: 6.0558 avg training loss: 7.5130
batch: [20000/21305] batch time: 0.056 trainign loss: 6.7044 avg training loss: 7.5128
batch: [20010/21305] batch time: 0.051 trainign loss: 6.0162 avg training loss: 7.5127
batch: [20020/21305] batch time: 0.056 trainign loss: 1.0112 avg training loss: 7.5125
batch: [20030/21305] batch time: 0.325 trainign loss: 6.9417 avg training loss: 7.5123
batch: [20040/21305] batch time: 0.057 trainign loss: 5.4549 avg training loss: 7.5122
batch: [20050/21305] batch time: 0.431 trainign loss: 2.2975 avg training loss: 7.5120
batch: [20060/21305] batch time: 0.055 trainign loss: 4.8772 avg training loss: 7.5118
batch: [20070/21305] batch time: 0.063 trainign loss: 6.8977 avg training loss: 7.5117
batch: [20080/21305] batch time: 0.730 trainign loss: 6.7785 avg training loss: 7.5117
batch: [20090/21305] batch time: 0.056 trainign loss: 4.8437 avg training loss: 7.5116
batch: [20100/21305] batch time: 0.907 trainign loss: 10.3650 avg training loss: 7.5113
batch: [20110/21305] batch time: 0.056 trainign loss: 6.8793 avg training loss: 7.5113
batch: [20120/21305] batch time: 0.290 trainign loss: 7.7002 avg training loss: 7.5113
batch: [20130/21305] batch time: 0.056 trainign loss: 7.0727 avg training loss: 7.5113
batch: [20140/21305] batch time: 0.051 trainign loss: 6.9179 avg training loss: 7.5112
batch: [20150/21305] batch time: 0.056 trainign loss: 6.0537 avg training loss: 7.5112
batch: [20160/21305] batch time: 0.054 trainign loss: 7.0636 avg training loss: 7.5111
batch: [20170/21305] batch time: 0.057 trainign loss: 7.1041 avg training loss: 7.5110
batch: [20180/21305] batch time: 0.051 trainign loss: 6.3495 avg training loss: 7.5109
batch: [20190/21305] batch time: 0.051 trainign loss: 3.4349 avg training loss: 7.5108
batch: [20200/21305] batch time: 0.054 trainign loss: 5.3974 avg training loss: 7.5107
batch: [20210/21305] batch time: 0.056 trainign loss: 7.2025 avg training loss: 7.5105
batch: [20220/21305] batch time: 0.063 trainign loss: 7.3912 avg training loss: 7.5105
batch: [20230/21305] batch time: 0.056 trainign loss: 6.6797 avg training loss: 7.5104
batch: [20240/21305] batch time: 0.056 trainign loss: 5.6132 avg training loss: 7.5103
batch: [20250/21305] batch time: 0.062 trainign loss: 6.9718 avg training loss: 7.5102
batch: [20260/21305] batch time: 0.052 trainign loss: 7.0067 avg training loss: 7.5101
batch: [20270/21305] batch time: 0.051 trainign loss: 1.7807 avg training loss: 7.5099
batch: [20280/21305] batch time: 0.063 trainign loss: 7.1984 avg training loss: 7.5098
batch: [20290/21305] batch time: 0.058 trainign loss: 7.4796 avg training loss: 7.5098
batch: [20300/21305] batch time: 0.053 trainign loss: 6.8859 avg training loss: 7.5097
batch: [20310/21305] batch time: 1.049 trainign loss: 6.8055 avg training loss: 7.5097
batch: [20320/21305] batch time: 0.056 trainign loss: 6.6621 avg training loss: 7.5096
batch: [20330/21305] batch time: 1.476 trainign loss: 4.7676 avg training loss: 7.5095
batch: [20340/21305] batch time: 0.057 trainign loss: 6.3506 avg training loss: 7.5093
batch: [20350/21305] batch time: 1.215 trainign loss: 7.2658 avg training loss: 7.5093
batch: [20360/21305] batch time: 0.060 trainign loss: 6.9552 avg training loss: 7.5092
batch: [20370/21305] batch time: 0.578 trainign loss: 6.6910 avg training loss: 7.5091
batch: [20380/21305] batch time: 0.063 trainign loss: 6.4654 avg training loss: 7.5090
batch: [20390/21305] batch time: 0.056 trainign loss: 7.2468 avg training loss: 7.5090
batch: [20400/21305] batch time: 0.062 trainign loss: 6.7820 avg training loss: 7.5089
batch: [20410/21305] batch time: 0.206 trainign loss: 6.6324 avg training loss: 7.5088
batch: [20420/21305] batch time: 0.062 trainign loss: 6.0233 avg training loss: 7.5087
batch: [20430/21305] batch time: 0.246 trainign loss: 7.1983 avg training loss: 7.5085
batch: [20440/21305] batch time: 0.056 trainign loss: 6.2293 avg training loss: 7.5084
batch: [20450/21305] batch time: 0.974 trainign loss: 5.9012 avg training loss: 7.5084
batch: [20460/21305] batch time: 0.056 trainign loss: 7.1998 avg training loss: 7.5083
batch: [20470/21305] batch time: 0.360 trainign loss: 4.8624 avg training loss: 7.5082
batch: [20480/21305] batch time: 0.058 trainign loss: 0.0855 avg training loss: 7.5078
batch: [20490/21305] batch time: 0.480 trainign loss: 13.8798 avg training loss: 7.5074
batch: [20500/21305] batch time: 0.056 trainign loss: 8.6101 avg training loss: 7.5076
batch: [20510/21305] batch time: 0.305 trainign loss: 7.7062 avg training loss: 7.5076
batch: [20520/21305] batch time: 0.063 trainign loss: 6.8318 avg training loss: 7.5076
batch: [20530/21305] batch time: 0.661 trainign loss: 6.4005 avg training loss: 7.5075
batch: [20540/21305] batch time: 0.061 trainign loss: 9.1626 avg training loss: 7.5073
batch: [20550/21305] batch time: 0.052 trainign loss: 7.0596 avg training loss: 7.5073
batch: [20560/21305] batch time: 0.057 trainign loss: 6.6908 avg training loss: 7.5071
batch: [20570/21305] batch time: 0.056 trainign loss: 7.8855 avg training loss: 7.5071
batch: [20580/21305] batch time: 0.056 trainign loss: 6.7669 avg training loss: 7.5071
batch: [20590/21305] batch time: 0.062 trainign loss: 6.3750 avg training loss: 7.5070
batch: [20600/21305] batch time: 0.380 trainign loss: 6.4863 avg training loss: 7.5069
batch: [20610/21305] batch time: 0.056 trainign loss: 7.3617 avg training loss: 7.5069
batch: [20620/21305] batch time: 0.060 trainign loss: 7.1134 avg training loss: 7.5069
batch: [20630/21305] batch time: 0.062 trainign loss: 6.6621 avg training loss: 7.5068
batch: [20640/21305] batch time: 0.059 trainign loss: 6.7942 avg training loss: 7.5067
batch: [20650/21305] batch time: 0.056 trainign loss: 7.0339 avg training loss: 7.5067
batch: [20660/21305] batch time: 0.576 trainign loss: 5.8277 avg training loss: 7.5066
batch: [20670/21305] batch time: 0.062 trainign loss: 7.5402 avg training loss: 7.5065
batch: [20680/21305] batch time: 1.667 trainign loss: 5.8229 avg training loss: 7.5065
batch: [20690/21305] batch time: 0.056 trainign loss: 6.6456 avg training loss: 7.5064
batch: [20700/21305] batch time: 1.519 trainign loss: 7.0232 avg training loss: 7.5063
batch: [20710/21305] batch time: 0.057 trainign loss: 6.9239 avg training loss: 7.5062
batch: [20720/21305] batch time: 0.524 trainign loss: 7.0453 avg training loss: 7.5062
batch: [20730/21305] batch time: 0.056 trainign loss: 6.6470 avg training loss: 7.5061
batch: [20740/21305] batch time: 0.172 trainign loss: 6.4198 avg training loss: 7.5061
batch: [20750/21305] batch time: 0.061 trainign loss: 5.3748 avg training loss: 7.5060
batch: [20760/21305] batch time: 0.055 trainign loss: 7.5460 avg training loss: 7.5059
batch: [20770/21305] batch time: 0.056 trainign loss: 6.7266 avg training loss: 7.5059
batch: [20780/21305] batch time: 0.623 trainign loss: 6.8664 avg training loss: 7.5059
batch: [20790/21305] batch time: 0.056 trainign loss: 6.1704 avg training loss: 7.5058
batch: [20800/21305] batch time: 1.251 trainign loss: 6.5898 avg training loss: 7.5057
batch: [20810/21305] batch time: 0.057 trainign loss: 7.3244 avg training loss: 7.5057
batch: [20820/21305] batch time: 0.689 trainign loss: 6.5687 avg training loss: 7.5056
batch: [20830/21305] batch time: 0.056 trainign loss: 6.6436 avg training loss: 7.5056
batch: [20840/21305] batch time: 1.645 trainign loss: 5.9769 avg training loss: 7.5055
batch: [20850/21305] batch time: 0.062 trainign loss: 5.8730 avg training loss: 7.5053
batch: [20860/21305] batch time: 1.935 trainign loss: 6.6550 avg training loss: 7.5053
batch: [20870/21305] batch time: 0.056 trainign loss: 6.3136 avg training loss: 7.5052
batch: [20880/21305] batch time: 2.067 trainign loss: 5.6205 avg training loss: 7.5051
batch: [20890/21305] batch time: 0.055 trainign loss: 7.4901 avg training loss: 7.5050
batch: [20900/21305] batch time: 2.161 trainign loss: 4.8021 avg training loss: 7.5049
batch: [20910/21305] batch time: 0.057 trainign loss: 5.9286 avg training loss: 7.5047
batch: [20920/21305] batch time: 1.081 trainign loss: 5.1184 avg training loss: 7.5046
batch: [20930/21305] batch time: 0.058 trainign loss: 7.8884 avg training loss: 7.5045
batch: [20940/21305] batch time: 1.112 trainign loss: 2.8648 avg training loss: 7.5042
batch: [20950/21305] batch time: 0.061 trainign loss: 6.9819 avg training loss: 7.5042
batch: [20960/21305] batch time: 1.582 trainign loss: 7.0842 avg training loss: 7.5042
batch: [20970/21305] batch time: 0.058 trainign loss: 5.9226 avg training loss: 7.5042
batch: [20980/21305] batch time: 1.558 trainign loss: 7.3370 avg training loss: 7.5041
batch: [20990/21305] batch time: 0.061 trainign loss: 7.4201 avg training loss: 7.5041
batch: [21000/21305] batch time: 1.649 trainign loss: 6.4419 avg training loss: 7.5040
batch: [21010/21305] batch time: 0.056 trainign loss: 6.8873 avg training loss: 7.5039
batch: [21020/21305] batch time: 0.326 trainign loss: 6.3011 avg training loss: 7.5039
batch: [21030/21305] batch time: 0.056 trainign loss: 6.9170 avg training loss: 7.5038
batch: [21040/21305] batch time: 0.257 trainign loss: 7.4604 avg training loss: 7.5038
batch: [21050/21305] batch time: 0.056 trainign loss: 6.6582 avg training loss: 7.5037
batch: [21060/21305] batch time: 0.139 trainign loss: 6.2385 avg training loss: 7.5036
batch: [21070/21305] batch time: 0.062 trainign loss: 6.0544 avg training loss: 7.5035
batch: [21080/21305] batch time: 0.222 trainign loss: 6.4177 avg training loss: 7.5033
batch: [21090/21305] batch time: 0.062 trainign loss: 7.3289 avg training loss: 7.5032
batch: [21100/21305] batch time: 0.362 trainign loss: 4.5166 avg training loss: 7.5031
batch: [21110/21305] batch time: 0.056 trainign loss: 7.8673 avg training loss: 7.5030
batch: [21120/21305] batch time: 0.056 trainign loss: 7.5408 avg training loss: 7.5029
batch: [21130/21305] batch time: 0.063 trainign loss: 7.4403 avg training loss: 7.5029
batch: [21140/21305] batch time: 0.481 trainign loss: 7.0091 avg training loss: 7.5028
batch: [21150/21305] batch time: 0.056 trainign loss: 7.1459 avg training loss: 7.5027
batch: [21160/21305] batch time: 0.211 trainign loss: 6.7477 avg training loss: 7.5026
batch: [21170/21305] batch time: 0.059 trainign loss: 4.8094 avg training loss: 7.5024
batch: [21180/21305] batch time: 0.213 trainign loss: 7.6319 avg training loss: 7.5024
batch: [21190/21305] batch time: 0.062 trainign loss: 7.1862 avg training loss: 7.5024
batch: [21200/21305] batch time: 0.056 trainign loss: 6.8383 avg training loss: 7.5023
batch: [21210/21305] batch time: 0.056 trainign loss: 7.1341 avg training loss: 7.5023
batch: [21220/21305] batch time: 0.051 trainign loss: 6.1175 avg training loss: 7.5022
batch: [21230/21305] batch time: 0.056 trainign loss: 6.5789 avg training loss: 7.5021
batch: [21240/21305] batch time: 0.056 trainign loss: 5.2411 avg training loss: 7.5020
batch: [21250/21305] batch time: 0.056 trainign loss: 7.1201 avg training loss: 7.5020
batch: [21260/21305] batch time: 0.051 trainign loss: 7.3218 avg training loss: 7.5020
batch: [21270/21305] batch time: 0.061 trainign loss: 3.8394 avg training loss: 7.5018
batch: [21280/21305] batch time: 0.051 trainign loss: 6.2001 avg training loss: 7.5017
batch: [21290/21305] batch time: 0.056 trainign loss: 4.8564 avg training loss: 7.5015
batch: [21300/21305] batch time: 0.056 trainign loss: 7.5782 avg training loss: 7.5015
Epoch: 7
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.862 trainign loss: 7.1468 avg training loss: 7.5015
batch: [10/21305] batch time: 0.056 trainign loss: 8.3407 avg training loss: 7.5012
batch: [20/21305] batch time: 2.350 trainign loss: 7.7936 avg training loss: 7.5012
batch: [30/21305] batch time: 0.058 trainign loss: 7.1434 avg training loss: 7.5012
batch: [40/21305] batch time: 1.595 trainign loss: 7.1071 avg training loss: 7.5011
batch: [50/21305] batch time: 0.056 trainign loss: 4.3807 avg training loss: 7.5010
batch: [60/21305] batch time: 0.911 trainign loss: 7.3634 avg training loss: 7.5010
batch: [70/21305] batch time: 0.057 trainign loss: 7.5696 avg training loss: 7.5009
batch: [80/21305] batch time: 1.029 trainign loss: 6.9636 avg training loss: 7.5009
batch: [90/21305] batch time: 0.061 trainign loss: 6.8145 avg training loss: 7.5008
batch: [100/21305] batch time: 0.287 trainign loss: 6.8821 avg training loss: 7.5007
batch: [110/21305] batch time: 0.055 trainign loss: 6.2214 avg training loss: 7.5006
batch: [120/21305] batch time: 0.294 trainign loss: 6.7689 avg training loss: 7.5005
batch: [130/21305] batch time: 0.057 trainign loss: 7.3222 avg training loss: 7.5004
batch: [140/21305] batch time: 0.063 trainign loss: 5.1001 avg training loss: 7.5003
batch: [150/21305] batch time: 0.056 trainign loss: 8.0484 avg training loss: 7.5002
batch: [160/21305] batch time: 0.057 trainign loss: 6.8334 avg training loss: 7.5002
batch: [170/21305] batch time: 0.062 trainign loss: 6.7105 avg training loss: 7.5001
batch: [180/21305] batch time: 0.734 trainign loss: 6.3081 avg training loss: 7.5000
batch: [190/21305] batch time: 0.058 trainign loss: 5.1175 avg training loss: 7.4999
batch: [200/21305] batch time: 0.062 trainign loss: 0.0076 avg training loss: 7.4994
batch: [210/21305] batch time: 0.055 trainign loss: 8.9454 avg training loss: 7.4994
batch: [220/21305] batch time: 0.051 trainign loss: 7.7387 avg training loss: 7.4994
batch: [230/21305] batch time: 0.063 trainign loss: 6.3771 avg training loss: 7.4994
batch: [240/21305] batch time: 0.056 trainign loss: 7.6590 avg training loss: 7.4994
batch: [250/21305] batch time: 0.056 trainign loss: 7.6107 avg training loss: 7.4993
batch: [260/21305] batch time: 0.051 trainign loss: 6.8978 avg training loss: 7.4993
batch: [270/21305] batch time: 0.056 trainign loss: 5.1690 avg training loss: 7.4992
batch: [280/21305] batch time: 0.051 trainign loss: 6.3055 avg training loss: 7.4991
batch: [290/21305] batch time: 0.056 trainign loss: 6.8210 avg training loss: 7.4990
batch: [300/21305] batch time: 0.061 trainign loss: 6.9814 avg training loss: 7.4989
batch: [310/21305] batch time: 0.061 trainign loss: 5.9017 avg training loss: 7.4989
batch: [320/21305] batch time: 0.057 trainign loss: 7.2636 avg training loss: 7.4988
batch: [330/21305] batch time: 0.058 trainign loss: 6.7829 avg training loss: 7.4987
batch: [340/21305] batch time: 0.056 trainign loss: 7.6374 avg training loss: 7.4987
batch: [350/21305] batch time: 0.057 trainign loss: 7.8085 avg training loss: 7.4987
batch: [360/21305] batch time: 0.051 trainign loss: 7.3257 avg training loss: 7.4986
batch: [370/21305] batch time: 0.056 trainign loss: 5.9555 avg training loss: 7.4985
batch: [380/21305] batch time: 0.057 trainign loss: 6.5713 avg training loss: 7.4985
batch: [390/21305] batch time: 0.056 trainign loss: 4.0262 avg training loss: 7.4983
batch: [400/21305] batch time: 0.056 trainign loss: 6.8036 avg training loss: 7.4982
batch: [410/21305] batch time: 1.388 trainign loss: 7.6951 avg training loss: 7.4981
batch: [420/21305] batch time: 0.063 trainign loss: 7.1988 avg training loss: 7.4980
batch: [430/21305] batch time: 0.327 trainign loss: 6.9008 avg training loss: 7.4980
batch: [440/21305] batch time: 0.053 trainign loss: 6.2835 avg training loss: 7.4979
batch: [450/21305] batch time: 0.607 trainign loss: 5.5538 avg training loss: 7.4978
batch: [460/21305] batch time: 0.057 trainign loss: 7.3553 avg training loss: 7.4977
batch: [470/21305] batch time: 0.391 trainign loss: 6.8201 avg training loss: 7.4977
batch: [480/21305] batch time: 0.062 trainign loss: 7.3106 avg training loss: 7.4977
batch: [490/21305] batch time: 0.542 trainign loss: 6.9565 avg training loss: 7.4976
batch: [500/21305] batch time: 0.055 trainign loss: 4.3026 avg training loss: 7.4975
batch: [510/21305] batch time: 0.056 trainign loss: 5.7139 avg training loss: 7.4975
batch: [520/21305] batch time: 0.051 trainign loss: 4.1185 avg training loss: 7.4974
batch: [530/21305] batch time: 0.056 trainign loss: 6.5946 avg training loss: 7.4973
batch: [540/21305] batch time: 0.062 trainign loss: 6.5745 avg training loss: 7.4972
batch: [550/21305] batch time: 0.057 trainign loss: 4.6367 avg training loss: 7.4970
batch: [560/21305] batch time: 0.056 trainign loss: 6.9588 avg training loss: 7.4970
batch: [570/21305] batch time: 0.056 trainign loss: 6.3484 avg training loss: 7.4969
batch: [580/21305] batch time: 0.051 trainign loss: 6.8105 avg training loss: 7.4968
batch: [590/21305] batch time: 0.056 trainign loss: 6.9452 avg training loss: 7.4967
batch: [600/21305] batch time: 0.051 trainign loss: 6.6881 avg training loss: 7.4966
batch: [610/21305] batch time: 0.056 trainign loss: 7.6573 avg training loss: 7.4966
batch: [620/21305] batch time: 0.057 trainign loss: 6.9872 avg training loss: 7.4966
batch: [630/21305] batch time: 0.056 trainign loss: 6.2860 avg training loss: 7.4965
batch: [640/21305] batch time: 0.054 trainign loss: 7.1677 avg training loss: 7.4965
batch: [650/21305] batch time: 0.056 trainign loss: 6.1897 avg training loss: 7.4964
batch: [660/21305] batch time: 0.063 trainign loss: 5.8662 avg training loss: 7.4962
batch: [670/21305] batch time: 0.061 trainign loss: 8.2918 avg training loss: 7.4961
batch: [680/21305] batch time: 0.054 trainign loss: 8.2192 avg training loss: 7.4961
batch: [690/21305] batch time: 0.057 trainign loss: 6.6940 avg training loss: 7.4961
batch: [700/21305] batch time: 0.056 trainign loss: 7.7386 avg training loss: 7.4960
batch: [710/21305] batch time: 0.056 trainign loss: 6.7306 avg training loss: 7.4959
batch: [720/21305] batch time: 0.056 trainign loss: 6.4854 avg training loss: 7.4959
batch: [730/21305] batch time: 0.056 trainign loss: 7.7016 avg training loss: 7.4958
batch: [740/21305] batch time: 0.056 trainign loss: 7.7041 avg training loss: 7.4958
batch: [750/21305] batch time: 0.062 trainign loss: 6.8998 avg training loss: 7.4957
batch: [760/21305] batch time: 0.056 trainign loss: 6.5605 avg training loss: 7.4956
batch: [770/21305] batch time: 0.063 trainign loss: 4.8037 avg training loss: 7.4955
batch: [780/21305] batch time: 0.057 trainign loss: 7.5143 avg training loss: 7.4954
batch: [790/21305] batch time: 0.056 trainign loss: 7.2767 avg training loss: 7.4954
batch: [800/21305] batch time: 0.056 trainign loss: 7.1129 avg training loss: 7.4954
batch: [810/21305] batch time: 0.056 trainign loss: 6.4300 avg training loss: 7.4953
batch: [820/21305] batch time: 0.051 trainign loss: 7.0608 avg training loss: 7.4952
batch: [830/21305] batch time: 0.062 trainign loss: 6.4974 avg training loss: 7.4952
batch: [840/21305] batch time: 0.062 trainign loss: 6.9400 avg training loss: 7.4951
batch: [850/21305] batch time: 0.058 trainign loss: 5.7352 avg training loss: 7.4950
batch: [860/21305] batch time: 0.052 trainign loss: 5.3342 avg training loss: 7.4949
batch: [870/21305] batch time: 0.059 trainign loss: 5.2273 avg training loss: 7.4948
batch: [880/21305] batch time: 0.056 trainign loss: 7.0135 avg training loss: 7.4946
batch: [890/21305] batch time: 0.058 trainign loss: 8.2912 avg training loss: 7.4945
batch: [900/21305] batch time: 0.063 trainign loss: 6.2969 avg training loss: 7.4945
batch: [910/21305] batch time: 0.053 trainign loss: 6.7397 avg training loss: 7.4945
batch: [920/21305] batch time: 0.056 trainign loss: 4.3040 avg training loss: 7.4943
batch: [930/21305] batch time: 0.062 trainign loss: 6.7966 avg training loss: 7.4942
batch: [940/21305] batch time: 0.061 trainign loss: 4.2456 avg training loss: 7.4941
batch: [950/21305] batch time: 0.062 trainign loss: 0.0042 avg training loss: 7.4936
batch: [960/21305] batch time: 0.053 trainign loss: 7.9431 avg training loss: 7.4936
batch: [970/21305] batch time: 0.062 trainign loss: 7.9278 avg training loss: 7.4937
batch: [980/21305] batch time: 0.062 trainign loss: 7.2714 avg training loss: 7.4937
batch: [990/21305] batch time: 0.058 trainign loss: 7.0130 avg training loss: 7.4936
batch: [1000/21305] batch time: 0.063 trainign loss: 4.3743 avg training loss: 7.4935
batch: [1010/21305] batch time: 0.456 trainign loss: 7.3575 avg training loss: 7.4934
batch: [1020/21305] batch time: 0.056 trainign loss: 7.9279 avg training loss: 7.4934
batch: [1030/21305] batch time: 0.053 trainign loss: 7.1205 avg training loss: 7.4933
batch: [1040/21305] batch time: 0.057 trainign loss: 7.2612 avg training loss: 7.4933
batch: [1050/21305] batch time: 0.053 trainign loss: 6.9007 avg training loss: 7.4932
batch: [1060/21305] batch time: 0.055 trainign loss: 5.7910 avg training loss: 7.4931
batch: [1070/21305] batch time: 0.062 trainign loss: 5.0724 avg training loss: 7.4930
batch: [1080/21305] batch time: 0.063 trainign loss: 0.0773 avg training loss: 7.4926
batch: [1090/21305] batch time: 0.056 trainign loss: 7.8439 avg training loss: 7.4925
batch: [1100/21305] batch time: 0.056 trainign loss: 6.2662 avg training loss: 7.4924
batch: [1110/21305] batch time: 0.056 trainign loss: 6.6953 avg training loss: 7.4923
batch: [1120/21305] batch time: 0.056 trainign loss: 7.6351 avg training loss: 7.4923
batch: [1130/21305] batch time: 0.056 trainign loss: 6.1024 avg training loss: 7.4923
batch: [1140/21305] batch time: 0.057 trainign loss: 7.2235 avg training loss: 7.4922
batch: [1150/21305] batch time: 0.051 trainign loss: 6.9236 avg training loss: 7.4921
batch: [1160/21305] batch time: 0.056 trainign loss: 7.1079 avg training loss: 7.4920
batch: [1170/21305] batch time: 0.056 trainign loss: 6.8566 avg training loss: 7.4920
batch: [1180/21305] batch time: 0.051 trainign loss: 6.6280 avg training loss: 7.4919
batch: [1190/21305] batch time: 0.058 trainign loss: 7.2263 avg training loss: 7.4918
batch: [1200/21305] batch time: 0.063 trainign loss: 5.1969 avg training loss: 7.4917
batch: [1210/21305] batch time: 0.057 trainign loss: 7.0949 avg training loss: 7.4916
batch: [1220/21305] batch time: 0.053 trainign loss: 7.7159 avg training loss: 7.4916
batch: [1230/21305] batch time: 0.052 trainign loss: 6.5320 avg training loss: 7.4915
batch: [1240/21305] batch time: 0.060 trainign loss: 6.7127 avg training loss: 7.4914
batch: [1250/21305] batch time: 0.058 trainign loss: 7.2740 avg training loss: 7.4914
batch: [1260/21305] batch time: 0.058 trainign loss: 5.8741 avg training loss: 7.4913
batch: [1270/21305] batch time: 0.055 trainign loss: 6.8176 avg training loss: 7.4913
batch: [1280/21305] batch time: 0.056 trainign loss: 7.4307 avg training loss: 7.4912
batch: [1290/21305] batch time: 0.057 trainign loss: 6.5483 avg training loss: 7.4912
batch: [1300/21305] batch time: 0.055 trainign loss: 6.6035 avg training loss: 7.4911
batch: [1310/21305] batch time: 0.053 trainign loss: 7.0754 avg training loss: 7.4911
batch: [1320/21305] batch time: 0.058 trainign loss: 5.5535 avg training loss: 7.4909
batch: [1330/21305] batch time: 0.055 trainign loss: 7.3973 avg training loss: 7.4909
batch: [1340/21305] batch time: 0.063 trainign loss: 7.4188 avg training loss: 7.4909
batch: [1350/21305] batch time: 0.056 trainign loss: 7.1797 avg training loss: 7.4908
batch: [1360/21305] batch time: 0.062 trainign loss: 4.6681 avg training loss: 7.4907
batch: [1370/21305] batch time: 0.056 trainign loss: 7.8769 avg training loss: 7.4906
batch: [1380/21305] batch time: 0.056 trainign loss: 6.1507 avg training loss: 7.4905
batch: [1390/21305] batch time: 0.056 trainign loss: 6.8738 avg training loss: 7.4904
batch: [1400/21305] batch time: 0.056 trainign loss: 6.3769 avg training loss: 7.4904
batch: [1410/21305] batch time: 0.056 trainign loss: 6.6562 avg training loss: 7.4902
batch: [1420/21305] batch time: 0.058 trainign loss: 6.1750 avg training loss: 7.4900
batch: [1430/21305] batch time: 0.056 trainign loss: 6.0071 avg training loss: 7.4900
batch: [1440/21305] batch time: 0.056 trainign loss: 5.4919 avg training loss: 7.4899
batch: [1450/21305] batch time: 0.136 trainign loss: 6.5279 avg training loss: 7.4898
batch: [1460/21305] batch time: 0.061 trainign loss: 6.7041 avg training loss: 7.4898
batch: [1470/21305] batch time: 0.964 trainign loss: 5.8231 avg training loss: 7.4896
batch: [1480/21305] batch time: 0.062 trainign loss: 6.9661 avg training loss: 7.4895
batch: [1490/21305] batch time: 1.249 trainign loss: 7.0330 avg training loss: 7.4895
batch: [1500/21305] batch time: 0.063 trainign loss: 6.6156 avg training loss: 7.4894
batch: [1510/21305] batch time: 0.827 trainign loss: 6.7895 avg training loss: 7.4893
batch: [1520/21305] batch time: 0.062 trainign loss: 2.8864 avg training loss: 7.4891
batch: [1530/21305] batch time: 1.030 trainign loss: 11.6977 avg training loss: 7.4887
batch: [1540/21305] batch time: 0.057 trainign loss: 7.4864 avg training loss: 7.4888
batch: [1550/21305] batch time: 0.812 trainign loss: 5.2805 avg training loss: 7.4888
batch: [1560/21305] batch time: 0.056 trainign loss: 6.4831 avg training loss: 7.4888
batch: [1570/21305] batch time: 0.057 trainign loss: 6.1786 avg training loss: 7.4887
batch: [1580/21305] batch time: 0.062 trainign loss: 6.3174 avg training loss: 7.4886
batch: [1590/21305] batch time: 0.051 trainign loss: 8.6445 avg training loss: 7.4883
batch: [1600/21305] batch time: 0.056 trainign loss: 7.9305 avg training loss: 7.4883
batch: [1610/21305] batch time: 0.546 trainign loss: 7.3232 avg training loss: 7.4884
batch: [1620/21305] batch time: 0.056 trainign loss: 6.5406 avg training loss: 7.4883
batch: [1630/21305] batch time: 1.138 trainign loss: 7.4549 avg training loss: 7.4882
batch: [1640/21305] batch time: 0.062 trainign loss: 7.0995 avg training loss: 7.4881
batch: [1650/21305] batch time: 2.501 trainign loss: 5.9294 avg training loss: 7.4881
batch: [1660/21305] batch time: 0.062 trainign loss: 6.0194 avg training loss: 7.4880
batch: [1670/21305] batch time: 1.280 trainign loss: 6.7502 avg training loss: 7.4879
batch: [1680/21305] batch time: 0.062 trainign loss: 6.0303 avg training loss: 7.4879
batch: [1690/21305] batch time: 1.056 trainign loss: 6.5372 avg training loss: 7.4878
batch: [1700/21305] batch time: 0.056 trainign loss: 6.5673 avg training loss: 7.4878
batch: [1710/21305] batch time: 1.449 trainign loss: 4.9668 avg training loss: 7.4876
batch: [1720/21305] batch time: 0.062 trainign loss: 7.2718 avg training loss: 7.4875
batch: [1730/21305] batch time: 1.069 trainign loss: 6.3629 avg training loss: 7.4875
batch: [1740/21305] batch time: 0.057 trainign loss: 6.0692 avg training loss: 7.4874
batch: [1750/21305] batch time: 1.480 trainign loss: 6.4101 avg training loss: 7.4873
batch: [1760/21305] batch time: 0.058 trainign loss: 6.6863 avg training loss: 7.4872
batch: [1770/21305] batch time: 2.126 trainign loss: 5.9117 avg training loss: 7.4871
batch: [1780/21305] batch time: 0.063 trainign loss: 6.3040 avg training loss: 7.4870
batch: [1790/21305] batch time: 2.059 trainign loss: 6.5263 avg training loss: 7.4869
batch: [1800/21305] batch time: 0.057 trainign loss: 6.4897 avg training loss: 7.4869
batch: [1810/21305] batch time: 2.475 trainign loss: 6.2171 avg training loss: 7.4868
batch: [1820/21305] batch time: 0.055 trainign loss: 4.3369 avg training loss: 7.4866
batch: [1830/21305] batch time: 2.202 trainign loss: 7.7677 avg training loss: 7.4865
batch: [1840/21305] batch time: 0.577 trainign loss: 7.3165 avg training loss: 7.4865
batch: [1850/21305] batch time: 1.678 trainign loss: 6.8269 avg training loss: 7.4865
batch: [1860/21305] batch time: 0.930 trainign loss: 6.3934 avg training loss: 7.4864
batch: [1870/21305] batch time: 0.544 trainign loss: 6.6605 avg training loss: 7.4864
batch: [1880/21305] batch time: 1.527 trainign loss: 4.3259 avg training loss: 7.4863
batch: [1890/21305] batch time: 0.940 trainign loss: 4.5833 avg training loss: 7.4860
batch: [1900/21305] batch time: 0.329 trainign loss: 7.4310 avg training loss: 7.4859
batch: [1910/21305] batch time: 1.067 trainign loss: 6.7438 avg training loss: 7.4859
batch: [1920/21305] batch time: 0.266 trainign loss: 5.9601 avg training loss: 7.4858
batch: [1930/21305] batch time: 2.075 trainign loss: 2.6469 avg training loss: 7.4856
batch: [1940/21305] batch time: 0.060 trainign loss: 6.2823 avg training loss: 7.4855
batch: [1950/21305] batch time: 2.138 trainign loss: 6.1002 avg training loss: 7.4854
batch: [1960/21305] batch time: 0.062 trainign loss: 4.5458 avg training loss: 7.4853
batch: [1970/21305] batch time: 2.213 trainign loss: 6.7039 avg training loss: 7.4852
batch: [1980/21305] batch time: 0.057 trainign loss: 4.4532 avg training loss: 7.4851
batch: [1990/21305] batch time: 2.239 trainign loss: 6.5319 avg training loss: 7.4850
batch: [2000/21305] batch time: 0.057 trainign loss: 5.4819 avg training loss: 7.4849
batch: [2010/21305] batch time: 1.724 trainign loss: 6.5814 avg training loss: 7.4848
batch: [2020/21305] batch time: 0.064 trainign loss: 2.9034 avg training loss: 7.4847
batch: [2030/21305] batch time: 2.031 trainign loss: 6.0496 avg training loss: 7.4845
batch: [2040/21305] batch time: 1.091 trainign loss: 6.8456 avg training loss: 7.4845
batch: [2050/21305] batch time: 1.000 trainign loss: 5.6393 avg training loss: 7.4844
batch: [2060/21305] batch time: 1.341 trainign loss: 2.5422 avg training loss: 7.4842
batch: [2070/21305] batch time: 1.190 trainign loss: 1.2590 avg training loss: 7.4840
batch: [2080/21305] batch time: 0.891 trainign loss: 0.0501 avg training loss: 7.4836
batch: [2090/21305] batch time: 1.667 trainign loss: 7.8441 avg training loss: 7.4834
batch: [2100/21305] batch time: 1.467 trainign loss: 8.0285 avg training loss: 7.4834
batch: [2110/21305] batch time: 0.628 trainign loss: 6.3634 avg training loss: 7.4834
batch: [2120/21305] batch time: 1.594 trainign loss: 0.4684 avg training loss: 7.4831
batch: [2130/21305] batch time: 0.410 trainign loss: 0.0004 avg training loss: 7.4825
batch: [2140/21305] batch time: 1.878 trainign loss: 0.0001 avg training loss: 7.4820
batch: [2150/21305] batch time: 0.130 trainign loss: 7.7425 avg training loss: 7.4821
batch: [2160/21305] batch time: 0.560 trainign loss: 7.8720 avg training loss: 7.4821
batch: [2170/21305] batch time: 1.195 trainign loss: 7.7189 avg training loss: 7.4821
batch: [2180/21305] batch time: 0.923 trainign loss: 7.3316 avg training loss: 7.4821
batch: [2190/21305] batch time: 0.645 trainign loss: 6.3958 avg training loss: 7.4820
batch: [2200/21305] batch time: 1.448 trainign loss: 6.5486 avg training loss: 7.4819
batch: [2210/21305] batch time: 0.725 trainign loss: 5.6388 avg training loss: 7.4818
batch: [2220/21305] batch time: 2.139 trainign loss: 6.8296 avg training loss: 7.4818
batch: [2230/21305] batch time: 0.879 trainign loss: 6.5696 avg training loss: 7.4817
batch: [2240/21305] batch time: 1.191 trainign loss: 6.5826 avg training loss: 7.4815
batch: [2250/21305] batch time: 0.795 trainign loss: 6.3052 avg training loss: 7.4815
batch: [2260/21305] batch time: 2.029 trainign loss: 5.8289 avg training loss: 7.4813
batch: [2270/21305] batch time: 0.873 trainign loss: 3.5676 avg training loss: 7.4811
batch: [2280/21305] batch time: 1.634 trainign loss: 6.2333 avg training loss: 7.4809
batch: [2290/21305] batch time: 0.545 trainign loss: 4.3509 avg training loss: 7.4807
batch: [2300/21305] batch time: 1.311 trainign loss: 5.6314 avg training loss: 7.4807
batch: [2310/21305] batch time: 1.028 trainign loss: 7.4084 avg training loss: 7.4806
batch: [2320/21305] batch time: 1.322 trainign loss: 8.2256 avg training loss: 7.4805
batch: [2330/21305] batch time: 1.345 trainign loss: 7.4703 avg training loss: 7.4805
batch: [2340/21305] batch time: 0.793 trainign loss: 5.1934 avg training loss: 7.4805
batch: [2350/21305] batch time: 1.328 trainign loss: 5.4412 avg training loss: 7.4804
batch: [2360/21305] batch time: 0.155 trainign loss: 7.0338 avg training loss: 7.4803
batch: [2370/21305] batch time: 2.264 trainign loss: 5.0146 avg training loss: 7.4801
batch: [2380/21305] batch time: 0.054 trainign loss: 6.0356 avg training loss: 7.4801
batch: [2390/21305] batch time: 2.036 trainign loss: 6.7017 avg training loss: 7.4800
batch: [2400/21305] batch time: 0.057 trainign loss: 5.8083 avg training loss: 7.4799
batch: [2410/21305] batch time: 1.722 trainign loss: 7.9434 avg training loss: 7.4799
batch: [2420/21305] batch time: 0.061 trainign loss: 6.9351 avg training loss: 7.4798
batch: [2430/21305] batch time: 2.183 trainign loss: 5.1777 avg training loss: 7.4798
batch: [2440/21305] batch time: 0.056 trainign loss: 6.9410 avg training loss: 7.4797
batch: [2450/21305] batch time: 2.095 trainign loss: 7.2290 avg training loss: 7.4796
batch: [2460/21305] batch time: 0.062 trainign loss: 6.5560 avg training loss: 7.4795
batch: [2470/21305] batch time: 2.501 trainign loss: 6.5586 avg training loss: 7.4794
batch: [2480/21305] batch time: 0.056 trainign loss: 7.1737 avg training loss: 7.4793
batch: [2490/21305] batch time: 1.980 trainign loss: 6.5412 avg training loss: 7.4792
batch: [2500/21305] batch time: 0.057 trainign loss: 7.6035 avg training loss: 7.4791
batch: [2510/21305] batch time: 0.788 trainign loss: 5.7056 avg training loss: 7.4791
batch: [2520/21305] batch time: 0.056 trainign loss: 5.2246 avg training loss: 7.4790
batch: [2530/21305] batch time: 0.204 trainign loss: 5.3509 avg training loss: 7.4789
batch: [2540/21305] batch time: 0.062 trainign loss: 6.6235 avg training loss: 7.4788
batch: [2550/21305] batch time: 0.450 trainign loss: 5.8955 avg training loss: 7.4787
batch: [2560/21305] batch time: 0.062 trainign loss: 6.1526 avg training loss: 7.4787
batch: [2570/21305] batch time: 0.891 trainign loss: 6.9358 avg training loss: 7.4786
batch: [2580/21305] batch time: 0.059 trainign loss: 5.4149 avg training loss: 7.4785
batch: [2590/21305] batch time: 1.739 trainign loss: 5.8248 avg training loss: 7.4784
batch: [2600/21305] batch time: 0.063 trainign loss: 0.2781 avg training loss: 7.4781
batch: [2610/21305] batch time: 1.874 trainign loss: 7.9190 avg training loss: 7.4780
batch: [2620/21305] batch time: 0.056 trainign loss: 8.0844 avg training loss: 7.4781
batch: [2630/21305] batch time: 1.511 trainign loss: 6.9516 avg training loss: 7.4780
batch: [2640/21305] batch time: 0.057 trainign loss: 3.7495 avg training loss: 7.4779
batch: [2650/21305] batch time: 2.150 trainign loss: 7.8073 avg training loss: 7.4778
batch: [2660/21305] batch time: 0.058 trainign loss: 7.4160 avg training loss: 7.4777
batch: [2670/21305] batch time: 2.165 trainign loss: 6.6346 avg training loss: 7.4777
batch: [2680/21305] batch time: 0.248 trainign loss: 5.0488 avg training loss: 7.4775
batch: [2690/21305] batch time: 0.880 trainign loss: 7.6433 avg training loss: 7.4775
batch: [2700/21305] batch time: 1.154 trainign loss: 7.2515 avg training loss: 7.4775
batch: [2710/21305] batch time: 1.493 trainign loss: 5.1656 avg training loss: 7.4773
batch: [2720/21305] batch time: 0.886 trainign loss: 2.0398 avg training loss: 7.4771
batch: [2730/21305] batch time: 0.896 trainign loss: 7.8597 avg training loss: 7.4771
batch: [2740/21305] batch time: 1.511 trainign loss: 7.5427 avg training loss: 7.4770
batch: [2750/21305] batch time: 0.794 trainign loss: 6.9414 avg training loss: 7.4770
batch: [2760/21305] batch time: 1.832 trainign loss: 6.9135 avg training loss: 7.4769
batch: [2770/21305] batch time: 0.061 trainign loss: 7.2409 avg training loss: 7.4769
batch: [2780/21305] batch time: 2.452 trainign loss: 7.5237 avg training loss: 7.4768
batch: [2790/21305] batch time: 0.056 trainign loss: 6.8158 avg training loss: 7.4768
batch: [2800/21305] batch time: 2.234 trainign loss: 6.8444 avg training loss: 7.4767
batch: [2810/21305] batch time: 0.605 trainign loss: 6.2257 avg training loss: 7.4766
batch: [2820/21305] batch time: 0.589 trainign loss: 6.6329 avg training loss: 7.4764
batch: [2830/21305] batch time: 2.085 trainign loss: 7.2941 avg training loss: 7.4763
batch: [2840/21305] batch time: 0.434 trainign loss: 1.8517 avg training loss: 7.4762
batch: [2850/21305] batch time: 1.264 trainign loss: 5.3988 avg training loss: 7.4760
batch: [2860/21305] batch time: 1.526 trainign loss: 6.8333 avg training loss: 7.4760
batch: [2870/21305] batch time: 0.484 trainign loss: 7.9773 avg training loss: 7.4758
batch: [2880/21305] batch time: 2.135 trainign loss: 6.9032 avg training loss: 7.4758
batch: [2890/21305] batch time: 0.514 trainign loss: 6.8792 avg training loss: 7.4757
batch: [2900/21305] batch time: 1.347 trainign loss: 6.5282 avg training loss: 7.4756
batch: [2910/21305] batch time: 0.602 trainign loss: 7.1670 avg training loss: 7.4756
batch: [2920/21305] batch time: 1.909 trainign loss: 6.3896 avg training loss: 7.4756
batch: [2930/21305] batch time: 0.056 trainign loss: 5.2912 avg training loss: 7.4755
batch: [2940/21305] batch time: 2.305 trainign loss: 5.1347 avg training loss: 7.4753
batch: [2950/21305] batch time: 0.056 trainign loss: 0.4304 avg training loss: 7.4750
batch: [2960/21305] batch time: 2.064 trainign loss: 7.0090 avg training loss: 7.4750
batch: [2970/21305] batch time: 0.053 trainign loss: 6.2995 avg training loss: 7.4750
batch: [2980/21305] batch time: 2.057 trainign loss: 6.9850 avg training loss: 7.4749
batch: [2990/21305] batch time: 0.061 trainign loss: 6.6016 avg training loss: 7.4749
batch: [3000/21305] batch time: 2.543 trainign loss: 4.6793 avg training loss: 7.4748
batch: [3010/21305] batch time: 0.056 trainign loss: 8.0037 avg training loss: 7.4747
batch: [3020/21305] batch time: 2.064 trainign loss: 5.0677 avg training loss: 7.4746
batch: [3030/21305] batch time: 0.062 trainign loss: 6.1832 avg training loss: 7.4745
batch: [3040/21305] batch time: 1.920 trainign loss: 6.3587 avg training loss: 7.4745
batch: [3050/21305] batch time: 0.054 trainign loss: 7.3805 avg training loss: 7.4744
batch: [3060/21305] batch time: 2.473 trainign loss: 6.3872 avg training loss: 7.4744
batch: [3070/21305] batch time: 0.056 trainign loss: 6.2511 avg training loss: 7.4742
batch: [3080/21305] batch time: 2.290 trainign loss: 7.0099 avg training loss: 7.4741
batch: [3090/21305] batch time: 0.056 trainign loss: 6.3298 avg training loss: 7.4741
batch: [3100/21305] batch time: 2.082 trainign loss: 6.9343 avg training loss: 7.4740
batch: [3110/21305] batch time: 0.057 trainign loss: 7.2499 avg training loss: 7.4739
batch: [3120/21305] batch time: 2.284 trainign loss: 5.7894 avg training loss: 7.4738
batch: [3130/21305] batch time: 0.063 trainign loss: 0.5975 avg training loss: 7.4735
batch: [3140/21305] batch time: 1.906 trainign loss: 7.5737 avg training loss: 7.4735
batch: [3150/21305] batch time: 0.056 trainign loss: 6.8736 avg training loss: 7.4735
batch: [3160/21305] batch time: 1.647 trainign loss: 7.0924 avg training loss: 7.4734
batch: [3170/21305] batch time: 0.054 trainign loss: 6.2578 avg training loss: 7.4733
batch: [3180/21305] batch time: 2.180 trainign loss: 5.9722 avg training loss: 7.4733
batch: [3190/21305] batch time: 0.062 trainign loss: 6.2805 avg training loss: 7.4732
batch: [3200/21305] batch time: 1.653 trainign loss: 6.9937 avg training loss: 7.4731
batch: [3210/21305] batch time: 0.051 trainign loss: 6.9821 avg training loss: 7.4731
batch: [3220/21305] batch time: 0.872 trainign loss: 6.8303 avg training loss: 7.4730
batch: [3230/21305] batch time: 0.063 trainign loss: 6.8970 avg training loss: 7.4729
batch: [3240/21305] batch time: 1.393 trainign loss: 5.9653 avg training loss: 7.4729
batch: [3250/21305] batch time: 0.062 trainign loss: 6.1747 avg training loss: 7.4727
batch: [3260/21305] batch time: 0.581 trainign loss: 7.1165 avg training loss: 7.4727
batch: [3270/21305] batch time: 0.388 trainign loss: 5.4441 avg training loss: 7.4725
batch: [3280/21305] batch time: 0.199 trainign loss: 3.1679 avg training loss: 7.4724
batch: [3290/21305] batch time: 0.056 trainign loss: 6.2007 avg training loss: 7.4722
batch: [3300/21305] batch time: 0.060 trainign loss: 5.5359 avg training loss: 7.4720
batch: [3310/21305] batch time: 0.063 trainign loss: 0.4644 avg training loss: 7.4718
batch: [3320/21305] batch time: 0.060 trainign loss: 7.3815 avg training loss: 7.4717
batch: [3330/21305] batch time: 0.056 trainign loss: 6.5588 avg training loss: 7.4717
batch: [3340/21305] batch time: 0.056 trainign loss: 4.3362 avg training loss: 7.4716
batch: [3350/21305] batch time: 0.051 trainign loss: 5.1271 avg training loss: 7.4715
batch: [3360/21305] batch time: 0.056 trainign loss: 7.5661 avg training loss: 7.4713
batch: [3370/21305] batch time: 0.056 trainign loss: 3.6953 avg training loss: 7.4711
batch: [3380/21305] batch time: 0.057 trainign loss: 8.0373 avg training loss: 7.4711
batch: [3390/21305] batch time: 0.054 trainign loss: 6.9483 avg training loss: 7.4710
batch: [3400/21305] batch time: 0.056 trainign loss: 6.2459 avg training loss: 7.4710
batch: [3410/21305] batch time: 0.056 trainign loss: 5.8426 avg training loss: 7.4709
batch: [3420/21305] batch time: 0.056 trainign loss: 6.8608 avg training loss: 7.4707
batch: [3430/21305] batch time: 0.055 trainign loss: 6.9853 avg training loss: 7.4707
batch: [3440/21305] batch time: 0.058 trainign loss: 7.1295 avg training loss: 7.4707
batch: [3450/21305] batch time: 0.054 trainign loss: 5.6887 avg training loss: 7.4705
batch: [3460/21305] batch time: 0.057 trainign loss: 5.9465 avg training loss: 7.4704
batch: [3470/21305] batch time: 0.057 trainign loss: 7.2230 avg training loss: 7.4704
batch: [3480/21305] batch time: 0.057 trainign loss: 7.3342 avg training loss: 7.4703
batch: [3490/21305] batch time: 0.056 trainign loss: 6.8600 avg training loss: 7.4703
batch: [3500/21305] batch time: 0.057 trainign loss: 6.3960 avg training loss: 7.4702
batch: [3510/21305] batch time: 0.052 trainign loss: 7.0443 avg training loss: 7.4702
batch: [3520/21305] batch time: 0.056 trainign loss: 6.3661 avg training loss: 7.4701
batch: [3530/21305] batch time: 0.051 trainign loss: 6.9898 avg training loss: 7.4700
batch: [3540/21305] batch time: 0.057 trainign loss: 5.3781 avg training loss: 7.4699
batch: [3550/21305] batch time: 0.056 trainign loss: 7.2199 avg training loss: 7.4697
batch: [3560/21305] batch time: 0.057 trainign loss: 6.5454 avg training loss: 7.4696
batch: [3570/21305] batch time: 0.157 trainign loss: 7.6158 avg training loss: 7.4696
batch: [3580/21305] batch time: 0.062 trainign loss: 6.2703 avg training loss: 7.4695
batch: [3590/21305] batch time: 0.055 trainign loss: 4.7132 avg training loss: 7.4695
batch: [3600/21305] batch time: 0.063 trainign loss: 7.2079 avg training loss: 7.4693
batch: [3610/21305] batch time: 0.056 trainign loss: 8.0043 avg training loss: 7.4692
batch: [3620/21305] batch time: 0.062 trainign loss: 7.2337 avg training loss: 7.4692
batch: [3630/21305] batch time: 0.051 trainign loss: 6.9069 avg training loss: 7.4692
batch: [3640/21305] batch time: 0.056 trainign loss: 6.5980 avg training loss: 7.4691
batch: [3650/21305] batch time: 0.056 trainign loss: 5.9171 avg training loss: 7.4690
batch: [3660/21305] batch time: 0.056 trainign loss: 5.6208 avg training loss: 7.4689
batch: [3670/21305] batch time: 0.051 trainign loss: 5.2363 avg training loss: 7.4688
batch: [3680/21305] batch time: 0.059 trainign loss: 7.2700 avg training loss: 7.4687
batch: [3690/21305] batch time: 0.054 trainign loss: 6.4335 avg training loss: 7.4687
batch: [3700/21305] batch time: 0.063 trainign loss: 5.5352 avg training loss: 7.4686
batch: [3710/21305] batch time: 0.056 trainign loss: 5.4750 avg training loss: 7.4685
batch: [3720/21305] batch time: 0.706 trainign loss: 6.1698 avg training loss: 7.4684
batch: [3730/21305] batch time: 0.056 trainign loss: 6.0109 avg training loss: 7.4683
batch: [3740/21305] batch time: 1.445 trainign loss: 7.2439 avg training loss: 7.4683
batch: [3750/21305] batch time: 0.056 trainign loss: 6.8905 avg training loss: 7.4682
batch: [3760/21305] batch time: 1.457 trainign loss: 6.2605 avg training loss: 7.4681
batch: [3770/21305] batch time: 0.590 trainign loss: 4.3180 avg training loss: 7.4679
batch: [3780/21305] batch time: 1.464 trainign loss: 6.7337 avg training loss: 7.4679
batch: [3790/21305] batch time: 0.063 trainign loss: 4.9487 avg training loss: 7.4678
batch: [3800/21305] batch time: 1.095 trainign loss: 6.1797 avg training loss: 7.4677
batch: [3810/21305] batch time: 0.056 trainign loss: 7.2191 avg training loss: 7.4677
batch: [3820/21305] batch time: 0.590 trainign loss: 4.4455 avg training loss: 7.4675
batch: [3830/21305] batch time: 0.054 trainign loss: 6.4527 avg training loss: 7.4675
batch: [3840/21305] batch time: 0.249 trainign loss: 6.4416 avg training loss: 7.4674
batch: [3850/21305] batch time: 0.059 trainign loss: 6.2551 avg training loss: 7.4673
batch: [3860/21305] batch time: 0.336 trainign loss: 6.2701 avg training loss: 7.4672
batch: [3870/21305] batch time: 0.055 trainign loss: 5.0274 avg training loss: 7.4671
batch: [3880/21305] batch time: 1.266 trainign loss: 0.1071 avg training loss: 7.4668
batch: [3890/21305] batch time: 0.056 trainign loss: 7.3706 avg training loss: 7.4666
batch: [3900/21305] batch time: 1.842 trainign loss: 8.0013 avg training loss: 7.4666
batch: [3910/21305] batch time: 0.056 trainign loss: 7.2263 avg training loss: 7.4666
batch: [3920/21305] batch time: 2.528 trainign loss: 4.7952 avg training loss: 7.4665
batch: [3930/21305] batch time: 0.056 trainign loss: 6.3887 avg training loss: 7.4664
batch: [3940/21305] batch time: 2.518 trainign loss: 5.8224 avg training loss: 7.4663
batch: [3950/21305] batch time: 0.056 trainign loss: 6.4587 avg training loss: 7.4662
batch: [3960/21305] batch time: 2.390 trainign loss: 7.4148 avg training loss: 7.4662
batch: [3970/21305] batch time: 0.056 trainign loss: 6.7945 avg training loss: 7.4661
batch: [3980/21305] batch time: 1.951 trainign loss: 5.7576 avg training loss: 7.4660
batch: [3990/21305] batch time: 0.055 trainign loss: 5.2444 avg training loss: 7.4659
batch: [4000/21305] batch time: 2.265 trainign loss: 7.6662 avg training loss: 7.4658
batch: [4010/21305] batch time: 0.056 trainign loss: 7.1827 avg training loss: 7.4657
batch: [4020/21305] batch time: 2.212 trainign loss: 4.4827 avg training loss: 7.4656
batch: [4030/21305] batch time: 0.057 trainign loss: 6.7170 avg training loss: 7.4655
batch: [4040/21305] batch time: 2.475 trainign loss: 6.6669 avg training loss: 7.4655
batch: [4050/21305] batch time: 0.056 trainign loss: 4.4427 avg training loss: 7.4653
batch: [4060/21305] batch time: 2.391 trainign loss: 10.0095 avg training loss: 7.4649
batch: [4070/21305] batch time: 0.058 trainign loss: 9.6153 avg training loss: 7.4649
batch: [4080/21305] batch time: 2.309 trainign loss: 8.2186 avg training loss: 7.4649
batch: [4090/21305] batch time: 0.056 trainign loss: 7.7506 avg training loss: 7.4649
batch: [4100/21305] batch time: 2.326 trainign loss: 6.5509 avg training loss: 7.4649
batch: [4110/21305] batch time: 0.054 trainign loss: 5.4091 avg training loss: 7.4648
batch: [4120/21305] batch time: 2.173 trainign loss: 5.5703 avg training loss: 7.4647
batch: [4130/21305] batch time: 0.055 trainign loss: 6.4301 avg training loss: 7.4646
batch: [4140/21305] batch time: 1.704 trainign loss: 6.1200 avg training loss: 7.4644
batch: [4150/21305] batch time: 0.058 trainign loss: 8.0763 avg training loss: 7.4644
batch: [4160/21305] batch time: 1.891 trainign loss: 7.2473 avg training loss: 7.4644
batch: [4170/21305] batch time: 0.056 trainign loss: 6.0399 avg training loss: 7.4643
batch: [4180/21305] batch time: 2.213 trainign loss: 6.9898 avg training loss: 7.4642
batch: [4190/21305] batch time: 0.056 trainign loss: 7.1873 avg training loss: 7.4641
batch: [4200/21305] batch time: 1.947 trainign loss: 6.1492 avg training loss: 7.4640
batch: [4210/21305] batch time: 0.056 trainign loss: 1.2248 avg training loss: 7.4638
batch: [4220/21305] batch time: 2.269 trainign loss: 6.7009 avg training loss: 7.4637
batch: [4230/21305] batch time: 0.056 trainign loss: 6.8134 avg training loss: 7.4635
batch: [4240/21305] batch time: 2.450 trainign loss: 5.0825 avg training loss: 7.4634
batch: [4250/21305] batch time: 0.057 trainign loss: 0.4879 avg training loss: 7.4630
batch: [4260/21305] batch time: 2.459 trainign loss: 7.5167 avg training loss: 7.4630
batch: [4270/21305] batch time: 0.056 trainign loss: 8.6249 avg training loss: 7.4630
batch: [4280/21305] batch time: 2.297 trainign loss: 7.3973 avg training loss: 7.4631
batch: [4290/21305] batch time: 0.058 trainign loss: 6.8645 avg training loss: 7.4630
batch: [4300/21305] batch time: 2.228 trainign loss: 6.3554 avg training loss: 7.4630
batch: [4310/21305] batch time: 0.063 trainign loss: 7.0225 avg training loss: 7.4629
batch: [4320/21305] batch time: 2.081 trainign loss: 5.4933 avg training loss: 7.4628
batch: [4330/21305] batch time: 0.055 trainign loss: 4.5511 avg training loss: 7.4627
batch: [4340/21305] batch time: 2.524 trainign loss: 5.3612 avg training loss: 7.4625
batch: [4350/21305] batch time: 0.056 trainign loss: 5.9260 avg training loss: 7.4625
batch: [4360/21305] batch time: 2.143 trainign loss: 4.8619 avg training loss: 7.4623
batch: [4370/21305] batch time: 0.060 trainign loss: 7.6807 avg training loss: 7.4621
batch: [4380/21305] batch time: 2.468 trainign loss: 6.7140 avg training loss: 7.4621
batch: [4390/21305] batch time: 0.056 trainign loss: 6.1227 avg training loss: 7.4620
batch: [4400/21305] batch time: 2.065 trainign loss: 7.2896 avg training loss: 7.4619
batch: [4410/21305] batch time: 0.054 trainign loss: 6.7442 avg training loss: 7.4619
batch: [4420/21305] batch time: 2.641 trainign loss: 5.7292 avg training loss: 7.4618
batch: [4430/21305] batch time: 0.057 trainign loss: 1.2437 avg training loss: 7.4616
batch: [4440/21305] batch time: 1.784 trainign loss: 8.3895 avg training loss: 7.4613
batch: [4450/21305] batch time: 0.052 trainign loss: 6.8232 avg training loss: 7.4612
batch: [4460/21305] batch time: 2.378 trainign loss: 3.8512 avg training loss: 7.4611
batch: [4470/21305] batch time: 0.057 trainign loss: 8.5819 avg training loss: 7.4608
batch: [4480/21305] batch time: 2.059 trainign loss: 6.9628 avg training loss: 7.4608
batch: [4490/21305] batch time: 0.059 trainign loss: 3.9807 avg training loss: 7.4607
batch: [4500/21305] batch time: 2.196 trainign loss: 7.9655 avg training loss: 7.4607
batch: [4510/21305] batch time: 0.055 trainign loss: 7.3845 avg training loss: 7.4606
batch: [4520/21305] batch time: 1.659 trainign loss: 7.7101 avg training loss: 7.4606
batch: [4530/21305] batch time: 0.056 trainign loss: 7.4628 avg training loss: 7.4605
batch: [4540/21305] batch time: 1.159 trainign loss: 6.5407 avg training loss: 7.4605
batch: [4550/21305] batch time: 0.063 trainign loss: 6.1178 avg training loss: 7.4604
batch: [4560/21305] batch time: 1.025 trainign loss: 7.2177 avg training loss: 7.4603
batch: [4570/21305] batch time: 0.057 trainign loss: 7.1055 avg training loss: 7.4602
batch: [4580/21305] batch time: 1.218 trainign loss: 7.1435 avg training loss: 7.4602
batch: [4590/21305] batch time: 0.054 trainign loss: 5.3584 avg training loss: 7.4601
batch: [4600/21305] batch time: 0.844 trainign loss: 6.6028 avg training loss: 7.4600
batch: [4610/21305] batch time: 0.056 trainign loss: 6.1923 avg training loss: 7.4599
batch: [4620/21305] batch time: 0.632 trainign loss: 6.3911 avg training loss: 7.4598
batch: [4630/21305] batch time: 0.056 trainign loss: 6.2310 avg training loss: 7.4597
batch: [4640/21305] batch time: 1.087 trainign loss: 5.5552 avg training loss: 7.4597
batch: [4650/21305] batch time: 0.057 trainign loss: 6.5206 avg training loss: 7.4596
batch: [4660/21305] batch time: 0.967 trainign loss: 7.1746 avg training loss: 7.4596
batch: [4670/21305] batch time: 0.056 trainign loss: 6.7729 avg training loss: 7.4594
batch: [4680/21305] batch time: 0.735 trainign loss: 7.7426 avg training loss: 7.4594
batch: [4690/21305] batch time: 0.056 trainign loss: 5.2901 avg training loss: 7.4593
batch: [4700/21305] batch time: 0.865 trainign loss: 6.4912 avg training loss: 7.4592
batch: [4710/21305] batch time: 0.056 trainign loss: 7.8727 avg training loss: 7.4592
batch: [4720/21305] batch time: 0.977 trainign loss: 7.0125 avg training loss: 7.4592
batch: [4730/21305] batch time: 0.056 trainign loss: 6.3096 avg training loss: 7.4591
batch: [4740/21305] batch time: 1.024 trainign loss: 7.6937 avg training loss: 7.4590
batch: [4750/21305] batch time: 0.056 trainign loss: 7.1891 avg training loss: 7.4590
batch: [4760/21305] batch time: 2.134 trainign loss: 4.7561 avg training loss: 7.4589
batch: [4770/21305] batch time: 0.156 trainign loss: 6.1472 avg training loss: 7.4587
batch: [4780/21305] batch time: 2.201 trainign loss: 4.0369 avg training loss: 7.4586
batch: [4790/21305] batch time: 0.056 trainign loss: 5.9620 avg training loss: 7.4585
batch: [4800/21305] batch time: 2.145 trainign loss: 7.4483 avg training loss: 7.4584
batch: [4810/21305] batch time: 0.056 trainign loss: 5.5769 avg training loss: 7.4584
batch: [4820/21305] batch time: 2.291 trainign loss: 6.6075 avg training loss: 7.4583
batch: [4830/21305] batch time: 0.056 trainign loss: 5.9147 avg training loss: 7.4582
batch: [4840/21305] batch time: 2.409 trainign loss: 1.2134 avg training loss: 7.4579
batch: [4850/21305] batch time: 0.060 trainign loss: 9.9946 avg training loss: 7.4577
batch: [4860/21305] batch time: 2.224 trainign loss: 6.7589 avg training loss: 7.4577
batch: [4870/21305] batch time: 0.056 trainign loss: 8.4128 avg training loss: 7.4577
batch: [4880/21305] batch time: 2.263 trainign loss: 7.3957 avg training loss: 7.4577
batch: [4890/21305] batch time: 0.057 trainign loss: 6.8186 avg training loss: 7.4577
batch: [4900/21305] batch time: 1.098 trainign loss: 6.1525 avg training loss: 7.4575
batch: [4910/21305] batch time: 0.057 trainign loss: 4.9410 avg training loss: 7.4574
batch: [4920/21305] batch time: 1.463 trainign loss: 8.0668 avg training loss: 7.4574
batch: [4930/21305] batch time: 0.063 trainign loss: 5.5040 avg training loss: 7.4573
batch: [4940/21305] batch time: 0.519 trainign loss: 7.4779 avg training loss: 7.4572
batch: [4950/21305] batch time: 0.056 trainign loss: 5.8558 avg training loss: 7.4571
batch: [4960/21305] batch time: 0.063 trainign loss: 6.8287 avg training loss: 7.4570
batch: [4970/21305] batch time: 0.051 trainign loss: 6.3295 avg training loss: 7.4569
batch: [4980/21305] batch time: 0.063 trainign loss: 4.4948 avg training loss: 7.4568
batch: [4990/21305] batch time: 0.058 trainign loss: 0.0146 avg training loss: 7.4563
batch: [5000/21305] batch time: 0.057 trainign loss: 6.9096 avg training loss: 7.4562
batch: [5010/21305] batch time: 0.057 trainign loss: 8.0216 avg training loss: 7.4563
batch: [5020/21305] batch time: 0.059 trainign loss: 6.8204 avg training loss: 7.4562
batch: [5030/21305] batch time: 0.056 trainign loss: 6.1096 avg training loss: 7.4561
batch: [5040/21305] batch time: 0.062 trainign loss: 4.0801 avg training loss: 7.4560
batch: [5050/21305] batch time: 0.056 trainign loss: 6.1508 avg training loss: 7.4558
batch: [5060/21305] batch time: 0.057 trainign loss: 7.4957 avg training loss: 7.4557
batch: [5070/21305] batch time: 0.318 trainign loss: 4.5530 avg training loss: 7.4557
batch: [5080/21305] batch time: 0.057 trainign loss: 0.7622 avg training loss: 7.4552
batch: [5090/21305] batch time: 0.051 trainign loss: 6.8344 avg training loss: 7.4553
batch: [5100/21305] batch time: 0.063 trainign loss: 7.3354 avg training loss: 7.4552
batch: [5110/21305] batch time: 0.595 trainign loss: 6.5157 avg training loss: 7.4552
batch: [5120/21305] batch time: 0.062 trainign loss: 6.9092 avg training loss: 7.4552
batch: [5130/21305] batch time: 0.561 trainign loss: 6.7960 avg training loss: 7.4551
batch: [5140/21305] batch time: 0.062 trainign loss: 6.5605 avg training loss: 7.4549
batch: [5150/21305] batch time: 0.233 trainign loss: 7.6392 avg training loss: 7.4549
batch: [5160/21305] batch time: 0.056 trainign loss: 7.2987 avg training loss: 7.4549
batch: [5170/21305] batch time: 0.623 trainign loss: 7.0740 avg training loss: 7.4548
batch: [5180/21305] batch time: 0.063 trainign loss: 6.5707 avg training loss: 7.4548
batch: [5190/21305] batch time: 0.214 trainign loss: 6.5733 avg training loss: 7.4547
batch: [5200/21305] batch time: 0.058 trainign loss: 5.8927 avg training loss: 7.4546
batch: [5210/21305] batch time: 0.500 trainign loss: 5.8157 avg training loss: 7.4545
batch: [5220/21305] batch time: 0.062 trainign loss: 6.3216 avg training loss: 7.4544
batch: [5230/21305] batch time: 0.455 trainign loss: 6.1613 avg training loss: 7.4544
batch: [5240/21305] batch time: 0.056 trainign loss: 4.7731 avg training loss: 7.4543
batch: [5250/21305] batch time: 0.056 trainign loss: 5.5826 avg training loss: 7.4541
batch: [5260/21305] batch time: 0.055 trainign loss: 6.9520 avg training loss: 7.4541
batch: [5270/21305] batch time: 0.056 trainign loss: 7.0007 avg training loss: 7.4540
batch: [5280/21305] batch time: 0.056 trainign loss: 7.1390 avg training loss: 7.4540
batch: [5290/21305] batch time: 0.056 trainign loss: 5.7610 avg training loss: 7.4539
batch: [5300/21305] batch time: 0.057 trainign loss: 6.4591 avg training loss: 7.4538
batch: [5310/21305] batch time: 0.051 trainign loss: 6.9004 avg training loss: 7.4537
batch: [5320/21305] batch time: 0.056 trainign loss: 7.2036 avg training loss: 7.4537
batch: [5330/21305] batch time: 0.052 trainign loss: 6.7429 avg training loss: 7.4536
batch: [5340/21305] batch time: 0.056 trainign loss: 6.4049 avg training loss: 7.4535
batch: [5350/21305] batch time: 0.101 trainign loss: 5.7741 avg training loss: 7.4534
batch: [5360/21305] batch time: 0.057 trainign loss: 8.0658 avg training loss: 7.4533
batch: [5370/21305] batch time: 0.231 trainign loss: 7.5076 avg training loss: 7.4533
batch: [5380/21305] batch time: 0.072 trainign loss: 7.1175 avg training loss: 7.4532
batch: [5390/21305] batch time: 0.637 trainign loss: 5.0632 avg training loss: 7.4531
batch: [5400/21305] batch time: 0.059 trainign loss: 6.8871 avg training loss: 7.4531
batch: [5410/21305] batch time: 0.537 trainign loss: 6.8962 avg training loss: 7.4530
batch: [5420/21305] batch time: 0.060 trainign loss: 7.2010 avg training loss: 7.4529
batch: [5430/21305] batch time: 0.181 trainign loss: 6.2362 avg training loss: 7.4528
batch: [5440/21305] batch time: 0.056 trainign loss: 6.3060 avg training loss: 7.4527
batch: [5450/21305] batch time: 0.056 trainign loss: 5.4473 avg training loss: 7.4526
batch: [5460/21305] batch time: 0.056 trainign loss: 7.5924 avg training loss: 7.4525
batch: [5470/21305] batch time: 0.056 trainign loss: 6.0420 avg training loss: 7.4524
batch: [5480/21305] batch time: 0.057 trainign loss: 7.4461 avg training loss: 7.4524
batch: [5490/21305] batch time: 0.056 trainign loss: 5.4300 avg training loss: 7.4523
batch: [5500/21305] batch time: 0.059 trainign loss: 5.6888 avg training loss: 7.4522
batch: [5510/21305] batch time: 0.367 trainign loss: 2.5498 avg training loss: 7.4520
batch: [5520/21305] batch time: 0.056 trainign loss: 7.4469 avg training loss: 7.4519
batch: [5530/21305] batch time: 0.516 trainign loss: 7.1986 avg training loss: 7.4519
batch: [5540/21305] batch time: 0.058 trainign loss: 6.7203 avg training loss: 7.4518
batch: [5550/21305] batch time: 0.062 trainign loss: 7.6585 avg training loss: 7.4517
batch: [5560/21305] batch time: 0.062 trainign loss: 5.3197 avg training loss: 7.4516
batch: [5570/21305] batch time: 0.056 trainign loss: 3.1278 avg training loss: 7.4515
batch: [5580/21305] batch time: 0.056 trainign loss: 8.6361 avg training loss: 7.4513
batch: [5590/21305] batch time: 0.056 trainign loss: 7.3676 avg training loss: 7.4514
batch: [5600/21305] batch time: 0.057 trainign loss: 6.7650 avg training loss: 7.4513
batch: [5610/21305] batch time: 0.056 trainign loss: 7.4665 avg training loss: 7.4512
batch: [5620/21305] batch time: 0.056 trainign loss: 7.3203 avg training loss: 7.4512
batch: [5630/21305] batch time: 0.062 trainign loss: 6.7274 avg training loss: 7.4511
batch: [5640/21305] batch time: 0.058 trainign loss: 5.9949 avg training loss: 7.4511
batch: [5650/21305] batch time: 0.057 trainign loss: 7.4724 avg training loss: 7.4510
batch: [5660/21305] batch time: 0.063 trainign loss: 4.4857 avg training loss: 7.4509
batch: [5670/21305] batch time: 0.056 trainign loss: 6.8805 avg training loss: 7.4508
batch: [5680/21305] batch time: 0.056 trainign loss: 6.0560 avg training loss: 7.4507
batch: [5690/21305] batch time: 0.053 trainign loss: 5.8075 avg training loss: 7.4507
batch: [5700/21305] batch time: 0.062 trainign loss: 6.3618 avg training loss: 7.4505
batch: [5710/21305] batch time: 0.052 trainign loss: 5.1811 avg training loss: 7.4504
batch: [5720/21305] batch time: 0.060 trainign loss: 6.6278 avg training loss: 7.4502
batch: [5730/21305] batch time: 0.055 trainign loss: 7.0562 avg training loss: 7.4502
batch: [5740/21305] batch time: 0.058 trainign loss: 7.2555 avg training loss: 7.4502
batch: [5750/21305] batch time: 0.056 trainign loss: 6.5815 avg training loss: 7.4502
batch: [5760/21305] batch time: 0.056 trainign loss: 5.8174 avg training loss: 7.4501
batch: [5770/21305] batch time: 0.050 trainign loss: 6.6608 avg training loss: 7.4501
batch: [5780/21305] batch time: 0.061 trainign loss: 6.2931 avg training loss: 7.4500
batch: [5790/21305] batch time: 0.050 trainign loss: 7.4673 avg training loss: 7.4499
batch: [5800/21305] batch time: 0.059 trainign loss: 6.2153 avg training loss: 7.4499
batch: [5810/21305] batch time: 0.056 trainign loss: 7.8331 avg training loss: 7.4499
batch: [5820/21305] batch time: 0.063 trainign loss: 7.3741 avg training loss: 7.4498
batch: [5830/21305] batch time: 0.063 trainign loss: 6.0258 avg training loss: 7.4498
batch: [5840/21305] batch time: 0.053 trainign loss: 6.7467 avg training loss: 7.4497
batch: [5850/21305] batch time: 0.053 trainign loss: 6.5137 avg training loss: 7.4496
batch: [5860/21305] batch time: 0.061 trainign loss: 7.3840 avg training loss: 7.4496
batch: [5870/21305] batch time: 0.061 trainign loss: 6.8814 avg training loss: 7.4496
batch: [5880/21305] batch time: 1.049 trainign loss: 6.0690 avg training loss: 7.4495
batch: [5890/21305] batch time: 0.056 trainign loss: 6.5441 avg training loss: 7.4494
batch: [5900/21305] batch time: 0.822 trainign loss: 5.3241 avg training loss: 7.4493
batch: [5910/21305] batch time: 0.056 trainign loss: 5.2603 avg training loss: 7.4492
batch: [5920/21305] batch time: 0.057 trainign loss: 5.5363 avg training loss: 7.4490
batch: [5930/21305] batch time: 0.062 trainign loss: 6.6158 avg training loss: 7.4490
batch: [5940/21305] batch time: 0.346 trainign loss: 5.4170 avg training loss: 7.4489
batch: [5950/21305] batch time: 0.062 trainign loss: 5.5106 avg training loss: 7.4488
batch: [5960/21305] batch time: 1.378 trainign loss: 4.5536 avg training loss: 7.4486
batch: [5970/21305] batch time: 0.057 trainign loss: 7.7510 avg training loss: 7.4486
batch: [5980/21305] batch time: 1.215 trainign loss: 6.1045 avg training loss: 7.4486
batch: [5990/21305] batch time: 0.056 trainign loss: 7.2335 avg training loss: 7.4485
batch: [6000/21305] batch time: 1.073 trainign loss: 6.9757 avg training loss: 7.4485
batch: [6010/21305] batch time: 0.062 trainign loss: 5.6942 avg training loss: 7.4484
batch: [6020/21305] batch time: 1.128 trainign loss: 6.8072 avg training loss: 7.4484
batch: [6030/21305] batch time: 0.060 trainign loss: 5.9656 avg training loss: 7.4482
batch: [6040/21305] batch time: 1.355 trainign loss: 2.0439 avg training loss: 7.4480
batch: [6050/21305] batch time: 0.056 trainign loss: 7.1292 avg training loss: 7.4479
batch: [6060/21305] batch time: 1.259 trainign loss: 7.4121 avg training loss: 7.4478
batch: [6070/21305] batch time: 0.061 trainign loss: 3.1063 avg training loss: 7.4477
batch: [6080/21305] batch time: 0.652 trainign loss: 4.5327 avg training loss: 7.4475
batch: [6090/21305] batch time: 0.052 trainign loss: 6.8638 avg training loss: 7.4474
batch: [6100/21305] batch time: 0.073 trainign loss: 8.0605 avg training loss: 7.4474
batch: [6110/21305] batch time: 1.385 trainign loss: 5.6799 avg training loss: 7.4474
batch: [6120/21305] batch time: 0.062 trainign loss: 6.7003 avg training loss: 7.4473
batch: [6130/21305] batch time: 0.784 trainign loss: 6.8623 avg training loss: 7.4471
batch: [6140/21305] batch time: 0.061 trainign loss: 6.0845 avg training loss: 7.4471
batch: [6150/21305] batch time: 1.054 trainign loss: 6.5481 avg training loss: 7.4470
batch: [6160/21305] batch time: 0.435 trainign loss: 6.1451 avg training loss: 7.4470
batch: [6170/21305] batch time: 0.922 trainign loss: 4.4207 avg training loss: 7.4469
batch: [6180/21305] batch time: 0.356 trainign loss: 7.3160 avg training loss: 7.4467
batch: [6190/21305] batch time: 0.209 trainign loss: 7.7611 avg training loss: 7.4468
batch: [6200/21305] batch time: 1.112 trainign loss: 6.7757 avg training loss: 7.4467
batch: [6210/21305] batch time: 0.062 trainign loss: 6.3476 avg training loss: 7.4467
batch: [6220/21305] batch time: 0.969 trainign loss: 6.8811 avg training loss: 7.4466
batch: [6230/21305] batch time: 0.058 trainign loss: 6.6167 avg training loss: 7.4465
batch: [6240/21305] batch time: 1.088 trainign loss: 7.3723 avg training loss: 7.4464
batch: [6250/21305] batch time: 0.063 trainign loss: 6.7895 avg training loss: 7.4464
batch: [6260/21305] batch time: 1.095 trainign loss: 7.5238 avg training loss: 7.4463
batch: [6270/21305] batch time: 0.056 trainign loss: 6.7883 avg training loss: 7.4463
batch: [6280/21305] batch time: 0.826 trainign loss: 7.0444 avg training loss: 7.4462
batch: [6290/21305] batch time: 0.062 trainign loss: 4.0596 avg training loss: 7.4461
batch: [6300/21305] batch time: 1.397 trainign loss: 7.5065 avg training loss: 7.4461
batch: [6310/21305] batch time: 0.062 trainign loss: 6.7125 avg training loss: 7.4460
batch: [6320/21305] batch time: 0.502 trainign loss: 7.1038 avg training loss: 7.4460
batch: [6330/21305] batch time: 0.056 trainign loss: 6.5594 avg training loss: 7.4459
batch: [6340/21305] batch time: 0.472 trainign loss: 4.3526 avg training loss: 7.4457
batch: [6350/21305] batch time: 0.056 trainign loss: 6.8802 avg training loss: 7.4456
batch: [6360/21305] batch time: 0.055 trainign loss: 7.3953 avg training loss: 7.4456
batch: [6370/21305] batch time: 0.056 trainign loss: 6.2243 avg training loss: 7.4456
batch: [6380/21305] batch time: 0.075 trainign loss: 6.6464 avg training loss: 7.4455
batch: [6390/21305] batch time: 0.063 trainign loss: 4.5212 avg training loss: 7.4454
batch: [6400/21305] batch time: 0.053 trainign loss: 6.6998 avg training loss: 7.4453
batch: [6410/21305] batch time: 0.056 trainign loss: 7.4663 avg training loss: 7.4452
batch: [6420/21305] batch time: 0.063 trainign loss: 6.3353 avg training loss: 7.4450
batch: [6430/21305] batch time: 0.056 trainign loss: 5.4546 avg training loss: 7.4449
batch: [6440/21305] batch time: 0.056 trainign loss: 6.2214 avg training loss: 7.4448
batch: [6450/21305] batch time: 0.057 trainign loss: 7.9678 avg training loss: 7.4446
batch: [6460/21305] batch time: 0.051 trainign loss: 7.7152 avg training loss: 7.4447
batch: [6470/21305] batch time: 0.056 trainign loss: 7.4215 avg training loss: 7.4446
batch: [6480/21305] batch time: 0.054 trainign loss: 6.5441 avg training loss: 7.4446
batch: [6490/21305] batch time: 0.062 trainign loss: 5.9062 avg training loss: 7.4445
batch: [6500/21305] batch time: 0.052 trainign loss: 5.6199 avg training loss: 7.4444
batch: [6510/21305] batch time: 0.055 trainign loss: 6.5275 avg training loss: 7.4444
batch: [6520/21305] batch time: 0.061 trainign loss: 6.4072 avg training loss: 7.4443
batch: [6530/21305] batch time: 0.058 trainign loss: 5.3172 avg training loss: 7.4442
batch: [6540/21305] batch time: 0.058 trainign loss: 7.0681 avg training loss: 7.4441
batch: [6550/21305] batch time: 0.050 trainign loss: 5.4870 avg training loss: 7.4440
batch: [6560/21305] batch time: 0.056 trainign loss: 6.2736 avg training loss: 7.4439
batch: [6570/21305] batch time: 0.062 trainign loss: 6.1226 avg training loss: 7.4438
batch: [6580/21305] batch time: 0.060 trainign loss: 4.5139 avg training loss: 7.4437
batch: [6590/21305] batch time: 0.062 trainign loss: 6.0178 avg training loss: 7.4437
batch: [6600/21305] batch time: 0.063 trainign loss: 7.2600 avg training loss: 7.4436
batch: [6610/21305] batch time: 0.063 trainign loss: 7.5436 avg training loss: 7.4435
batch: [6620/21305] batch time: 0.056 trainign loss: 6.5777 avg training loss: 7.4435
batch: [6630/21305] batch time: 0.050 trainign loss: 6.3221 avg training loss: 7.4433
batch: [6640/21305] batch time: 0.056 trainign loss: 4.7959 avg training loss: 7.4432
batch: [6650/21305] batch time: 0.057 trainign loss: 5.8384 avg training loss: 7.4432
batch: [6660/21305] batch time: 0.051 trainign loss: 5.6331 avg training loss: 7.4430
batch: [6670/21305] batch time: 0.056 trainign loss: 6.2797 avg training loss: 7.4429
batch: [6680/21305] batch time: 0.056 trainign loss: 7.3700 avg training loss: 7.4429
batch: [6690/21305] batch time: 0.056 trainign loss: 7.0216 avg training loss: 7.4429
batch: [6700/21305] batch time: 0.052 trainign loss: 4.6637 avg training loss: 7.4427
batch: [6710/21305] batch time: 0.056 trainign loss: 5.4338 avg training loss: 7.4426
batch: [6720/21305] batch time: 0.060 trainign loss: 6.6897 avg training loss: 7.4425
batch: [6730/21305] batch time: 0.056 trainign loss: 5.7236 avg training loss: 7.4424
batch: [6740/21305] batch time: 0.056 trainign loss: 7.5166 avg training loss: 7.4424
batch: [6750/21305] batch time: 0.056 trainign loss: 7.4376 avg training loss: 7.4424
batch: [6760/21305] batch time: 0.056 trainign loss: 6.6813 avg training loss: 7.4423
batch: [6770/21305] batch time: 0.056 trainign loss: 5.4313 avg training loss: 7.4422
batch: [6780/21305] batch time: 0.051 trainign loss: 5.8536 avg training loss: 7.4421
batch: [6790/21305] batch time: 0.058 trainign loss: 6.8676 avg training loss: 7.4421
batch: [6800/21305] batch time: 0.052 trainign loss: 5.7007 avg training loss: 7.4420
batch: [6810/21305] batch time: 0.056 trainign loss: 6.5965 avg training loss: 7.4419
batch: [6820/21305] batch time: 0.055 trainign loss: 5.2585 avg training loss: 7.4418
batch: [6830/21305] batch time: 0.056 trainign loss: 6.3384 avg training loss: 7.4417
batch: [6840/21305] batch time: 0.051 trainign loss: 7.7554 avg training loss: 7.4414
batch: [6850/21305] batch time: 0.056 trainign loss: 5.6653 avg training loss: 7.4414
batch: [6860/21305] batch time: 0.051 trainign loss: 6.3302 avg training loss: 7.4414
batch: [6870/21305] batch time: 0.062 trainign loss: 6.4308 avg training loss: 7.4413
batch: [6880/21305] batch time: 0.056 trainign loss: 6.0477 avg training loss: 7.4412
batch: [6890/21305] batch time: 0.056 trainign loss: 7.5828 avg training loss: 7.4410
batch: [6900/21305] batch time: 0.056 trainign loss: 6.6449 avg training loss: 7.4410
batch: [6910/21305] batch time: 0.057 trainign loss: 5.8769 avg training loss: 7.4409
batch: [6920/21305] batch time: 0.054 trainign loss: 6.9764 avg training loss: 7.4409
batch: [6930/21305] batch time: 0.056 trainign loss: 5.5118 avg training loss: 7.4408
batch: [6940/21305] batch time: 0.057 trainign loss: 5.1951 avg training loss: 7.4406
batch: [6950/21305] batch time: 0.057 trainign loss: 6.3514 avg training loss: 7.4405
batch: [6960/21305] batch time: 0.062 trainign loss: 4.7326 avg training loss: 7.4404
batch: [6970/21305] batch time: 0.056 trainign loss: 4.0401 avg training loss: 7.4403
batch: [6980/21305] batch time: 0.056 trainign loss: 6.6198 avg training loss: 7.4402
batch: [6990/21305] batch time: 0.057 trainign loss: 7.2830 avg training loss: 7.4402
batch: [7000/21305] batch time: 0.056 trainign loss: 5.7562 avg training loss: 7.4402
batch: [7010/21305] batch time: 0.056 trainign loss: 6.3238 avg training loss: 7.4401
batch: [7020/21305] batch time: 0.056 trainign loss: 7.7055 avg training loss: 7.4400
batch: [7030/21305] batch time: 0.062 trainign loss: 6.5263 avg training loss: 7.4400
batch: [7040/21305] batch time: 0.056 trainign loss: 7.1590 avg training loss: 7.4399
batch: [7050/21305] batch time: 0.057 trainign loss: 6.0773 avg training loss: 7.4398
batch: [7060/21305] batch time: 0.057 trainign loss: 6.0913 avg training loss: 7.4397
batch: [7070/21305] batch time: 0.063 trainign loss: 7.1127 avg training loss: 7.4397
batch: [7080/21305] batch time: 0.057 trainign loss: 6.3992 avg training loss: 7.4396
batch: [7090/21305] batch time: 0.056 trainign loss: 6.1114 avg training loss: 7.4395
batch: [7100/21305] batch time: 0.051 trainign loss: 6.2321 avg training loss: 7.4395
batch: [7110/21305] batch time: 0.056 trainign loss: 6.2621 avg training loss: 7.4393
batch: [7120/21305] batch time: 0.059 trainign loss: 4.1922 avg training loss: 7.4391
batch: [7130/21305] batch time: 0.062 trainign loss: 7.6054 avg training loss: 7.4390
batch: [7140/21305] batch time: 0.420 trainign loss: 7.6395 avg training loss: 7.4390
batch: [7150/21305] batch time: 0.056 trainign loss: 5.8997 avg training loss: 7.4390
batch: [7160/21305] batch time: 0.453 trainign loss: 7.5717 avg training loss: 7.4388
batch: [7170/21305] batch time: 0.056 trainign loss: 6.2064 avg training loss: 7.4388
batch: [7180/21305] batch time: 0.056 trainign loss: 6.7416 avg training loss: 7.4387
batch: [7190/21305] batch time: 0.060 trainign loss: 5.1076 avg training loss: 7.4387
batch: [7200/21305] batch time: 0.055 trainign loss: 5.6774 avg training loss: 7.4385
batch: [7210/21305] batch time: 0.057 trainign loss: 6.2290 avg training loss: 7.4385
batch: [7220/21305] batch time: 0.969 trainign loss: 5.3299 avg training loss: 7.4383
batch: [7230/21305] batch time: 0.056 trainign loss: 4.7622 avg training loss: 7.4382
batch: [7240/21305] batch time: 1.449 trainign loss: 7.3593 avg training loss: 7.4381
batch: [7250/21305] batch time: 0.057 trainign loss: 5.9539 avg training loss: 7.4381
batch: [7260/21305] batch time: 2.244 trainign loss: 5.2149 avg training loss: 7.4380
batch: [7270/21305] batch time: 0.056 trainign loss: 7.0850 avg training loss: 7.4380
batch: [7280/21305] batch time: 2.068 trainign loss: 6.4984 avg training loss: 7.4379
batch: [7290/21305] batch time: 0.055 trainign loss: 5.4987 avg training loss: 7.4379
batch: [7300/21305] batch time: 2.268 trainign loss: 6.6925 avg training loss: 7.4378
batch: [7310/21305] batch time: 0.055 trainign loss: 7.1259 avg training loss: 7.4377
batch: [7320/21305] batch time: 2.223 trainign loss: 6.9467 avg training loss: 7.4377
batch: [7330/21305] batch time: 0.057 trainign loss: 5.6325 avg training loss: 7.4376
batch: [7340/21305] batch time: 1.871 trainign loss: 7.1087 avg training loss: 7.4375
batch: [7350/21305] batch time: 0.057 trainign loss: 7.1860 avg training loss: 7.4374
batch: [7360/21305] batch time: 2.390 trainign loss: 6.8271 avg training loss: 7.4373
batch: [7370/21305] batch time: 0.061 trainign loss: 2.7804 avg training loss: 7.4372
batch: [7380/21305] batch time: 2.013 trainign loss: 6.6630 avg training loss: 7.4371
batch: [7390/21305] batch time: 0.876 trainign loss: 7.4682 avg training loss: 7.4370
batch: [7400/21305] batch time: 1.672 trainign loss: 4.6754 avg training loss: 7.4369
batch: [7410/21305] batch time: 0.643 trainign loss: 7.3192 avg training loss: 7.4367
batch: [7420/21305] batch time: 1.619 trainign loss: 7.1962 avg training loss: 7.4367
batch: [7430/21305] batch time: 0.662 trainign loss: 6.7228 avg training loss: 7.4367
batch: [7440/21305] batch time: 2.027 trainign loss: 6.0693 avg training loss: 7.4366
batch: [7450/21305] batch time: 0.056 trainign loss: 6.9516 avg training loss: 7.4366
batch: [7460/21305] batch time: 2.309 trainign loss: 3.1135 avg training loss: 7.4363
batch: [7470/21305] batch time: 0.490 trainign loss: 4.0737 avg training loss: 7.4363
batch: [7480/21305] batch time: 1.680 trainign loss: 6.7618 avg training loss: 7.4362
batch: [7490/21305] batch time: 1.453 trainign loss: 6.5736 avg training loss: 7.4362
batch: [7500/21305] batch time: 0.922 trainign loss: 7.3753 avg training loss: 7.4361
batch: [7510/21305] batch time: 1.291 trainign loss: 6.2718 avg training loss: 7.4360
batch: [7520/21305] batch time: 1.474 trainign loss: 7.1172 avg training loss: 7.4360
batch: [7530/21305] batch time: 0.738 trainign loss: 5.7332 avg training loss: 7.4359
batch: [7540/21305] batch time: 2.002 trainign loss: 7.0674 avg training loss: 7.4358
batch: [7550/21305] batch time: 0.749 trainign loss: 7.6598 avg training loss: 7.4358
batch: [7560/21305] batch time: 1.212 trainign loss: 6.6619 avg training loss: 7.4358
batch: [7570/21305] batch time: 0.215 trainign loss: 6.8303 avg training loss: 7.4357
batch: [7580/21305] batch time: 1.068 trainign loss: 6.4390 avg training loss: 7.4356
batch: [7590/21305] batch time: 0.071 trainign loss: 7.4147 avg training loss: 7.4356
batch: [7600/21305] batch time: 1.384 trainign loss: 6.2488 avg training loss: 7.4355
batch: [7610/21305] batch time: 0.058 trainign loss: 5.9962 avg training loss: 7.4354
batch: [7620/21305] batch time: 0.539 trainign loss: 4.6287 avg training loss: 7.4353
batch: [7630/21305] batch time: 0.059 trainign loss: 7.3229 avg training loss: 7.4352
batch: [7640/21305] batch time: 0.062 trainign loss: 6.5309 avg training loss: 7.4351
batch: [7650/21305] batch time: 0.058 trainign loss: 5.2961 avg training loss: 7.4350
batch: [7660/21305] batch time: 0.245 trainign loss: 7.0574 avg training loss: 7.4349
batch: [7670/21305] batch time: 0.056 trainign loss: 7.6448 avg training loss: 7.4348
batch: [7680/21305] batch time: 0.300 trainign loss: 5.4835 avg training loss: 7.4348
batch: [7690/21305] batch time: 0.063 trainign loss: 4.8611 avg training loss: 7.4346
batch: [7700/21305] batch time: 0.056 trainign loss: 6.9724 avg training loss: 7.4345
batch: [7710/21305] batch time: 0.062 trainign loss: 5.5074 avg training loss: 7.4343
batch: [7720/21305] batch time: 0.656 trainign loss: 5.9856 avg training loss: 7.4343
batch: [7730/21305] batch time: 0.058 trainign loss: 4.5096 avg training loss: 7.4340
batch: [7740/21305] batch time: 0.829 trainign loss: 7.4722 avg training loss: 7.4339
batch: [7750/21305] batch time: 0.056 trainign loss: 7.8015 avg training loss: 7.4339
batch: [7760/21305] batch time: 0.944 trainign loss: 6.9791 avg training loss: 7.4339
batch: [7770/21305] batch time: 0.056 trainign loss: 4.8272 avg training loss: 7.4338
batch: [7780/21305] batch time: 2.094 trainign loss: 7.1016 avg training loss: 7.4337
batch: [7790/21305] batch time: 0.054 trainign loss: 7.3320 avg training loss: 7.4337
batch: [7800/21305] batch time: 2.191 trainign loss: 7.0088 avg training loss: 7.4336
batch: [7810/21305] batch time: 0.056 trainign loss: 5.9428 avg training loss: 7.4335
batch: [7820/21305] batch time: 1.791 trainign loss: 6.7023 avg training loss: 7.4335
batch: [7830/21305] batch time: 0.062 trainign loss: 6.1744 avg training loss: 7.4334
batch: [7840/21305] batch time: 1.379 trainign loss: 6.8548 avg training loss: 7.4334
batch: [7850/21305] batch time: 0.056 trainign loss: 5.2908 avg training loss: 7.4333
batch: [7860/21305] batch time: 1.900 trainign loss: 5.4264 avg training loss: 7.4332
batch: [7870/21305] batch time: 0.056 trainign loss: 6.3355 avg training loss: 7.4331
batch: [7880/21305] batch time: 1.241 trainign loss: 4.6105 avg training loss: 7.4330
batch: [7890/21305] batch time: 0.056 trainign loss: 5.7508 avg training loss: 7.4329
batch: [7900/21305] batch time: 1.687 trainign loss: 5.7372 avg training loss: 7.4327
batch: [7910/21305] batch time: 0.062 trainign loss: 7.4375 avg training loss: 7.4327
batch: [7920/21305] batch time: 1.912 trainign loss: 5.9941 avg training loss: 7.4327
batch: [7930/21305] batch time: 0.056 trainign loss: 6.1531 avg training loss: 7.4326
batch: [7940/21305] batch time: 1.172 trainign loss: 7.2251 avg training loss: 7.4326
batch: [7950/21305] batch time: 0.056 trainign loss: 7.1777 avg training loss: 7.4325
batch: [7960/21305] batch time: 1.192 trainign loss: 5.8210 avg training loss: 7.4324
batch: [7970/21305] batch time: 0.056 trainign loss: 6.4563 avg training loss: 7.4324
batch: [7980/21305] batch time: 0.918 trainign loss: 7.0168 avg training loss: 7.4323
batch: [7990/21305] batch time: 0.063 trainign loss: 7.3392 avg training loss: 7.4322
batch: [8000/21305] batch time: 1.333 trainign loss: 6.9180 avg training loss: 7.4321
batch: [8010/21305] batch time: 0.056 trainign loss: 5.5251 avg training loss: 7.4320
batch: [8020/21305] batch time: 1.979 trainign loss: 2.4437 avg training loss: 7.4318
batch: [8030/21305] batch time: 0.054 trainign loss: 5.7231 avg training loss: 7.4316
batch: [8040/21305] batch time: 2.425 trainign loss: 7.2001 avg training loss: 7.4315
batch: [8050/21305] batch time: 0.061 trainign loss: 8.2243 avg training loss: 7.4315
batch: [8060/21305] batch time: 2.223 trainign loss: 7.3045 avg training loss: 7.4316
batch: [8070/21305] batch time: 0.052 trainign loss: 6.4177 avg training loss: 7.4315
batch: [8080/21305] batch time: 2.270 trainign loss: 6.9847 avg training loss: 7.4315
batch: [8090/21305] batch time: 0.056 trainign loss: 7.0230 avg training loss: 7.4314
batch: [8100/21305] batch time: 2.192 trainign loss: 6.2960 avg training loss: 7.4314
batch: [8110/21305] batch time: 0.052 trainign loss: 7.1901 avg training loss: 7.4313
batch: [8120/21305] batch time: 2.368 trainign loss: 6.5777 avg training loss: 7.4312
batch: [8130/21305] batch time: 0.061 trainign loss: 5.8772 avg training loss: 7.4311
batch: [8140/21305] batch time: 2.154 trainign loss: 6.1037 avg training loss: 7.4310
batch: [8150/21305] batch time: 0.056 trainign loss: 5.9588 avg training loss: 7.4309
batch: [8160/21305] batch time: 2.139 trainign loss: 6.6061 avg training loss: 7.4309
batch: [8170/21305] batch time: 0.059 trainign loss: 6.4631 avg training loss: 7.4308
batch: [8180/21305] batch time: 1.963 trainign loss: 6.0059 avg training loss: 7.4307
batch: [8190/21305] batch time: 0.056 trainign loss: 6.8287 avg training loss: 7.4306
batch: [8200/21305] batch time: 1.513 trainign loss: 6.7723 avg training loss: 7.4306
batch: [8210/21305] batch time: 0.056 trainign loss: 6.2364 avg training loss: 7.4305
batch: [8220/21305] batch time: 1.889 trainign loss: 6.1124 avg training loss: 7.4305
batch: [8230/21305] batch time: 0.056 trainign loss: 6.8539 avg training loss: 7.4303
batch: [8240/21305] batch time: 1.718 trainign loss: 7.2493 avg training loss: 7.4303
batch: [8250/21305] batch time: 0.056 trainign loss: 6.3629 avg training loss: 7.4303
batch: [8260/21305] batch time: 1.745 trainign loss: 6.4660 avg training loss: 7.4301
batch: [8270/21305] batch time: 0.052 trainign loss: 7.7513 avg training loss: 7.4301
batch: [8280/21305] batch time: 1.860 trainign loss: 6.6356 avg training loss: 7.4301
batch: [8290/21305] batch time: 0.056 trainign loss: 6.0558 avg training loss: 7.4300
batch: [8300/21305] batch time: 2.475 trainign loss: 5.7498 avg training loss: 7.4299
batch: [8310/21305] batch time: 0.055 trainign loss: 7.3027 avg training loss: 7.4298
batch: [8320/21305] batch time: 2.226 trainign loss: 6.6639 avg training loss: 7.4298
batch: [8330/21305] batch time: 0.056 trainign loss: 6.7326 avg training loss: 7.4297
batch: [8340/21305] batch time: 2.141 trainign loss: 5.8672 avg training loss: 7.4296
batch: [8350/21305] batch time: 0.062 trainign loss: 4.8565 avg training loss: 7.4295
batch: [8360/21305] batch time: 1.655 trainign loss: 5.3678 avg training loss: 7.4293
batch: [8370/21305] batch time: 0.062 trainign loss: 7.2126 avg training loss: 7.4292
batch: [8380/21305] batch time: 1.328 trainign loss: 5.1589 avg training loss: 7.4292
batch: [8390/21305] batch time: 0.062 trainign loss: 4.6640 avg training loss: 7.4289
batch: [8400/21305] batch time: 0.693 trainign loss: 6.4941 avg training loss: 7.4287
batch: [8410/21305] batch time: 0.057 trainign loss: 5.8569 avg training loss: 7.4286
batch: [8420/21305] batch time: 0.056 trainign loss: 6.9515 avg training loss: 7.4283
batch: [8430/21305] batch time: 0.060 trainign loss: 7.9616 avg training loss: 7.4283
batch: [8440/21305] batch time: 0.056 trainign loss: 8.0242 avg training loss: 7.4283
batch: [8450/21305] batch time: 0.056 trainign loss: 7.4592 avg training loss: 7.4283
batch: [8460/21305] batch time: 0.062 trainign loss: 7.3683 avg training loss: 7.4283
batch: [8470/21305] batch time: 0.053 trainign loss: 7.5046 avg training loss: 7.4282
batch: [8480/21305] batch time: 0.057 trainign loss: 6.6357 avg training loss: 7.4282
batch: [8490/21305] batch time: 0.052 trainign loss: 6.5806 avg training loss: 7.4281
batch: [8500/21305] batch time: 0.056 trainign loss: 5.7128 avg training loss: 7.4279
batch: [8510/21305] batch time: 0.057 trainign loss: 5.8458 avg training loss: 7.4278
batch: [8520/21305] batch time: 0.062 trainign loss: 6.6890 avg training loss: 7.4277
batch: [8530/21305] batch time: 0.051 trainign loss: 6.4698 avg training loss: 7.4276
batch: [8540/21305] batch time: 0.058 trainign loss: 6.8307 avg training loss: 7.4276
batch: [8550/21305] batch time: 0.063 trainign loss: 7.6056 avg training loss: 7.4275
batch: [8560/21305] batch time: 0.056 trainign loss: 6.9783 avg training loss: 7.4274
batch: [8570/21305] batch time: 0.051 trainign loss: 5.3777 avg training loss: 7.4273
batch: [8580/21305] batch time: 0.062 trainign loss: 6.5744 avg training loss: 7.4273
batch: [8590/21305] batch time: 0.055 trainign loss: 7.2389 avg training loss: 7.4272
batch: [8600/21305] batch time: 0.056 trainign loss: 6.6049 avg training loss: 7.4271
batch: [8610/21305] batch time: 0.056 trainign loss: 6.0867 avg training loss: 7.4270
batch: [8620/21305] batch time: 0.056 trainign loss: 7.0393 avg training loss: 7.4270
batch: [8630/21305] batch time: 0.052 trainign loss: 4.8914 avg training loss: 7.4269
batch: [8640/21305] batch time: 0.056 trainign loss: 6.9740 avg training loss: 7.4268
batch: [8650/21305] batch time: 0.056 trainign loss: 4.3810 avg training loss: 7.4267
batch: [8660/21305] batch time: 0.057 trainign loss: 4.5377 avg training loss: 7.4266
batch: [8670/21305] batch time: 0.056 trainign loss: 5.9318 avg training loss: 7.4265
batch: [8680/21305] batch time: 0.057 trainign loss: 7.2246 avg training loss: 7.4265
batch: [8690/21305] batch time: 0.051 trainign loss: 5.9230 avg training loss: 7.4264
batch: [8700/21305] batch time: 0.062 trainign loss: 5.0511 avg training loss: 7.4263
batch: [8710/21305] batch time: 0.057 trainign loss: 1.3178 avg training loss: 7.4261
batch: [8720/21305] batch time: 0.058 trainign loss: 0.0012 avg training loss: 7.4256
batch: [8730/21305] batch time: 0.055 trainign loss: 0.0001 avg training loss: 7.4250
batch: [8740/21305] batch time: 0.056 trainign loss: 8.3172 avg training loss: 7.4250
batch: [8750/21305] batch time: 0.056 trainign loss: 8.0312 avg training loss: 7.4251
batch: [8760/21305] batch time: 0.056 trainign loss: 7.1761 avg training loss: 7.4250
batch: [8770/21305] batch time: 0.052 trainign loss: 6.6786 avg training loss: 7.4249
batch: [8780/21305] batch time: 0.056 trainign loss: 6.0906 avg training loss: 7.4248
batch: [8790/21305] batch time: 0.054 trainign loss: 5.9442 avg training loss: 7.4248
batch: [8800/21305] batch time: 0.056 trainign loss: 5.9110 avg training loss: 7.4247
batch: [8810/21305] batch time: 0.055 trainign loss: 7.4302 avg training loss: 7.4246
batch: [8820/21305] batch time: 0.062 trainign loss: 6.5420 avg training loss: 7.4246
batch: [8830/21305] batch time: 0.059 trainign loss: 6.8888 avg training loss: 7.4246
batch: [8840/21305] batch time: 0.053 trainign loss: 6.4237 avg training loss: 7.4245
batch: [8850/21305] batch time: 0.052 trainign loss: 6.1200 avg training loss: 7.4244
batch: [8860/21305] batch time: 0.063 trainign loss: 7.0862 avg training loss: 7.4243
batch: [8870/21305] batch time: 0.058 trainign loss: 6.9594 avg training loss: 7.4243
batch: [8880/21305] batch time: 0.053 trainign loss: 5.4730 avg training loss: 7.4242
batch: [8890/21305] batch time: 0.062 trainign loss: 6.9595 avg training loss: 7.4242
batch: [8900/21305] batch time: 0.057 trainign loss: 6.0138 avg training loss: 7.4241
batch: [8910/21305] batch time: 0.056 trainign loss: 5.5077 avg training loss: 7.4240
batch: [8920/21305] batch time: 0.056 trainign loss: 2.5326 avg training loss: 7.4238
batch: [8930/21305] batch time: 0.062 trainign loss: 7.7972 avg training loss: 7.4237
batch: [8940/21305] batch time: 0.063 trainign loss: 6.5823 avg training loss: 7.4237
batch: [8950/21305] batch time: 0.056 trainign loss: 5.9472 avg training loss: 7.4236
batch: [8960/21305] batch time: 0.051 trainign loss: 6.2984 avg training loss: 7.4235
batch: [8970/21305] batch time: 0.059 trainign loss: 6.5911 avg training loss: 7.4235
batch: [8980/21305] batch time: 0.053 trainign loss: 5.3602 avg training loss: 7.4234
batch: [8990/21305] batch time: 0.056 trainign loss: 6.5506 avg training loss: 7.4233
batch: [9000/21305] batch time: 0.061 trainign loss: 0.9889 avg training loss: 7.4230
batch: [9010/21305] batch time: 0.057 trainign loss: 7.6843 avg training loss: 7.4229
batch: [9020/21305] batch time: 0.057 trainign loss: 6.1671 avg training loss: 7.4229
batch: [9030/21305] batch time: 0.058 trainign loss: 6.0655 avg training loss: 7.4228
batch: [9040/21305] batch time: 0.051 trainign loss: 6.8077 avg training loss: 7.4228
batch: [9050/21305] batch time: 0.057 trainign loss: 6.2789 avg training loss: 7.4227
batch: [9060/21305] batch time: 0.058 trainign loss: 5.3370 avg training loss: 7.4226
batch: [9070/21305] batch time: 0.056 trainign loss: 3.2786 avg training loss: 7.4224
batch: [9080/21305] batch time: 0.053 trainign loss: 8.0018 avg training loss: 7.4224
batch: [9090/21305] batch time: 0.057 trainign loss: 6.7362 avg training loss: 7.4224
batch: [9100/21305] batch time: 0.061 trainign loss: 6.9246 avg training loss: 7.4223
batch: [9110/21305] batch time: 0.061 trainign loss: 5.4956 avg training loss: 7.4223
batch: [9120/21305] batch time: 0.051 trainign loss: 7.0814 avg training loss: 7.4221
batch: [9130/21305] batch time: 0.056 trainign loss: 5.0817 avg training loss: 7.4220
batch: [9140/21305] batch time: 0.057 trainign loss: 6.7880 avg training loss: 7.4218
batch: [9150/21305] batch time: 0.056 trainign loss: 6.5914 avg training loss: 7.4218
batch: [9160/21305] batch time: 0.050 trainign loss: 7.0752 avg training loss: 7.4218
batch: [9170/21305] batch time: 0.056 trainign loss: 5.5506 avg training loss: 7.4216
batch: [9180/21305] batch time: 0.057 trainign loss: 7.1878 avg training loss: 7.4216
batch: [9190/21305] batch time: 0.174 trainign loss: 6.9314 avg training loss: 7.4216
batch: [9200/21305] batch time: 0.056 trainign loss: 7.0211 avg training loss: 7.4215
batch: [9210/21305] batch time: 0.062 trainign loss: 6.3051 avg training loss: 7.4214
batch: [9220/21305] batch time: 0.060 trainign loss: 4.8782 avg training loss: 7.4213
batch: [9230/21305] batch time: 0.566 trainign loss: 6.6770 avg training loss: 7.4213
batch: [9240/21305] batch time: 0.056 trainign loss: 5.1882 avg training loss: 7.4212
batch: [9250/21305] batch time: 1.114 trainign loss: 6.9916 avg training loss: 7.4211
batch: [9260/21305] batch time: 0.056 trainign loss: 6.5092 avg training loss: 7.4210
batch: [9270/21305] batch time: 0.143 trainign loss: 6.4699 avg training loss: 7.4209
batch: [9280/21305] batch time: 0.117 trainign loss: 6.2599 avg training loss: 7.4208
batch: [9290/21305] batch time: 0.308 trainign loss: 4.9057 avg training loss: 7.4207
batch: [9300/21305] batch time: 0.054 trainign loss: 7.3309 avg training loss: 7.4205
batch: [9310/21305] batch time: 0.835 trainign loss: 7.3514 avg training loss: 7.4205
batch: [9320/21305] batch time: 0.055 trainign loss: 5.0405 avg training loss: 7.4204
batch: [9330/21305] batch time: 1.299 trainign loss: 4.9980 avg training loss: 7.4203
batch: [9340/21305] batch time: 0.058 trainign loss: 7.1322 avg training loss: 7.4203
batch: [9350/21305] batch time: 1.734 trainign loss: 7.2490 avg training loss: 7.4203
batch: [9360/21305] batch time: 0.063 trainign loss: 5.1016 avg training loss: 7.4202
batch: [9370/21305] batch time: 2.407 trainign loss: 5.1980 avg training loss: 7.4201
batch: [9380/21305] batch time: 0.062 trainign loss: 5.6012 avg training loss: 7.4200
batch: [9390/21305] batch time: 2.433 trainign loss: 6.5646 avg training loss: 7.4199
batch: [9400/21305] batch time: 0.057 trainign loss: 6.6011 avg training loss: 7.4198
batch: [9410/21305] batch time: 2.190 trainign loss: 5.7982 avg training loss: 7.4197
batch: [9420/21305] batch time: 0.054 trainign loss: 3.7296 avg training loss: 7.4196
batch: [9430/21305] batch time: 1.986 trainign loss: 7.2981 avg training loss: 7.4195
batch: [9440/21305] batch time: 0.056 trainign loss: 7.3612 avg training loss: 7.4195
batch: [9450/21305] batch time: 2.364 trainign loss: 4.8132 avg training loss: 7.4193
batch: [9460/21305] batch time: 0.056 trainign loss: 7.3014 avg training loss: 7.4192
batch: [9470/21305] batch time: 2.401 trainign loss: 7.6542 avg training loss: 7.4192
batch: [9480/21305] batch time: 0.062 trainign loss: 5.7051 avg training loss: 7.4191
batch: [9490/21305] batch time: 2.257 trainign loss: 7.3445 avg training loss: 7.4190
batch: [9500/21305] batch time: 0.053 trainign loss: 7.4040 avg training loss: 7.4190
batch: [9510/21305] batch time: 2.328 trainign loss: 5.9075 avg training loss: 7.4188
batch: [9520/21305] batch time: 0.057 trainign loss: 7.0395 avg training loss: 7.4188
batch: [9530/21305] batch time: 1.038 trainign loss: 6.7568 avg training loss: 7.4187
batch: [9540/21305] batch time: 0.062 trainign loss: 6.0241 avg training loss: 7.4186
batch: [9550/21305] batch time: 0.468 trainign loss: 4.9338 avg training loss: 7.4185
batch: [9560/21305] batch time: 0.062 trainign loss: 5.5617 avg training loss: 7.4184
batch: [9570/21305] batch time: 0.145 trainign loss: 6.9619 avg training loss: 7.4184
batch: [9580/21305] batch time: 0.062 trainign loss: 8.2117 avg training loss: 7.4182
batch: [9590/21305] batch time: 0.055 trainign loss: 7.2700 avg training loss: 7.4182
batch: [9600/21305] batch time: 0.052 trainign loss: 6.8339 avg training loss: 7.4182
batch: [9610/21305] batch time: 0.056 trainign loss: 6.0785 avg training loss: 7.4180
batch: [9620/21305] batch time: 0.058 trainign loss: 6.8363 avg training loss: 7.4180
batch: [9630/21305] batch time: 0.053 trainign loss: 7.4682 avg training loss: 7.4179
batch: [9640/21305] batch time: 0.056 trainign loss: 7.2620 avg training loss: 7.4178
batch: [9650/21305] batch time: 0.051 trainign loss: 6.1513 avg training loss: 7.4178
batch: [9660/21305] batch time: 0.057 trainign loss: 3.9538 avg training loss: 7.4176
batch: [9670/21305] batch time: 0.255 trainign loss: 6.6188 avg training loss: 7.4176
batch: [9680/21305] batch time: 0.063 trainign loss: 7.0940 avg training loss: 7.4175
batch: [9690/21305] batch time: 0.056 trainign loss: 6.8450 avg training loss: 7.4175
batch: [9700/21305] batch time: 0.056 trainign loss: 6.4596 avg training loss: 7.4174
batch: [9710/21305] batch time: 0.417 trainign loss: 6.2515 avg training loss: 7.4173
batch: [9720/21305] batch time: 0.052 trainign loss: 7.0079 avg training loss: 7.4173
batch: [9730/21305] batch time: 0.063 trainign loss: 6.8295 avg training loss: 7.4172
batch: [9740/21305] batch time: 0.055 trainign loss: 6.2628 avg training loss: 7.4172
batch: [9750/21305] batch time: 0.062 trainign loss: 7.3552 avg training loss: 7.4171
batch: [9760/21305] batch time: 0.056 trainign loss: 6.6663 avg training loss: 7.4170
batch: [9770/21305] batch time: 0.057 trainign loss: 6.7161 avg training loss: 7.4169
batch: [9780/21305] batch time: 0.057 trainign loss: 7.7475 avg training loss: 7.4169
batch: [9790/21305] batch time: 0.493 trainign loss: 6.5003 avg training loss: 7.4168
batch: [9800/21305] batch time: 0.061 trainign loss: 6.0747 avg training loss: 7.4167
batch: [9810/21305] batch time: 0.417 trainign loss: 7.5974 avg training loss: 7.4166
batch: [9820/21305] batch time: 0.063 trainign loss: 5.7480 avg training loss: 7.4165
batch: [9830/21305] batch time: 1.219 trainign loss: 5.7518 avg training loss: 7.4164
batch: [9840/21305] batch time: 0.056 trainign loss: 4.5026 avg training loss: 7.4163
batch: [9850/21305] batch time: 1.904 trainign loss: 7.0043 avg training loss: 7.4162
batch: [9860/21305] batch time: 0.058 trainign loss: 3.5938 avg training loss: 7.4161
batch: [9870/21305] batch time: 1.028 trainign loss: 5.2209 avg training loss: 7.4160
batch: [9880/21305] batch time: 0.056 trainign loss: 7.3542 avg training loss: 7.4159
batch: [9890/21305] batch time: 0.987 trainign loss: 6.4815 avg training loss: 7.4158
batch: [9900/21305] batch time: 0.062 trainign loss: 6.2354 avg training loss: 7.4158
batch: [9910/21305] batch time: 0.767 trainign loss: 6.8316 avg training loss: 7.4157
batch: [9920/21305] batch time: 0.057 trainign loss: 6.3971 avg training loss: 7.4157
batch: [9930/21305] batch time: 0.124 trainign loss: 5.7922 avg training loss: 7.4156
batch: [9940/21305] batch time: 0.063 trainign loss: 7.3645 avg training loss: 7.4155
batch: [9950/21305] batch time: 1.224 trainign loss: 6.4399 avg training loss: 7.4154
batch: [9960/21305] batch time: 0.056 trainign loss: 7.2504 avg training loss: 7.4154
batch: [9970/21305] batch time: 1.043 trainign loss: 6.7111 avg training loss: 7.4153
batch: [9980/21305] batch time: 0.683 trainign loss: 5.9654 avg training loss: 7.4152
batch: [9990/21305] batch time: 0.378 trainign loss: 6.8453 avg training loss: 7.4152
batch: [10000/21305] batch time: 1.144 trainign loss: 5.4159 avg training loss: 7.4151
batch: [10010/21305] batch time: 0.056 trainign loss: 6.4920 avg training loss: 7.4150
batch: [10020/21305] batch time: 1.012 trainign loss: 7.2976 avg training loss: 7.4150
batch: [10030/21305] batch time: 0.058 trainign loss: 6.2811 avg training loss: 7.4149
batch: [10040/21305] batch time: 1.145 trainign loss: 6.0806 avg training loss: 7.4149
batch: [10050/21305] batch time: 0.056 trainign loss: 5.5917 avg training loss: 7.4148
batch: [10060/21305] batch time: 1.416 trainign loss: 4.7195 avg training loss: 7.4146
batch: [10070/21305] batch time: 0.056 trainign loss: 6.5524 avg training loss: 7.4145
batch: [10080/21305] batch time: 0.957 trainign loss: 5.3496 avg training loss: 7.4144
batch: [10090/21305] batch time: 0.059 trainign loss: 8.2052 avg training loss: 7.4142
batch: [10100/21305] batch time: 0.057 trainign loss: 8.1319 avg training loss: 7.4142
batch: [10110/21305] batch time: 0.058 trainign loss: 6.5733 avg training loss: 7.4142
batch: [10120/21305] batch time: 0.376 trainign loss: 6.9943 avg training loss: 7.4141
batch: [10130/21305] batch time: 0.058 trainign loss: 5.9999 avg training loss: 7.4141
batch: [10140/21305] batch time: 0.061 trainign loss: 7.2083 avg training loss: 7.4140
batch: [10150/21305] batch time: 0.056 trainign loss: 6.7118 avg training loss: 7.4140
batch: [10160/21305] batch time: 0.052 trainign loss: 6.1036 avg training loss: 7.4139
batch: [10170/21305] batch time: 0.063 trainign loss: 6.8465 avg training loss: 7.4139
batch: [10180/21305] batch time: 0.054 trainign loss: 6.8845 avg training loss: 7.4138
batch: [10190/21305] batch time: 0.056 trainign loss: 6.9093 avg training loss: 7.4137
batch: [10200/21305] batch time: 0.056 trainign loss: 5.5397 avg training loss: 7.4136
batch: [10210/21305] batch time: 0.057 trainign loss: 6.6457 avg training loss: 7.4136
batch: [10220/21305] batch time: 0.063 trainign loss: 6.0811 avg training loss: 7.4134
batch: [10230/21305] batch time: 0.056 trainign loss: 5.3093 avg training loss: 7.4133
batch: [10240/21305] batch time: 0.051 trainign loss: 6.8412 avg training loss: 7.4132
batch: [10250/21305] batch time: 0.058 trainign loss: 7.1690 avg training loss: 7.4131
batch: [10260/21305] batch time: 0.062 trainign loss: 7.4795 avg training loss: 7.4131
batch: [10270/21305] batch time: 0.061 trainign loss: 7.0512 avg training loss: 7.4130
batch: [10280/21305] batch time: 0.056 trainign loss: 6.8330 avg training loss: 7.4129
batch: [10290/21305] batch time: 0.051 trainign loss: 6.5767 avg training loss: 7.4128
batch: [10300/21305] batch time: 0.056 trainign loss: 6.0551 avg training loss: 7.4128
batch: [10310/21305] batch time: 0.056 trainign loss: 5.5338 avg training loss: 7.4126
batch: [10320/21305] batch time: 0.056 trainign loss: 2.5713 avg training loss: 7.4125
batch: [10330/21305] batch time: 0.056 trainign loss: 7.3315 avg training loss: 7.4124
batch: [10340/21305] batch time: 0.057 trainign loss: 7.0297 avg training loss: 7.4124
batch: [10350/21305] batch time: 0.057 trainign loss: 7.2613 avg training loss: 7.4124
batch: [10360/21305] batch time: 0.063 trainign loss: 6.4917 avg training loss: 7.4123
batch: [10370/21305] batch time: 0.054 trainign loss: 6.4567 avg training loss: 7.4123
batch: [10380/21305] batch time: 0.056 trainign loss: 6.7812 avg training loss: 7.4122
batch: [10390/21305] batch time: 0.058 trainign loss: 6.9781 avg training loss: 7.4121
batch: [10400/21305] batch time: 0.056 trainign loss: 6.2138 avg training loss: 7.4120
batch: [10410/21305] batch time: 0.051 trainign loss: 6.2031 avg training loss: 7.4119
batch: [10420/21305] batch time: 0.057 trainign loss: 7.7641 avg training loss: 7.4118
batch: [10430/21305] batch time: 0.056 trainign loss: 5.4721 avg training loss: 7.4118
batch: [10440/21305] batch time: 0.062 trainign loss: 6.2608 avg training loss: 7.4116
batch: [10450/21305] batch time: 0.056 trainign loss: 6.4383 avg training loss: 7.4116
batch: [10460/21305] batch time: 0.061 trainign loss: 6.7030 avg training loss: 7.4115
batch: [10470/21305] batch time: 0.056 trainign loss: 5.5254 avg training loss: 7.4114
batch: [10480/21305] batch time: 0.058 trainign loss: 7.2308 avg training loss: 7.4113
batch: [10490/21305] batch time: 0.063 trainign loss: 6.9317 avg training loss: 7.4112
batch: [10500/21305] batch time: 0.056 trainign loss: 5.4296 avg training loss: 7.4112
batch: [10510/21305] batch time: 0.051 trainign loss: 4.4480 avg training loss: 7.4110
batch: [10520/21305] batch time: 0.053 trainign loss: 6.9127 avg training loss: 7.4109
batch: [10530/21305] batch time: 0.051 trainign loss: 6.8587 avg training loss: 7.4109
batch: [10540/21305] batch time: 0.057 trainign loss: 5.4629 avg training loss: 7.4108
batch: [10550/21305] batch time: 0.061 trainign loss: 4.5218 avg training loss: 7.4107
batch: [10560/21305] batch time: 0.057 trainign loss: 9.0549 avg training loss: 7.4104
batch: [10570/21305] batch time: 0.059 trainign loss: 6.9841 avg training loss: 7.4104
batch: [10580/21305] batch time: 0.056 trainign loss: 7.5596 avg training loss: 7.4104
batch: [10590/21305] batch time: 0.056 trainign loss: 7.1953 avg training loss: 7.4103
batch: [10600/21305] batch time: 0.056 trainign loss: 6.7954 avg training loss: 7.4103
batch: [10610/21305] batch time: 0.057 trainign loss: 4.5500 avg training loss: 7.4102
batch: [10620/21305] batch time: 0.062 trainign loss: 6.9748 avg training loss: 7.4101
batch: [10630/21305] batch time: 0.054 trainign loss: 6.2893 avg training loss: 7.4100
batch: [10640/21305] batch time: 0.056 trainign loss: 5.9297 avg training loss: 7.4099
batch: [10650/21305] batch time: 0.060 trainign loss: 8.0039 avg training loss: 7.4098
batch: [10660/21305] batch time: 0.324 trainign loss: 6.9677 avg training loss: 7.4098
batch: [10670/21305] batch time: 0.057 trainign loss: 4.5941 avg training loss: 7.4097
batch: [10680/21305] batch time: 1.126 trainign loss: 6.9638 avg training loss: 7.4096
batch: [10690/21305] batch time: 0.056 trainign loss: 6.3101 avg training loss: 7.4095
batch: [10700/21305] batch time: 0.882 trainign loss: 7.2300 avg training loss: 7.4094
batch: [10710/21305] batch time: 0.056 trainign loss: 6.6365 avg training loss: 7.4092
batch: [10720/21305] batch time: 0.322 trainign loss: 6.8987 avg training loss: 7.4091
batch: [10730/21305] batch time: 0.063 trainign loss: 7.6499 avg training loss: 7.4090
batch: [10740/21305] batch time: 0.723 trainign loss: 7.0701 avg training loss: 7.4090
batch: [10750/21305] batch time: 0.621 trainign loss: 6.6385 avg training loss: 7.4089
batch: [10760/21305] batch time: 0.555 trainign loss: 4.5713 avg training loss: 7.4088
batch: [10770/21305] batch time: 1.192 trainign loss: 6.5068 avg training loss: 7.4087
batch: [10780/21305] batch time: 0.060 trainign loss: 7.3936 avg training loss: 7.4087
batch: [10790/21305] batch time: 1.067 trainign loss: 6.5289 avg training loss: 7.4086
batch: [10800/21305] batch time: 0.056 trainign loss: 5.6431 avg training loss: 7.4085
batch: [10810/21305] batch time: 0.487 trainign loss: 6.8218 avg training loss: 7.4085
batch: [10820/21305] batch time: 0.056 trainign loss: 4.0379 avg training loss: 7.4083
batch: [10830/21305] batch time: 0.056 trainign loss: 1.6413 avg training loss: 7.4081
batch: [10840/21305] batch time: 0.052 trainign loss: 7.3883 avg training loss: 7.4081
batch: [10850/21305] batch time: 0.063 trainign loss: 7.2571 avg training loss: 7.4081
batch: [10860/21305] batch time: 0.060 trainign loss: 6.4855 avg training loss: 7.4080
batch: [10870/21305] batch time: 0.061 trainign loss: 5.6837 avg training loss: 7.4080
batch: [10880/21305] batch time: 0.059 trainign loss: 5.2543 avg training loss: 7.4079
batch: [10890/21305] batch time: 0.052 trainign loss: 6.9445 avg training loss: 7.4078
batch: [10900/21305] batch time: 0.055 trainign loss: 7.3537 avg training loss: 7.4077
batch: [10910/21305] batch time: 0.063 trainign loss: 7.0180 avg training loss: 7.4076
batch: [10920/21305] batch time: 0.057 trainign loss: 6.8862 avg training loss: 7.4076
batch: [10930/21305] batch time: 0.258 trainign loss: 5.3016 avg training loss: 7.4075
batch: [10940/21305] batch time: 0.324 trainign loss: 6.7847 avg training loss: 7.4074
batch: [10950/21305] batch time: 0.754 trainign loss: 6.2337 avg training loss: 7.4072
batch: [10960/21305] batch time: 0.783 trainign loss: 6.7317 avg training loss: 7.4072
batch: [10970/21305] batch time: 0.941 trainign loss: 6.9937 avg training loss: 7.4072
batch: [10980/21305] batch time: 0.936 trainign loss: 5.8392 avg training loss: 7.4071
batch: [10990/21305] batch time: 0.726 trainign loss: 6.9849 avg training loss: 7.4070
batch: [11000/21305] batch time: 0.062 trainign loss: 6.1322 avg training loss: 7.4069
batch: [11010/21305] batch time: 1.459 trainign loss: 6.1248 avg training loss: 7.4067
batch: [11020/21305] batch time: 0.465 trainign loss: 7.4143 avg training loss: 7.4067
batch: [11030/21305] batch time: 1.549 trainign loss: 5.6088 avg training loss: 7.4066
batch: [11040/21305] batch time: 0.199 trainign loss: 6.9452 avg training loss: 7.4065
batch: [11050/21305] batch time: 2.029 trainign loss: 6.4372 avg training loss: 7.4065
batch: [11060/21305] batch time: 0.158 trainign loss: 7.0115 avg training loss: 7.4064
batch: [11070/21305] batch time: 2.103 trainign loss: 5.6010 avg training loss: 7.4063
batch: [11080/21305] batch time: 1.053 trainign loss: 7.3347 avg training loss: 7.4062
batch: [11090/21305] batch time: 1.115 trainign loss: 7.4896 avg training loss: 7.4062
batch: [11100/21305] batch time: 1.749 trainign loss: 6.1484 avg training loss: 7.4062
batch: [11110/21305] batch time: 0.582 trainign loss: 6.8188 avg training loss: 7.4061
batch: [11120/21305] batch time: 1.854 trainign loss: 6.7589 avg training loss: 7.4061
batch: [11130/21305] batch time: 0.416 trainign loss: 4.7676 avg training loss: 7.4060
batch: [11140/21305] batch time: 1.217 trainign loss: 4.3716 avg training loss: 7.4058
batch: [11150/21305] batch time: 0.844 trainign loss: 6.7159 avg training loss: 7.4057
batch: [11160/21305] batch time: 1.231 trainign loss: 6.6939 avg training loss: 7.4057
batch: [11170/21305] batch time: 0.677 trainign loss: 3.8626 avg training loss: 7.4056
batch: [11180/21305] batch time: 1.407 trainign loss: 7.6018 avg training loss: 7.4053
batch: [11190/21305] batch time: 1.071 trainign loss: 7.3156 avg training loss: 7.4052
batch: [11200/21305] batch time: 1.389 trainign loss: 7.2677 avg training loss: 7.4052
batch: [11210/21305] batch time: 0.811 trainign loss: 5.4238 avg training loss: 7.4051
batch: [11220/21305] batch time: 0.742 trainign loss: 6.4489 avg training loss: 7.4051
batch: [11230/21305] batch time: 1.181 trainign loss: 5.5793 avg training loss: 7.4050
batch: [11240/21305] batch time: 1.030 trainign loss: 6.0185 avg training loss: 7.4049
batch: [11250/21305] batch time: 1.778 trainign loss: 0.7989 avg training loss: 7.4046
batch: [11260/21305] batch time: 0.063 trainign loss: 7.6598 avg training loss: 7.4046
batch: [11270/21305] batch time: 2.490 trainign loss: 6.6123 avg training loss: 7.4046
batch: [11280/21305] batch time: 0.052 trainign loss: 5.6095 avg training loss: 7.4045
batch: [11290/21305] batch time: 2.106 trainign loss: 6.8786 avg training loss: 7.4044
batch: [11300/21305] batch time: 0.061 trainign loss: 1.8540 avg training loss: 7.4043
batch: [11310/21305] batch time: 2.379 trainign loss: 6.6776 avg training loss: 7.4042
batch: [11320/21305] batch time: 0.056 trainign loss: 7.1019 avg training loss: 7.4041
batch: [11330/21305] batch time: 1.598 trainign loss: 8.2605 avg training loss: 7.4041
batch: [11340/21305] batch time: 0.056 trainign loss: 7.5879 avg training loss: 7.4041
batch: [11350/21305] batch time: 1.402 trainign loss: 6.9539 avg training loss: 7.4040
batch: [11360/21305] batch time: 0.056 trainign loss: 6.3661 avg training loss: 7.4040
batch: [11370/21305] batch time: 2.261 trainign loss: 6.1625 avg training loss: 7.4038
batch: [11380/21305] batch time: 0.056 trainign loss: 4.3368 avg training loss: 7.4037
batch: [11390/21305] batch time: 2.441 trainign loss: 7.0659 avg training loss: 7.4033
batch: [11400/21305] batch time: 0.054 trainign loss: 8.0722 avg training loss: 7.4034
batch: [11410/21305] batch time: 2.350 trainign loss: 6.6723 avg training loss: 7.4034
batch: [11420/21305] batch time: 0.058 trainign loss: 5.8572 avg training loss: 7.4033
batch: [11430/21305] batch time: 2.461 trainign loss: 6.6303 avg training loss: 7.4033
batch: [11440/21305] batch time: 0.057 trainign loss: 6.5483 avg training loss: 7.4032
batch: [11450/21305] batch time: 2.216 trainign loss: 6.3146 avg training loss: 7.4031
batch: [11460/21305] batch time: 0.053 trainign loss: 6.5980 avg training loss: 7.4031
batch: [11470/21305] batch time: 2.538 trainign loss: 6.8139 avg training loss: 7.4030
batch: [11480/21305] batch time: 0.058 trainign loss: 6.4527 avg training loss: 7.4029
batch: [11490/21305] batch time: 2.340 trainign loss: 6.7950 avg training loss: 7.4027
batch: [11500/21305] batch time: 0.057 trainign loss: 6.5011 avg training loss: 7.4027
batch: [11510/21305] batch time: 2.885 trainign loss: 5.9395 avg training loss: 7.4026
batch: [11520/21305] batch time: 0.056 trainign loss: 7.1552 avg training loss: 7.4025
batch: [11530/21305] batch time: 2.292 trainign loss: 5.6225 avg training loss: 7.4024
batch: [11540/21305] batch time: 0.056 trainign loss: 8.0632 avg training loss: 7.4024
batch: [11550/21305] batch time: 2.274 trainign loss: 5.6121 avg training loss: 7.4023
batch: [11560/21305] batch time: 0.051 trainign loss: 6.9335 avg training loss: 7.4023
batch: [11570/21305] batch time: 2.224 trainign loss: 5.4434 avg training loss: 7.4022
batch: [11580/21305] batch time: 0.056 trainign loss: 7.6893 avg training loss: 7.4021
batch: [11590/21305] batch time: 2.608 trainign loss: 6.8683 avg training loss: 7.4021
batch: [11600/21305] batch time: 0.055 trainign loss: 4.6468 avg training loss: 7.4020
batch: [11610/21305] batch time: 2.374 trainign loss: 5.9145 avg training loss: 7.4019
batch: [11620/21305] batch time: 0.063 trainign loss: 6.6186 avg training loss: 7.4018
batch: [11630/21305] batch time: 2.474 trainign loss: 7.2627 avg training loss: 7.4017
batch: [11640/21305] batch time: 0.055 trainign loss: 6.4860 avg training loss: 7.4016
batch: [11650/21305] batch time: 1.990 trainign loss: 7.4170 avg training loss: 7.4016
batch: [11660/21305] batch time: 0.056 trainign loss: 5.5758 avg training loss: 7.4015
batch: [11670/21305] batch time: 2.048 trainign loss: 6.7323 avg training loss: 7.4014
batch: [11680/21305] batch time: 0.056 trainign loss: 5.9762 avg training loss: 7.4014
batch: [11690/21305] batch time: 1.755 trainign loss: 6.0186 avg training loss: 7.4012
batch: [11700/21305] batch time: 0.058 trainign loss: 6.5656 avg training loss: 7.4011
batch: [11710/21305] batch time: 1.243 trainign loss: 4.4815 avg training loss: 7.4010
batch: [11720/21305] batch time: 0.056 trainign loss: 7.4018 avg training loss: 7.4010
batch: [11730/21305] batch time: 1.266 trainign loss: 5.7429 avg training loss: 7.4009
batch: [11740/21305] batch time: 0.058 trainign loss: 5.9477 avg training loss: 7.4008
batch: [11750/21305] batch time: 2.164 trainign loss: 6.8835 avg training loss: 7.4008
batch: [11760/21305] batch time: 0.059 trainign loss: 6.6564 avg training loss: 7.4007
batch: [11770/21305] batch time: 2.462 trainign loss: 6.4255 avg training loss: 7.4007
batch: [11780/21305] batch time: 0.062 trainign loss: 4.8337 avg training loss: 7.4005
batch: [11790/21305] batch time: 2.202 trainign loss: 1.2220 avg training loss: 7.4003
batch: [11800/21305] batch time: 0.057 trainign loss: 0.0007 avg training loss: 7.3998
batch: [11810/21305] batch time: 1.963 trainign loss: 6.9554 avg training loss: 7.3997
batch: [11820/21305] batch time: 0.062 trainign loss: 7.3987 avg training loss: 7.3997
batch: [11830/21305] batch time: 2.138 trainign loss: 7.4065 avg training loss: 7.3997
batch: [11840/21305] batch time: 0.056 trainign loss: 6.8477 avg training loss: 7.3996
batch: [11850/21305] batch time: 1.978 trainign loss: 5.7482 avg training loss: 7.3994
batch: [11860/21305] batch time: 0.056 trainign loss: 4.2045 avg training loss: 7.3994
batch: [11870/21305] batch time: 1.611 trainign loss: 5.8027 avg training loss: 7.3992
batch: [11880/21305] batch time: 0.056 trainign loss: 7.8989 avg training loss: 7.3990
batch: [11890/21305] batch time: 1.976 trainign loss: 7.7656 avg training loss: 7.3990
batch: [11900/21305] batch time: 0.062 trainign loss: 5.8200 avg training loss: 7.3990
batch: [11910/21305] batch time: 1.961 trainign loss: 7.3818 avg training loss: 7.3989
batch: [11920/21305] batch time: 0.057 trainign loss: 6.5874 avg training loss: 7.3989
batch: [11930/21305] batch time: 1.573 trainign loss: 5.4208 avg training loss: 7.3987
batch: [11940/21305] batch time: 0.056 trainign loss: 7.5227 avg training loss: 7.3987
batch: [11950/21305] batch time: 2.015 trainign loss: 6.8930 avg training loss: 7.3987
batch: [11960/21305] batch time: 0.058 trainign loss: 7.1470 avg training loss: 7.3986
batch: [11970/21305] batch time: 0.921 trainign loss: 6.5734 avg training loss: 7.3986
batch: [11980/21305] batch time: 0.056 trainign loss: 6.7776 avg training loss: 7.3985
batch: [11990/21305] batch time: 1.485 trainign loss: 5.9546 avg training loss: 7.3985
batch: [12000/21305] batch time: 0.678 trainign loss: 6.4580 avg training loss: 7.3984
batch: [12010/21305] batch time: 1.336 trainign loss: 7.6014 avg training loss: 7.3983
batch: [12020/21305] batch time: 0.506 trainign loss: 6.9203 avg training loss: 7.3983
batch: [12030/21305] batch time: 1.305 trainign loss: 6.2251 avg training loss: 7.3983
batch: [12040/21305] batch time: 0.869 trainign loss: 5.9968 avg training loss: 7.3982
batch: [12050/21305] batch time: 0.561 trainign loss: 6.1501 avg training loss: 7.3981
batch: [12060/21305] batch time: 1.570 trainign loss: 6.9720 avg training loss: 7.3980
batch: [12070/21305] batch time: 0.770 trainign loss: 3.8730 avg training loss: 7.3979
batch: [12080/21305] batch time: 1.239 trainign loss: 6.6667 avg training loss: 7.3976
batch: [12090/21305] batch time: 1.519 trainign loss: 5.9650 avg training loss: 7.3976
batch: [12100/21305] batch time: 0.817 trainign loss: 6.3786 avg training loss: 7.3975
batch: [12110/21305] batch time: 1.470 trainign loss: 7.1240 avg training loss: 7.3975
batch: [12120/21305] batch time: 0.911 trainign loss: 4.3909 avg training loss: 7.3973
batch: [12130/21305] batch time: 1.667 trainign loss: 6.9592 avg training loss: 7.3972
batch: [12140/21305] batch time: 1.340 trainign loss: 5.5718 avg training loss: 7.3971
batch: [12150/21305] batch time: 0.465 trainign loss: 7.2800 avg training loss: 7.3970
batch: [12160/21305] batch time: 1.405 trainign loss: 7.5672 avg training loss: 7.3970
batch: [12170/21305] batch time: 0.768 trainign loss: 6.9033 avg training loss: 7.3970
batch: [12180/21305] batch time: 1.409 trainign loss: 6.9272 avg training loss: 7.3970
batch: [12190/21305] batch time: 0.056 trainign loss: 7.2005 avg training loss: 7.3970
batch: [12200/21305] batch time: 2.295 trainign loss: 6.0007 avg training loss: 7.3969
batch: [12210/21305] batch time: 0.056 trainign loss: 7.0946 avg training loss: 7.3968
batch: [12220/21305] batch time: 1.791 trainign loss: 6.7539 avg training loss: 7.3968
batch: [12230/21305] batch time: 0.063 trainign loss: 7.2160 avg training loss: 7.3968
batch: [12240/21305] batch time: 2.168 trainign loss: 6.2144 avg training loss: 7.3967
batch: [12250/21305] batch time: 0.057 trainign loss: 6.0577 avg training loss: 7.3966
batch: [12260/21305] batch time: 1.498 trainign loss: 6.9036 avg training loss: 7.3966
batch: [12270/21305] batch time: 0.056 trainign loss: 5.2993 avg training loss: 7.3965
batch: [12280/21305] batch time: 1.544 trainign loss: 4.4372 avg training loss: 7.3963
batch: [12290/21305] batch time: 0.056 trainign loss: 7.1076 avg training loss: 7.3962
batch: [12300/21305] batch time: 1.667 trainign loss: 7.4736 avg training loss: 7.3962
batch: [12310/21305] batch time: 0.056 trainign loss: 6.7510 avg training loss: 7.3962
batch: [12320/21305] batch time: 1.988 trainign loss: 6.3322 avg training loss: 7.3961
batch: [12330/21305] batch time: 0.060 trainign loss: 6.9754 avg training loss: 7.3960
batch: [12340/21305] batch time: 1.336 trainign loss: 6.7160 avg training loss: 7.3960
batch: [12350/21305] batch time: 0.056 trainign loss: 6.7174 avg training loss: 7.3959
batch: [12360/21305] batch time: 0.480 trainign loss: 5.0366 avg training loss: 7.3958
batch: [12370/21305] batch time: 0.056 trainign loss: 4.9942 avg training loss: 7.3957
batch: [12380/21305] batch time: 1.423 trainign loss: 6.4413 avg training loss: 7.3956
batch: [12390/21305] batch time: 0.056 trainign loss: 7.6123 avg training loss: 7.3956
batch: [12400/21305] batch time: 0.781 trainign loss: 6.0284 avg training loss: 7.3955
batch: [12410/21305] batch time: 0.062 trainign loss: 7.1019 avg training loss: 7.3955
batch: [12420/21305] batch time: 1.041 trainign loss: 5.2918 avg training loss: 7.3954
batch: [12430/21305] batch time: 0.062 trainign loss: 5.2566 avg training loss: 7.3953
batch: [12440/21305] batch time: 1.068 trainign loss: 6.8819 avg training loss: 7.3951
batch: [12450/21305] batch time: 0.056 trainign loss: 7.7313 avg training loss: 7.3951
batch: [12460/21305] batch time: 0.893 trainign loss: 7.3335 avg training loss: 7.3951
batch: [12470/21305] batch time: 0.056 trainign loss: 5.6097 avg training loss: 7.3950
batch: [12480/21305] batch time: 0.889 trainign loss: 5.5851 avg training loss: 7.3949
batch: [12490/21305] batch time: 0.056 trainign loss: 5.9016 avg training loss: 7.3948
batch: [12500/21305] batch time: 0.861 trainign loss: 7.0245 avg training loss: 7.3947
batch: [12510/21305] batch time: 0.061 trainign loss: 7.4299 avg training loss: 7.3947
batch: [12520/21305] batch time: 0.408 trainign loss: 7.0233 avg training loss: 7.3946
batch: [12530/21305] batch time: 0.056 trainign loss: 6.2874 avg training loss: 7.3945
batch: [12540/21305] batch time: 0.853 trainign loss: 5.3446 avg training loss: 7.3944
batch: [12550/21305] batch time: 0.056 trainign loss: 6.6187 avg training loss: 7.3943
batch: [12560/21305] batch time: 2.431 trainign loss: 3.6095 avg training loss: 7.3942
batch: [12570/21305] batch time: 0.061 trainign loss: 0.0045 avg training loss: 7.3937
batch: [12580/21305] batch time: 2.324 trainign loss: 7.7323 avg training loss: 7.3938
batch: [12590/21305] batch time: 0.061 trainign loss: 3.7223 avg training loss: 7.3937
batch: [12600/21305] batch time: 2.170 trainign loss: 7.8662 avg training loss: 7.3936
batch: [12610/21305] batch time: 0.058 trainign loss: 4.4622 avg training loss: 7.3936
batch: [12620/21305] batch time: 2.056 trainign loss: 6.6906 avg training loss: 7.3936
batch: [12630/21305] batch time: 0.056 trainign loss: 6.3681 avg training loss: 7.3935
batch: [12640/21305] batch time: 2.123 trainign loss: 7.0235 avg training loss: 7.3935
batch: [12650/21305] batch time: 0.057 trainign loss: 5.3700 avg training loss: 7.3934
batch: [12660/21305] batch time: 2.239 trainign loss: 6.1160 avg training loss: 7.3933
batch: [12670/21305] batch time: 0.053 trainign loss: 7.3023 avg training loss: 7.3933
batch: [12680/21305] batch time: 2.553 trainign loss: 4.4700 avg training loss: 7.3932
batch: [12690/21305] batch time: 0.059 trainign loss: 7.5694 avg training loss: 7.3930
batch: [12700/21305] batch time: 2.190 trainign loss: 6.9028 avg training loss: 7.3930
batch: [12710/21305] batch time: 0.055 trainign loss: 5.9694 avg training loss: 7.3929
batch: [12720/21305] batch time: 2.533 trainign loss: 5.5375 avg training loss: 7.3928
batch: [12730/21305] batch time: 0.061 trainign loss: 6.1386 avg training loss: 7.3927
batch: [12740/21305] batch time: 2.400 trainign loss: 4.5669 avg training loss: 7.3926
batch: [12750/21305] batch time: 0.055 trainign loss: 3.5264 avg training loss: 7.3924
batch: [12760/21305] batch time: 2.059 trainign loss: 6.6407 avg training loss: 7.3924
batch: [12770/21305] batch time: 0.056 trainign loss: 6.3676 avg training loss: 7.3924
batch: [12780/21305] batch time: 2.338 trainign loss: 7.6553 avg training loss: 7.3923
batch: [12790/21305] batch time: 0.056 trainign loss: 5.5525 avg training loss: 7.3923
batch: [12800/21305] batch time: 2.283 trainign loss: 7.0650 avg training loss: 7.3922
batch: [12810/21305] batch time: 0.062 trainign loss: 7.2131 avg training loss: 7.3922
batch: [12820/21305] batch time: 2.256 trainign loss: 6.3870 avg training loss: 7.3921
batch: [12830/21305] batch time: 0.053 trainign loss: 4.5990 avg training loss: 7.3920
batch: [12840/21305] batch time: 2.488 trainign loss: 6.7814 avg training loss: 7.3919
batch: [12850/21305] batch time: 0.054 trainign loss: 6.9646 avg training loss: 7.3919
batch: [12860/21305] batch time: 2.371 trainign loss: 6.8160 avg training loss: 7.3918
batch: [12870/21305] batch time: 0.057 trainign loss: 6.3338 avg training loss: 7.3918
batch: [12880/21305] batch time: 2.297 trainign loss: 5.8117 avg training loss: 7.3917
batch: [12890/21305] batch time: 0.062 trainign loss: 6.2634 avg training loss: 7.3916
batch: [12900/21305] batch time: 2.155 trainign loss: 7.2538 avg training loss: 7.3916
batch: [12910/21305] batch time: 0.056 trainign loss: 6.3573 avg training loss: 7.3915
batch: [12920/21305] batch time: 2.121 trainign loss: 6.3678 avg training loss: 7.3914
batch: [12930/21305] batch time: 0.058 trainign loss: 6.0815 avg training loss: 7.3914
batch: [12940/21305] batch time: 2.378 trainign loss: 6.3176 avg training loss: 7.3913
batch: [12950/21305] batch time: 0.056 trainign loss: 4.9305 avg training loss: 7.3911
batch: [12960/21305] batch time: 2.336 trainign loss: 6.9921 avg training loss: 7.3911
batch: [12970/21305] batch time: 0.063 trainign loss: 5.0211 avg training loss: 7.3910
batch: [12980/21305] batch time: 2.028 trainign loss: 6.5768 avg training loss: 7.3908
batch: [12990/21305] batch time: 0.062 trainign loss: 6.5897 avg training loss: 7.3908
batch: [13000/21305] batch time: 2.521 trainign loss: 6.8147 avg training loss: 7.3908
batch: [13010/21305] batch time: 0.054 trainign loss: 6.6003 avg training loss: 7.3906
batch: [13020/21305] batch time: 2.237 trainign loss: 7.5762 avg training loss: 7.3906
batch: [13030/21305] batch time: 0.056 trainign loss: 7.1351 avg training loss: 7.3906
batch: [13040/21305] batch time: 2.170 trainign loss: 7.0347 avg training loss: 7.3905
batch: [13050/21305] batch time: 0.057 trainign loss: 2.1416 avg training loss: 7.3903
batch: [13060/21305] batch time: 2.407 trainign loss: 7.0647 avg training loss: 7.3903
batch: [13070/21305] batch time: 0.061 trainign loss: 6.9337 avg training loss: 7.3902
batch: [13080/21305] batch time: 2.290 trainign loss: 6.4461 avg training loss: 7.3902
batch: [13090/21305] batch time: 0.062 trainign loss: 4.8119 avg training loss: 7.3900
batch: [13100/21305] batch time: 2.108 trainign loss: 7.4065 avg training loss: 7.3899
batch: [13110/21305] batch time: 0.056 trainign loss: 6.7188 avg training loss: 7.3897
batch: [13120/21305] batch time: 2.349 trainign loss: 6.6687 avg training loss: 7.3897
batch: [13130/21305] batch time: 0.056 trainign loss: 5.6761 avg training loss: 7.3897
batch: [13140/21305] batch time: 2.516 trainign loss: 2.4156 avg training loss: 7.3895
batch: [13150/21305] batch time: 0.060 trainign loss: 7.2172 avg training loss: 7.3894
batch: [13160/21305] batch time: 2.164 trainign loss: 5.7019 avg training loss: 7.3894
batch: [13170/21305] batch time: 0.060 trainign loss: 6.8005 avg training loss: 7.3893
batch: [13180/21305] batch time: 1.981 trainign loss: 5.1935 avg training loss: 7.3892
batch: [13190/21305] batch time: 0.052 trainign loss: 6.3338 avg training loss: 7.3891
batch: [13200/21305] batch time: 2.097 trainign loss: 3.3961 avg training loss: 7.3889
batch: [13210/21305] batch time: 0.061 trainign loss: 0.0022 avg training loss: 7.3884
batch: [13220/21305] batch time: 1.760 trainign loss: 0.0001 avg training loss: 7.3879
batch: [13230/21305] batch time: 0.061 trainign loss: 0.0001 avg training loss: 7.3874
batch: [13240/21305] batch time: 2.221 trainign loss: 8.3724 avg training loss: 7.3874
batch: [13250/21305] batch time: 0.063 trainign loss: 7.8063 avg training loss: 7.3874
batch: [13260/21305] batch time: 2.325 trainign loss: 7.7165 avg training loss: 7.3874
batch: [13270/21305] batch time: 0.055 trainign loss: 6.7850 avg training loss: 7.3874
batch: [13280/21305] batch time: 2.383 trainign loss: 7.3182 avg training loss: 7.3873
batch: [13290/21305] batch time: 0.055 trainign loss: 6.7940 avg training loss: 7.3872
batch: [13300/21305] batch time: 2.205 trainign loss: 7.0623 avg training loss: 7.3871
batch: [13310/21305] batch time: 0.062 trainign loss: 4.8967 avg training loss: 7.3870
batch: [13320/21305] batch time: 2.454 trainign loss: 5.3926 avg training loss: 7.3868
batch: [13330/21305] batch time: 0.056 trainign loss: 7.2070 avg training loss: 7.3868
batch: [13340/21305] batch time: 2.323 trainign loss: 7.1838 avg training loss: 7.3867
batch: [13350/21305] batch time: 0.063 trainign loss: 5.7417 avg training loss: 7.3867
batch: [13360/21305] batch time: 1.991 trainign loss: 7.6780 avg training loss: 7.3866
batch: [13370/21305] batch time: 0.098 trainign loss: 6.0091 avg training loss: 7.3866
batch: [13380/21305] batch time: 2.077 trainign loss: 4.6961 avg training loss: 7.3864
batch: [13390/21305] batch time: 0.059 trainign loss: 6.3635 avg training loss: 7.3863
batch: [13400/21305] batch time: 1.884 trainign loss: 4.4988 avg training loss: 7.3862
batch: [13410/21305] batch time: 0.062 trainign loss: 7.2230 avg training loss: 7.3862
batch: [13420/21305] batch time: 1.777 trainign loss: 6.6094 avg training loss: 7.3862
batch: [13430/21305] batch time: 0.063 trainign loss: 6.5557 avg training loss: 7.3861
batch: [13440/21305] batch time: 1.048 trainign loss: 5.8171 avg training loss: 7.3861
batch: [13450/21305] batch time: 0.063 trainign loss: 6.9108 avg training loss: 7.3860
batch: [13460/21305] batch time: 0.304 trainign loss: 6.5079 avg training loss: 7.3860
batch: [13470/21305] batch time: 0.056 trainign loss: 3.9243 avg training loss: 7.3858
batch: [13480/21305] batch time: 0.056 trainign loss: 4.5952 avg training loss: 7.3857
batch: [13490/21305] batch time: 0.055 trainign loss: 7.0313 avg training loss: 7.3856
batch: [13500/21305] batch time: 0.062 trainign loss: 6.6532 avg training loss: 7.3856
batch: [13510/21305] batch time: 0.056 trainign loss: 5.9603 avg training loss: 7.3855
batch: [13520/21305] batch time: 0.061 trainign loss: 6.3150 avg training loss: 7.3854
batch: [13530/21305] batch time: 0.056 trainign loss: 5.8599 avg training loss: 7.3853
batch: [13540/21305] batch time: 0.461 trainign loss: 3.2859 avg training loss: 7.3852
batch: [13550/21305] batch time: 0.062 trainign loss: 7.2736 avg training loss: 7.3852
batch: [13560/21305] batch time: 0.844 trainign loss: 6.2730 avg training loss: 7.3851
batch: [13570/21305] batch time: 0.591 trainign loss: 6.8316 avg training loss: 7.3850
batch: [13580/21305] batch time: 0.652 trainign loss: 7.0597 avg training loss: 7.3849
batch: [13590/21305] batch time: 1.078 trainign loss: 5.0668 avg training loss: 7.3849
batch: [13600/21305] batch time: 0.677 trainign loss: 6.1341 avg training loss: 7.3848
batch: [13610/21305] batch time: 0.933 trainign loss: 7.6160 avg training loss: 7.3847
batch: [13620/21305] batch time: 0.216 trainign loss: 6.5637 avg training loss: 7.3846
batch: [13630/21305] batch time: 0.225 trainign loss: 6.6780 avg training loss: 7.3846
batch: [13640/21305] batch time: 0.056 trainign loss: 7.0129 avg training loss: 7.3845
batch: [13650/21305] batch time: 0.056 trainign loss: 6.1289 avg training loss: 7.3845
batch: [13660/21305] batch time: 0.060 trainign loss: 5.8058 avg training loss: 7.3844
batch: [13670/21305] batch time: 0.057 trainign loss: 6.6413 avg training loss: 7.3843
batch: [13680/21305] batch time: 0.419 trainign loss: 5.7467 avg training loss: 7.3842
batch: [13690/21305] batch time: 0.191 trainign loss: 3.9370 avg training loss: 7.3841
batch: [13700/21305] batch time: 0.056 trainign loss: 7.7503 avg training loss: 7.3841
batch: [13710/21305] batch time: 0.640 trainign loss: 6.7917 avg training loss: 7.3841
batch: [13720/21305] batch time: 0.057 trainign loss: 6.3326 avg training loss: 7.3840
batch: [13730/21305] batch time: 0.057 trainign loss: 7.0782 avg training loss: 7.3839
batch: [13740/21305] batch time: 0.055 trainign loss: 5.6022 avg training loss: 7.3838
batch: [13750/21305] batch time: 0.057 trainign loss: 7.3936 avg training loss: 7.3837
batch: [13760/21305] batch time: 0.056 trainign loss: 7.4202 avg training loss: 7.3837
batch: [13770/21305] batch time: 0.882 trainign loss: 6.9235 avg training loss: 7.3837
batch: [13780/21305] batch time: 0.056 trainign loss: 5.5848 avg training loss: 7.3836
batch: [13790/21305] batch time: 0.925 trainign loss: 7.0430 avg training loss: 7.3835
batch: [13800/21305] batch time: 0.056 trainign loss: 6.2095 avg training loss: 7.3835
batch: [13810/21305] batch time: 0.710 trainign loss: 6.5471 avg training loss: 7.3834
batch: [13820/21305] batch time: 0.056 trainign loss: 7.1517 avg training loss: 7.3834
batch: [13830/21305] batch time: 0.062 trainign loss: 5.9531 avg training loss: 7.3833
batch: [13840/21305] batch time: 0.146 trainign loss: 5.8451 avg training loss: 7.3832
batch: [13850/21305] batch time: 0.057 trainign loss: 0.7485 avg training loss: 7.3829
batch: [13860/21305] batch time: 0.617 trainign loss: 6.3025 avg training loss: 7.3828
batch: [13870/21305] batch time: 0.059 trainign loss: 7.4611 avg training loss: 7.3828
batch: [13880/21305] batch time: 1.154 trainign loss: 5.9391 avg training loss: 7.3827
batch: [13890/21305] batch time: 0.062 trainign loss: 7.0942 avg training loss: 7.3826
batch: [13900/21305] batch time: 0.928 trainign loss: 5.0227 avg training loss: 7.3826
batch: [13910/21305] batch time: 0.057 trainign loss: 7.0500 avg training loss: 7.3825
batch: [13920/21305] batch time: 0.409 trainign loss: 4.4067 avg training loss: 7.3823
batch: [13930/21305] batch time: 0.054 trainign loss: 6.2700 avg training loss: 7.3822
batch: [13940/21305] batch time: 0.057 trainign loss: 7.6317 avg training loss: 7.3822
batch: [13950/21305] batch time: 0.058 trainign loss: 5.5003 avg training loss: 7.3821
batch: [13960/21305] batch time: 0.363 trainign loss: 5.9167 avg training loss: 7.3821
batch: [13970/21305] batch time: 0.057 trainign loss: 6.9063 avg training loss: 7.3820
batch: [13980/21305] batch time: 0.179 trainign loss: 6.8217 avg training loss: 7.3820
batch: [13990/21305] batch time: 0.056 trainign loss: 6.5941 avg training loss: 7.3819
batch: [14000/21305] batch time: 0.199 trainign loss: 6.0362 avg training loss: 7.3818
batch: [14010/21305] batch time: 0.057 trainign loss: 4.8994 avg training loss: 7.3816
batch: [14020/21305] batch time: 0.056 trainign loss: 7.4127 avg training loss: 7.3814
batch: [14030/21305] batch time: 0.054 trainign loss: 7.2121 avg training loss: 7.3814
batch: [14040/21305] batch time: 0.056 trainign loss: 7.1962 avg training loss: 7.3814
batch: [14050/21305] batch time: 0.054 trainign loss: 7.0953 avg training loss: 7.3814
batch: [14060/21305] batch time: 0.056 trainign loss: 6.6062 avg training loss: 7.3813
batch: [14070/21305] batch time: 0.062 trainign loss: 6.9083 avg training loss: 7.3813
batch: [14080/21305] batch time: 0.056 trainign loss: 6.8076 avg training loss: 7.3813
batch: [14090/21305] batch time: 0.051 trainign loss: 6.8050 avg training loss: 7.3812
batch: [14100/21305] batch time: 0.061 trainign loss: 6.2409 avg training loss: 7.3811
batch: [14110/21305] batch time: 0.062 trainign loss: 6.0449 avg training loss: 7.3811
batch: [14120/21305] batch time: 0.057 trainign loss: 7.2384 avg training loss: 7.3809
batch: [14130/21305] batch time: 0.060 trainign loss: 5.8946 avg training loss: 7.3808
batch: [14140/21305] batch time: 0.063 trainign loss: 6.5551 avg training loss: 7.3807
batch: [14150/21305] batch time: 0.052 trainign loss: 6.7870 avg training loss: 7.3806
batch: [14160/21305] batch time: 0.056 trainign loss: 6.5536 avg training loss: 7.3805
batch: [14170/21305] batch time: 0.052 trainign loss: 6.5214 avg training loss: 7.3805
batch: [14180/21305] batch time: 0.062 trainign loss: 6.0372 avg training loss: 7.3804
batch: [14190/21305] batch time: 0.056 trainign loss: 2.4527 avg training loss: 7.3802
batch: [14200/21305] batch time: 0.051 trainign loss: 7.1855 avg training loss: 7.3801
batch: [14210/21305] batch time: 0.052 trainign loss: 6.2702 avg training loss: 7.3801
batch: [14220/21305] batch time: 0.055 trainign loss: 7.8094 avg training loss: 7.3799
batch: [14230/21305] batch time: 0.051 trainign loss: 6.6289 avg training loss: 7.3799
batch: [14240/21305] batch time: 0.062 trainign loss: 7.1161 avg training loss: 7.3799
batch: [14250/21305] batch time: 0.056 trainign loss: 7.0064 avg training loss: 7.3799
batch: [14260/21305] batch time: 0.056 trainign loss: 5.4687 avg training loss: 7.3798
batch: [14270/21305] batch time: 0.055 trainign loss: 5.1203 avg training loss: 7.3797
batch: [14280/21305] batch time: 0.056 trainign loss: 5.0266 avg training loss: 7.3796
batch: [14290/21305] batch time: 0.062 trainign loss: 6.4704 avg training loss: 7.3795
batch: [14300/21305] batch time: 0.062 trainign loss: 7.0214 avg training loss: 7.3795
batch: [14310/21305] batch time: 0.052 trainign loss: 6.6541 avg training loss: 7.3794
batch: [14320/21305] batch time: 0.063 trainign loss: 2.4410 avg training loss: 7.3792
batch: [14330/21305] batch time: 0.057 trainign loss: 6.9033 avg training loss: 7.3791
batch: [14340/21305] batch time: 0.056 trainign loss: 6.2739 avg training loss: 7.3791
batch: [14350/21305] batch time: 0.050 trainign loss: 5.4141 avg training loss: 7.3791
batch: [14360/21305] batch time: 0.056 trainign loss: 5.6246 avg training loss: 7.3790
batch: [14370/21305] batch time: 0.057 trainign loss: 5.8347 avg training loss: 7.3789
batch: [14380/21305] batch time: 0.063 trainign loss: 6.4310 avg training loss: 7.3789
batch: [14390/21305] batch time: 0.056 trainign loss: 6.7243 avg training loss: 7.3788
batch: [14400/21305] batch time: 0.063 trainign loss: 6.0126 avg training loss: 7.3787
batch: [14410/21305] batch time: 0.056 trainign loss: 6.6218 avg training loss: 7.3786
batch: [14420/21305] batch time: 0.062 trainign loss: 6.2017 avg training loss: 7.3785
batch: [14430/21305] batch time: 0.062 trainign loss: 6.6469 avg training loss: 7.3785
batch: [14440/21305] batch time: 0.056 trainign loss: 6.8077 avg training loss: 7.3785
batch: [14450/21305] batch time: 0.056 trainign loss: 5.3697 avg training loss: 7.3784
batch: [14460/21305] batch time: 0.063 trainign loss: 6.8604 avg training loss: 7.3783
batch: [14470/21305] batch time: 0.053 trainign loss: 5.7617 avg training loss: 7.3782
batch: [14480/21305] batch time: 0.053 trainign loss: 4.2317 avg training loss: 7.3781
batch: [14490/21305] batch time: 0.061 trainign loss: 5.4453 avg training loss: 7.3780
batch: [14500/21305] batch time: 0.063 trainign loss: 1.4288 avg training loss: 7.3778
batch: [14510/21305] batch time: 0.200 trainign loss: 0.0013 avg training loss: 7.3773
batch: [14520/21305] batch time: 0.063 trainign loss: 0.0002 avg training loss: 7.3768
batch: [14530/21305] batch time: 0.347 trainign loss: 0.0001 avg training loss: 7.3762
batch: [14540/21305] batch time: 0.058 trainign loss: 0.0001 avg training loss: 7.3757
batch: [14550/21305] batch time: 0.356 trainign loss: 0.0001 avg training loss: 7.3752
batch: [14560/21305] batch time: 0.415 trainign loss: 0.0001 avg training loss: 7.3747
batch: [14570/21305] batch time: 0.109 trainign loss: 0.0001 avg training loss: 7.3742
batch: [14580/21305] batch time: 0.387 trainign loss: 0.0001 avg training loss: 7.3737
batch: [14590/21305] batch time: 0.338 trainign loss: 0.0001 avg training loss: 7.3731
batch: [14600/21305] batch time: 0.568 trainign loss: 6.8011 avg training loss: 7.3730
batch: [14610/21305] batch time: 0.142 trainign loss: 7.7404 avg training loss: 7.3731
batch: [14620/21305] batch time: 1.188 trainign loss: 6.7662 avg training loss: 7.3731
batch: [14630/21305] batch time: 0.125 trainign loss: 6.0550 avg training loss: 7.3729
batch: [14640/21305] batch time: 1.178 trainign loss: 5.6262 avg training loss: 7.3729
batch: [14650/21305] batch time: 0.572 trainign loss: 7.0766 avg training loss: 7.3729
batch: [14660/21305] batch time: 1.246 trainign loss: 6.0909 avg training loss: 7.3728
batch: [14670/21305] batch time: 0.350 trainign loss: 5.1710 avg training loss: 7.3727
batch: [14680/21305] batch time: 1.370 trainign loss: 1.7763 avg training loss: 7.3725
batch: [14690/21305] batch time: 0.401 trainign loss: 6.5798 avg training loss: 7.3724
batch: [14700/21305] batch time: 0.874 trainign loss: 4.4024 avg training loss: 7.3723
batch: [14710/21305] batch time: 0.660 trainign loss: 0.0053 avg training loss: 7.3719
batch: [14720/21305] batch time: 0.631 trainign loss: 7.5051 avg training loss: 7.3718
batch: [14730/21305] batch time: 1.058 trainign loss: 5.1428 avg training loss: 7.3718
batch: [14740/21305] batch time: 0.388 trainign loss: 5.7021 avg training loss: 7.3717
batch: [14750/21305] batch time: 0.868 trainign loss: 7.5807 avg training loss: 7.3717
batch: [14760/21305] batch time: 1.179 trainign loss: 7.3792 avg training loss: 7.3716
batch: [14770/21305] batch time: 0.056 trainign loss: 6.8966 avg training loss: 7.3716
batch: [14780/21305] batch time: 0.536 trainign loss: 6.3872 avg training loss: 7.3716
batch: [14790/21305] batch time: 0.506 trainign loss: 7.0780 avg training loss: 7.3715
batch: [14800/21305] batch time: 1.070 trainign loss: 6.4909 avg training loss: 7.3715
batch: [14810/21305] batch time: 0.056 trainign loss: 6.0183 avg training loss: 7.3714
batch: [14820/21305] batch time: 0.644 trainign loss: 6.6346 avg training loss: 7.3713
batch: [14830/21305] batch time: 0.553 trainign loss: 6.6600 avg training loss: 7.3713
batch: [14840/21305] batch time: 0.620 trainign loss: 5.4129 avg training loss: 7.3712
batch: [14850/21305] batch time: 0.055 trainign loss: 7.0762 avg training loss: 7.3712
batch: [14860/21305] batch time: 1.317 trainign loss: 7.1937 avg training loss: 7.3711
batch: [14870/21305] batch time: 0.127 trainign loss: 6.4275 avg training loss: 7.3711
batch: [14880/21305] batch time: 0.988 trainign loss: 6.5810 avg training loss: 7.3710
batch: [14890/21305] batch time: 0.056 trainign loss: 6.1465 avg training loss: 7.3709
batch: [14900/21305] batch time: 1.388 trainign loss: 6.2483 avg training loss: 7.3709
batch: [14910/21305] batch time: 0.056 trainign loss: 2.9842 avg training loss: 7.3707
batch: [14920/21305] batch time: 0.968 trainign loss: 6.3446 avg training loss: 7.3707
batch: [14930/21305] batch time: 0.063 trainign loss: 6.0268 avg training loss: 7.3706
batch: [14940/21305] batch time: 1.481 trainign loss: 6.6866 avg training loss: 7.3705
batch: [14950/21305] batch time: 0.056 trainign loss: 6.9447 avg training loss: 7.3704
batch: [14960/21305] batch time: 0.623 trainign loss: 7.1458 avg training loss: 7.3703
batch: [14970/21305] batch time: 0.063 trainign loss: 7.6363 avg training loss: 7.3702
batch: [14980/21305] batch time: 0.687 trainign loss: 7.6839 avg training loss: 7.3702
batch: [14990/21305] batch time: 0.056 trainign loss: 5.1698 avg training loss: 7.3701
batch: [15000/21305] batch time: 0.060 trainign loss: 6.2721 avg training loss: 7.3700
batch: [15010/21305] batch time: 0.056 trainign loss: 7.6122 avg training loss: 7.3700
batch: [15020/21305] batch time: 0.056 trainign loss: 5.7289 avg training loss: 7.3699
batch: [15030/21305] batch time: 0.050 trainign loss: 7.1226 avg training loss: 7.3699
batch: [15040/21305] batch time: 0.418 trainign loss: 2.6601 avg training loss: 7.3697
batch: [15050/21305] batch time: 0.056 trainign loss: 7.1217 avg training loss: 7.3697
batch: [15060/21305] batch time: 0.553 trainign loss: 6.3946 avg training loss: 7.3696
batch: [15070/21305] batch time: 0.058 trainign loss: 5.8557 avg training loss: 7.3695
batch: [15080/21305] batch time: 1.353 trainign loss: 3.7370 avg training loss: 7.3692
batch: [15090/21305] batch time: 0.062 trainign loss: 7.3963 avg training loss: 7.3693
batch: [15100/21305] batch time: 2.189 trainign loss: 5.9934 avg training loss: 7.3693
batch: [15110/21305] batch time: 0.056 trainign loss: 2.9622 avg training loss: 7.3692
batch: [15120/21305] batch time: 1.614 trainign loss: 7.1234 avg training loss: 7.3691
batch: [15130/21305] batch time: 0.053 trainign loss: 6.3590 avg training loss: 7.3690
batch: [15140/21305] batch time: 1.794 trainign loss: 7.6574 avg training loss: 7.3690
batch: [15150/21305] batch time: 0.056 trainign loss: 6.8641 avg training loss: 7.3690
batch: [15160/21305] batch time: 1.549 trainign loss: 6.8868 avg training loss: 7.3689
batch: [15170/21305] batch time: 0.057 trainign loss: 6.4132 avg training loss: 7.3688
batch: [15180/21305] batch time: 1.131 trainign loss: 5.4186 avg training loss: 7.3687
batch: [15190/21305] batch time: 0.056 trainign loss: 6.7572 avg training loss: 7.3686
batch: [15200/21305] batch time: 1.810 trainign loss: 7.2089 avg training loss: 7.3687
batch: [15210/21305] batch time: 0.062 trainign loss: 5.7855 avg training loss: 7.3685
batch: [15220/21305] batch time: 1.878 trainign loss: 6.5436 avg training loss: 7.3685
batch: [15230/21305] batch time: 0.149 trainign loss: 6.9653 avg training loss: 7.3684
batch: [15240/21305] batch time: 1.042 trainign loss: 6.6394 avg training loss: 7.3684
batch: [15250/21305] batch time: 1.394 trainign loss: 6.9305 avg training loss: 7.3683
batch: [15260/21305] batch time: 1.469 trainign loss: 5.5080 avg training loss: 7.3682
batch: [15270/21305] batch time: 0.889 trainign loss: 7.5500 avg training loss: 7.3681
batch: [15280/21305] batch time: 0.592 trainign loss: 6.4535 avg training loss: 7.3681
batch: [15290/21305] batch time: 1.244 trainign loss: 7.2063 avg training loss: 7.3680
batch: [15300/21305] batch time: 0.324 trainign loss: 6.6818 avg training loss: 7.3680
batch: [15310/21305] batch time: 1.027 trainign loss: 5.9583 avg training loss: 7.3678
batch: [15320/21305] batch time: 0.056 trainign loss: 7.5850 avg training loss: 7.3678
batch: [15330/21305] batch time: 1.597 trainign loss: 6.6157 avg training loss: 7.3678
batch: [15340/21305] batch time: 0.319 trainign loss: 5.4207 avg training loss: 7.3677
batch: [15350/21305] batch time: 2.129 trainign loss: 6.1380 avg training loss: 7.3676
batch: [15360/21305] batch time: 0.056 trainign loss: 7.4092 avg training loss: 7.3675
batch: [15370/21305] batch time: 1.910 trainign loss: 5.1646 avg training loss: 7.3674
batch: [15380/21305] batch time: 0.058 trainign loss: 5.9611 avg training loss: 7.3674
batch: [15390/21305] batch time: 0.374 trainign loss: 6.6907 avg training loss: 7.3673
batch: [15400/21305] batch time: 0.680 trainign loss: 6.8839 avg training loss: 7.3673
batch: [15410/21305] batch time: 0.056 trainign loss: 6.8526 avg training loss: 7.3672
batch: [15420/21305] batch time: 0.057 trainign loss: 6.3863 avg training loss: 7.3672
batch: [15430/21305] batch time: 0.061 trainign loss: 5.9169 avg training loss: 7.3671
batch: [15440/21305] batch time: 0.056 trainign loss: 7.1126 avg training loss: 7.3670
batch: [15450/21305] batch time: 0.052 trainign loss: 6.8265 avg training loss: 7.3669
batch: [15460/21305] batch time: 0.056 trainign loss: 6.9218 avg training loss: 7.3669
batch: [15470/21305] batch time: 0.056 trainign loss: 6.3883 avg training loss: 7.3668
batch: [15480/21305] batch time: 0.057 trainign loss: 6.9453 avg training loss: 7.3668
batch: [15490/21305] batch time: 0.053 trainign loss: 6.0597 avg training loss: 7.3667
batch: [15500/21305] batch time: 0.056 trainign loss: 4.8791 avg training loss: 7.3666
batch: [15510/21305] batch time: 0.056 trainign loss: 3.9842 avg training loss: 7.3665
batch: [15520/21305] batch time: 0.056 trainign loss: 7.9652 avg training loss: 7.3663
batch: [15530/21305] batch time: 0.052 trainign loss: 7.5863 avg training loss: 7.3663
batch: [15540/21305] batch time: 0.056 trainign loss: 7.2849 avg training loss: 7.3663
batch: [15550/21305] batch time: 0.051 trainign loss: 5.6496 avg training loss: 7.3663
batch: [15560/21305] batch time: 0.062 trainign loss: 7.8179 avg training loss: 7.3662
batch: [15570/21305] batch time: 0.052 trainign loss: 6.6057 avg training loss: 7.3662
batch: [15580/21305] batch time: 0.056 trainign loss: 6.6032 avg training loss: 7.3661
batch: [15590/21305] batch time: 0.056 trainign loss: 5.9534 avg training loss: 7.3661
batch: [15600/21305] batch time: 0.056 trainign loss: 6.4201 avg training loss: 7.3660
batch: [15610/21305] batch time: 0.056 trainign loss: 6.3224 avg training loss: 7.3659
batch: [15620/21305] batch time: 0.056 trainign loss: 6.2941 avg training loss: 7.3659
batch: [15630/21305] batch time: 0.056 trainign loss: 5.8916 avg training loss: 7.3658
batch: [15640/21305] batch time: 0.056 trainign loss: 7.1603 avg training loss: 7.3657
batch: [15650/21305] batch time: 0.056 trainign loss: 5.4626 avg training loss: 7.3657
batch: [15660/21305] batch time: 0.056 trainign loss: 5.0879 avg training loss: 7.3656
batch: [15670/21305] batch time: 0.054 trainign loss: 6.3148 avg training loss: 7.3655
batch: [15680/21305] batch time: 0.057 trainign loss: 7.1035 avg training loss: 7.3654
batch: [15690/21305] batch time: 0.055 trainign loss: 6.8389 avg training loss: 7.3654
batch: [15700/21305] batch time: 0.060 trainign loss: 6.5978 avg training loss: 7.3653
batch: [15710/21305] batch time: 0.051 trainign loss: 6.1991 avg training loss: 7.3652
batch: [15720/21305] batch time: 0.057 trainign loss: 4.6560 avg training loss: 7.3651
batch: [15730/21305] batch time: 0.057 trainign loss: 0.2456 avg training loss: 7.3648
batch: [15740/21305] batch time: 0.056 trainign loss: 12.0604 avg training loss: 7.3645
batch: [15750/21305] batch time: 0.054 trainign loss: 7.4993 avg training loss: 7.3646
batch: [15760/21305] batch time: 0.056 trainign loss: 6.8914 avg training loss: 7.3646
batch: [15770/21305] batch time: 0.058 trainign loss: 6.2872 avg training loss: 7.3645
batch: [15780/21305] batch time: 0.059 trainign loss: 5.9613 avg training loss: 7.3645
batch: [15790/21305] batch time: 0.062 trainign loss: 6.2410 avg training loss: 7.3644
batch: [15800/21305] batch time: 0.056 trainign loss: 4.7153 avg training loss: 7.3643
batch: [15810/21305] batch time: 0.056 trainign loss: 7.2692 avg training loss: 7.3642
batch: [15820/21305] batch time: 0.058 trainign loss: 6.3084 avg training loss: 7.3642
batch: [15830/21305] batch time: 0.056 trainign loss: 6.6143 avg training loss: 7.3642
batch: [15840/21305] batch time: 0.056 trainign loss: 6.0118 avg training loss: 7.3641
batch: [15850/21305] batch time: 0.057 trainign loss: 4.7229 avg training loss: 7.3640
batch: [15860/21305] batch time: 0.056 trainign loss: 7.6054 avg training loss: 7.3640
batch: [15870/21305] batch time: 0.054 trainign loss: 5.7788 avg training loss: 7.3639
batch: [15880/21305] batch time: 0.841 trainign loss: 7.0279 avg training loss: 7.3639
batch: [15890/21305] batch time: 0.056 trainign loss: 7.2539 avg training loss: 7.3639
batch: [15900/21305] batch time: 1.334 trainign loss: 5.6041 avg training loss: 7.3638
batch: [15910/21305] batch time: 0.060 trainign loss: 6.3570 avg training loss: 7.3637
batch: [15920/21305] batch time: 0.999 trainign loss: 6.4328 avg training loss: 7.3637
batch: [15930/21305] batch time: 0.056 trainign loss: 6.7876 avg training loss: 7.3636
batch: [15940/21305] batch time: 0.313 trainign loss: 6.3695 avg training loss: 7.3634
batch: [15950/21305] batch time: 0.062 trainign loss: 7.1094 avg training loss: 7.3634
batch: [15960/21305] batch time: 0.547 trainign loss: 4.7974 avg training loss: 7.3633
batch: [15970/21305] batch time: 0.061 trainign loss: 4.9186 avg training loss: 7.3632
batch: [15980/21305] batch time: 1.536 trainign loss: 5.7640 avg training loss: 7.3632
batch: [15990/21305] batch time: 0.057 trainign loss: 6.9031 avg training loss: 7.3631
batch: [16000/21305] batch time: 0.672 trainign loss: 6.3781 avg training loss: 7.3630
batch: [16010/21305] batch time: 0.056 trainign loss: 0.7897 avg training loss: 7.3628
batch: [16020/21305] batch time: 0.467 trainign loss: 7.1671 avg training loss: 7.3627
batch: [16030/21305] batch time: 0.056 trainign loss: 7.2082 avg training loss: 7.3627
batch: [16040/21305] batch time: 0.061 trainign loss: 4.6654 avg training loss: 7.3626
batch: [16050/21305] batch time: 0.062 trainign loss: 6.5833 avg training loss: 7.3625
batch: [16060/21305] batch time: 0.061 trainign loss: 6.0654 avg training loss: 7.3625
batch: [16070/21305] batch time: 0.052 trainign loss: 7.5140 avg training loss: 7.3624
batch: [16080/21305] batch time: 0.056 trainign loss: 6.8202 avg training loss: 7.3624
batch: [16090/21305] batch time: 0.054 trainign loss: 6.0572 avg training loss: 7.3623
batch: [16100/21305] batch time: 0.056 trainign loss: 5.6731 avg training loss: 7.3622
batch: [16110/21305] batch time: 0.056 trainign loss: 5.9614 avg training loss: 7.3622
batch: [16120/21305] batch time: 0.056 trainign loss: 4.6930 avg training loss: 7.3621
batch: [16130/21305] batch time: 0.052 trainign loss: 6.2189 avg training loss: 7.3620
batch: [16140/21305] batch time: 0.056 trainign loss: 6.6144 avg training loss: 7.3619
batch: [16150/21305] batch time: 0.056 trainign loss: 6.5076 avg training loss: 7.3619
batch: [16160/21305] batch time: 0.056 trainign loss: 4.1144 avg training loss: 7.3618
batch: [16170/21305] batch time: 0.056 trainign loss: 7.5037 avg training loss: 7.3617
batch: [16180/21305] batch time: 0.062 trainign loss: 7.1330 avg training loss: 7.3617
batch: [16190/21305] batch time: 0.052 trainign loss: 7.0361 avg training loss: 7.3617
batch: [16200/21305] batch time: 0.062 trainign loss: 5.3765 avg training loss: 7.3616
batch: [16210/21305] batch time: 0.055 trainign loss: 6.9284 avg training loss: 7.3616
batch: [16220/21305] batch time: 0.056 trainign loss: 6.7558 avg training loss: 7.3615
batch: [16230/21305] batch time: 0.052 trainign loss: 6.7542 avg training loss: 7.3614
batch: [16240/21305] batch time: 0.063 trainign loss: 7.2417 avg training loss: 7.3614
batch: [16250/21305] batch time: 0.052 trainign loss: 6.7330 avg training loss: 7.3613
batch: [16260/21305] batch time: 0.057 trainign loss: 6.8114 avg training loss: 7.3613
batch: [16270/21305] batch time: 0.057 trainign loss: 7.0973 avg training loss: 7.3612
batch: [16280/21305] batch time: 0.056 trainign loss: 7.1235 avg training loss: 7.3612
batch: [16290/21305] batch time: 0.054 trainign loss: 6.6692 avg training loss: 7.3611
batch: [16300/21305] batch time: 0.607 trainign loss: 6.6055 avg training loss: 7.3610
batch: [16310/21305] batch time: 0.056 trainign loss: 6.7787 avg training loss: 7.3609
batch: [16320/21305] batch time: 0.653 trainign loss: 6.9265 avg training loss: 7.3609
batch: [16330/21305] batch time: 0.051 trainign loss: 5.6648 avg training loss: 7.3608
batch: [16340/21305] batch time: 0.443 trainign loss: 5.0201 avg training loss: 7.3607
batch: [16350/21305] batch time: 0.058 trainign loss: 7.7729 avg training loss: 7.3606
batch: [16360/21305] batch time: 0.252 trainign loss: 6.1712 avg training loss: 7.3605
batch: [16370/21305] batch time: 0.063 trainign loss: 5.9452 avg training loss: 7.3604
batch: [16380/21305] batch time: 0.318 trainign loss: 6.7939 avg training loss: 7.3603
batch: [16390/21305] batch time: 0.062 trainign loss: 6.0884 avg training loss: 7.3603
batch: [16400/21305] batch time: 0.056 trainign loss: 7.1063 avg training loss: 7.3602
batch: [16410/21305] batch time: 0.056 trainign loss: 7.2675 avg training loss: 7.3602
batch: [16420/21305] batch time: 0.056 trainign loss: 6.6787 avg training loss: 7.3601
batch: [16430/21305] batch time: 0.054 trainign loss: 5.8688 avg training loss: 7.3600
batch: [16440/21305] batch time: 0.972 trainign loss: 7.0457 avg training loss: 7.3600
batch: [16450/21305] batch time: 0.061 trainign loss: 7.3905 avg training loss: 7.3599
batch: [16460/21305] batch time: 0.664 trainign loss: 7.1027 avg training loss: 7.3598
batch: [16470/21305] batch time: 0.056 trainign loss: 6.7521 avg training loss: 7.3598
batch: [16480/21305] batch time: 1.092 trainign loss: 5.7649 avg training loss: 7.3597
batch: [16490/21305] batch time: 0.052 trainign loss: 5.5346 avg training loss: 7.3597
batch: [16500/21305] batch time: 1.125 trainign loss: 5.7896 avg training loss: 7.3596
batch: [16510/21305] batch time: 0.057 trainign loss: 6.5915 avg training loss: 7.3595
batch: [16520/21305] batch time: 1.721 trainign loss: 6.1788 avg training loss: 7.3595
batch: [16530/21305] batch time: 0.061 trainign loss: 6.6677 avg training loss: 7.3594
batch: [16540/21305] batch time: 2.294 trainign loss: 6.2064 avg training loss: 7.3592
batch: [16550/21305] batch time: 0.056 trainign loss: 7.0042 avg training loss: 7.3592
batch: [16560/21305] batch time: 1.810 trainign loss: 7.1453 avg training loss: 7.3591
batch: [16570/21305] batch time: 0.056 trainign loss: 6.2090 avg training loss: 7.3591
batch: [16580/21305] batch time: 2.530 trainign loss: 7.0980 avg training loss: 7.3590
batch: [16590/21305] batch time: 0.053 trainign loss: 7.1031 avg training loss: 7.3589
batch: [16600/21305] batch time: 2.328 trainign loss: 6.6688 avg training loss: 7.3589
batch: [16610/21305] batch time: 0.056 trainign loss: 5.2709 avg training loss: 7.3587
batch: [16620/21305] batch time: 1.293 trainign loss: 6.7721 avg training loss: 7.3585
batch: [16630/21305] batch time: 0.059 trainign loss: 5.4501 avg training loss: 7.3584
batch: [16640/21305] batch time: 0.916 trainign loss: 7.5542 avg training loss: 7.3584
batch: [16650/21305] batch time: 0.056 trainign loss: 7.2814 avg training loss: 7.3584
batch: [16660/21305] batch time: 0.799 trainign loss: 6.8100 avg training loss: 7.3583
batch: [16670/21305] batch time: 0.060 trainign loss: 6.6689 avg training loss: 7.3582
batch: [16680/21305] batch time: 0.920 trainign loss: 7.3416 avg training loss: 7.3582
batch: [16690/21305] batch time: 0.052 trainign loss: 7.1736 avg training loss: 7.3582
batch: [16700/21305] batch time: 0.614 trainign loss: 6.8584 avg training loss: 7.3581
batch: [16710/21305] batch time: 0.062 trainign loss: 6.8243 avg training loss: 7.3581
batch: [16720/21305] batch time: 0.152 trainign loss: 7.0610 avg training loss: 7.3581
batch: [16730/21305] batch time: 0.051 trainign loss: 5.0684 avg training loss: 7.3580
batch: [16740/21305] batch time: 0.240 trainign loss: 6.5719 avg training loss: 7.3579
batch: [16750/21305] batch time: 0.056 trainign loss: 6.7398 avg training loss: 7.3579
batch: [16760/21305] batch time: 1.151 trainign loss: 6.2893 avg training loss: 7.3579
batch: [16770/21305] batch time: 0.056 trainign loss: 7.1609 avg training loss: 7.3578
batch: [16780/21305] batch time: 0.899 trainign loss: 6.2648 avg training loss: 7.3577
batch: [16790/21305] batch time: 0.058 trainign loss: 6.3636 avg training loss: 7.3577
batch: [16800/21305] batch time: 0.430 trainign loss: 2.6379 avg training loss: 7.3574
batch: [16810/21305] batch time: 0.056 trainign loss: 7.7400 avg training loss: 7.3574
batch: [16820/21305] batch time: 0.928 trainign loss: 6.1617 avg training loss: 7.3574
batch: [16830/21305] batch time: 0.062 trainign loss: 6.5201 avg training loss: 7.3574
batch: [16840/21305] batch time: 1.200 trainign loss: 7.0870 avg training loss: 7.3573
batch: [16850/21305] batch time: 0.056 trainign loss: 6.8442 avg training loss: 7.3572
batch: [16860/21305] batch time: 0.684 trainign loss: 6.9895 avg training loss: 7.3571
batch: [16870/21305] batch time: 0.053 trainign loss: 6.5510 avg training loss: 7.3570
batch: [16880/21305] batch time: 0.939 trainign loss: 6.3210 avg training loss: 7.3570
batch: [16890/21305] batch time: 0.056 trainign loss: 6.6943 avg training loss: 7.3569
batch: [16900/21305] batch time: 0.354 trainign loss: 4.7526 avg training loss: 7.3568
batch: [16910/21305] batch time: 0.060 trainign loss: 0.0102 avg training loss: 7.3563
batch: [16920/21305] batch time: 1.004 trainign loss: 8.2136 avg training loss: 7.3564
batch: [16930/21305] batch time: 0.056 trainign loss: 4.5160 avg training loss: 7.3563
batch: [16940/21305] batch time: 1.607 trainign loss: 6.7892 avg training loss: 7.3563
batch: [16950/21305] batch time: 0.060 trainign loss: 7.5443 avg training loss: 7.3562
batch: [16960/21305] batch time: 1.766 trainign loss: 4.7127 avg training loss: 7.3562
batch: [16970/21305] batch time: 0.056 trainign loss: 5.9290 avg training loss: 7.3561
batch: [16980/21305] batch time: 1.585 trainign loss: 3.3525 avg training loss: 7.3560
batch: [16990/21305] batch time: 0.056 trainign loss: 3.1636 avg training loss: 7.3558
batch: [17000/21305] batch time: 2.334 trainign loss: 1.9890 avg training loss: 7.3556
batch: [17010/21305] batch time: 0.052 trainign loss: 7.2433 avg training loss: 7.3556
batch: [17020/21305] batch time: 2.290 trainign loss: 0.6123 avg training loss: 7.3554
batch: [17030/21305] batch time: 0.056 trainign loss: 6.8061 avg training loss: 7.3552
batch: [17040/21305] batch time: 2.089 trainign loss: 7.3618 avg training loss: 7.3552
batch: [17050/21305] batch time: 0.054 trainign loss: 6.5751 avg training loss: 7.3552
batch: [17060/21305] batch time: 2.097 trainign loss: 6.6899 avg training loss: 7.3550
batch: [17070/21305] batch time: 0.058 trainign loss: 7.2309 avg training loss: 7.3549
batch: [17080/21305] batch time: 2.227 trainign loss: 7.6725 avg training loss: 7.3549
batch: [17090/21305] batch time: 0.056 trainign loss: 6.4145 avg training loss: 7.3549
batch: [17100/21305] batch time: 2.299 trainign loss: 2.2685 avg training loss: 7.3547
batch: [17110/21305] batch time: 0.062 trainign loss: 7.5930 avg training loss: 7.3547
batch: [17120/21305] batch time: 1.913 trainign loss: 7.2098 avg training loss: 7.3547
batch: [17130/21305] batch time: 0.056 trainign loss: 5.8346 avg training loss: 7.3546
batch: [17140/21305] batch time: 2.181 trainign loss: 5.9005 avg training loss: 7.3545
batch: [17150/21305] batch time: 0.056 trainign loss: 6.3327 avg training loss: 7.3545
batch: [17160/21305] batch time: 2.363 trainign loss: 6.5488 avg training loss: 7.3544
batch: [17170/21305] batch time: 0.060 trainign loss: 5.0519 avg training loss: 7.3542
batch: [17180/21305] batch time: 2.269 trainign loss: 7.3761 avg training loss: 7.3542
batch: [17190/21305] batch time: 0.062 trainign loss: 5.6686 avg training loss: 7.3541
batch: [17200/21305] batch time: 2.284 trainign loss: 6.1904 avg training loss: 7.3540
batch: [17210/21305] batch time: 0.054 trainign loss: 6.4322 avg training loss: 7.3539
batch: [17220/21305] batch time: 2.398 trainign loss: 6.7113 avg training loss: 7.3539
batch: [17230/21305] batch time: 0.061 trainign loss: 4.5568 avg training loss: 7.3538
batch: [17240/21305] batch time: 2.288 trainign loss: 6.6512 avg training loss: 7.3536
batch: [17250/21305] batch time: 0.057 trainign loss: 8.1372 avg training loss: 7.3535
batch: [17260/21305] batch time: 1.012 trainign loss: 7.3608 avg training loss: 7.3535
batch: [17270/21305] batch time: 0.062 trainign loss: 6.4148 avg training loss: 7.3535
batch: [17280/21305] batch time: 1.841 trainign loss: 6.9649 avg training loss: 7.3534
batch: [17290/21305] batch time: 0.056 trainign loss: 4.7809 avg training loss: 7.3534
batch: [17300/21305] batch time: 0.750 trainign loss: 6.2117 avg training loss: 7.3533
batch: [17310/21305] batch time: 0.058 trainign loss: 3.6255 avg training loss: 7.3531
batch: [17320/21305] batch time: 0.276 trainign loss: 10.9429 avg training loss: 7.3528
batch: [17330/21305] batch time: 0.057 trainign loss: 8.1239 avg training loss: 7.3528
batch: [17340/21305] batch time: 0.058 trainign loss: 7.1450 avg training loss: 7.3528
batch: [17350/21305] batch time: 0.063 trainign loss: 7.7870 avg training loss: 7.3526
batch: [17360/21305] batch time: 0.055 trainign loss: 6.3037 avg training loss: 7.3525
batch: [17370/21305] batch time: 0.056 trainign loss: 6.0445 avg training loss: 7.3525
batch: [17380/21305] batch time: 0.057 trainign loss: 7.0250 avg training loss: 7.3524
batch: [17390/21305] batch time: 0.062 trainign loss: 7.2658 avg training loss: 7.3524
batch: [17400/21305] batch time: 0.052 trainign loss: 6.7801 avg training loss: 7.3524
batch: [17410/21305] batch time: 0.053 trainign loss: 6.7704 avg training loss: 7.3523
batch: [17420/21305] batch time: 0.053 trainign loss: 6.0301 avg training loss: 7.3523
batch: [17430/21305] batch time: 0.056 trainign loss: 5.5942 avg training loss: 7.3522
batch: [17440/21305] batch time: 0.058 trainign loss: 6.5393 avg training loss: 7.3522
batch: [17450/21305] batch time: 0.060 trainign loss: 6.4037 avg training loss: 7.3521
batch: [17460/21305] batch time: 0.061 trainign loss: 5.2284 avg training loss: 7.3520
batch: [17470/21305] batch time: 0.061 trainign loss: 6.6939 avg training loss: 7.3519
batch: [17480/21305] batch time: 0.063 trainign loss: 5.8609 avg training loss: 7.3518
batch: [17490/21305] batch time: 0.052 trainign loss: 6.7594 avg training loss: 7.3517
batch: [17500/21305] batch time: 0.060 trainign loss: 6.2057 avg training loss: 7.3517
batch: [17510/21305] batch time: 0.057 trainign loss: 6.0678 avg training loss: 7.3516
batch: [17520/21305] batch time: 0.057 trainign loss: 5.4283 avg training loss: 7.3515
batch: [17530/21305] batch time: 0.056 trainign loss: 5.3355 avg training loss: 7.3514
batch: [17540/21305] batch time: 0.056 trainign loss: 6.2200 avg training loss: 7.3513
batch: [17550/21305] batch time: 0.056 trainign loss: 6.3534 avg training loss: 7.3513
batch: [17560/21305] batch time: 0.056 trainign loss: 5.7579 avg training loss: 7.3512
batch: [17570/21305] batch time: 0.056 trainign loss: 6.8560 avg training loss: 7.3511
batch: [17580/21305] batch time: 0.051 trainign loss: 1.4409 avg training loss: 7.3508
batch: [17590/21305] batch time: 0.058 trainign loss: 11.4849 avg training loss: 7.3504
batch: [17600/21305] batch time: 0.056 trainign loss: 8.0988 avg training loss: 7.3504
batch: [17610/21305] batch time: 0.057 trainign loss: 8.3296 avg training loss: 7.3504
batch: [17620/21305] batch time: 0.052 trainign loss: 6.1070 avg training loss: 7.3504
batch: [17630/21305] batch time: 0.057 trainign loss: 6.9286 avg training loss: 7.3504
batch: [17640/21305] batch time: 0.056 trainign loss: 5.2324 avg training loss: 7.3503
batch: [17650/21305] batch time: 0.290 trainign loss: 5.9980 avg training loss: 7.3503
batch: [17660/21305] batch time: 0.056 trainign loss: 6.1283 avg training loss: 7.3502
batch: [17670/21305] batch time: 0.559 trainign loss: 5.9049 avg training loss: 7.3501
batch: [17680/21305] batch time: 0.056 trainign loss: 6.6033 avg training loss: 7.3500
batch: [17690/21305] batch time: 0.365 trainign loss: 5.6899 avg training loss: 7.3500
batch: [17700/21305] batch time: 0.055 trainign loss: 6.2413 avg training loss: 7.3499
batch: [17710/21305] batch time: 0.056 trainign loss: 6.9309 avg training loss: 7.3498
batch: [17720/21305] batch time: 0.063 trainign loss: 6.5463 avg training loss: 7.3498
batch: [17730/21305] batch time: 0.057 trainign loss: 6.4620 avg training loss: 7.3497
batch: [17740/21305] batch time: 0.271 trainign loss: 5.6190 avg training loss: 7.3496
batch: [17750/21305] batch time: 0.056 trainign loss: 6.3673 avg training loss: 7.3496
batch: [17760/21305] batch time: 0.055 trainign loss: 6.3771 avg training loss: 7.3496
batch: [17770/21305] batch time: 0.057 trainign loss: 6.6957 avg training loss: 7.3495
batch: [17780/21305] batch time: 0.062 trainign loss: 5.9191 avg training loss: 7.3494
batch: [17790/21305] batch time: 0.057 trainign loss: 7.0477 avg training loss: 7.3494
batch: [17800/21305] batch time: 0.056 trainign loss: 5.9958 avg training loss: 7.3493
batch: [17810/21305] batch time: 0.061 trainign loss: 6.7950 avg training loss: 7.3492
batch: [17820/21305] batch time: 0.051 trainign loss: 6.4111 avg training loss: 7.3492
batch: [17830/21305] batch time: 0.144 trainign loss: 5.4404 avg training loss: 7.3491
batch: [17840/21305] batch time: 0.059 trainign loss: 7.1743 avg training loss: 7.3491
batch: [17850/21305] batch time: 0.056 trainign loss: 6.8265 avg training loss: 7.3490
batch: [17860/21305] batch time: 0.054 trainign loss: 6.4373 avg training loss: 7.3489
batch: [17870/21305] batch time: 0.063 trainign loss: 5.6990 avg training loss: 7.3489
batch: [17880/21305] batch time: 0.052 trainign loss: 6.6419 avg training loss: 7.3488
batch: [17890/21305] batch time: 0.056 trainign loss: 6.6189 avg training loss: 7.3487
batch: [17900/21305] batch time: 0.057 trainign loss: 6.1214 avg training loss: 7.3486
batch: [17910/21305] batch time: 0.057 trainign loss: 3.2194 avg training loss: 7.3485
batch: [17920/21305] batch time: 0.055 trainign loss: 7.8148 avg training loss: 7.3485
batch: [17930/21305] batch time: 0.209 trainign loss: 5.9432 avg training loss: 7.3484
batch: [17940/21305] batch time: 0.050 trainign loss: 6.1969 avg training loss: 7.3484
batch: [17950/21305] batch time: 0.058 trainign loss: 6.3666 avg training loss: 7.3482
batch: [17960/21305] batch time: 0.056 trainign loss: 6.7972 avg training loss: 7.3481
batch: [17970/21305] batch time: 0.062 trainign loss: 5.9740 avg training loss: 7.3481
batch: [17980/21305] batch time: 0.056 trainign loss: 1.5150 avg training loss: 7.3479
batch: [17990/21305] batch time: 0.280 trainign loss: 5.2083 avg training loss: 7.3478
batch: [18000/21305] batch time: 0.597 trainign loss: 6.8437 avg training loss: 7.3478
batch: [18010/21305] batch time: 0.056 trainign loss: 8.0368 avg training loss: 7.3477
batch: [18020/21305] batch time: 0.065 trainign loss: 4.0251 avg training loss: 7.3476
batch: [18030/21305] batch time: 0.056 trainign loss: 6.7084 avg training loss: 7.3476
batch: [18040/21305] batch time: 0.325 trainign loss: 6.1384 avg training loss: 7.3475
batch: [18050/21305] batch time: 0.056 trainign loss: 6.2764 avg training loss: 7.3474
batch: [18060/21305] batch time: 0.062 trainign loss: 7.2065 avg training loss: 7.3473
batch: [18070/21305] batch time: 0.052 trainign loss: 7.3771 avg training loss: 7.3473
batch: [18080/21305] batch time: 0.052 trainign loss: 6.3441 avg training loss: 7.3473
batch: [18090/21305] batch time: 0.056 trainign loss: 5.3379 avg training loss: 7.3472
batch: [18100/21305] batch time: 0.053 trainign loss: 5.9632 avg training loss: 7.3471
batch: [18110/21305] batch time: 0.239 trainign loss: 5.5006 avg training loss: 7.3470
batch: [18120/21305] batch time: 0.057 trainign loss: 6.7165 avg training loss: 7.3469
batch: [18130/21305] batch time: 0.483 trainign loss: 7.5486 avg training loss: 7.3468
batch: [18140/21305] batch time: 0.056 trainign loss: 7.2254 avg training loss: 7.3468
batch: [18150/21305] batch time: 0.761 trainign loss: 7.0643 avg training loss: 7.3468
batch: [18160/21305] batch time: 0.056 trainign loss: 5.0894 avg training loss: 7.3466
batch: [18170/21305] batch time: 0.929 trainign loss: 5.8842 avg training loss: 7.3466
batch: [18180/21305] batch time: 0.055 trainign loss: 0.0898 avg training loss: 7.3463
batch: [18190/21305] batch time: 1.275 trainign loss: 7.2396 avg training loss: 7.3462
batch: [18200/21305] batch time: 0.056 trainign loss: 7.1364 avg training loss: 7.3462
batch: [18210/21305] batch time: 1.151 trainign loss: 6.6341 avg training loss: 7.3462
batch: [18220/21305] batch time: 0.056 trainign loss: 4.0078 avg training loss: 7.3460
batch: [18230/21305] batch time: 1.362 trainign loss: 7.8857 avg training loss: 7.3460
batch: [18240/21305] batch time: 0.056 trainign loss: 5.9590 avg training loss: 7.3460
batch: [18250/21305] batch time: 1.509 trainign loss: 5.6784 avg training loss: 7.3459
batch: [18260/21305] batch time: 0.063 trainign loss: 2.6678 avg training loss: 7.3457
batch: [18270/21305] batch time: 0.916 trainign loss: 5.5446 avg training loss: 7.3456
batch: [18280/21305] batch time: 0.056 trainign loss: 6.3451 avg training loss: 7.3455
batch: [18290/21305] batch time: 0.799 trainign loss: 7.1790 avg training loss: 7.3455
batch: [18300/21305] batch time: 0.056 trainign loss: 6.1912 avg training loss: 7.3455
batch: [18310/21305] batch time: 0.767 trainign loss: 6.5774 avg training loss: 7.3454
batch: [18320/21305] batch time: 0.060 trainign loss: 5.3421 avg training loss: 7.3453
batch: [18330/21305] batch time: 0.886 trainign loss: 6.2594 avg training loss: 7.3452
batch: [18340/21305] batch time: 0.057 trainign loss: 7.5557 avg training loss: 7.3452
batch: [18350/21305] batch time: 1.338 trainign loss: 7.1467 avg training loss: 7.3452
batch: [18360/21305] batch time: 0.056 trainign loss: 5.6478 avg training loss: 7.3451
batch: [18370/21305] batch time: 0.419 trainign loss: 6.7738 avg training loss: 7.3450
batch: [18380/21305] batch time: 0.056 trainign loss: 6.4254 avg training loss: 7.3449
batch: [18390/21305] batch time: 0.056 trainign loss: 5.8333 avg training loss: 7.3448
batch: [18400/21305] batch time: 0.061 trainign loss: 7.3248 avg training loss: 7.3448
batch: [18410/21305] batch time: 0.059 trainign loss: 5.0649 avg training loss: 7.3447
batch: [18420/21305] batch time: 0.057 trainign loss: 6.2488 avg training loss: 7.3447
batch: [18430/21305] batch time: 0.057 trainign loss: 6.1117 avg training loss: 7.3446
batch: [18440/21305] batch time: 0.056 trainign loss: 0.0786 avg training loss: 7.3443
batch: [18450/21305] batch time: 0.058 trainign loss: 7.6788 avg training loss: 7.3442
batch: [18460/21305] batch time: 0.052 trainign loss: 7.0255 avg training loss: 7.3441
batch: [18470/21305] batch time: 0.061 trainign loss: 7.3211 avg training loss: 7.3441
batch: [18480/21305] batch time: 0.057 trainign loss: 6.2374 avg training loss: 7.3441
batch: [18490/21305] batch time: 0.056 trainign loss: 4.4608 avg training loss: 7.3439
batch: [18500/21305] batch time: 0.056 trainign loss: 0.0063 avg training loss: 7.3435
batch: [18510/21305] batch time: 0.056 trainign loss: 8.4563 avg training loss: 7.3435
batch: [18520/21305] batch time: 0.057 trainign loss: 8.0061 avg training loss: 7.3435
batch: [18530/21305] batch time: 0.056 trainign loss: 6.0125 avg training loss: 7.3435
batch: [18540/21305] batch time: 0.055 trainign loss: 6.1168 avg training loss: 7.3434
batch: [18550/21305] batch time: 0.058 trainign loss: 4.4055 avg training loss: 7.3433
batch: [18560/21305] batch time: 0.063 trainign loss: 7.4678 avg training loss: 7.3432
batch: [18570/21305] batch time: 0.056 trainign loss: 7.1882 avg training loss: 7.3432
batch: [18580/21305] batch time: 0.052 trainign loss: 6.8786 avg training loss: 7.3431
batch: [18590/21305] batch time: 0.057 trainign loss: 6.8315 avg training loss: 7.3431
batch: [18600/21305] batch time: 0.053 trainign loss: 5.4641 avg training loss: 7.3430
batch: [18610/21305] batch time: 0.116 trainign loss: 6.4210 avg training loss: 7.3428
batch: [18620/21305] batch time: 0.056 trainign loss: 7.7396 avg training loss: 7.3428
batch: [18630/21305] batch time: 0.751 trainign loss: 6.2175 avg training loss: 7.3427
batch: [18640/21305] batch time: 0.056 trainign loss: 5.0768 avg training loss: 7.3427
batch: [18650/21305] batch time: 0.457 trainign loss: 3.7052 avg training loss: 7.3425
batch: [18660/21305] batch time: 0.056 trainign loss: 0.0025 avg training loss: 7.3421
batch: [18670/21305] batch time: 0.443 trainign loss: 8.9872 avg training loss: 7.3420
batch: [18680/21305] batch time: 0.062 trainign loss: 7.5552 avg training loss: 7.3421
batch: [18690/21305] batch time: 0.369 trainign loss: 7.6593 avg training loss: 7.3421
batch: [18700/21305] batch time: 0.056 trainign loss: 6.9644 avg training loss: 7.3421
batch: [18710/21305] batch time: 1.169 trainign loss: 6.9888 avg training loss: 7.3420
batch: [18720/21305] batch time: 0.062 trainign loss: 6.3216 avg training loss: 7.3420
batch: [18730/21305] batch time: 0.899 trainign loss: 5.3162 avg training loss: 7.3419
batch: [18740/21305] batch time: 0.057 trainign loss: 7.4729 avg training loss: 7.3418
batch: [18750/21305] batch time: 1.664 trainign loss: 5.7316 avg training loss: 7.3418
batch: [18760/21305] batch time: 0.056 trainign loss: 5.1044 avg training loss: 7.3417
batch: [18770/21305] batch time: 1.426 trainign loss: 7.7695 avg training loss: 7.3416
batch: [18780/21305] batch time: 0.058 trainign loss: 7.2346 avg training loss: 7.3416
batch: [18790/21305] batch time: 1.755 trainign loss: 6.1579 avg training loss: 7.3415
batch: [18800/21305] batch time: 0.061 trainign loss: 7.1459 avg training loss: 7.3415
batch: [18810/21305] batch time: 1.892 trainign loss: 6.8319 avg training loss: 7.3415
batch: [18820/21305] batch time: 0.058 trainign loss: 4.7764 avg training loss: 7.3414
batch: [18830/21305] batch time: 2.234 trainign loss: 7.2416 avg training loss: 7.3413
batch: [18840/21305] batch time: 0.062 trainign loss: 6.3919 avg training loss: 7.3412
batch: [18850/21305] batch time: 2.129 trainign loss: 7.3134 avg training loss: 7.3411
batch: [18860/21305] batch time: 0.057 trainign loss: 6.9193 avg training loss: 7.3411
batch: [18870/21305] batch time: 1.837 trainign loss: 6.5988 avg training loss: 7.3410
batch: [18880/21305] batch time: 0.056 trainign loss: 4.4844 avg training loss: 7.3409
batch: [18890/21305] batch time: 1.776 trainign loss: 7.2933 avg training loss: 7.3409
batch: [18900/21305] batch time: 0.062 trainign loss: 6.0071 avg training loss: 7.3409
batch: [18910/21305] batch time: 1.999 trainign loss: 6.5494 avg training loss: 7.3408
batch: [18920/21305] batch time: 0.056 trainign loss: 6.8818 avg training loss: 7.3407
batch: [18930/21305] batch time: 2.219 trainign loss: 7.2669 avg training loss: 7.3407
batch: [18940/21305] batch time: 0.062 trainign loss: 6.5081 avg training loss: 7.3407
batch: [18950/21305] batch time: 1.896 trainign loss: 6.9446 avg training loss: 7.3406
batch: [18960/21305] batch time: 0.059 trainign loss: 6.8342 avg training loss: 7.3405
batch: [18970/21305] batch time: 2.315 trainign loss: 6.4325 avg training loss: 7.3404
batch: [18980/21305] batch time: 0.062 trainign loss: 8.0786 avg training loss: 7.3402
batch: [18990/21305] batch time: 2.401 trainign loss: 7.5241 avg training loss: 7.3402
batch: [19000/21305] batch time: 0.062 trainign loss: 5.7839 avg training loss: 7.3401
batch: [19010/21305] batch time: 2.089 trainign loss: 7.6058 avg training loss: 7.3401
batch: [19020/21305] batch time: 0.056 trainign loss: 6.6774 avg training loss: 7.3400
batch: [19030/21305] batch time: 2.238 trainign loss: 7.3900 avg training loss: 7.3399
batch: [19040/21305] batch time: 0.057 trainign loss: 7.0545 avg training loss: 7.3399
batch: [19050/21305] batch time: 2.467 trainign loss: 7.2537 avg training loss: 7.3399
batch: [19060/21305] batch time: 0.056 trainign loss: 5.3512 avg training loss: 7.3397
batch: [19070/21305] batch time: 1.360 trainign loss: 6.8963 avg training loss: 7.3396
batch: [19080/21305] batch time: 0.059 trainign loss: 7.0362 avg training loss: 7.3396
batch: [19090/21305] batch time: 1.426 trainign loss: 8.0271 avg training loss: 7.3396
batch: [19100/21305] batch time: 0.063 trainign loss: 7.2990 avg training loss: 7.3396
batch: [19110/21305] batch time: 1.820 trainign loss: 3.9969 avg training loss: 7.3395
batch: [19120/21305] batch time: 0.052 trainign loss: 6.1840 avg training loss: 7.3394
batch: [19130/21305] batch time: 1.786 trainign loss: 7.2896 avg training loss: 7.3394
batch: [19140/21305] batch time: 0.057 trainign loss: 6.2542 avg training loss: 7.3393
batch: [19150/21305] batch time: 1.208 trainign loss: 6.1478 avg training loss: 7.3393
batch: [19160/21305] batch time: 0.056 trainign loss: 6.9512 avg training loss: 7.3392
batch: [19170/21305] batch time: 1.549 trainign loss: 6.5170 avg training loss: 7.3392
batch: [19180/21305] batch time: 0.062 trainign loss: 4.7108 avg training loss: 7.3391
batch: [19190/21305] batch time: 0.868 trainign loss: 6.6594 avg training loss: 7.3390
batch: [19200/21305] batch time: 0.056 trainign loss: 4.0636 avg training loss: 7.3389
batch: [19210/21305] batch time: 0.947 trainign loss: 7.0138 avg training loss: 7.3388
batch: [19220/21305] batch time: 0.062 trainign loss: 6.9712 avg training loss: 7.3387
batch: [19230/21305] batch time: 1.455 trainign loss: 6.1869 avg training loss: 7.3387
batch: [19240/21305] batch time: 0.056 trainign loss: 5.8109 avg training loss: 7.3387
batch: [19250/21305] batch time: 1.034 trainign loss: 3.5620 avg training loss: 7.3385
batch: [19260/21305] batch time: 0.055 trainign loss: 4.6951 avg training loss: 7.3384
batch: [19270/21305] batch time: 0.720 trainign loss: 7.1703 avg training loss: 7.3383
batch: [19280/21305] batch time: 0.059 trainign loss: 8.2191 avg training loss: 7.3383
batch: [19290/21305] batch time: 1.904 trainign loss: 7.0642 avg training loss: 7.3383
batch: [19300/21305] batch time: 0.057 trainign loss: 7.2306 avg training loss: 7.3383
batch: [19310/21305] batch time: 1.148 trainign loss: 7.3392 avg training loss: 7.3382
batch: [19320/21305] batch time: 0.060 trainign loss: 7.2914 avg training loss: 7.3382
batch: [19330/21305] batch time: 0.692 trainign loss: 6.5954 avg training loss: 7.3381
batch: [19340/21305] batch time: 0.057 trainign loss: 5.2651 avg training loss: 7.3381
batch: [19350/21305] batch time: 0.815 trainign loss: 6.7873 avg training loss: 7.3380
batch: [19360/21305] batch time: 0.056 trainign loss: 6.1890 avg training loss: 7.3380
batch: [19370/21305] batch time: 0.340 trainign loss: 6.5417 avg training loss: 7.3379
batch: [19380/21305] batch time: 0.056 trainign loss: 6.4112 avg training loss: 7.3378
batch: [19390/21305] batch time: 0.056 trainign loss: 7.9375 avg training loss: 7.3378
batch: [19400/21305] batch time: 0.052 trainign loss: 7.1220 avg training loss: 7.3378
batch: [19410/21305] batch time: 0.053 trainign loss: 5.3277 avg training loss: 7.3377
batch: [19420/21305] batch time: 0.057 trainign loss: 6.4988 avg training loss: 7.3376
batch: [19430/21305] batch time: 0.058 trainign loss: 6.8614 avg training loss: 7.3376
batch: [19440/21305] batch time: 0.057 trainign loss: 7.2357 avg training loss: 7.3375
batch: [19450/21305] batch time: 0.063 trainign loss: 6.9602 avg training loss: 7.3375
batch: [19460/21305] batch time: 0.059 trainign loss: 6.4076 avg training loss: 7.3375
batch: [19470/21305] batch time: 0.232 trainign loss: 7.0766 avg training loss: 7.3375
batch: [19480/21305] batch time: 0.060 trainign loss: 5.7557 avg training loss: 7.3374
batch: [19490/21305] batch time: 0.374 trainign loss: 5.1184 avg training loss: 7.3373
batch: [19500/21305] batch time: 0.062 trainign loss: 6.7482 avg training loss: 7.3372
batch: [19510/21305] batch time: 0.062 trainign loss: 6.1506 avg training loss: 7.3372
batch: [19520/21305] batch time: 0.053 trainign loss: 6.5691 avg training loss: 7.3372
batch: [19530/21305] batch time: 0.365 trainign loss: 6.8370 avg training loss: 7.3371
batch: [19540/21305] batch time: 0.056 trainign loss: 6.3305 avg training loss: 7.3370
batch: [19550/21305] batch time: 0.295 trainign loss: 6.2791 avg training loss: 7.3369
batch: [19560/21305] batch time: 0.058 trainign loss: 7.1427 avg training loss: 7.3368
batch: [19570/21305] batch time: 0.054 trainign loss: 6.7737 avg training loss: 7.3368
batch: [19580/21305] batch time: 0.056 trainign loss: 5.5598 avg training loss: 7.3367
batch: [19590/21305] batch time: 0.353 trainign loss: 5.6722 avg training loss: 7.3366
batch: [19600/21305] batch time: 0.056 trainign loss: 6.4244 avg training loss: 7.3365
batch: [19610/21305] batch time: 0.056 trainign loss: 6.6758 avg training loss: 7.3365
batch: [19620/21305] batch time: 0.058 trainign loss: 6.7902 avg training loss: 7.3364
batch: [19630/21305] batch time: 0.056 trainign loss: 7.0276 avg training loss: 7.3364
batch: [19640/21305] batch time: 0.063 trainign loss: 7.2537 avg training loss: 7.3363
batch: [19650/21305] batch time: 0.515 trainign loss: 6.5664 avg training loss: 7.3363
batch: [19660/21305] batch time: 0.547 trainign loss: 6.1784 avg training loss: 7.3362
batch: [19670/21305] batch time: 0.127 trainign loss: 6.1510 avg training loss: 7.3361
batch: [19680/21305] batch time: 0.057 trainign loss: 0.0233 avg training loss: 7.3357
batch: [19690/21305] batch time: 0.486 trainign loss: 7.3486 avg training loss: 7.3358
batch: [19700/21305] batch time: 0.061 trainign loss: 7.7373 avg training loss: 7.3358
batch: [19710/21305] batch time: 0.051 trainign loss: 6.5766 avg training loss: 7.3357
batch: [19720/21305] batch time: 0.056 trainign loss: 6.2130 avg training loss: 7.3357
batch: [19730/21305] batch time: 0.060 trainign loss: 5.7895 avg training loss: 7.3356
batch: [19740/21305] batch time: 0.056 trainign loss: 5.9489 avg training loss: 7.3355
batch: [19750/21305] batch time: 0.063 trainign loss: 7.2910 avg training loss: 7.3355
batch: [19760/21305] batch time: 0.058 trainign loss: 6.5530 avg training loss: 7.3355
batch: [19770/21305] batch time: 0.057 trainign loss: 5.9041 avg training loss: 7.3354
batch: [19780/21305] batch time: 0.064 trainign loss: 5.1052 avg training loss: 7.3353
batch: [19790/21305] batch time: 0.062 trainign loss: 6.0644 avg training loss: 7.3352
batch: [19800/21305] batch time: 0.572 trainign loss: 5.9711 avg training loss: 7.3352
batch: [19810/21305] batch time: 0.056 trainign loss: 5.1208 avg training loss: 7.3350
batch: [19820/21305] batch time: 0.753 trainign loss: 6.7696 avg training loss: 7.3349
batch: [19830/21305] batch time: 0.056 trainign loss: 7.2698 avg training loss: 7.3349
batch: [19840/21305] batch time: 1.868 trainign loss: 6.1691 avg training loss: 7.3349
batch: [19850/21305] batch time: 0.054 trainign loss: 6.5308 avg training loss: 7.3348
batch: [19860/21305] batch time: 1.750 trainign loss: 7.4077 avg training loss: 7.3348
batch: [19870/21305] batch time: 0.055 trainign loss: 7.0804 avg training loss: 7.3347
batch: [19880/21305] batch time: 1.911 trainign loss: 5.0994 avg training loss: 7.3346
batch: [19890/21305] batch time: 0.056 trainign loss: 5.0397 avg training loss: 7.3345
batch: [19900/21305] batch time: 2.028 trainign loss: 8.7804 avg training loss: 7.3343
batch: [19910/21305] batch time: 0.056 trainign loss: 1.6196 avg training loss: 7.3340
batch: [19920/21305] batch time: 1.805 trainign loss: 7.0316 avg training loss: 7.3341
batch: [19930/21305] batch time: 0.056 trainign loss: 7.5331 avg training loss: 7.3341
batch: [19940/21305] batch time: 1.883 trainign loss: 6.0537 avg training loss: 7.3340
batch: [19950/21305] batch time: 0.056 trainign loss: 6.3986 avg training loss: 7.3339
batch: [19960/21305] batch time: 1.616 trainign loss: 6.8270 avg training loss: 7.3337
batch: [19970/21305] batch time: 0.056 trainign loss: 7.1361 avg training loss: 7.3337
batch: [19980/21305] batch time: 1.823 trainign loss: 7.1736 avg training loss: 7.3337
batch: [19990/21305] batch time: 0.056 trainign loss: 5.8738 avg training loss: 7.3336
batch: [20000/21305] batch time: 0.646 trainign loss: 6.7401 avg training loss: 7.3335
batch: [20010/21305] batch time: 0.056 trainign loss: 6.1981 avg training loss: 7.3335
batch: [20020/21305] batch time: 0.594 trainign loss: 0.9642 avg training loss: 7.3333
batch: [20030/21305] batch time: 0.062 trainign loss: 6.6906 avg training loss: 7.3331
batch: [20040/21305] batch time: 0.864 trainign loss: 4.9527 avg training loss: 7.3330
batch: [20050/21305] batch time: 0.056 trainign loss: 2.1902 avg training loss: 7.3328
batch: [20060/21305] batch time: 0.901 trainign loss: 4.8093 avg training loss: 7.3327
batch: [20070/21305] batch time: 0.056 trainign loss: 6.6553 avg training loss: 7.3326
batch: [20080/21305] batch time: 0.061 trainign loss: 6.6150 avg training loss: 7.3326
batch: [20090/21305] batch time: 0.057 trainign loss: 4.7551 avg training loss: 7.3325
batch: [20100/21305] batch time: 0.053 trainign loss: 9.7100 avg training loss: 7.3322
batch: [20110/21305] batch time: 0.056 trainign loss: 6.7370 avg training loss: 7.3322
batch: [20120/21305] batch time: 0.054 trainign loss: 7.4922 avg training loss: 7.3323
batch: [20130/21305] batch time: 0.057 trainign loss: 6.8261 avg training loss: 7.3322
batch: [20140/21305] batch time: 0.056 trainign loss: 6.6848 avg training loss: 7.3322
batch: [20150/21305] batch time: 0.057 trainign loss: 5.8915 avg training loss: 7.3321
batch: [20160/21305] batch time: 0.056 trainign loss: 7.1594 avg training loss: 7.3320
batch: [20170/21305] batch time: 0.053 trainign loss: 7.2107 avg training loss: 7.3320
batch: [20180/21305] batch time: 0.063 trainign loss: 6.0713 avg training loss: 7.3320
batch: [20190/21305] batch time: 0.062 trainign loss: 3.3867 avg training loss: 7.3318
batch: [20200/21305] batch time: 0.062 trainign loss: 4.9831 avg training loss: 7.3317
batch: [20210/21305] batch time: 0.059 trainign loss: 7.0850 avg training loss: 7.3316
batch: [20220/21305] batch time: 0.059 trainign loss: 6.9692 avg training loss: 7.3316
batch: [20230/21305] batch time: 0.056 trainign loss: 6.3198 avg training loss: 7.3315
batch: [20240/21305] batch time: 0.056 trainign loss: 5.2660 avg training loss: 7.3314
batch: [20250/21305] batch time: 0.051 trainign loss: 6.8354 avg training loss: 7.3313
batch: [20260/21305] batch time: 0.058 trainign loss: 6.7649 avg training loss: 7.3312
batch: [20270/21305] batch time: 0.056 trainign loss: 1.0990 avg training loss: 7.3310
batch: [20280/21305] batch time: 0.056 trainign loss: 7.0007 avg training loss: 7.3309
batch: [20290/21305] batch time: 0.056 trainign loss: 6.9652 avg training loss: 7.3309
batch: [20300/21305] batch time: 0.056 trainign loss: 6.6839 avg training loss: 7.3309
batch: [20310/21305] batch time: 0.056 trainign loss: 6.4246 avg training loss: 7.3308
batch: [20320/21305] batch time: 0.058 trainign loss: 6.5078 avg training loss: 7.3308
batch: [20330/21305] batch time: 0.056 trainign loss: 4.5011 avg training loss: 7.3306
batch: [20340/21305] batch time: 0.062 trainign loss: 6.9978 avg training loss: 7.3305
batch: [20350/21305] batch time: 0.050 trainign loss: 6.9799 avg training loss: 7.3305
batch: [20360/21305] batch time: 0.055 trainign loss: 6.9135 avg training loss: 7.3304
batch: [20370/21305] batch time: 0.057 trainign loss: 6.5372 avg training loss: 7.3303
batch: [20380/21305] batch time: 0.051 trainign loss: 5.9142 avg training loss: 7.3303
batch: [20390/21305] batch time: 0.051 trainign loss: 7.2305 avg training loss: 7.3302
batch: [20400/21305] batch time: 0.060 trainign loss: 6.7870 avg training loss: 7.3301
batch: [20410/21305] batch time: 0.053 trainign loss: 6.4986 avg training loss: 7.3301
batch: [20420/21305] batch time: 0.058 trainign loss: 6.1113 avg training loss: 7.3300
batch: [20430/21305] batch time: 0.056 trainign loss: 6.8457 avg training loss: 7.3298
batch: [20440/21305] batch time: 0.056 trainign loss: 6.2599 avg training loss: 7.3298
batch: [20450/21305] batch time: 0.056 trainign loss: 5.2441 avg training loss: 7.3297
batch: [20460/21305] batch time: 0.056 trainign loss: 7.0464 avg training loss: 7.3297
batch: [20470/21305] batch time: 0.056 trainign loss: 4.8812 avg training loss: 7.3296
batch: [20480/21305] batch time: 0.056 trainign loss: 0.0878 avg training loss: 7.3292
batch: [20490/21305] batch time: 0.054 trainign loss: 13.3746 avg training loss: 7.3289
batch: [20500/21305] batch time: 0.060 trainign loss: 8.1724 avg training loss: 7.3290
batch: [20510/21305] batch time: 0.063 trainign loss: 7.3631 avg training loss: 7.3291
batch: [20520/21305] batch time: 0.058 trainign loss: 6.4255 avg training loss: 7.3290
batch: [20530/21305] batch time: 0.051 trainign loss: 6.0608 avg training loss: 7.3289
batch: [20540/21305] batch time: 0.056 trainign loss: 8.3522 avg training loss: 7.3287
batch: [20550/21305] batch time: 0.056 trainign loss: 6.6750 avg training loss: 7.3287
batch: [20560/21305] batch time: 0.056 trainign loss: 6.1153 avg training loss: 7.3286
batch: [20570/21305] batch time: 0.051 trainign loss: 7.6727 avg training loss: 7.3286
batch: [20580/21305] batch time: 0.060 trainign loss: 6.7585 avg training loss: 7.3285
batch: [20590/21305] batch time: 0.056 trainign loss: 6.1595 avg training loss: 7.3285
batch: [20600/21305] batch time: 0.057 trainign loss: 6.6209 avg training loss: 7.3284
batch: [20610/21305] batch time: 0.055 trainign loss: 7.2447 avg training loss: 7.3284
batch: [20620/21305] batch time: 0.059 trainign loss: 6.9517 avg training loss: 7.3284
batch: [20630/21305] batch time: 0.056 trainign loss: 6.7021 avg training loss: 7.3283
batch: [20640/21305] batch time: 0.056 trainign loss: 6.7711 avg training loss: 7.3283
batch: [20650/21305] batch time: 0.052 trainign loss: 6.9937 avg training loss: 7.3282
batch: [20660/21305] batch time: 0.063 trainign loss: 5.8456 avg training loss: 7.3282
batch: [20670/21305] batch time: 0.053 trainign loss: 7.5363 avg training loss: 7.3281
batch: [20680/21305] batch time: 0.058 trainign loss: 5.5120 avg training loss: 7.3281
batch: [20690/21305] batch time: 0.060 trainign loss: 6.5392 avg training loss: 7.3280
batch: [20700/21305] batch time: 0.057 trainign loss: 7.1241 avg training loss: 7.3279
batch: [20710/21305] batch time: 0.057 trainign loss: 6.8183 avg training loss: 7.3279
batch: [20720/21305] batch time: 0.056 trainign loss: 6.9223 avg training loss: 7.3279
batch: [20730/21305] batch time: 0.056 trainign loss: 6.4862 avg training loss: 7.3278
batch: [20740/21305] batch time: 0.056 trainign loss: 6.4397 avg training loss: 7.3278
batch: [20750/21305] batch time: 0.053 trainign loss: 5.5790 avg training loss: 7.3277
batch: [20760/21305] batch time: 0.057 trainign loss: 7.2859 avg training loss: 7.3277
batch: [20770/21305] batch time: 0.056 trainign loss: 6.5589 avg training loss: 7.3276
batch: [20780/21305] batch time: 0.061 trainign loss: 6.8494 avg training loss: 7.3276
batch: [20790/21305] batch time: 0.053 trainign loss: 5.7788 avg training loss: 7.3275
batch: [20800/21305] batch time: 0.056 trainign loss: 6.1955 avg training loss: 7.3274
batch: [20810/21305] batch time: 0.052 trainign loss: 7.4012 avg training loss: 7.3274
batch: [20820/21305] batch time: 0.056 trainign loss: 6.3213 avg training loss: 7.3274
batch: [20830/21305] batch time: 0.055 trainign loss: 6.3962 avg training loss: 7.3274
batch: [20840/21305] batch time: 0.056 trainign loss: 5.4596 avg training loss: 7.3273
batch: [20850/21305] batch time: 0.059 trainign loss: 5.5890 avg training loss: 7.3271
batch: [20860/21305] batch time: 0.184 trainign loss: 6.2227 avg training loss: 7.3270
batch: [20870/21305] batch time: 0.054 trainign loss: 6.1862 avg training loss: 7.3270
batch: [20880/21305] batch time: 0.056 trainign loss: 5.8025 avg training loss: 7.3269
batch: [20890/21305] batch time: 0.055 trainign loss: 7.0845 avg training loss: 7.3268
batch: [20900/21305] batch time: 0.308 trainign loss: 4.3892 avg training loss: 7.3267
batch: [20910/21305] batch time: 0.060 trainign loss: 5.8865 avg training loss: 7.3266
batch: [20920/21305] batch time: 0.263 trainign loss: 4.9416 avg training loss: 7.3265
batch: [20930/21305] batch time: 0.062 trainign loss: 7.3268 avg training loss: 7.3263
batch: [20940/21305] batch time: 0.063 trainign loss: 2.3350 avg training loss: 7.3261
batch: [20950/21305] batch time: 0.057 trainign loss: 7.3138 avg training loss: 7.3261
batch: [20960/21305] batch time: 0.218 trainign loss: 7.1539 avg training loss: 7.3262
batch: [20970/21305] batch time: 0.057 trainign loss: 5.7864 avg training loss: 7.3261
batch: [20980/21305] batch time: 0.061 trainign loss: 7.2194 avg training loss: 7.3261
batch: [20990/21305] batch time: 0.056 trainign loss: 7.2711 avg training loss: 7.3261
batch: [21000/21305] batch time: 0.061 trainign loss: 6.2631 avg training loss: 7.3260
batch: [21010/21305] batch time: 0.052 trainign loss: 6.6999 avg training loss: 7.3260
batch: [21020/21305] batch time: 0.062 trainign loss: 6.2412 avg training loss: 7.3259
batch: [21030/21305] batch time: 0.063 trainign loss: 7.0460 avg training loss: 7.3259
batch: [21040/21305] batch time: 0.056 trainign loss: 7.1781 avg training loss: 7.3258
batch: [21050/21305] batch time: 0.059 trainign loss: 6.5056 avg training loss: 7.3257
batch: [21060/21305] batch time: 0.063 trainign loss: 6.0803 avg training loss: 7.3257
batch: [21070/21305] batch time: 0.054 trainign loss: 5.9171 avg training loss: 7.3256
batch: [21080/21305] batch time: 0.063 trainign loss: 6.1155 avg training loss: 7.3254
batch: [21090/21305] batch time: 0.056 trainign loss: 7.1039 avg training loss: 7.3253
batch: [21100/21305] batch time: 0.053 trainign loss: 4.6999 avg training loss: 7.3252
batch: [21110/21305] batch time: 0.056 trainign loss: 7.8233 avg training loss: 7.3252
batch: [21120/21305] batch time: 0.062 trainign loss: 7.5626 avg training loss: 7.3251
batch: [21130/21305] batch time: 0.639 trainign loss: 7.2781 avg training loss: 7.3251
batch: [21140/21305] batch time: 0.057 trainign loss: 6.8337 avg training loss: 7.3250
batch: [21150/21305] batch time: 1.644 trainign loss: 6.8173 avg training loss: 7.3250
batch: [21160/21305] batch time: 0.059 trainign loss: 6.4778 avg training loss: 7.3249
batch: [21170/21305] batch time: 2.137 trainign loss: 4.5899 avg training loss: 7.3247
batch: [21180/21305] batch time: 0.061 trainign loss: 7.4679 avg training loss: 7.3247
batch: [21190/21305] batch time: 2.488 trainign loss: 6.9564 avg training loss: 7.3247
batch: [21200/21305] batch time: 0.062 trainign loss: 6.3955 avg training loss: 7.3246
batch: [21210/21305] batch time: 2.326 trainign loss: 6.4667 avg training loss: 7.3246
batch: [21220/21305] batch time: 0.057 trainign loss: 6.0847 avg training loss: 7.3245
batch: [21230/21305] batch time: 2.025 trainign loss: 6.6197 avg training loss: 7.3245
batch: [21240/21305] batch time: 0.056 trainign loss: 5.1232 avg training loss: 7.3244
batch: [21250/21305] batch time: 2.251 trainign loss: 6.8387 avg training loss: 7.3243
batch: [21260/21305] batch time: 0.059 trainign loss: 7.1601 avg training loss: 7.3243
batch: [21270/21305] batch time: 2.048 trainign loss: 3.9708 avg training loss: 7.3242
batch: [21280/21305] batch time: 0.055 trainign loss: 5.9148 avg training loss: 7.3241
batch: [21290/21305] batch time: 2.006 trainign loss: 4.6277 avg training loss: 7.3239
batch: [21300/21305] batch time: 0.056 trainign loss: 7.1749 avg training loss: 7.3239
Epoch: 8
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.736 trainign loss: 6.6446 avg training loss: 7.3239
batch: [10/21305] batch time: 0.062 trainign loss: 8.6233 avg training loss: 7.3236
batch: [20/21305] batch time: 2.346 trainign loss: 7.7527 avg training loss: 7.3237
batch: [30/21305] batch time: 0.057 trainign loss: 7.0192 avg training loss: 7.3236
batch: [40/21305] batch time: 2.163 trainign loss: 6.9217 avg training loss: 7.3236
batch: [50/21305] batch time: 0.057 trainign loss: 3.9153 avg training loss: 7.3235
batch: [60/21305] batch time: 2.610 trainign loss: 7.0514 avg training loss: 7.3234
batch: [70/21305] batch time: 0.056 trainign loss: 7.4560 avg training loss: 7.3233
batch: [80/21305] batch time: 1.922 trainign loss: 6.7797 avg training loss: 7.3233
batch: [90/21305] batch time: 0.057 trainign loss: 6.9631 avg training loss: 7.3233
batch: [100/21305] batch time: 2.216 trainign loss: 6.9239 avg training loss: 7.3232
batch: [110/21305] batch time: 0.062 trainign loss: 6.2092 avg training loss: 7.3232
batch: [120/21305] batch time: 2.228 trainign loss: 6.4829 avg training loss: 7.3231
batch: [130/21305] batch time: 0.056 trainign loss: 7.3254 avg training loss: 7.3230
batch: [140/21305] batch time: 2.392 trainign loss: 5.2518 avg training loss: 7.3229
batch: [150/21305] batch time: 0.056 trainign loss: 8.1626 avg training loss: 7.3229
batch: [160/21305] batch time: 2.325 trainign loss: 7.0163 avg training loss: 7.3228
batch: [170/21305] batch time: 0.056 trainign loss: 6.4526 avg training loss: 7.3228
batch: [180/21305] batch time: 2.147 trainign loss: 5.9839 avg training loss: 7.3227
batch: [190/21305] batch time: 0.054 trainign loss: 5.5223 avg training loss: 7.3226
batch: [200/21305] batch time: 1.827 trainign loss: 0.0098 avg training loss: 7.3222
batch: [210/21305] batch time: 0.057 trainign loss: 8.5571 avg training loss: 7.3222
batch: [220/21305] batch time: 1.955 trainign loss: 7.4969 avg training loss: 7.3222
batch: [230/21305] batch time: 0.056 trainign loss: 5.8726 avg training loss: 7.3222
batch: [240/21305] batch time: 2.210 trainign loss: 7.2652 avg training loss: 7.3221
batch: [250/21305] batch time: 0.059 trainign loss: 7.5512 avg training loss: 7.3221
batch: [260/21305] batch time: 1.969 trainign loss: 6.6523 avg training loss: 7.3221
batch: [270/21305] batch time: 0.056 trainign loss: 4.5803 avg training loss: 7.3220
batch: [280/21305] batch time: 2.686 trainign loss: 6.2638 avg training loss: 7.3219
batch: [290/21305] batch time: 0.057 trainign loss: 6.8071 avg training loss: 7.3218
batch: [300/21305] batch time: 1.978 trainign loss: 6.9176 avg training loss: 7.3218
batch: [310/21305] batch time: 0.062 trainign loss: 6.0674 avg training loss: 7.3217
batch: [320/21305] batch time: 0.999 trainign loss: 6.7434 avg training loss: 7.3217
batch: [330/21305] batch time: 0.057 trainign loss: 6.4482 avg training loss: 7.3216
batch: [340/21305] batch time: 2.127 trainign loss: 7.4118 avg training loss: 7.3215
batch: [350/21305] batch time: 0.062 trainign loss: 7.6391 avg training loss: 7.3215
batch: [360/21305] batch time: 1.125 trainign loss: 7.1109 avg training loss: 7.3215
batch: [370/21305] batch time: 0.055 trainign loss: 5.9469 avg training loss: 7.3214
batch: [380/21305] batch time: 1.151 trainign loss: 6.3676 avg training loss: 7.3214
batch: [390/21305] batch time: 0.056 trainign loss: 4.2371 avg training loss: 7.3212
batch: [400/21305] batch time: 2.150 trainign loss: 6.4759 avg training loss: 7.3211
batch: [410/21305] batch time: 0.057 trainign loss: 7.3491 avg training loss: 7.3211
batch: [420/21305] batch time: 1.771 trainign loss: 7.1069 avg training loss: 7.3210
batch: [430/21305] batch time: 0.056 trainign loss: 7.1080 avg training loss: 7.3210
batch: [440/21305] batch time: 1.526 trainign loss: 6.2107 avg training loss: 7.3209
batch: [450/21305] batch time: 0.056 trainign loss: 5.4422 avg training loss: 7.3208
batch: [460/21305] batch time: 1.703 trainign loss: 7.0780 avg training loss: 7.3207
batch: [470/21305] batch time: 0.056 trainign loss: 6.6110 avg training loss: 7.3208
batch: [480/21305] batch time: 1.492 trainign loss: 7.2882 avg training loss: 7.3207
batch: [490/21305] batch time: 0.739 trainign loss: 6.7728 avg training loss: 7.3207
batch: [500/21305] batch time: 2.037 trainign loss: 4.3200 avg training loss: 7.3206
batch: [510/21305] batch time: 0.056 trainign loss: 5.9232 avg training loss: 7.3206
batch: [520/21305] batch time: 1.299 trainign loss: 3.7792 avg training loss: 7.3205
batch: [530/21305] batch time: 0.540 trainign loss: 6.4087 avg training loss: 7.3204
batch: [540/21305] batch time: 0.683 trainign loss: 6.4736 avg training loss: 7.3204
batch: [550/21305] batch time: 0.527 trainign loss: 4.5571 avg training loss: 7.3202
batch: [560/21305] batch time: 1.177 trainign loss: 6.0178 avg training loss: 7.3202
batch: [570/21305] batch time: 0.376 trainign loss: 6.0116 avg training loss: 7.3201
batch: [580/21305] batch time: 1.691 trainign loss: 6.4973 avg training loss: 7.3200
batch: [590/21305] batch time: 0.510 trainign loss: 6.7642 avg training loss: 7.3199
batch: [600/21305] batch time: 2.057 trainign loss: 6.1371 avg training loss: 7.3198
batch: [610/21305] batch time: 0.308 trainign loss: 7.4118 avg training loss: 7.3198
batch: [620/21305] batch time: 1.961 trainign loss: 6.7450 avg training loss: 7.3198
batch: [630/21305] batch time: 0.056 trainign loss: 6.2763 avg training loss: 7.3197
batch: [640/21305] batch time: 1.766 trainign loss: 7.0387 avg training loss: 7.3197
batch: [650/21305] batch time: 0.062 trainign loss: 6.0835 avg training loss: 7.3196
batch: [660/21305] batch time: 1.525 trainign loss: 5.6857 avg training loss: 7.3195
batch: [670/21305] batch time: 0.056 trainign loss: 8.0894 avg training loss: 7.3194
batch: [680/21305] batch time: 1.840 trainign loss: 7.9789 avg training loss: 7.3193
batch: [690/21305] batch time: 0.059 trainign loss: 6.3476 avg training loss: 7.3193
batch: [700/21305] batch time: 2.479 trainign loss: 7.6618 avg training loss: 7.3193
batch: [710/21305] batch time: 0.063 trainign loss: 6.3952 avg training loss: 7.3192
batch: [720/21305] batch time: 2.256 trainign loss: 6.6143 avg training loss: 7.3191
batch: [730/21305] batch time: 0.062 trainign loss: 7.4660 avg training loss: 7.3191
batch: [740/21305] batch time: 2.581 trainign loss: 7.5976 avg training loss: 7.3191
batch: [750/21305] batch time: 0.059 trainign loss: 6.7176 avg training loss: 7.3190
batch: [760/21305] batch time: 1.796 trainign loss: 6.2546 avg training loss: 7.3189
batch: [770/21305] batch time: 0.056 trainign loss: 4.8852 avg training loss: 7.3188
batch: [780/21305] batch time: 1.801 trainign loss: 7.6240 avg training loss: 7.3188
batch: [790/21305] batch time: 0.058 trainign loss: 7.0958 avg training loss: 7.3188
batch: [800/21305] batch time: 2.108 trainign loss: 6.8584 avg training loss: 7.3188
batch: [810/21305] batch time: 0.057 trainign loss: 6.4632 avg training loss: 7.3187
batch: [820/21305] batch time: 2.244 trainign loss: 6.8834 avg training loss: 7.3186
batch: [830/21305] batch time: 0.063 trainign loss: 6.1160 avg training loss: 7.3186
batch: [840/21305] batch time: 2.281 trainign loss: 6.8246 avg training loss: 7.3185
batch: [850/21305] batch time: 0.062 trainign loss: 5.8455 avg training loss: 7.3184
batch: [860/21305] batch time: 2.812 trainign loss: 5.4686 avg training loss: 7.3184
batch: [870/21305] batch time: 0.056 trainign loss: 5.6052 avg training loss: 7.3183
batch: [880/21305] batch time: 2.152 trainign loss: 7.3544 avg training loss: 7.3181
batch: [890/21305] batch time: 0.058 trainign loss: 8.3210 avg training loss: 7.3181
batch: [900/21305] batch time: 2.353 trainign loss: 6.2633 avg training loss: 7.3181
batch: [910/21305] batch time: 0.056 trainign loss: 6.7649 avg training loss: 7.3181
batch: [920/21305] batch time: 1.657 trainign loss: 4.7231 avg training loss: 7.3180
batch: [930/21305] batch time: 0.062 trainign loss: 6.7431 avg training loss: 7.3179
batch: [940/21305] batch time: 1.485 trainign loss: 4.4737 avg training loss: 7.3178
batch: [950/21305] batch time: 0.063 trainign loss: 0.0046 avg training loss: 7.3174
batch: [960/21305] batch time: 0.461 trainign loss: 7.9026 avg training loss: 7.3174
batch: [970/21305] batch time: 0.056 trainign loss: 7.6591 avg training loss: 7.3174
batch: [980/21305] batch time: 0.057 trainign loss: 6.9916 avg training loss: 7.3174
batch: [990/21305] batch time: 0.056 trainign loss: 6.7155 avg training loss: 7.3174
batch: [1000/21305] batch time: 0.051 trainign loss: 4.4983 avg training loss: 7.3173
batch: [1010/21305] batch time: 0.057 trainign loss: 7.1698 avg training loss: 7.3172
batch: [1020/21305] batch time: 0.364 trainign loss: 7.9739 avg training loss: 7.3172
batch: [1030/21305] batch time: 0.057 trainign loss: 7.1491 avg training loss: 7.3171
batch: [1040/21305] batch time: 0.976 trainign loss: 7.2245 avg training loss: 7.3171
batch: [1050/21305] batch time: 0.057 trainign loss: 6.6619 avg training loss: 7.3171
batch: [1060/21305] batch time: 0.736 trainign loss: 5.6091 avg training loss: 7.3170
batch: [1070/21305] batch time: 0.056 trainign loss: 4.9429 avg training loss: 7.3169
batch: [1080/21305] batch time: 0.888 trainign loss: 0.0365 avg training loss: 7.3166
batch: [1090/21305] batch time: 0.439 trainign loss: 8.6830 avg training loss: 7.3165
batch: [1100/21305] batch time: 1.387 trainign loss: 6.2005 avg training loss: 7.3164
batch: [1110/21305] batch time: 0.057 trainign loss: 6.8743 avg training loss: 7.3164
batch: [1120/21305] batch time: 0.930 trainign loss: 7.6118 avg training loss: 7.3164
batch: [1130/21305] batch time: 1.200 trainign loss: 6.2957 avg training loss: 7.3164
batch: [1140/21305] batch time: 0.656 trainign loss: 7.2662 avg training loss: 7.3163
batch: [1150/21305] batch time: 1.291 trainign loss: 7.0548 avg training loss: 7.3163
batch: [1160/21305] batch time: 0.748 trainign loss: 7.0020 avg training loss: 7.3162
batch: [1170/21305] batch time: 0.407 trainign loss: 6.7559 avg training loss: 7.3162
batch: [1180/21305] batch time: 0.917 trainign loss: 6.4549 avg training loss: 7.3161
batch: [1190/21305] batch time: 1.435 trainign loss: 7.2386 avg training loss: 7.3161
batch: [1200/21305] batch time: 1.439 trainign loss: 4.9191 avg training loss: 7.3160
batch: [1210/21305] batch time: 0.056 trainign loss: 7.0446 avg training loss: 7.3159
batch: [1220/21305] batch time: 2.192 trainign loss: 7.5124 avg training loss: 7.3159
batch: [1230/21305] batch time: 1.056 trainign loss: 6.2753 avg training loss: 7.3158
batch: [1240/21305] batch time: 1.637 trainign loss: 6.4150 avg training loss: 7.3157
batch: [1250/21305] batch time: 0.700 trainign loss: 6.8962 avg training loss: 7.3157
batch: [1260/21305] batch time: 1.794 trainign loss: 5.8149 avg training loss: 7.3156
batch: [1270/21305] batch time: 0.401 trainign loss: 6.6784 avg training loss: 7.3156
batch: [1280/21305] batch time: 2.253 trainign loss: 7.2380 avg training loss: 7.3155
batch: [1290/21305] batch time: 0.056 trainign loss: 6.6441 avg training loss: 7.3155
batch: [1300/21305] batch time: 1.813 trainign loss: 6.5009 avg training loss: 7.3154
batch: [1310/21305] batch time: 0.086 trainign loss: 7.1235 avg training loss: 7.3154
batch: [1320/21305] batch time: 2.412 trainign loss: 5.7718 avg training loss: 7.3153
batch: [1330/21305] batch time: 0.054 trainign loss: 7.0147 avg training loss: 7.3153
batch: [1340/21305] batch time: 2.321 trainign loss: 7.0088 avg training loss: 7.3153
batch: [1350/21305] batch time: 0.060 trainign loss: 7.0399 avg training loss: 7.3152
batch: [1360/21305] batch time: 1.359 trainign loss: 4.6667 avg training loss: 7.3152
batch: [1370/21305] batch time: 1.138 trainign loss: 7.6319 avg training loss: 7.3150
batch: [1380/21305] batch time: 1.589 trainign loss: 5.9447 avg training loss: 7.3150
batch: [1390/21305] batch time: 1.471 trainign loss: 6.8383 avg training loss: 7.3149
batch: [1400/21305] batch time: 0.991 trainign loss: 6.3485 avg training loss: 7.3149
batch: [1410/21305] batch time: 1.822 trainign loss: 6.6974 avg training loss: 7.3147
batch: [1420/21305] batch time: 0.572 trainign loss: 6.3117 avg training loss: 7.3146
batch: [1430/21305] batch time: 1.343 trainign loss: 6.1438 avg training loss: 7.3146
batch: [1440/21305] batch time: 0.720 trainign loss: 5.3194 avg training loss: 7.3145
batch: [1450/21305] batch time: 1.348 trainign loss: 6.4106 avg training loss: 7.3144
batch: [1460/21305] batch time: 0.477 trainign loss: 6.3186 avg training loss: 7.3144
batch: [1470/21305] batch time: 1.797 trainign loss: 5.6323 avg training loss: 7.3143
batch: [1480/21305] batch time: 0.821 trainign loss: 6.9687 avg training loss: 7.3142
batch: [1490/21305] batch time: 1.444 trainign loss: 7.0566 avg training loss: 7.3141
batch: [1500/21305] batch time: 0.598 trainign loss: 6.5990 avg training loss: 7.3141
batch: [1510/21305] batch time: 1.657 trainign loss: 6.6943 avg training loss: 7.3140
batch: [1520/21305] batch time: 0.610 trainign loss: 2.8488 avg training loss: 7.3139
batch: [1530/21305] batch time: 2.147 trainign loss: 11.1683 avg training loss: 7.3135
batch: [1540/21305] batch time: 0.953 trainign loss: 7.2253 avg training loss: 7.3136
batch: [1550/21305] batch time: 1.305 trainign loss: 5.0087 avg training loss: 7.3135
batch: [1560/21305] batch time: 0.865 trainign loss: 6.3345 avg training loss: 7.3135
batch: [1570/21305] batch time: 1.283 trainign loss: 5.8933 avg training loss: 7.3135
batch: [1580/21305] batch time: 0.905 trainign loss: 5.9253 avg training loss: 7.3133
batch: [1590/21305] batch time: 1.192 trainign loss: 8.1244 avg training loss: 7.3131
batch: [1600/21305] batch time: 0.448 trainign loss: 7.6681 avg training loss: 7.3131
batch: [1610/21305] batch time: 2.353 trainign loss: 6.9677 avg training loss: 7.3131
batch: [1620/21305] batch time: 0.057 trainign loss: 6.1511 avg training loss: 7.3131
batch: [1630/21305] batch time: 2.560 trainign loss: 7.6594 avg training loss: 7.3130
batch: [1640/21305] batch time: 0.056 trainign loss: 6.9812 avg training loss: 7.3129
batch: [1650/21305] batch time: 2.358 trainign loss: 5.8145 avg training loss: 7.3129
batch: [1660/21305] batch time: 0.056 trainign loss: 6.1863 avg training loss: 7.3129
batch: [1670/21305] batch time: 2.329 trainign loss: 6.6281 avg training loss: 7.3128
batch: [1680/21305] batch time: 0.058 trainign loss: 6.1400 avg training loss: 7.3128
batch: [1690/21305] batch time: 2.103 trainign loss: 6.4699 avg training loss: 7.3127
batch: [1700/21305] batch time: 0.306 trainign loss: 6.4543 avg training loss: 7.3127
batch: [1710/21305] batch time: 2.209 trainign loss: 4.4751 avg training loss: 7.3126
batch: [1720/21305] batch time: 0.510 trainign loss: 7.2674 avg training loss: 7.3125
batch: [1730/21305] batch time: 0.907 trainign loss: 6.2339 avg training loss: 7.3124
batch: [1740/21305] batch time: 0.979 trainign loss: 5.8701 avg training loss: 7.3123
batch: [1750/21305] batch time: 0.119 trainign loss: 6.3709 avg training loss: 7.3123
batch: [1760/21305] batch time: 2.127 trainign loss: 6.7969 avg training loss: 7.3122
batch: [1770/21305] batch time: 0.315 trainign loss: 6.0036 avg training loss: 7.3121
batch: [1780/21305] batch time: 1.804 trainign loss: 6.4485 avg training loss: 7.3121
batch: [1790/21305] batch time: 0.955 trainign loss: 6.2584 avg training loss: 7.3120
batch: [1800/21305] batch time: 0.955 trainign loss: 6.2878 avg training loss: 7.3120
batch: [1810/21305] batch time: 1.351 trainign loss: 6.4063 avg training loss: 7.3119
batch: [1820/21305] batch time: 0.845 trainign loss: 4.2299 avg training loss: 7.3118
batch: [1830/21305] batch time: 1.977 trainign loss: 7.5783 avg training loss: 7.3117
batch: [1840/21305] batch time: 0.892 trainign loss: 7.0622 avg training loss: 7.3117
batch: [1850/21305] batch time: 1.544 trainign loss: 6.6008 avg training loss: 7.3117
batch: [1860/21305] batch time: 0.986 trainign loss: 6.0486 avg training loss: 7.3116
batch: [1870/21305] batch time: 1.305 trainign loss: 6.3550 avg training loss: 7.3115
batch: [1880/21305] batch time: 0.879 trainign loss: 4.4675 avg training loss: 7.3114
batch: [1890/21305] batch time: 0.955 trainign loss: 4.5176 avg training loss: 7.3112
batch: [1900/21305] batch time: 1.739 trainign loss: 7.2133 avg training loss: 7.3112
batch: [1910/21305] batch time: 0.623 trainign loss: 7.0918 avg training loss: 7.3112
batch: [1920/21305] batch time: 1.690 trainign loss: 5.9984 avg training loss: 7.3111
batch: [1930/21305] batch time: 0.056 trainign loss: 2.4091 avg training loss: 7.3110
batch: [1940/21305] batch time: 0.870 trainign loss: 6.3177 avg training loss: 7.3109
batch: [1950/21305] batch time: 0.352 trainign loss: 6.2091 avg training loss: 7.3109
batch: [1960/21305] batch time: 0.597 trainign loss: 4.5694 avg training loss: 7.3108
batch: [1970/21305] batch time: 0.580 trainign loss: 6.8450 avg training loss: 7.3107
batch: [1980/21305] batch time: 0.634 trainign loss: 5.0245 avg training loss: 7.3106
batch: [1990/21305] batch time: 0.215 trainign loss: 6.4499 avg training loss: 7.3106
batch: [2000/21305] batch time: 0.624 trainign loss: 5.3732 avg training loss: 7.3105
batch: [2010/21305] batch time: 0.993 trainign loss: 6.7409 avg training loss: 7.3104
batch: [2020/21305] batch time: 1.153 trainign loss: 3.0188 avg training loss: 7.3103
batch: [2030/21305] batch time: 1.290 trainign loss: 5.8358 avg training loss: 7.3102
batch: [2040/21305] batch time: 1.123 trainign loss: 6.5768 avg training loss: 7.3102
batch: [2050/21305] batch time: 1.897 trainign loss: 5.7013 avg training loss: 7.3101
batch: [2060/21305] batch time: 0.131 trainign loss: 2.3863 avg training loss: 7.3100
batch: [2070/21305] batch time: 3.081 trainign loss: 0.9798 avg training loss: 7.3098
batch: [2080/21305] batch time: 0.057 trainign loss: 0.0352 avg training loss: 7.3094
batch: [2090/21305] batch time: 2.365 trainign loss: 7.0771 avg training loss: 7.3092
batch: [2100/21305] batch time: 0.062 trainign loss: 7.5592 avg training loss: 7.3092
batch: [2110/21305] batch time: 2.600 trainign loss: 5.9710 avg training loss: 7.3092
batch: [2120/21305] batch time: 0.054 trainign loss: 0.2672 avg training loss: 7.3089
batch: [2130/21305] batch time: 2.214 trainign loss: 0.0004 avg training loss: 7.3085
batch: [2140/21305] batch time: 0.056 trainign loss: 0.0001 avg training loss: 7.3080
batch: [2150/21305] batch time: 2.380 trainign loss: 7.5547 avg training loss: 7.3082
batch: [2160/21305] batch time: 0.058 trainign loss: 7.5853 avg training loss: 7.3082
batch: [2170/21305] batch time: 2.571 trainign loss: 7.6043 avg training loss: 7.3082
batch: [2180/21305] batch time: 0.062 trainign loss: 7.3998 avg training loss: 7.3081
batch: [2190/21305] batch time: 2.230 trainign loss: 6.4698 avg training loss: 7.3081
batch: [2200/21305] batch time: 0.061 trainign loss: 6.5285 avg training loss: 7.3080
batch: [2210/21305] batch time: 2.400 trainign loss: 5.9013 avg training loss: 7.3080
batch: [2220/21305] batch time: 0.054 trainign loss: 6.9541 avg training loss: 7.3079
batch: [2230/21305] batch time: 2.236 trainign loss: 6.6277 avg training loss: 7.3079
batch: [2240/21305] batch time: 0.056 trainign loss: 6.7690 avg training loss: 7.3078
batch: [2250/21305] batch time: 2.321 trainign loss: 6.1906 avg training loss: 7.3077
batch: [2260/21305] batch time: 0.199 trainign loss: 5.4295 avg training loss: 7.3076
batch: [2270/21305] batch time: 2.191 trainign loss: 3.9772 avg training loss: 7.3074
batch: [2280/21305] batch time: 0.061 trainign loss: 6.3735 avg training loss: 7.3072
batch: [2290/21305] batch time: 2.140 trainign loss: 3.9078 avg training loss: 7.3071
batch: [2300/21305] batch time: 0.180 trainign loss: 6.2622 avg training loss: 7.3071
batch: [2310/21305] batch time: 2.075 trainign loss: 7.2729 avg training loss: 7.3070
batch: [2320/21305] batch time: 0.057 trainign loss: 7.8883 avg training loss: 7.3070
batch: [2330/21305] batch time: 1.714 trainign loss: 7.3682 avg training loss: 7.3070
batch: [2340/21305] batch time: 0.056 trainign loss: 5.0542 avg training loss: 7.3069
batch: [2350/21305] batch time: 1.572 trainign loss: 5.5048 avg training loss: 7.3068
batch: [2360/21305] batch time: 0.052 trainign loss: 7.2481 avg training loss: 7.3068
batch: [2370/21305] batch time: 2.056 trainign loss: 4.9410 avg training loss: 7.3067
batch: [2380/21305] batch time: 0.056 trainign loss: 6.2427 avg training loss: 7.3066
batch: [2390/21305] batch time: 1.964 trainign loss: 7.1729 avg training loss: 7.3066
batch: [2400/21305] batch time: 0.450 trainign loss: 5.9797 avg training loss: 7.3066
batch: [2410/21305] batch time: 2.363 trainign loss: 7.5992 avg training loss: 7.3065
batch: [2420/21305] batch time: 0.057 trainign loss: 6.8719 avg training loss: 7.3065
batch: [2430/21305] batch time: 1.948 trainign loss: 5.6361 avg training loss: 7.3065
batch: [2440/21305] batch time: 0.507 trainign loss: 6.7680 avg training loss: 7.3064
batch: [2450/21305] batch time: 2.088 trainign loss: 6.7472 avg training loss: 7.3063
batch: [2460/21305] batch time: 0.670 trainign loss: 6.3867 avg training loss: 7.3063
batch: [2470/21305] batch time: 2.213 trainign loss: 6.4814 avg training loss: 7.3061
batch: [2480/21305] batch time: 0.303 trainign loss: 6.9896 avg training loss: 7.3061
batch: [2490/21305] batch time: 2.149 trainign loss: 6.9728 avg training loss: 7.3060
batch: [2500/21305] batch time: 0.056 trainign loss: 7.5246 avg training loss: 7.3059
batch: [2510/21305] batch time: 2.234 trainign loss: 6.0369 avg training loss: 7.3059
batch: [2520/21305] batch time: 0.052 trainign loss: 5.4805 avg training loss: 7.3059
batch: [2530/21305] batch time: 1.924 trainign loss: 5.5463 avg training loss: 7.3058
batch: [2540/21305] batch time: 0.056 trainign loss: 6.6401 avg training loss: 7.3057
batch: [2550/21305] batch time: 1.716 trainign loss: 5.6383 avg training loss: 7.3057
batch: [2560/21305] batch time: 0.052 trainign loss: 6.1126 avg training loss: 7.3056
batch: [2570/21305] batch time: 1.636 trainign loss: 7.1249 avg training loss: 7.3056
batch: [2580/21305] batch time: 0.685 trainign loss: 5.4603 avg training loss: 7.3055
batch: [2590/21305] batch time: 2.294 trainign loss: 5.7479 avg training loss: 7.3054
batch: [2600/21305] batch time: 0.063 trainign loss: 0.3373 avg training loss: 7.3052
batch: [2610/21305] batch time: 2.191 trainign loss: 8.3999 avg training loss: 7.3051
batch: [2620/21305] batch time: 0.056 trainign loss: 8.3200 avg training loss: 7.3052
batch: [2630/21305] batch time: 2.358 trainign loss: 7.0422 avg training loss: 7.3052
batch: [2640/21305] batch time: 0.056 trainign loss: 3.4391 avg training loss: 7.3051
batch: [2650/21305] batch time: 2.242 trainign loss: 8.0881 avg training loss: 7.3050
batch: [2660/21305] batch time: 0.062 trainign loss: 7.0431 avg training loss: 7.3050
batch: [2670/21305] batch time: 2.188 trainign loss: 6.3811 avg training loss: 7.3049
batch: [2680/21305] batch time: 0.056 trainign loss: 4.6643 avg training loss: 7.3048
batch: [2690/21305] batch time: 2.479 trainign loss: 7.4093 avg training loss: 7.3047
batch: [2700/21305] batch time: 0.056 trainign loss: 7.0451 avg training loss: 7.3047
batch: [2710/21305] batch time: 2.147 trainign loss: 5.1401 avg training loss: 7.3046
batch: [2720/21305] batch time: 0.057 trainign loss: 2.0207 avg training loss: 7.3044
batch: [2730/21305] batch time: 1.879 trainign loss: 7.7972 avg training loss: 7.3043
batch: [2740/21305] batch time: 0.053 trainign loss: 7.6205 avg training loss: 7.3043
batch: [2750/21305] batch time: 2.174 trainign loss: 6.8179 avg training loss: 7.3043
batch: [2760/21305] batch time: 0.056 trainign loss: 6.5899 avg training loss: 7.3042
batch: [2770/21305] batch time: 2.310 trainign loss: 7.0140 avg training loss: 7.3042
batch: [2780/21305] batch time: 0.063 trainign loss: 7.3261 avg training loss: 7.3041
batch: [2790/21305] batch time: 2.376 trainign loss: 6.6787 avg training loss: 7.3041
batch: [2800/21305] batch time: 0.055 trainign loss: 6.9910 avg training loss: 7.3040
batch: [2810/21305] batch time: 2.235 trainign loss: 6.3527 avg training loss: 7.3040
batch: [2820/21305] batch time: 0.056 trainign loss: 6.9334 avg training loss: 7.3038
batch: [2830/21305] batch time: 2.451 trainign loss: 7.1159 avg training loss: 7.3038
batch: [2840/21305] batch time: 0.056 trainign loss: 1.4704 avg training loss: 7.3036
batch: [2850/21305] batch time: 2.509 trainign loss: 5.0779 avg training loss: 7.3035
batch: [2860/21305] batch time: 0.057 trainign loss: 6.8269 avg training loss: 7.3035
batch: [2870/21305] batch time: 2.250 trainign loss: 7.7220 avg training loss: 7.3033
batch: [2880/21305] batch time: 0.057 trainign loss: 6.5724 avg training loss: 7.3033
batch: [2890/21305] batch time: 2.663 trainign loss: 6.9357 avg training loss: 7.3033
batch: [2900/21305] batch time: 0.056 trainign loss: 6.5875 avg training loss: 7.3032
batch: [2910/21305] batch time: 2.036 trainign loss: 7.1479 avg training loss: 7.3032
batch: [2920/21305] batch time: 0.056 trainign loss: 6.2478 avg training loss: 7.3031
batch: [2930/21305] batch time: 1.940 trainign loss: 5.1732 avg training loss: 7.3031
batch: [2940/21305] batch time: 0.056 trainign loss: 5.1419 avg training loss: 7.3030
batch: [2950/21305] batch time: 2.092 trainign loss: 0.6167 avg training loss: 7.3027
batch: [2960/21305] batch time: 0.062 trainign loss: 6.4510 avg training loss: 7.3027
batch: [2970/21305] batch time: 2.296 trainign loss: 6.4041 avg training loss: 7.3027
batch: [2980/21305] batch time: 0.057 trainign loss: 6.7864 avg training loss: 7.3026
batch: [2990/21305] batch time: 2.607 trainign loss: 6.5201 avg training loss: 7.3026
batch: [3000/21305] batch time: 0.056 trainign loss: 4.9500 avg training loss: 7.3025
batch: [3010/21305] batch time: 1.979 trainign loss: 7.7826 avg training loss: 7.3024
batch: [3020/21305] batch time: 0.062 trainign loss: 5.2454 avg training loss: 7.3024
batch: [3030/21305] batch time: 2.421 trainign loss: 5.8966 avg training loss: 7.3023
batch: [3040/21305] batch time: 0.056 trainign loss: 6.2976 avg training loss: 7.3023
batch: [3050/21305] batch time: 2.213 trainign loss: 7.3385 avg training loss: 7.3022
batch: [3060/21305] batch time: 0.290 trainign loss: 6.4687 avg training loss: 7.3022
batch: [3070/21305] batch time: 1.985 trainign loss: 6.4793 avg training loss: 7.3021
batch: [3080/21305] batch time: 0.056 trainign loss: 6.9490 avg training loss: 7.3020
batch: [3090/21305] batch time: 2.300 trainign loss: 6.2751 avg training loss: 7.3020
batch: [3100/21305] batch time: 0.060 trainign loss: 6.7188 avg training loss: 7.3019
batch: [3110/21305] batch time: 2.288 trainign loss: 7.2277 avg training loss: 7.3019
batch: [3120/21305] batch time: 0.080 trainign loss: 5.6831 avg training loss: 7.3018
batch: [3130/21305] batch time: 2.018 trainign loss: 0.4583 avg training loss: 7.3015
batch: [3140/21305] batch time: 0.054 trainign loss: 7.4780 avg training loss: 7.3015
batch: [3150/21305] batch time: 2.716 trainign loss: 6.6735 avg training loss: 7.3015
batch: [3160/21305] batch time: 0.059 trainign loss: 7.1041 avg training loss: 7.3014
batch: [3170/21305] batch time: 2.610 trainign loss: 6.5917 avg training loss: 7.3014
batch: [3180/21305] batch time: 0.056 trainign loss: 5.9451 avg training loss: 7.3014
batch: [3190/21305] batch time: 2.591 trainign loss: 6.2556 avg training loss: 7.3013
batch: [3200/21305] batch time: 0.059 trainign loss: 6.7714 avg training loss: 7.3013
batch: [3210/21305] batch time: 2.046 trainign loss: 6.9685 avg training loss: 7.3012
batch: [3220/21305] batch time: 0.063 trainign loss: 6.8045 avg training loss: 7.3011
batch: [3230/21305] batch time: 2.130 trainign loss: 6.9642 avg training loss: 7.3011
batch: [3240/21305] batch time: 0.061 trainign loss: 5.7233 avg training loss: 7.3011
batch: [3250/21305] batch time: 1.913 trainign loss: 6.2355 avg training loss: 7.3009
batch: [3260/21305] batch time: 0.057 trainign loss: 7.1299 avg training loss: 7.3009
batch: [3270/21305] batch time: 2.346 trainign loss: 5.5473 avg training loss: 7.3008
batch: [3280/21305] batch time: 0.057 trainign loss: 2.8144 avg training loss: 7.3007
batch: [3290/21305] batch time: 2.238 trainign loss: 5.9842 avg training loss: 7.3005
batch: [3300/21305] batch time: 0.055 trainign loss: 5.4964 avg training loss: 7.3004
batch: [3310/21305] batch time: 2.142 trainign loss: 0.5567 avg training loss: 7.3002
batch: [3320/21305] batch time: 0.056 trainign loss: 7.4685 avg training loss: 7.3001
batch: [3330/21305] batch time: 2.226 trainign loss: 6.7682 avg training loss: 7.3002
batch: [3340/21305] batch time: 0.053 trainign loss: 4.1036 avg training loss: 7.3001
batch: [3350/21305] batch time: 2.138 trainign loss: 5.4784 avg training loss: 7.3000
batch: [3360/21305] batch time: 0.063 trainign loss: 7.0165 avg training loss: 7.2999
batch: [3370/21305] batch time: 2.106 trainign loss: 3.4314 avg training loss: 7.2997
batch: [3380/21305] batch time: 0.062 trainign loss: 8.2733 avg training loss: 7.2997
batch: [3390/21305] batch time: 1.712 trainign loss: 7.2266 avg training loss: 7.2996
batch: [3400/21305] batch time: 0.056 trainign loss: 6.4143 avg training loss: 7.2996
batch: [3410/21305] batch time: 1.625 trainign loss: 5.4368 avg training loss: 7.2995
batch: [3420/21305] batch time: 0.056 trainign loss: 6.6807 avg training loss: 7.2994
batch: [3430/21305] batch time: 1.471 trainign loss: 6.4393 avg training loss: 7.2994
batch: [3440/21305] batch time: 0.056 trainign loss: 7.0124 avg training loss: 7.2993
batch: [3450/21305] batch time: 1.945 trainign loss: 5.7314 avg training loss: 7.2992
batch: [3460/21305] batch time: 0.056 trainign loss: 5.9780 avg training loss: 7.2991
batch: [3470/21305] batch time: 2.074 trainign loss: 7.3299 avg training loss: 7.2991
batch: [3480/21305] batch time: 0.057 trainign loss: 7.2525 avg training loss: 7.2991
batch: [3490/21305] batch time: 1.421 trainign loss: 6.5806 avg training loss: 7.2990
batch: [3500/21305] batch time: 0.056 trainign loss: 6.0056 avg training loss: 7.2990
batch: [3510/21305] batch time: 1.574 trainign loss: 6.4900 avg training loss: 7.2989
batch: [3520/21305] batch time: 0.055 trainign loss: 6.1650 avg training loss: 7.2988
batch: [3530/21305] batch time: 0.600 trainign loss: 6.8994 avg training loss: 7.2988
batch: [3540/21305] batch time: 0.056 trainign loss: 5.2777 avg training loss: 7.2987
batch: [3550/21305] batch time: 1.344 trainign loss: 6.7227 avg training loss: 7.2985
batch: [3560/21305] batch time: 0.057 trainign loss: 6.6639 avg training loss: 7.2984
batch: [3570/21305] batch time: 2.212 trainign loss: 7.6931 avg training loss: 7.2984
batch: [3580/21305] batch time: 0.056 trainign loss: 6.4133 avg training loss: 7.2984
batch: [3590/21305] batch time: 1.705 trainign loss: 4.3480 avg training loss: 7.2983
batch: [3600/21305] batch time: 0.063 trainign loss: 7.4333 avg training loss: 7.2982
batch: [3610/21305] batch time: 2.093 trainign loss: 8.0405 avg training loss: 7.2982
batch: [3620/21305] batch time: 0.063 trainign loss: 7.3101 avg training loss: 7.2982
batch: [3630/21305] batch time: 2.453 trainign loss: 6.8646 avg training loss: 7.2982
batch: [3640/21305] batch time: 0.063 trainign loss: 6.6511 avg training loss: 7.2981
batch: [3650/21305] batch time: 2.000 trainign loss: 5.9670 avg training loss: 7.2980
batch: [3660/21305] batch time: 0.053 trainign loss: 5.8995 avg training loss: 7.2980
batch: [3670/21305] batch time: 1.749 trainign loss: 4.8762 avg training loss: 7.2979
batch: [3680/21305] batch time: 0.621 trainign loss: 7.3710 avg training loss: 7.2979
batch: [3690/21305] batch time: 1.795 trainign loss: 6.3734 avg training loss: 7.2978
batch: [3700/21305] batch time: 0.618 trainign loss: 5.5249 avg training loss: 7.2978
batch: [3710/21305] batch time: 1.988 trainign loss: 5.3978 avg training loss: 7.2977
batch: [3720/21305] batch time: 0.220 trainign loss: 6.0828 avg training loss: 7.2976
batch: [3730/21305] batch time: 2.222 trainign loss: 5.7684 avg training loss: 7.2975
batch: [3740/21305] batch time: 0.508 trainign loss: 7.4557 avg training loss: 7.2975
batch: [3750/21305] batch time: 2.441 trainign loss: 6.8261 avg training loss: 7.2975
batch: [3760/21305] batch time: 0.056 trainign loss: 6.6435 avg training loss: 7.2973
batch: [3770/21305] batch time: 2.297 trainign loss: 4.7085 avg training loss: 7.2973
batch: [3780/21305] batch time: 0.056 trainign loss: 6.5220 avg training loss: 7.2972
batch: [3790/21305] batch time: 2.214 trainign loss: 4.3248 avg training loss: 7.2971
batch: [3800/21305] batch time: 0.054 trainign loss: 6.2925 avg training loss: 7.2971
batch: [3810/21305] batch time: 2.168 trainign loss: 7.2330 avg training loss: 7.2970
batch: [3820/21305] batch time: 0.268 trainign loss: 4.7664 avg training loss: 7.2970
batch: [3830/21305] batch time: 1.864 trainign loss: 6.1780 avg training loss: 7.2969
batch: [3840/21305] batch time: 0.056 trainign loss: 6.2693 avg training loss: 7.2969
batch: [3850/21305] batch time: 1.440 trainign loss: 6.2307 avg training loss: 7.2968
batch: [3860/21305] batch time: 0.058 trainign loss: 6.3508 avg training loss: 7.2967
batch: [3870/21305] batch time: 1.938 trainign loss: 5.1837 avg training loss: 7.2967
batch: [3880/21305] batch time: 0.290 trainign loss: 0.1066 avg training loss: 7.2963
batch: [3890/21305] batch time: 1.839 trainign loss: 8.9592 avg training loss: 7.2962
batch: [3900/21305] batch time: 0.062 trainign loss: 7.7071 avg training loss: 7.2962
batch: [3910/21305] batch time: 1.628 trainign loss: 7.1412 avg training loss: 7.2962
batch: [3920/21305] batch time: 0.839 trainign loss: 5.1303 avg training loss: 7.2962
batch: [3930/21305] batch time: 0.670 trainign loss: 6.4714 avg training loss: 7.2961
batch: [3940/21305] batch time: 0.699 trainign loss: 5.9481 avg training loss: 7.2960
batch: [3950/21305] batch time: 0.263 trainign loss: 6.3762 avg training loss: 7.2960
batch: [3960/21305] batch time: 1.621 trainign loss: 7.5736 avg training loss: 7.2959
batch: [3970/21305] batch time: 0.056 trainign loss: 6.9735 avg training loss: 7.2959
batch: [3980/21305] batch time: 1.603 trainign loss: 5.7167 avg training loss: 7.2959
batch: [3990/21305] batch time: 0.056 trainign loss: 5.1427 avg training loss: 7.2957
batch: [4000/21305] batch time: 1.874 trainign loss: 7.4715 avg training loss: 7.2956
batch: [4010/21305] batch time: 0.053 trainign loss: 7.1187 avg training loss: 7.2956
batch: [4020/21305] batch time: 2.000 trainign loss: 4.6287 avg training loss: 7.2955
batch: [4030/21305] batch time: 0.054 trainign loss: 6.6289 avg training loss: 7.2955
batch: [4040/21305] batch time: 2.555 trainign loss: 6.4363 avg training loss: 7.2954
batch: [4050/21305] batch time: 0.056 trainign loss: 4.4246 avg training loss: 7.2953
batch: [4060/21305] batch time: 2.248 trainign loss: 9.8314 avg training loss: 7.2949
batch: [4070/21305] batch time: 0.062 trainign loss: 8.4562 avg training loss: 7.2949
batch: [4080/21305] batch time: 2.161 trainign loss: 7.7932 avg training loss: 7.2949
batch: [4090/21305] batch time: 0.054 trainign loss: 7.5039 avg training loss: 7.2949
batch: [4100/21305] batch time: 2.543 trainign loss: 6.4017 avg training loss: 7.2949
batch: [4110/21305] batch time: 0.056 trainign loss: 5.4961 avg training loss: 7.2948
batch: [4120/21305] batch time: 2.506 trainign loss: 5.4641 avg training loss: 7.2947
batch: [4130/21305] batch time: 0.063 trainign loss: 6.2212 avg training loss: 7.2947
batch: [4140/21305] batch time: 2.184 trainign loss: 5.3880 avg training loss: 7.2945
batch: [4150/21305] batch time: 0.063 trainign loss: 7.9795 avg training loss: 7.2945
batch: [4160/21305] batch time: 2.053 trainign loss: 7.2510 avg training loss: 7.2944
batch: [4170/21305] batch time: 0.060 trainign loss: 5.8811 avg training loss: 7.2944
batch: [4180/21305] batch time: 2.301 trainign loss: 6.8804 avg training loss: 7.2943
batch: [4190/21305] batch time: 0.060 trainign loss: 7.1829 avg training loss: 7.2942
batch: [4200/21305] batch time: 2.523 trainign loss: 5.9918 avg training loss: 7.2942
batch: [4210/21305] batch time: 0.055 trainign loss: 1.0789 avg training loss: 7.2939
batch: [4220/21305] batch time: 2.289 trainign loss: 6.9404 avg training loss: 7.2939
batch: [4230/21305] batch time: 0.055 trainign loss: 6.8033 avg training loss: 7.2938
batch: [4240/21305] batch time: 2.346 trainign loss: 5.0037 avg training loss: 7.2937
batch: [4250/21305] batch time: 0.055 trainign loss: 0.4814 avg training loss: 7.2934
batch: [4260/21305] batch time: 2.362 trainign loss: 7.5153 avg training loss: 7.2934
batch: [4270/21305] batch time: 0.056 trainign loss: 8.3294 avg training loss: 7.2934
batch: [4280/21305] batch time: 2.307 trainign loss: 7.1754 avg training loss: 7.2934
batch: [4290/21305] batch time: 0.062 trainign loss: 6.6077 avg training loss: 7.2934
batch: [4300/21305] batch time: 1.917 trainign loss: 6.1263 avg training loss: 7.2933
batch: [4310/21305] batch time: 0.063 trainign loss: 6.8519 avg training loss: 7.2932
batch: [4320/21305] batch time: 2.219 trainign loss: 5.7690 avg training loss: 7.2932
batch: [4330/21305] batch time: 0.062 trainign loss: 4.4304 avg training loss: 7.2931
batch: [4340/21305] batch time: 2.520 trainign loss: 5.4550 avg training loss: 7.2930
batch: [4350/21305] batch time: 0.056 trainign loss: 5.9775 avg training loss: 7.2930
batch: [4360/21305] batch time: 2.482 trainign loss: 4.9952 avg training loss: 7.2928
batch: [4370/21305] batch time: 0.063 trainign loss: 7.2625 avg training loss: 7.2927
batch: [4380/21305] batch time: 2.493 trainign loss: 6.5995 avg training loss: 7.2927
batch: [4390/21305] batch time: 0.056 trainign loss: 6.6454 avg training loss: 7.2926
batch: [4400/21305] batch time: 2.417 trainign loss: 7.0839 avg training loss: 7.2926
batch: [4410/21305] batch time: 0.057 trainign loss: 6.6815 avg training loss: 7.2926
batch: [4420/21305] batch time: 2.076 trainign loss: 5.8971 avg training loss: 7.2925
batch: [4430/21305] batch time: 0.063 trainign loss: 1.1346 avg training loss: 7.2923
batch: [4440/21305] batch time: 2.017 trainign loss: 8.6659 avg training loss: 7.2921
batch: [4450/21305] batch time: 0.056 trainign loss: 6.8375 avg training loss: 7.2920
batch: [4460/21305] batch time: 2.066 trainign loss: 3.6987 avg training loss: 7.2919
batch: [4470/21305] batch time: 0.056 trainign loss: 8.5022 avg training loss: 7.2917
batch: [4480/21305] batch time: 2.020 trainign loss: 6.6816 avg training loss: 7.2917
batch: [4490/21305] batch time: 0.051 trainign loss: 3.9503 avg training loss: 7.2916
batch: [4500/21305] batch time: 2.235 trainign loss: 7.9519 avg training loss: 7.2916
batch: [4510/21305] batch time: 0.056 trainign loss: 7.0714 avg training loss: 7.2915
batch: [4520/21305] batch time: 2.031 trainign loss: 7.7417 avg training loss: 7.2915
batch: [4530/21305] batch time: 0.055 trainign loss: 7.3143 avg training loss: 7.2915
batch: [4540/21305] batch time: 2.303 trainign loss: 6.5579 avg training loss: 7.2914
batch: [4550/21305] batch time: 0.056 trainign loss: 6.2296 avg training loss: 7.2913
batch: [4560/21305] batch time: 2.284 trainign loss: 7.1679 avg training loss: 7.2913
batch: [4570/21305] batch time: 0.052 trainign loss: 6.9115 avg training loss: 7.2912
batch: [4580/21305] batch time: 2.013 trainign loss: 6.9761 avg training loss: 7.2912
batch: [4590/21305] batch time: 0.056 trainign loss: 5.3293 avg training loss: 7.2911
batch: [4600/21305] batch time: 0.543 trainign loss: 6.3869 avg training loss: 7.2911
batch: [4610/21305] batch time: 0.056 trainign loss: 5.9829 avg training loss: 7.2910
batch: [4620/21305] batch time: 0.158 trainign loss: 6.2866 avg training loss: 7.2909
batch: [4630/21305] batch time: 0.057 trainign loss: 6.2453 avg training loss: 7.2909
batch: [4640/21305] batch time: 0.056 trainign loss: 5.5787 avg training loss: 7.2908
batch: [4650/21305] batch time: 0.056 trainign loss: 6.2500 avg training loss: 7.2908
batch: [4660/21305] batch time: 0.062 trainign loss: 6.9078 avg training loss: 7.2907
batch: [4670/21305] batch time: 0.055 trainign loss: 6.5369 avg training loss: 7.2906
batch: [4680/21305] batch time: 0.056 trainign loss: 7.6374 avg training loss: 7.2906
batch: [4690/21305] batch time: 0.051 trainign loss: 5.6316 avg training loss: 7.2905
batch: [4700/21305] batch time: 0.062 trainign loss: 6.1029 avg training loss: 7.2904
batch: [4710/21305] batch time: 0.051 trainign loss: 7.5344 avg training loss: 7.2904
batch: [4720/21305] batch time: 0.061 trainign loss: 6.6842 avg training loss: 7.2904
batch: [4730/21305] batch time: 0.051 trainign loss: 6.2366 avg training loss: 7.2903
batch: [4740/21305] batch time: 0.063 trainign loss: 7.4139 avg training loss: 7.2902
batch: [4750/21305] batch time: 0.056 trainign loss: 6.7430 avg training loss: 7.2902
batch: [4760/21305] batch time: 0.062 trainign loss: 4.1406 avg training loss: 7.2900
batch: [4770/21305] batch time: 0.172 trainign loss: 6.2658 avg training loss: 7.2900
batch: [4780/21305] batch time: 0.282 trainign loss: 3.8075 avg training loss: 7.2899
batch: [4790/21305] batch time: 0.062 trainign loss: 5.5828 avg training loss: 7.2897
batch: [4800/21305] batch time: 1.445 trainign loss: 7.2874 avg training loss: 7.2897
batch: [4810/21305] batch time: 0.054 trainign loss: 5.1655 avg training loss: 7.2896
batch: [4820/21305] batch time: 1.361 trainign loss: 6.8942 avg training loss: 7.2896
batch: [4830/21305] batch time: 0.061 trainign loss: 5.5535 avg training loss: 7.2895
batch: [4840/21305] batch time: 2.901 trainign loss: 1.0802 avg training loss: 7.2893
batch: [4850/21305] batch time: 0.061 trainign loss: 10.0667 avg training loss: 7.2891
batch: [4860/21305] batch time: 2.145 trainign loss: 6.7605 avg training loss: 7.2891
batch: [4870/21305] batch time: 0.061 trainign loss: 8.2012 avg training loss: 7.2891
batch: [4880/21305] batch time: 2.427 trainign loss: 7.6294 avg training loss: 7.2891
batch: [4890/21305] batch time: 0.056 trainign loss: 6.5120 avg training loss: 7.2891
batch: [4900/21305] batch time: 2.396 trainign loss: 5.9940 avg training loss: 7.2890
batch: [4910/21305] batch time: 0.055 trainign loss: 4.2677 avg training loss: 7.2888
batch: [4920/21305] batch time: 2.083 trainign loss: 7.7897 avg training loss: 7.2888
batch: [4930/21305] batch time: 0.056 trainign loss: 5.4125 avg training loss: 7.2887
batch: [4940/21305] batch time: 2.084 trainign loss: 7.0771 avg training loss: 7.2886
batch: [4950/21305] batch time: 0.059 trainign loss: 5.4772 avg training loss: 7.2886
batch: [4960/21305] batch time: 2.101 trainign loss: 6.5673 avg training loss: 7.2884
batch: [4970/21305] batch time: 0.058 trainign loss: 6.4008 avg training loss: 7.2884
batch: [4980/21305] batch time: 1.240 trainign loss: 4.4568 avg training loss: 7.2883
batch: [4990/21305] batch time: 0.142 trainign loss: 0.0072 avg training loss: 7.2879
batch: [5000/21305] batch time: 1.354 trainign loss: 6.7356 avg training loss: 7.2878
batch: [5010/21305] batch time: 0.488 trainign loss: 7.5782 avg training loss: 7.2879
batch: [5020/21305] batch time: 0.837 trainign loss: 6.5618 avg training loss: 7.2878
batch: [5030/21305] batch time: 0.949 trainign loss: 6.1602 avg training loss: 7.2877
batch: [5040/21305] batch time: 0.990 trainign loss: 3.7069 avg training loss: 7.2876
batch: [5050/21305] batch time: 1.093 trainign loss: 6.3511 avg training loss: 7.2875
batch: [5060/21305] batch time: 0.581 trainign loss: 7.5678 avg training loss: 7.2874
batch: [5070/21305] batch time: 0.967 trainign loss: 5.1621 avg training loss: 7.2874
batch: [5080/21305] batch time: 0.234 trainign loss: 0.7085 avg training loss: 7.2870
batch: [5090/21305] batch time: 1.633 trainign loss: 6.8647 avg training loss: 7.2871
batch: [5100/21305] batch time: 1.634 trainign loss: 6.9730 avg training loss: 7.2870
batch: [5110/21305] batch time: 1.640 trainign loss: 6.0335 avg training loss: 7.2870
batch: [5120/21305] batch time: 1.523 trainign loss: 6.6284 avg training loss: 7.2870
batch: [5130/21305] batch time: 0.908 trainign loss: 6.6411 avg training loss: 7.2869
batch: [5140/21305] batch time: 0.446 trainign loss: 6.4095 avg training loss: 7.2867
batch: [5150/21305] batch time: 1.549 trainign loss: 7.4838 avg training loss: 7.2867
batch: [5160/21305] batch time: 0.374 trainign loss: 7.4162 avg training loss: 7.2867
batch: [5170/21305] batch time: 2.198 trainign loss: 7.0543 avg training loss: 7.2867
batch: [5180/21305] batch time: 0.468 trainign loss: 6.2307 avg training loss: 7.2866
batch: [5190/21305] batch time: 1.307 trainign loss: 6.3729 avg training loss: 7.2865
batch: [5200/21305] batch time: 1.323 trainign loss: 5.7493 avg training loss: 7.2865
batch: [5210/21305] batch time: 0.989 trainign loss: 5.5438 avg training loss: 7.2864
batch: [5220/21305] batch time: 1.046 trainign loss: 6.1651 avg training loss: 7.2863
batch: [5230/21305] batch time: 1.502 trainign loss: 6.2934 avg training loss: 7.2863
batch: [5240/21305] batch time: 1.077 trainign loss: 4.8572 avg training loss: 7.2862
batch: [5250/21305] batch time: 0.895 trainign loss: 5.3264 avg training loss: 7.2861
batch: [5260/21305] batch time: 1.375 trainign loss: 6.9742 avg training loss: 7.2860
batch: [5270/21305] batch time: 0.891 trainign loss: 6.9168 avg training loss: 7.2860
batch: [5280/21305] batch time: 0.131 trainign loss: 6.8262 avg training loss: 7.2860
batch: [5290/21305] batch time: 0.836 trainign loss: 5.5293 avg training loss: 7.2859
batch: [5300/21305] batch time: 0.057 trainign loss: 6.4555 avg training loss: 7.2858
batch: [5310/21305] batch time: 1.913 trainign loss: 6.6456 avg training loss: 7.2857
batch: [5320/21305] batch time: 0.058 trainign loss: 7.0937 avg training loss: 7.2857
batch: [5330/21305] batch time: 2.181 trainign loss: 6.4010 avg training loss: 7.2857
batch: [5340/21305] batch time: 0.169 trainign loss: 6.2340 avg training loss: 7.2855
batch: [5350/21305] batch time: 1.915 trainign loss: 5.7735 avg training loss: 7.2854
batch: [5360/21305] batch time: 0.062 trainign loss: 7.9292 avg training loss: 7.2854
batch: [5370/21305] batch time: 2.355 trainign loss: 7.4998 avg training loss: 7.2853
batch: [5380/21305] batch time: 0.263 trainign loss: 7.3545 avg training loss: 7.2853
batch: [5390/21305] batch time: 1.817 trainign loss: 5.3384 avg training loss: 7.2853
batch: [5400/21305] batch time: 0.056 trainign loss: 7.0766 avg training loss: 7.2852
batch: [5410/21305] batch time: 2.046 trainign loss: 6.7723 avg training loss: 7.2852
batch: [5420/21305] batch time: 0.057 trainign loss: 7.2775 avg training loss: 7.2851
batch: [5430/21305] batch time: 2.381 trainign loss: 5.9776 avg training loss: 7.2851
batch: [5440/21305] batch time: 0.056 trainign loss: 6.3175 avg training loss: 7.2850
batch: [5450/21305] batch time: 2.155 trainign loss: 5.2255 avg training loss: 7.2848
batch: [5460/21305] batch time: 0.056 trainign loss: 7.2101 avg training loss: 7.2848
batch: [5470/21305] batch time: 2.569 trainign loss: 5.7051 avg training loss: 7.2847
batch: [5480/21305] batch time: 0.062 trainign loss: 7.4364 avg training loss: 7.2847
batch: [5490/21305] batch time: 1.866 trainign loss: 5.9184 avg training loss: 7.2846
batch: [5500/21305] batch time: 0.058 trainign loss: 5.8967 avg training loss: 7.2846
batch: [5510/21305] batch time: 2.034 trainign loss: 2.3100 avg training loss: 7.2844
batch: [5520/21305] batch time: 0.053 trainign loss: 7.5045 avg training loss: 7.2844
batch: [5530/21305] batch time: 2.242 trainign loss: 7.2571 avg training loss: 7.2843
batch: [5540/21305] batch time: 0.061 trainign loss: 7.1084 avg training loss: 7.2843
batch: [5550/21305] batch time: 2.254 trainign loss: 7.7708 avg training loss: 7.2843
batch: [5560/21305] batch time: 0.056 trainign loss: 5.1048 avg training loss: 7.2842
batch: [5570/21305] batch time: 2.316 trainign loss: 2.7431 avg training loss: 7.2841
batch: [5580/21305] batch time: 0.056 trainign loss: 8.4902 avg training loss: 7.2839
batch: [5590/21305] batch time: 1.959 trainign loss: 7.0424 avg training loss: 7.2839
batch: [5600/21305] batch time: 0.056 trainign loss: 6.7126 avg training loss: 7.2839
batch: [5610/21305] batch time: 1.970 trainign loss: 7.3452 avg training loss: 7.2838
batch: [5620/21305] batch time: 0.056 trainign loss: 7.2751 avg training loss: 7.2838
batch: [5630/21305] batch time: 2.516 trainign loss: 6.7111 avg training loss: 7.2838
batch: [5640/21305] batch time: 0.051 trainign loss: 6.0279 avg training loss: 7.2837
batch: [5650/21305] batch time: 2.107 trainign loss: 7.3990 avg training loss: 7.2837
batch: [5660/21305] batch time: 0.056 trainign loss: 4.2898 avg training loss: 7.2836
batch: [5670/21305] batch time: 2.192 trainign loss: 7.0196 avg training loss: 7.2835
batch: [5680/21305] batch time: 0.063 trainign loss: 6.2153 avg training loss: 7.2835
batch: [5690/21305] batch time: 2.225 trainign loss: 6.1497 avg training loss: 7.2835
batch: [5700/21305] batch time: 0.056 trainign loss: 6.3270 avg training loss: 7.2834
batch: [5710/21305] batch time: 2.282 trainign loss: 4.9788 avg training loss: 7.2833
batch: [5720/21305] batch time: 0.063 trainign loss: 6.3785 avg training loss: 7.2831
batch: [5730/21305] batch time: 2.157 trainign loss: 7.5511 avg training loss: 7.2831
batch: [5740/21305] batch time: 0.057 trainign loss: 7.0116 avg training loss: 7.2831
batch: [5750/21305] batch time: 2.178 trainign loss: 6.5595 avg training loss: 7.2831
batch: [5760/21305] batch time: 0.057 trainign loss: 5.8366 avg training loss: 7.2830
batch: [5770/21305] batch time: 2.056 trainign loss: 6.4265 avg training loss: 7.2830
batch: [5780/21305] batch time: 0.063 trainign loss: 6.2549 avg training loss: 7.2829
batch: [5790/21305] batch time: 2.100 trainign loss: 7.3746 avg training loss: 7.2829
batch: [5800/21305] batch time: 0.058 trainign loss: 6.1543 avg training loss: 7.2828
batch: [5810/21305] batch time: 2.310 trainign loss: 7.6293 avg training loss: 7.2828
batch: [5820/21305] batch time: 0.053 trainign loss: 7.3868 avg training loss: 7.2828
batch: [5830/21305] batch time: 2.087 trainign loss: 6.2936 avg training loss: 7.2827
batch: [5840/21305] batch time: 0.055 trainign loss: 6.7609 avg training loss: 7.2827
batch: [5850/21305] batch time: 2.495 trainign loss: 6.3013 avg training loss: 7.2826
batch: [5860/21305] batch time: 0.055 trainign loss: 7.3943 avg training loss: 7.2826
batch: [5870/21305] batch time: 2.384 trainign loss: 6.7390 avg training loss: 7.2826
batch: [5880/21305] batch time: 0.056 trainign loss: 5.9691 avg training loss: 7.2825
batch: [5890/21305] batch time: 2.432 trainign loss: 6.3960 avg training loss: 7.2825
batch: [5900/21305] batch time: 0.062 trainign loss: 4.9932 avg training loss: 7.2824
batch: [5910/21305] batch time: 2.250 trainign loss: 5.5420 avg training loss: 7.2822
batch: [5920/21305] batch time: 0.062 trainign loss: 5.4131 avg training loss: 7.2821
batch: [5930/21305] batch time: 2.517 trainign loss: 6.8838 avg training loss: 7.2821
batch: [5940/21305] batch time: 0.056 trainign loss: 5.7160 avg training loss: 7.2821
batch: [5950/21305] batch time: 2.107 trainign loss: 5.5244 avg training loss: 7.2820
batch: [5960/21305] batch time: 0.060 trainign loss: 4.8428 avg training loss: 7.2819
batch: [5970/21305] batch time: 2.458 trainign loss: 7.8850 avg training loss: 7.2818
batch: [5980/21305] batch time: 0.052 trainign loss: 6.3219 avg training loss: 7.2818
batch: [5990/21305] batch time: 2.167 trainign loss: 7.4006 avg training loss: 7.2818
batch: [6000/21305] batch time: 0.056 trainign loss: 6.9361 avg training loss: 7.2818
batch: [6010/21305] batch time: 1.922 trainign loss: 5.8054 avg training loss: 7.2818
batch: [6020/21305] batch time: 0.056 trainign loss: 6.7472 avg training loss: 7.2817
batch: [6030/21305] batch time: 2.248 trainign loss: 5.5141 avg training loss: 7.2816
batch: [6040/21305] batch time: 0.056 trainign loss: 1.9698 avg training loss: 7.2814
batch: [6050/21305] batch time: 2.177 trainign loss: 6.9052 avg training loss: 7.2813
batch: [6060/21305] batch time: 0.056 trainign loss: 7.6420 avg training loss: 7.2812
batch: [6070/21305] batch time: 1.953 trainign loss: 3.4427 avg training loss: 7.2811
batch: [6080/21305] batch time: 0.055 trainign loss: 4.4618 avg training loss: 7.2810
batch: [6090/21305] batch time: 2.349 trainign loss: 6.8835 avg training loss: 7.2810
batch: [6100/21305] batch time: 0.055 trainign loss: 7.8997 avg training loss: 7.2810
batch: [6110/21305] batch time: 2.616 trainign loss: 5.6388 avg training loss: 7.2809
batch: [6120/21305] batch time: 0.061 trainign loss: 6.2860 avg training loss: 7.2808
batch: [6130/21305] batch time: 2.030 trainign loss: 6.6409 avg training loss: 7.2807
batch: [6140/21305] batch time: 0.057 trainign loss: 5.8369 avg training loss: 7.2806
batch: [6150/21305] batch time: 1.901 trainign loss: 6.3540 avg training loss: 7.2806
batch: [6160/21305] batch time: 0.054 trainign loss: 5.7017 avg training loss: 7.2805
batch: [6170/21305] batch time: 2.549 trainign loss: 5.0365 avg training loss: 7.2805
batch: [6180/21305] batch time: 0.061 trainign loss: 6.8865 avg training loss: 7.2803
batch: [6190/21305] batch time: 2.244 trainign loss: 7.8651 avg training loss: 7.2804
batch: [6200/21305] batch time: 0.060 trainign loss: 6.8046 avg training loss: 7.2804
batch: [6210/21305] batch time: 2.076 trainign loss: 6.4252 avg training loss: 7.2803
batch: [6220/21305] batch time: 0.057 trainign loss: 6.9598 avg training loss: 7.2803
batch: [6230/21305] batch time: 2.403 trainign loss: 6.5508 avg training loss: 7.2802
batch: [6240/21305] batch time: 0.059 trainign loss: 7.4385 avg training loss: 7.2802
batch: [6250/21305] batch time: 2.054 trainign loss: 6.7259 avg training loss: 7.2801
batch: [6260/21305] batch time: 0.745 trainign loss: 7.4370 avg training loss: 7.2800
batch: [6270/21305] batch time: 1.460 trainign loss: 6.5807 avg training loss: 7.2800
batch: [6280/21305] batch time: 0.632 trainign loss: 6.9036 avg training loss: 7.2800
batch: [6290/21305] batch time: 1.462 trainign loss: 4.0044 avg training loss: 7.2799
batch: [6300/21305] batch time: 1.408 trainign loss: 7.4314 avg training loss: 7.2798
batch: [6310/21305] batch time: 0.249 trainign loss: 6.6004 avg training loss: 7.2798
batch: [6320/21305] batch time: 2.116 trainign loss: 7.0484 avg training loss: 7.2797
batch: [6330/21305] batch time: 0.094 trainign loss: 6.5583 avg training loss: 7.2797
batch: [6340/21305] batch time: 1.189 trainign loss: 4.0444 avg training loss: 7.2796
batch: [6350/21305] batch time: 1.171 trainign loss: 6.6522 avg training loss: 7.2795
batch: [6360/21305] batch time: 1.584 trainign loss: 7.1828 avg training loss: 7.2795
batch: [6370/21305] batch time: 1.072 trainign loss: 6.0628 avg training loss: 7.2794
batch: [6380/21305] batch time: 1.263 trainign loss: 6.4929 avg training loss: 7.2793
batch: [6390/21305] batch time: 0.638 trainign loss: 4.5189 avg training loss: 7.2793
batch: [6400/21305] batch time: 2.135 trainign loss: 6.4893 avg training loss: 7.2792
batch: [6410/21305] batch time: 0.790 trainign loss: 7.2340 avg training loss: 7.2791
batch: [6420/21305] batch time: 1.057 trainign loss: 6.3586 avg training loss: 7.2790
batch: [6430/21305] batch time: 1.249 trainign loss: 4.9002 avg training loss: 7.2789
batch: [6440/21305] batch time: 0.979 trainign loss: 6.2873 avg training loss: 7.2787
batch: [6450/21305] batch time: 1.823 trainign loss: 7.2043 avg training loss: 7.2786
batch: [6460/21305] batch time: 0.253 trainign loss: 7.0399 avg training loss: 7.2787
batch: [6470/21305] batch time: 1.952 trainign loss: 7.3013 avg training loss: 7.2786
batch: [6480/21305] batch time: 1.013 trainign loss: 6.2322 avg training loss: 7.2786
batch: [6490/21305] batch time: 0.949 trainign loss: 5.8828 avg training loss: 7.2785
batch: [6500/21305] batch time: 1.576 trainign loss: 5.4729 avg training loss: 7.2784
batch: [6510/21305] batch time: 0.439 trainign loss: 6.6064 avg training loss: 7.2784
batch: [6520/21305] batch time: 1.331 trainign loss: 6.2187 avg training loss: 7.2783
batch: [6530/21305] batch time: 1.491 trainign loss: 5.1068 avg training loss: 7.2782
batch: [6540/21305] batch time: 1.095 trainign loss: 7.0363 avg training loss: 7.2781
batch: [6550/21305] batch time: 1.434 trainign loss: 5.7441 avg training loss: 7.2781
batch: [6560/21305] batch time: 1.488 trainign loss: 6.4371 avg training loss: 7.2780
batch: [6570/21305] batch time: 0.352 trainign loss: 6.0924 avg training loss: 7.2780
batch: [6580/21305] batch time: 1.834 trainign loss: 4.6893 avg training loss: 7.2779
batch: [6590/21305] batch time: 0.056 trainign loss: 6.2835 avg training loss: 7.2779
batch: [6600/21305] batch time: 1.400 trainign loss: 7.2598 avg training loss: 7.2778
batch: [6610/21305] batch time: 0.058 trainign loss: 7.5185 avg training loss: 7.2778
batch: [6620/21305] batch time: 1.627 trainign loss: 6.3591 avg training loss: 7.2778
batch: [6630/21305] batch time: 0.056 trainign loss: 6.2174 avg training loss: 7.2776
batch: [6640/21305] batch time: 2.024 trainign loss: 5.0427 avg training loss: 7.2776
batch: [6650/21305] batch time: 0.063 trainign loss: 6.2689 avg training loss: 7.2775
batch: [6660/21305] batch time: 1.265 trainign loss: 5.8629 avg training loss: 7.2774
batch: [6670/21305] batch time: 0.063 trainign loss: 6.0112 avg training loss: 7.2773
batch: [6680/21305] batch time: 1.192 trainign loss: 7.1591 avg training loss: 7.2773
batch: [6690/21305] batch time: 0.056 trainign loss: 6.7118 avg training loss: 7.2773
batch: [6700/21305] batch time: 1.277 trainign loss: 4.9590 avg training loss: 7.2772
batch: [6710/21305] batch time: 0.251 trainign loss: 5.2912 avg training loss: 7.2771
batch: [6720/21305] batch time: 1.070 trainign loss: 6.7565 avg training loss: 7.2770
batch: [6730/21305] batch time: 0.551 trainign loss: 5.5233 avg training loss: 7.2769
batch: [6740/21305] batch time: 0.671 trainign loss: 7.4894 avg training loss: 7.2769
batch: [6750/21305] batch time: 0.560 trainign loss: 7.4661 avg training loss: 7.2769
batch: [6760/21305] batch time: 0.758 trainign loss: 6.7516 avg training loss: 7.2769
batch: [6770/21305] batch time: 0.643 trainign loss: 5.0308 avg training loss: 7.2768
batch: [6780/21305] batch time: 0.914 trainign loss: 5.7211 avg training loss: 7.2767
batch: [6790/21305] batch time: 0.555 trainign loss: 7.0741 avg training loss: 7.2767
batch: [6800/21305] batch time: 0.835 trainign loss: 5.5977 avg training loss: 7.2766
batch: [6810/21305] batch time: 0.056 trainign loss: 6.4457 avg training loss: 7.2765
batch: [6820/21305] batch time: 0.487 trainign loss: 5.0023 avg training loss: 7.2764
batch: [6830/21305] batch time: 0.527 trainign loss: 6.3934 avg training loss: 7.2764
batch: [6840/21305] batch time: 0.397 trainign loss: 7.6649 avg training loss: 7.2761
batch: [6850/21305] batch time: 0.063 trainign loss: 5.8359 avg training loss: 7.2761
batch: [6860/21305] batch time: 0.326 trainign loss: 6.1124 avg training loss: 7.2761
batch: [6870/21305] batch time: 0.182 trainign loss: 6.1119 avg training loss: 7.2759
batch: [6880/21305] batch time: 0.155 trainign loss: 5.8390 avg training loss: 7.2758
batch: [6890/21305] batch time: 0.904 trainign loss: 7.2050 avg training loss: 7.2758
batch: [6900/21305] batch time: 0.288 trainign loss: 6.2485 avg training loss: 7.2757
batch: [6910/21305] batch time: 0.972 trainign loss: 5.9821 avg training loss: 7.2756
batch: [6920/21305] batch time: 0.117 trainign loss: 6.7492 avg training loss: 7.2756
batch: [6930/21305] batch time: 0.415 trainign loss: 5.4816 avg training loss: 7.2755
batch: [6940/21305] batch time: 0.710 trainign loss: 5.1583 avg training loss: 7.2754
batch: [6950/21305] batch time: 0.703 trainign loss: 6.1969 avg training loss: 7.2753
batch: [6960/21305] batch time: 0.336 trainign loss: 4.4108 avg training loss: 7.2752
batch: [6970/21305] batch time: 0.508 trainign loss: 3.8291 avg training loss: 7.2751
batch: [6980/21305] batch time: 0.478 trainign loss: 6.5213 avg training loss: 7.2750
batch: [6990/21305] batch time: 0.056 trainign loss: 7.2326 avg training loss: 7.2750
batch: [7000/21305] batch time: 1.138 trainign loss: 5.6712 avg training loss: 7.2750
batch: [7010/21305] batch time: 0.062 trainign loss: 6.1132 avg training loss: 7.2749
batch: [7020/21305] batch time: 1.136 trainign loss: 7.4649 avg training loss: 7.2748
batch: [7030/21305] batch time: 0.056 trainign loss: 6.3119 avg training loss: 7.2748
batch: [7040/21305] batch time: 1.174 trainign loss: 7.0762 avg training loss: 7.2748
batch: [7050/21305] batch time: 0.062 trainign loss: 5.8035 avg training loss: 7.2746
batch: [7060/21305] batch time: 0.644 trainign loss: 5.5834 avg training loss: 7.2746
batch: [7070/21305] batch time: 0.056 trainign loss: 6.7151 avg training loss: 7.2745
batch: [7080/21305] batch time: 0.526 trainign loss: 6.1760 avg training loss: 7.2745
batch: [7090/21305] batch time: 0.056 trainign loss: 5.7986 avg training loss: 7.2744
batch: [7100/21305] batch time: 1.150 trainign loss: 6.3161 avg training loss: 7.2743
batch: [7110/21305] batch time: 0.429 trainign loss: 6.0281 avg training loss: 7.2742
batch: [7120/21305] batch time: 1.181 trainign loss: 4.0001 avg training loss: 7.2740
batch: [7130/21305] batch time: 0.336 trainign loss: 6.9538 avg training loss: 7.2740
batch: [7140/21305] batch time: 0.368 trainign loss: 7.3334 avg training loss: 7.2739
batch: [7150/21305] batch time: 0.629 trainign loss: 5.4074 avg training loss: 7.2738
batch: [7160/21305] batch time: 0.058 trainign loss: 7.3068 avg training loss: 7.2737
batch: [7170/21305] batch time: 0.803 trainign loss: 6.2349 avg training loss: 7.2737
batch: [7180/21305] batch time: 0.054 trainign loss: 6.3360 avg training loss: 7.2736
batch: [7190/21305] batch time: 0.244 trainign loss: 4.4642 avg training loss: 7.2735
batch: [7200/21305] batch time: 0.061 trainign loss: 5.2049 avg training loss: 7.2734
batch: [7210/21305] batch time: 0.062 trainign loss: 6.3354 avg training loss: 7.2734
batch: [7220/21305] batch time: 0.055 trainign loss: 5.2789 avg training loss: 7.2733
batch: [7230/21305] batch time: 0.056 trainign loss: 5.1213 avg training loss: 7.2732
batch: [7240/21305] batch time: 0.056 trainign loss: 7.4470 avg training loss: 7.2731
batch: [7250/21305] batch time: 0.056 trainign loss: 5.9232 avg training loss: 7.2731
batch: [7260/21305] batch time: 0.053 trainign loss: 5.3329 avg training loss: 7.2730
batch: [7270/21305] batch time: 0.061 trainign loss: 7.0747 avg training loss: 7.2730
batch: [7280/21305] batch time: 0.052 trainign loss: 6.3907 avg training loss: 7.2730
batch: [7290/21305] batch time: 0.056 trainign loss: 5.0950 avg training loss: 7.2729
batch: [7300/21305] batch time: 0.051 trainign loss: 6.6767 avg training loss: 7.2728
batch: [7310/21305] batch time: 0.056 trainign loss: 7.0734 avg training loss: 7.2728
batch: [7320/21305] batch time: 0.057 trainign loss: 7.1592 avg training loss: 7.2728
batch: [7330/21305] batch time: 0.056 trainign loss: 5.5915 avg training loss: 7.2727
batch: [7340/21305] batch time: 0.056 trainign loss: 7.1937 avg training loss: 7.2726
batch: [7350/21305] batch time: 0.056 trainign loss: 6.9212 avg training loss: 7.2726
batch: [7360/21305] batch time: 0.056 trainign loss: 6.5963 avg training loss: 7.2725
batch: [7370/21305] batch time: 0.056 trainign loss: 2.6355 avg training loss: 7.2724
batch: [7380/21305] batch time: 0.063 trainign loss: 6.7456 avg training loss: 7.2723
batch: [7390/21305] batch time: 0.062 trainign loss: 7.0227 avg training loss: 7.2722
batch: [7400/21305] batch time: 0.059 trainign loss: 4.4985 avg training loss: 7.2722
batch: [7410/21305] batch time: 0.056 trainign loss: 7.2120 avg training loss: 7.2720
batch: [7420/21305] batch time: 0.051 trainign loss: 7.1280 avg training loss: 7.2720
batch: [7430/21305] batch time: 0.056 trainign loss: 6.6422 avg training loss: 7.2720
batch: [7440/21305] batch time: 0.063 trainign loss: 6.3266 avg training loss: 7.2719
batch: [7450/21305] batch time: 0.063 trainign loss: 7.0233 avg training loss: 7.2719
batch: [7460/21305] batch time: 0.052 trainign loss: 3.0932 avg training loss: 7.2717
batch: [7470/21305] batch time: 0.051 trainign loss: 4.0337 avg training loss: 7.2717
batch: [7480/21305] batch time: 0.062 trainign loss: 6.6997 avg training loss: 7.2716
batch: [7490/21305] batch time: 0.856 trainign loss: 6.4323 avg training loss: 7.2716
batch: [7500/21305] batch time: 0.057 trainign loss: 7.2916 avg training loss: 7.2715
batch: [7510/21305] batch time: 0.421 trainign loss: 5.9145 avg training loss: 7.2715
batch: [7520/21305] batch time: 0.056 trainign loss: 7.1151 avg training loss: 7.2714
batch: [7530/21305] batch time: 0.054 trainign loss: 5.2544 avg training loss: 7.2714
batch: [7540/21305] batch time: 0.055 trainign loss: 7.0751 avg training loss: 7.2713
batch: [7550/21305] batch time: 0.055 trainign loss: 7.3962 avg training loss: 7.2713
batch: [7560/21305] batch time: 0.052 trainign loss: 6.6107 avg training loss: 7.2712
batch: [7570/21305] batch time: 0.056 trainign loss: 6.8070 avg training loss: 7.2712
batch: [7580/21305] batch time: 0.053 trainign loss: 6.2858 avg training loss: 7.2711
batch: [7590/21305] batch time: 0.063 trainign loss: 7.3772 avg training loss: 7.2711
batch: [7600/21305] batch time: 0.053 trainign loss: 6.0659 avg training loss: 7.2710
batch: [7610/21305] batch time: 0.063 trainign loss: 6.2034 avg training loss: 7.2710
batch: [7620/21305] batch time: 0.057 trainign loss: 4.4496 avg training loss: 7.2708
batch: [7630/21305] batch time: 0.056 trainign loss: 7.4397 avg training loss: 7.2708
batch: [7640/21305] batch time: 0.052 trainign loss: 6.3049 avg training loss: 7.2707
batch: [7650/21305] batch time: 0.062 trainign loss: 5.3681 avg training loss: 7.2706
batch: [7660/21305] batch time: 0.050 trainign loss: 7.2157 avg training loss: 7.2706
batch: [7670/21305] batch time: 0.056 trainign loss: 7.3080 avg training loss: 7.2705
batch: [7680/21305] batch time: 0.063 trainign loss: 5.7638 avg training loss: 7.2705
batch: [7690/21305] batch time: 0.057 trainign loss: 5.0959 avg training loss: 7.2704
batch: [7700/21305] batch time: 0.053 trainign loss: 7.5123 avg training loss: 7.2703
batch: [7710/21305] batch time: 0.054 trainign loss: 5.5080 avg training loss: 7.2702
batch: [7720/21305] batch time: 0.062 trainign loss: 6.0085 avg training loss: 7.2701
batch: [7730/21305] batch time: 0.059 trainign loss: 4.6207 avg training loss: 7.2699
batch: [7740/21305] batch time: 0.191 trainign loss: 7.2087 avg training loss: 7.2699
batch: [7750/21305] batch time: 0.056 trainign loss: 7.7325 avg training loss: 7.2699
batch: [7760/21305] batch time: 0.062 trainign loss: 6.9581 avg training loss: 7.2699
batch: [7770/21305] batch time: 0.056 trainign loss: 4.6856 avg training loss: 7.2698
batch: [7780/21305] batch time: 0.053 trainign loss: 7.0282 avg training loss: 7.2698
batch: [7790/21305] batch time: 0.053 trainign loss: 7.2592 avg training loss: 7.2698
batch: [7800/21305] batch time: 0.056 trainign loss: 7.1897 avg training loss: 7.2697
batch: [7810/21305] batch time: 0.056 trainign loss: 6.1200 avg training loss: 7.2696
batch: [7820/21305] batch time: 0.055 trainign loss: 6.4912 avg training loss: 7.2696
batch: [7830/21305] batch time: 0.051 trainign loss: 6.1884 avg training loss: 7.2696
batch: [7840/21305] batch time: 0.052 trainign loss: 6.8288 avg training loss: 7.2695
batch: [7850/21305] batch time: 0.053 trainign loss: 5.0515 avg training loss: 7.2694
batch: [7860/21305] batch time: 0.557 trainign loss: 5.5587 avg training loss: 7.2693
batch: [7870/21305] batch time: 0.061 trainign loss: 5.9408 avg training loss: 7.2693
batch: [7880/21305] batch time: 0.228 trainign loss: 4.3463 avg training loss: 7.2692
batch: [7890/21305] batch time: 0.055 trainign loss: 5.8956 avg training loss: 7.2691
batch: [7900/21305] batch time: 0.070 trainign loss: 5.8709 avg training loss: 7.2689
batch: [7910/21305] batch time: 0.056 trainign loss: 7.2454 avg training loss: 7.2689
batch: [7920/21305] batch time: 0.057 trainign loss: 5.5230 avg training loss: 7.2689
batch: [7930/21305] batch time: 0.063 trainign loss: 6.0250 avg training loss: 7.2688
batch: [7940/21305] batch time: 0.055 trainign loss: 6.7379 avg training loss: 7.2688
batch: [7950/21305] batch time: 0.057 trainign loss: 6.7995 avg training loss: 7.2688
batch: [7960/21305] batch time: 0.523 trainign loss: 5.8910 avg training loss: 7.2687
batch: [7970/21305] batch time: 0.056 trainign loss: 6.3061 avg training loss: 7.2687
batch: [7980/21305] batch time: 1.154 trainign loss: 6.9475 avg training loss: 7.2686
batch: [7990/21305] batch time: 0.056 trainign loss: 7.1985 avg training loss: 7.2685
batch: [8000/21305] batch time: 0.574 trainign loss: 6.6207 avg training loss: 7.2685
batch: [8010/21305] batch time: 0.056 trainign loss: 5.3060 avg training loss: 7.2683
batch: [8020/21305] batch time: 1.345 trainign loss: 2.5327 avg training loss: 7.2682
batch: [8030/21305] batch time: 0.062 trainign loss: 4.9613 avg training loss: 7.2680
batch: [8040/21305] batch time: 1.124 trainign loss: 7.0646 avg training loss: 7.2679
batch: [8050/21305] batch time: 0.056 trainign loss: 7.8712 avg training loss: 7.2679
batch: [8060/21305] batch time: 1.396 trainign loss: 7.0510 avg training loss: 7.2679
batch: [8070/21305] batch time: 0.059 trainign loss: 6.0618 avg training loss: 7.2679
batch: [8080/21305] batch time: 2.091 trainign loss: 6.7766 avg training loss: 7.2679
batch: [8090/21305] batch time: 0.057 trainign loss: 7.0564 avg training loss: 7.2678
batch: [8100/21305] batch time: 2.014 trainign loss: 6.2874 avg training loss: 7.2678
batch: [8110/21305] batch time: 0.057 trainign loss: 7.0461 avg training loss: 7.2677
batch: [8120/21305] batch time: 2.265 trainign loss: 6.4026 avg training loss: 7.2676
batch: [8130/21305] batch time: 0.055 trainign loss: 6.0964 avg training loss: 7.2675
batch: [8140/21305] batch time: 1.951 trainign loss: 6.2843 avg training loss: 7.2675
batch: [8150/21305] batch time: 0.062 trainign loss: 6.3697 avg training loss: 7.2674
batch: [8160/21305] batch time: 2.056 trainign loss: 6.6777 avg training loss: 7.2674
batch: [8170/21305] batch time: 0.056 trainign loss: 6.3269 avg training loss: 7.2673
batch: [8180/21305] batch time: 2.298 trainign loss: 6.1402 avg training loss: 7.2672
batch: [8190/21305] batch time: 0.057 trainign loss: 6.7277 avg training loss: 7.2672
batch: [8200/21305] batch time: 2.255 trainign loss: 6.8012 avg training loss: 7.2672
batch: [8210/21305] batch time: 0.056 trainign loss: 6.0910 avg training loss: 7.2672
batch: [8220/21305] batch time: 2.413 trainign loss: 6.0863 avg training loss: 7.2671
batch: [8230/21305] batch time: 0.052 trainign loss: 7.0820 avg training loss: 7.2670
batch: [8240/21305] batch time: 2.400 trainign loss: 7.3075 avg training loss: 7.2670
batch: [8250/21305] batch time: 0.056 trainign loss: 6.3531 avg training loss: 7.2670
batch: [8260/21305] batch time: 2.433 trainign loss: 6.7897 avg training loss: 7.2669
batch: [8270/21305] batch time: 0.057 trainign loss: 7.5526 avg training loss: 7.2668
batch: [8280/21305] batch time: 2.361 trainign loss: 6.4884 avg training loss: 7.2668
batch: [8290/21305] batch time: 0.056 trainign loss: 5.9299 avg training loss: 7.2667
batch: [8300/21305] batch time: 2.010 trainign loss: 5.8063 avg training loss: 7.2667
batch: [8310/21305] batch time: 0.052 trainign loss: 7.0757 avg training loss: 7.2666
batch: [8320/21305] batch time: 2.333 trainign loss: 6.5641 avg training loss: 7.2666
batch: [8330/21305] batch time: 0.062 trainign loss: 6.8986 avg training loss: 7.2665
batch: [8340/21305] batch time: 2.263 trainign loss: 6.0119 avg training loss: 7.2665
batch: [8350/21305] batch time: 0.056 trainign loss: 4.7024 avg training loss: 7.2663
batch: [8360/21305] batch time: 2.235 trainign loss: 5.5172 avg training loss: 7.2663
batch: [8370/21305] batch time: 0.060 trainign loss: 7.1517 avg training loss: 7.2662
batch: [8380/21305] batch time: 2.362 trainign loss: 5.0520 avg training loss: 7.2661
batch: [8390/21305] batch time: 0.063 trainign loss: 4.3178 avg training loss: 7.2659
batch: [8400/21305] batch time: 2.276 trainign loss: 7.7376 avg training loss: 7.2657
batch: [8410/21305] batch time: 0.061 trainign loss: 6.2479 avg training loss: 7.2657
batch: [8420/21305] batch time: 2.298 trainign loss: 6.5961 avg training loss: 7.2655
batch: [8430/21305] batch time: 0.056 trainign loss: 7.7863 avg training loss: 7.2655
batch: [8440/21305] batch time: 2.247 trainign loss: 7.8520 avg training loss: 7.2655
batch: [8450/21305] batch time: 0.053 trainign loss: 7.1634 avg training loss: 7.2654
batch: [8460/21305] batch time: 2.385 trainign loss: 7.1968 avg training loss: 7.2654
batch: [8470/21305] batch time: 0.063 trainign loss: 7.4800 avg training loss: 7.2654
batch: [8480/21305] batch time: 2.252 trainign loss: 6.6509 avg training loss: 7.2653
batch: [8490/21305] batch time: 0.063 trainign loss: 6.5883 avg training loss: 7.2653
batch: [8500/21305] batch time: 2.280 trainign loss: 5.7693 avg training loss: 7.2652
batch: [8510/21305] batch time: 0.055 trainign loss: 5.6913 avg training loss: 7.2651
batch: [8520/21305] batch time: 1.984 trainign loss: 6.8203 avg training loss: 7.2650
batch: [8530/21305] batch time: 0.058 trainign loss: 6.5127 avg training loss: 7.2649
batch: [8540/21305] batch time: 2.089 trainign loss: 6.8750 avg training loss: 7.2649
batch: [8550/21305] batch time: 0.062 trainign loss: 7.7451 avg training loss: 7.2648
batch: [8560/21305] batch time: 2.057 trainign loss: 6.7744 avg training loss: 7.2648
batch: [8570/21305] batch time: 0.352 trainign loss: 5.4307 avg training loss: 7.2647
batch: [8580/21305] batch time: 1.688 trainign loss: 6.4265 avg training loss: 7.2647
batch: [8590/21305] batch time: 0.056 trainign loss: 6.9716 avg training loss: 7.2646
batch: [8600/21305] batch time: 1.746 trainign loss: 6.3078 avg training loss: 7.2645
batch: [8610/21305] batch time: 0.056 trainign loss: 5.6909 avg training loss: 7.2644
batch: [8620/21305] batch time: 1.117 trainign loss: 6.7951 avg training loss: 7.2644
batch: [8630/21305] batch time: 0.061 trainign loss: 5.1766 avg training loss: 7.2643
batch: [8640/21305] batch time: 0.352 trainign loss: 6.4063 avg training loss: 7.2643
batch: [8650/21305] batch time: 0.056 trainign loss: 4.0657 avg training loss: 7.2642
batch: [8660/21305] batch time: 1.573 trainign loss: 4.7390 avg training loss: 7.2640
batch: [8670/21305] batch time: 0.056 trainign loss: 5.5377 avg training loss: 7.2640
batch: [8680/21305] batch time: 1.638 trainign loss: 6.8081 avg training loss: 7.2639
batch: [8690/21305] batch time: 0.056 trainign loss: 5.6506 avg training loss: 7.2638
batch: [8700/21305] batch time: 1.829 trainign loss: 4.8220 avg training loss: 7.2638
batch: [8710/21305] batch time: 0.062 trainign loss: 1.4133 avg training loss: 7.2636
batch: [8720/21305] batch time: 2.098 trainign loss: 0.0010 avg training loss: 7.2631
batch: [8730/21305] batch time: 0.057 trainign loss: 0.0001 avg training loss: 7.2627
batch: [8740/21305] batch time: 2.638 trainign loss: 8.1631 avg training loss: 7.2627
batch: [8750/21305] batch time: 0.060 trainign loss: 7.9543 avg training loss: 7.2627
batch: [8760/21305] batch time: 2.165 trainign loss: 7.3545 avg training loss: 7.2627
batch: [8770/21305] batch time: 0.062 trainign loss: 6.2736 avg training loss: 7.2626
batch: [8780/21305] batch time: 2.057 trainign loss: 5.5106 avg training loss: 7.2625
batch: [8790/21305] batch time: 0.057 trainign loss: 5.9987 avg training loss: 7.2625
batch: [8800/21305] batch time: 2.180 trainign loss: 6.0709 avg training loss: 7.2624
batch: [8810/21305] batch time: 0.057 trainign loss: 7.3274 avg training loss: 7.2624
batch: [8820/21305] batch time: 2.239 trainign loss: 6.2866 avg training loss: 7.2624
batch: [8830/21305] batch time: 0.058 trainign loss: 6.9805 avg training loss: 7.2623
batch: [8840/21305] batch time: 2.256 trainign loss: 6.3629 avg training loss: 7.2623
batch: [8850/21305] batch time: 0.056 trainign loss: 5.9987 avg training loss: 7.2622
batch: [8860/21305] batch time: 1.938 trainign loss: 7.0006 avg training loss: 7.2621
batch: [8870/21305] batch time: 0.054 trainign loss: 6.7885 avg training loss: 7.2621
batch: [8880/21305] batch time: 2.294 trainign loss: 5.1996 avg training loss: 7.2621
batch: [8890/21305] batch time: 0.056 trainign loss: 7.0588 avg training loss: 7.2620
batch: [8900/21305] batch time: 1.634 trainign loss: 5.9602 avg training loss: 7.2619
batch: [8910/21305] batch time: 0.056 trainign loss: 5.4422 avg training loss: 7.2619
batch: [8920/21305] batch time: 1.029 trainign loss: 2.1852 avg training loss: 7.2617
batch: [8930/21305] batch time: 0.056 trainign loss: 7.5673 avg training loss: 7.2617
batch: [8940/21305] batch time: 0.825 trainign loss: 6.6947 avg training loss: 7.2616
batch: [8950/21305] batch time: 0.056 trainign loss: 5.7048 avg training loss: 7.2616
batch: [8960/21305] batch time: 0.995 trainign loss: 6.1577 avg training loss: 7.2615
batch: [8970/21305] batch time: 0.056 trainign loss: 6.1216 avg training loss: 7.2615
batch: [8980/21305] batch time: 0.906 trainign loss: 5.4714 avg training loss: 7.2614
batch: [8990/21305] batch time: 0.063 trainign loss: 6.4832 avg training loss: 7.2613
batch: [9000/21305] batch time: 1.578 trainign loss: 0.8583 avg training loss: 7.2611
batch: [9010/21305] batch time: 0.056 trainign loss: 7.7924 avg training loss: 7.2610
batch: [9020/21305] batch time: 0.854 trainign loss: 5.9711 avg training loss: 7.2610
batch: [9030/21305] batch time: 0.056 trainign loss: 6.2196 avg training loss: 7.2609
batch: [9040/21305] batch time: 0.601 trainign loss: 6.7838 avg training loss: 7.2609
batch: [9050/21305] batch time: 0.056 trainign loss: 6.1263 avg training loss: 7.2608
batch: [9060/21305] batch time: 0.535 trainign loss: 5.3813 avg training loss: 7.2607
batch: [9070/21305] batch time: 0.059 trainign loss: 3.3170 avg training loss: 7.2606
batch: [9080/21305] batch time: 0.051 trainign loss: 8.0238 avg training loss: 7.2606
batch: [9090/21305] batch time: 0.058 trainign loss: 6.3185 avg training loss: 7.2605
batch: [9100/21305] batch time: 0.051 trainign loss: 7.2045 avg training loss: 7.2605
batch: [9110/21305] batch time: 0.056 trainign loss: 5.7311 avg training loss: 7.2605
batch: [9120/21305] batch time: 0.056 trainign loss: 6.9750 avg training loss: 7.2604
batch: [9130/21305] batch time: 0.056 trainign loss: 5.1088 avg training loss: 7.2602
batch: [9140/21305] batch time: 0.052 trainign loss: 6.8232 avg training loss: 7.2601
batch: [9150/21305] batch time: 0.058 trainign loss: 6.7061 avg training loss: 7.2601
batch: [9160/21305] batch time: 0.056 trainign loss: 6.8493 avg training loss: 7.2600
batch: [9170/21305] batch time: 0.058 trainign loss: 5.9258 avg training loss: 7.2599
batch: [9180/21305] batch time: 0.058 trainign loss: 7.3552 avg training loss: 7.2599
batch: [9190/21305] batch time: 0.063 trainign loss: 6.7358 avg training loss: 7.2599
batch: [9200/21305] batch time: 0.051 trainign loss: 6.8706 avg training loss: 7.2598
batch: [9210/21305] batch time: 0.056 trainign loss: 5.8522 avg training loss: 7.2597
batch: [9220/21305] batch time: 0.055 trainign loss: 4.8945 avg training loss: 7.2596
batch: [9230/21305] batch time: 0.056 trainign loss: 6.5504 avg training loss: 7.2596
batch: [9240/21305] batch time: 0.056 trainign loss: 4.8866 avg training loss: 7.2596
batch: [9250/21305] batch time: 0.062 trainign loss: 6.9035 avg training loss: 7.2595
batch: [9260/21305] batch time: 0.052 trainign loss: 6.5888 avg training loss: 7.2594
batch: [9270/21305] batch time: 0.056 trainign loss: 6.3878 avg training loss: 7.2594
batch: [9280/21305] batch time: 0.056 trainign loss: 6.3350 avg training loss: 7.2593
batch: [9290/21305] batch time: 0.056 trainign loss: 4.8490 avg training loss: 7.2592
batch: [9300/21305] batch time: 0.056 trainign loss: 7.8620 avg training loss: 7.2591
batch: [9310/21305] batch time: 0.056 trainign loss: 7.2726 avg training loss: 7.2591
batch: [9320/21305] batch time: 0.056 trainign loss: 4.8599 avg training loss: 7.2590
batch: [9330/21305] batch time: 0.056 trainign loss: 4.9615 avg training loss: 7.2589
batch: [9340/21305] batch time: 0.053 trainign loss: 6.8672 avg training loss: 7.2588
batch: [9350/21305] batch time: 0.058 trainign loss: 7.2032 avg training loss: 7.2588
batch: [9360/21305] batch time: 0.056 trainign loss: 4.8935 avg training loss: 7.2587
batch: [9370/21305] batch time: 0.058 trainign loss: 5.1898 avg training loss: 7.2587
batch: [9380/21305] batch time: 0.052 trainign loss: 6.0513 avg training loss: 7.2586
batch: [9390/21305] batch time: 0.058 trainign loss: 6.3633 avg training loss: 7.2586
batch: [9400/21305] batch time: 0.056 trainign loss: 6.6762 avg training loss: 7.2585
batch: [9410/21305] batch time: 0.056 trainign loss: 6.0461 avg training loss: 7.2585
batch: [9420/21305] batch time: 0.053 trainign loss: 3.4397 avg training loss: 7.2584
batch: [9430/21305] batch time: 0.061 trainign loss: 7.2162 avg training loss: 7.2583
batch: [9440/21305] batch time: 0.057 trainign loss: 7.4588 avg training loss: 7.2583
batch: [9450/21305] batch time: 0.056 trainign loss: 5.1897 avg training loss: 7.2581
batch: [9460/21305] batch time: 0.052 trainign loss: 7.4120 avg training loss: 7.2581
batch: [9470/21305] batch time: 0.054 trainign loss: 7.4597 avg training loss: 7.2580
batch: [9480/21305] batch time: 0.056 trainign loss: 5.7723 avg training loss: 7.2580
batch: [9490/21305] batch time: 0.058 trainign loss: 6.6144 avg training loss: 7.2579
batch: [9500/21305] batch time: 0.058 trainign loss: 7.4924 avg training loss: 7.2579
batch: [9510/21305] batch time: 0.063 trainign loss: 5.8184 avg training loss: 7.2577
batch: [9520/21305] batch time: 0.058 trainign loss: 7.1230 avg training loss: 7.2577
batch: [9530/21305] batch time: 0.053 trainign loss: 6.7804 avg training loss: 7.2577
batch: [9540/21305] batch time: 0.056 trainign loss: 5.8978 avg training loss: 7.2576
batch: [9550/21305] batch time: 0.051 trainign loss: 5.1099 avg training loss: 7.2575
batch: [9560/21305] batch time: 0.056 trainign loss: 5.1454 avg training loss: 7.2574
batch: [9570/21305] batch time: 0.052 trainign loss: 6.6120 avg training loss: 7.2574
batch: [9580/21305] batch time: 0.054 trainign loss: 7.9435 avg training loss: 7.2572
batch: [9590/21305] batch time: 0.051 trainign loss: 7.1704 avg training loss: 7.2572
batch: [9600/21305] batch time: 0.062 trainign loss: 6.9281 avg training loss: 7.2572
batch: [9610/21305] batch time: 0.056 trainign loss: 6.1651 avg training loss: 7.2571
batch: [9620/21305] batch time: 0.056 trainign loss: 6.8561 avg training loss: 7.2570
batch: [9630/21305] batch time: 0.054 trainign loss: 7.3053 avg training loss: 7.2570
batch: [9640/21305] batch time: 0.062 trainign loss: 7.1249 avg training loss: 7.2569
batch: [9650/21305] batch time: 0.054 trainign loss: 6.0908 avg training loss: 7.2569
batch: [9660/21305] batch time: 0.056 trainign loss: 3.6027 avg training loss: 7.2568
batch: [9670/21305] batch time: 0.051 trainign loss: 6.6081 avg training loss: 7.2567
batch: [9680/21305] batch time: 0.057 trainign loss: 6.6325 avg training loss: 7.2567
batch: [9690/21305] batch time: 0.054 trainign loss: 6.5970 avg training loss: 7.2566
batch: [9700/21305] batch time: 0.059 trainign loss: 6.3327 avg training loss: 7.2566
batch: [9710/21305] batch time: 0.055 trainign loss: 5.9828 avg training loss: 7.2565
batch: [9720/21305] batch time: 0.056 trainign loss: 6.9602 avg training loss: 7.2565
batch: [9730/21305] batch time: 0.051 trainign loss: 6.7793 avg training loss: 7.2565
batch: [9740/21305] batch time: 0.062 trainign loss: 6.2236 avg training loss: 7.2564
batch: [9750/21305] batch time: 0.063 trainign loss: 6.8447 avg training loss: 7.2563
batch: [9760/21305] batch time: 1.129 trainign loss: 6.2535 avg training loss: 7.2562
batch: [9770/21305] batch time: 0.273 trainign loss: 6.6901 avg training loss: 7.2562
batch: [9780/21305] batch time: 0.974 trainign loss: 7.3426 avg training loss: 7.2561
batch: [9790/21305] batch time: 0.619 trainign loss: 6.2714 avg training loss: 7.2561
batch: [9800/21305] batch time: 1.438 trainign loss: 6.0688 avg training loss: 7.2560
batch: [9810/21305] batch time: 1.397 trainign loss: 7.3381 avg training loss: 7.2559
batch: [9820/21305] batch time: 0.800 trainign loss: 5.5844 avg training loss: 7.2558
batch: [9830/21305] batch time: 1.361 trainign loss: 5.2975 avg training loss: 7.2557
batch: [9840/21305] batch time: 0.905 trainign loss: 4.0969 avg training loss: 7.2556
batch: [9850/21305] batch time: 0.755 trainign loss: 6.7294 avg training loss: 7.2555
batch: [9860/21305] batch time: 1.498 trainign loss: 2.7508 avg training loss: 7.2554
batch: [9870/21305] batch time: 0.956 trainign loss: 5.5087 avg training loss: 7.2554
batch: [9880/21305] batch time: 1.332 trainign loss: 7.3273 avg training loss: 7.2553
batch: [9890/21305] batch time: 0.871 trainign loss: 6.7395 avg training loss: 7.2552
batch: [9900/21305] batch time: 1.793 trainign loss: 5.8336 avg training loss: 7.2552
batch: [9910/21305] batch time: 0.472 trainign loss: 6.3157 avg training loss: 7.2551
batch: [9920/21305] batch time: 1.519 trainign loss: 6.3092 avg training loss: 7.2550
batch: [9930/21305] batch time: 0.711 trainign loss: 5.5392 avg training loss: 7.2550
batch: [9940/21305] batch time: 1.996 trainign loss: 7.1835 avg training loss: 7.2549
batch: [9950/21305] batch time: 0.488 trainign loss: 6.5198 avg training loss: 7.2548
batch: [9960/21305] batch time: 2.224 trainign loss: 7.2826 avg training loss: 7.2548
batch: [9970/21305] batch time: 0.365 trainign loss: 6.5636 avg training loss: 7.2548
batch: [9980/21305] batch time: 2.153 trainign loss: 5.8582 avg training loss: 7.2547
batch: [9990/21305] batch time: 0.365 trainign loss: 6.5457 avg training loss: 7.2546
batch: [10000/21305] batch time: 2.326 trainign loss: 5.7307 avg training loss: 7.2546
batch: [10010/21305] batch time: 0.801 trainign loss: 6.6024 avg training loss: 7.2545
batch: [10020/21305] batch time: 2.121 trainign loss: 6.9281 avg training loss: 7.2545
batch: [10030/21305] batch time: 0.134 trainign loss: 6.0180 avg training loss: 7.2545
batch: [10040/21305] batch time: 2.117 trainign loss: 5.7033 avg training loss: 7.2544
batch: [10050/21305] batch time: 0.851 trainign loss: 5.5852 avg training loss: 7.2543
batch: [10060/21305] batch time: 1.843 trainign loss: 4.7262 avg training loss: 7.2541
batch: [10070/21305] batch time: 0.976 trainign loss: 6.1631 avg training loss: 7.2540
batch: [10080/21305] batch time: 1.255 trainign loss: 5.3129 avg training loss: 7.2540
batch: [10090/21305] batch time: 1.299 trainign loss: 7.9360 avg training loss: 7.2539
batch: [10100/21305] batch time: 0.447 trainign loss: 7.9000 avg training loss: 7.2538
batch: [10110/21305] batch time: 2.088 trainign loss: 6.5029 avg training loss: 7.2538
batch: [10120/21305] batch time: 0.799 trainign loss: 6.7013 avg training loss: 7.2537
batch: [10130/21305] batch time: 1.249 trainign loss: 6.1369 avg training loss: 7.2537
batch: [10140/21305] batch time: 0.062 trainign loss: 7.0743 avg training loss: 7.2537
batch: [10150/21305] batch time: 0.783 trainign loss: 6.4566 avg training loss: 7.2536
batch: [10160/21305] batch time: 0.056 trainign loss: 5.8774 avg training loss: 7.2536
batch: [10170/21305] batch time: 1.968 trainign loss: 6.8407 avg training loss: 7.2535
batch: [10180/21305] batch time: 0.056 trainign loss: 6.4936 avg training loss: 7.2535
batch: [10190/21305] batch time: 2.186 trainign loss: 6.8417 avg training loss: 7.2534
batch: [10200/21305] batch time: 0.057 trainign loss: 5.1447 avg training loss: 7.2533
batch: [10210/21305] batch time: 2.108 trainign loss: 6.5391 avg training loss: 7.2533
batch: [10220/21305] batch time: 0.056 trainign loss: 5.9077 avg training loss: 7.2531
batch: [10230/21305] batch time: 2.165 trainign loss: 5.4176 avg training loss: 7.2531
batch: [10240/21305] batch time: 0.058 trainign loss: 6.8481 avg training loss: 7.2530
batch: [10250/21305] batch time: 2.143 trainign loss: 6.9150 avg training loss: 7.2529
batch: [10260/21305] batch time: 0.056 trainign loss: 7.4612 avg training loss: 7.2528
batch: [10270/21305] batch time: 2.131 trainign loss: 7.0791 avg training loss: 7.2528
batch: [10280/21305] batch time: 0.056 trainign loss: 6.8097 avg training loss: 7.2528
batch: [10290/21305] batch time: 2.339 trainign loss: 6.3239 avg training loss: 7.2527
batch: [10300/21305] batch time: 0.056 trainign loss: 6.2355 avg training loss: 7.2526
batch: [10310/21305] batch time: 2.595 trainign loss: 5.3526 avg training loss: 7.2525
batch: [10320/21305] batch time: 0.056 trainign loss: 2.1741 avg training loss: 7.2524
batch: [10330/21305] batch time: 2.121 trainign loss: 6.9556 avg training loss: 7.2523
batch: [10340/21305] batch time: 0.050 trainign loss: 7.1132 avg training loss: 7.2523
batch: [10350/21305] batch time: 1.796 trainign loss: 7.3575 avg training loss: 7.2523
batch: [10360/21305] batch time: 0.056 trainign loss: 6.5026 avg training loss: 7.2522
batch: [10370/21305] batch time: 1.484 trainign loss: 6.1009 avg training loss: 7.2522
batch: [10380/21305] batch time: 0.056 trainign loss: 6.5970 avg training loss: 7.2521
batch: [10390/21305] batch time: 2.051 trainign loss: 6.3764 avg training loss: 7.2520
batch: [10400/21305] batch time: 0.219 trainign loss: 5.8348 avg training loss: 7.2519
batch: [10410/21305] batch time: 1.526 trainign loss: 6.3623 avg training loss: 7.2518
batch: [10420/21305] batch time: 0.519 trainign loss: 7.6466 avg training loss: 7.2518
batch: [10430/21305] batch time: 2.069 trainign loss: 5.1721 avg training loss: 7.2518
batch: [10440/21305] batch time: 0.057 trainign loss: 6.7496 avg training loss: 7.2516
batch: [10450/21305] batch time: 2.424 trainign loss: 6.4485 avg training loss: 7.2516
batch: [10460/21305] batch time: 0.054 trainign loss: 6.9250 avg training loss: 7.2516
batch: [10470/21305] batch time: 1.903 trainign loss: 5.7171 avg training loss: 7.2515
batch: [10480/21305] batch time: 0.069 trainign loss: 7.3915 avg training loss: 7.2514
batch: [10490/21305] batch time: 1.449 trainign loss: 6.8155 avg training loss: 7.2514
batch: [10500/21305] batch time: 1.142 trainign loss: 5.0709 avg training loss: 7.2513
batch: [10510/21305] batch time: 0.829 trainign loss: 4.4480 avg training loss: 7.2512
batch: [10520/21305] batch time: 2.259 trainign loss: 6.9693 avg training loss: 7.2511
batch: [10530/21305] batch time: 0.339 trainign loss: 6.7535 avg training loss: 7.2511
batch: [10540/21305] batch time: 2.326 trainign loss: 5.5515 avg training loss: 7.2510
batch: [10550/21305] batch time: 0.311 trainign loss: 4.5728 avg training loss: 7.2510
batch: [10560/21305] batch time: 2.197 trainign loss: 9.1021 avg training loss: 7.2507
batch: [10570/21305] batch time: 0.712 trainign loss: 6.8012 avg training loss: 7.2507
batch: [10580/21305] batch time: 1.230 trainign loss: 7.1792 avg training loss: 7.2507
batch: [10590/21305] batch time: 1.450 trainign loss: 6.8566 avg training loss: 7.2506
batch: [10600/21305] batch time: 0.074 trainign loss: 6.4385 avg training loss: 7.2506
batch: [10610/21305] batch time: 2.432 trainign loss: 4.1370 avg training loss: 7.2505
batch: [10620/21305] batch time: 0.057 trainign loss: 6.7841 avg training loss: 7.2504
batch: [10630/21305] batch time: 2.908 trainign loss: 6.2187 avg training loss: 7.2504
batch: [10640/21305] batch time: 0.371 trainign loss: 5.8371 avg training loss: 7.2502
batch: [10650/21305] batch time: 1.747 trainign loss: 7.8907 avg training loss: 7.2502
batch: [10660/21305] batch time: 0.425 trainign loss: 6.9385 avg training loss: 7.2502
batch: [10670/21305] batch time: 1.482 trainign loss: 4.4183 avg training loss: 7.2501
batch: [10680/21305] batch time: 0.916 trainign loss: 6.7559 avg training loss: 7.2500
batch: [10690/21305] batch time: 1.801 trainign loss: 6.3312 avg training loss: 7.2499
batch: [10700/21305] batch time: 0.051 trainign loss: 7.2283 avg training loss: 7.2498
batch: [10710/21305] batch time: 1.953 trainign loss: 6.6301 avg training loss: 7.2497
batch: [10720/21305] batch time: 0.858 trainign loss: 6.8950 avg training loss: 7.2496
batch: [10730/21305] batch time: 1.822 trainign loss: 7.7913 avg training loss: 7.2495
batch: [10740/21305] batch time: 1.093 trainign loss: 7.0651 avg training loss: 7.2495
batch: [10750/21305] batch time: 1.146 trainign loss: 6.3627 avg training loss: 7.2495
batch: [10760/21305] batch time: 0.993 trainign loss: 4.4944 avg training loss: 7.2494
batch: [10770/21305] batch time: 1.334 trainign loss: 6.1616 avg training loss: 7.2493
batch: [10780/21305] batch time: 0.882 trainign loss: 7.3188 avg training loss: 7.2493
batch: [10790/21305] batch time: 1.225 trainign loss: 6.4590 avg training loss: 7.2492
batch: [10800/21305] batch time: 0.753 trainign loss: 5.7225 avg training loss: 7.2492
batch: [10810/21305] batch time: 1.299 trainign loss: 6.7177 avg training loss: 7.2491
batch: [10820/21305] batch time: 0.892 trainign loss: 3.8404 avg training loss: 7.2490
batch: [10830/21305] batch time: 0.497 trainign loss: 1.5592 avg training loss: 7.2487
batch: [10840/21305] batch time: 2.183 trainign loss: 7.0063 avg training loss: 7.2488
batch: [10850/21305] batch time: 0.052 trainign loss: 7.1420 avg training loss: 7.2487
batch: [10860/21305] batch time: 2.535 trainign loss: 6.3877 avg training loss: 7.2487
batch: [10870/21305] batch time: 0.061 trainign loss: 5.7096 avg training loss: 7.2487
batch: [10880/21305] batch time: 2.195 trainign loss: 5.2464 avg training loss: 7.2486
batch: [10890/21305] batch time: 0.053 trainign loss: 7.1934 avg training loss: 7.2485
batch: [10900/21305] batch time: 2.430 trainign loss: 7.4872 avg training loss: 7.2485
batch: [10910/21305] batch time: 0.056 trainign loss: 7.0061 avg training loss: 7.2485
batch: [10920/21305] batch time: 2.388 trainign loss: 6.7100 avg training loss: 7.2484
batch: [10930/21305] batch time: 0.056 trainign loss: 5.2519 avg training loss: 7.2483
batch: [10940/21305] batch time: 2.656 trainign loss: 6.4285 avg training loss: 7.2482
batch: [10950/21305] batch time: 0.056 trainign loss: 5.7081 avg training loss: 7.2481
batch: [10960/21305] batch time: 2.239 trainign loss: 6.5423 avg training loss: 7.2481
batch: [10970/21305] batch time: 0.062 trainign loss: 6.7861 avg training loss: 7.2481
batch: [10980/21305] batch time: 2.250 trainign loss: 5.1833 avg training loss: 7.2480
batch: [10990/21305] batch time: 0.059 trainign loss: 6.9216 avg training loss: 7.2479
batch: [11000/21305] batch time: 2.016 trainign loss: 6.1151 avg training loss: 7.2478
batch: [11010/21305] batch time: 0.058 trainign loss: 5.6362 avg training loss: 7.2477
batch: [11020/21305] batch time: 2.231 trainign loss: 7.2842 avg training loss: 7.2476
batch: [11030/21305] batch time: 0.058 trainign loss: 5.5064 avg training loss: 7.2476
batch: [11040/21305] batch time: 2.016 trainign loss: 6.9965 avg training loss: 7.2475
batch: [11050/21305] batch time: 0.056 trainign loss: 6.3131 avg training loss: 7.2474
batch: [11060/21305] batch time: 2.475 trainign loss: 6.9513 avg training loss: 7.2474
batch: [11070/21305] batch time: 0.056 trainign loss: 5.3866 avg training loss: 7.2473
batch: [11080/21305] batch time: 2.372 trainign loss: 7.1818 avg training loss: 7.2472
batch: [11090/21305] batch time: 0.057 trainign loss: 7.5079 avg training loss: 7.2472
batch: [11100/21305] batch time: 2.717 trainign loss: 6.1080 avg training loss: 7.2472
batch: [11110/21305] batch time: 0.057 trainign loss: 6.6221 avg training loss: 7.2471
batch: [11120/21305] batch time: 2.059 trainign loss: 6.4642 avg training loss: 7.2471
batch: [11130/21305] batch time: 0.062 trainign loss: 5.0051 avg training loss: 7.2470
batch: [11140/21305] batch time: 2.342 trainign loss: 3.9504 avg training loss: 7.2469
batch: [11150/21305] batch time: 0.054 trainign loss: 6.2540 avg training loss: 7.2468
batch: [11160/21305] batch time: 1.926 trainign loss: 6.5409 avg training loss: 7.2468
batch: [11170/21305] batch time: 1.008 trainign loss: 3.2760 avg training loss: 7.2466
batch: [11180/21305] batch time: 1.430 trainign loss: 8.4424 avg training loss: 7.2465
batch: [11190/21305] batch time: 0.939 trainign loss: 7.3828 avg training loss: 7.2464
batch: [11200/21305] batch time: 1.164 trainign loss: 7.5350 avg training loss: 7.2464
batch: [11210/21305] batch time: 0.841 trainign loss: 5.0865 avg training loss: 7.2463
batch: [11220/21305] batch time: 1.190 trainign loss: 6.3465 avg training loss: 7.2463
batch: [11230/21305] batch time: 1.417 trainign loss: 5.5748 avg training loss: 7.2462
batch: [11240/21305] batch time: 0.828 trainign loss: 5.8243 avg training loss: 7.2461
batch: [11250/21305] batch time: 1.374 trainign loss: 0.6743 avg training loss: 7.2459
batch: [11260/21305] batch time: 0.918 trainign loss: 7.8338 avg training loss: 7.2459
batch: [11270/21305] batch time: 0.973 trainign loss: 6.1402 avg training loss: 7.2459
batch: [11280/21305] batch time: 1.414 trainign loss: 5.4639 avg training loss: 7.2458
batch: [11290/21305] batch time: 0.499 trainign loss: 6.5340 avg training loss: 7.2458
batch: [11300/21305] batch time: 1.916 trainign loss: 1.4138 avg training loss: 7.2456
batch: [11310/21305] batch time: 0.709 trainign loss: 6.3386 avg training loss: 7.2456
batch: [11320/21305] batch time: 1.362 trainign loss: 6.7270 avg training loss: 7.2455
batch: [11330/21305] batch time: 0.724 trainign loss: 8.0408 avg training loss: 7.2454
batch: [11340/21305] batch time: 1.638 trainign loss: 7.6561 avg training loss: 7.2454
batch: [11350/21305] batch time: 1.357 trainign loss: 6.9921 avg training loss: 7.2454
batch: [11360/21305] batch time: 1.068 trainign loss: 6.3121 avg training loss: 7.2453
batch: [11370/21305] batch time: 1.627 trainign loss: 5.9797 avg training loss: 7.2453
batch: [11380/21305] batch time: 1.098 trainign loss: 4.3592 avg training loss: 7.2451
batch: [11390/21305] batch time: 1.624 trainign loss: 6.8788 avg training loss: 7.2448
batch: [11400/21305] batch time: 1.008 trainign loss: 8.1515 avg training loss: 7.2449
batch: [11410/21305] batch time: 1.333 trainign loss: 6.6228 avg training loss: 7.2449
batch: [11420/21305] batch time: 1.510 trainign loss: 5.9197 avg training loss: 7.2448
batch: [11430/21305] batch time: 0.769 trainign loss: 6.6124 avg training loss: 7.2448
batch: [11440/21305] batch time: 2.308 trainign loss: 6.6004 avg training loss: 7.2448
batch: [11450/21305] batch time: 0.055 trainign loss: 6.2700 avg training loss: 7.2447
batch: [11460/21305] batch time: 2.049 trainign loss: 6.1451 avg training loss: 7.2447
batch: [11470/21305] batch time: 0.062 trainign loss: 6.6776 avg training loss: 7.2446
batch: [11480/21305] batch time: 2.551 trainign loss: 6.3688 avg training loss: 7.2445
batch: [11490/21305] batch time: 0.056 trainign loss: 6.9217 avg training loss: 7.2443
batch: [11500/21305] batch time: 1.851 trainign loss: 6.7048 avg training loss: 7.2443
batch: [11510/21305] batch time: 1.060 trainign loss: 5.6918 avg training loss: 7.2442
batch: [11520/21305] batch time: 1.531 trainign loss: 7.0194 avg training loss: 7.2442
batch: [11530/21305] batch time: 0.334 trainign loss: 5.5335 avg training loss: 7.2441
batch: [11540/21305] batch time: 2.138 trainign loss: 7.9151 avg training loss: 7.2441
batch: [11550/21305] batch time: 0.469 trainign loss: 5.2682 avg training loss: 7.2440
batch: [11560/21305] batch time: 2.364 trainign loss: 7.2353 avg training loss: 7.2440
batch: [11570/21305] batch time: 0.057 trainign loss: 5.2437 avg training loss: 7.2439
batch: [11580/21305] batch time: 1.981 trainign loss: 7.5178 avg training loss: 7.2439
batch: [11590/21305] batch time: 0.053 trainign loss: 6.6345 avg training loss: 7.2438
batch: [11600/21305] batch time: 1.901 trainign loss: 4.7671 avg training loss: 7.2438
batch: [11610/21305] batch time: 0.054 trainign loss: 5.6447 avg training loss: 7.2437
batch: [11620/21305] batch time: 2.036 trainign loss: 6.6554 avg training loss: 7.2436
batch: [11630/21305] batch time: 0.061 trainign loss: 7.2229 avg training loss: 7.2436
batch: [11640/21305] batch time: 1.984 trainign loss: 6.5136 avg training loss: 7.2435
batch: [11650/21305] batch time: 0.151 trainign loss: 7.0326 avg training loss: 7.2435
batch: [11660/21305] batch time: 2.187 trainign loss: 5.3056 avg training loss: 7.2434
batch: [11670/21305] batch time: 0.166 trainign loss: 6.7825 avg training loss: 7.2433
batch: [11680/21305] batch time: 2.062 trainign loss: 5.7404 avg training loss: 7.2433
batch: [11690/21305] batch time: 0.637 trainign loss: 6.0242 avg training loss: 7.2432
batch: [11700/21305] batch time: 1.819 trainign loss: 6.6105 avg training loss: 7.2431
batch: [11710/21305] batch time: 0.247 trainign loss: 4.3456 avg training loss: 7.2430
batch: [11720/21305] batch time: 1.226 trainign loss: 7.2599 avg training loss: 7.2430
batch: [11730/21305] batch time: 0.985 trainign loss: 5.8122 avg training loss: 7.2429
batch: [11740/21305] batch time: 1.198 trainign loss: 5.8101 avg training loss: 7.2428
batch: [11750/21305] batch time: 1.111 trainign loss: 6.6184 avg training loss: 7.2428
batch: [11760/21305] batch time: 0.901 trainign loss: 6.5360 avg training loss: 7.2427
batch: [11770/21305] batch time: 1.835 trainign loss: 6.4353 avg training loss: 7.2427
batch: [11780/21305] batch time: 0.791 trainign loss: 4.5932 avg training loss: 7.2426
batch: [11790/21305] batch time: 1.707 trainign loss: 1.1538 avg training loss: 7.2424
batch: [11800/21305] batch time: 0.063 trainign loss: 0.0008 avg training loss: 7.2419
batch: [11810/21305] batch time: 2.372 trainign loss: 5.8840 avg training loss: 7.2418
batch: [11820/21305] batch time: 0.062 trainign loss: 7.2367 avg training loss: 7.2419
batch: [11830/21305] batch time: 2.148 trainign loss: 7.6701 avg training loss: 7.2418
batch: [11840/21305] batch time: 0.057 trainign loss: 7.1265 avg training loss: 7.2417
batch: [11850/21305] batch time: 2.476 trainign loss: 5.6818 avg training loss: 7.2416
batch: [11860/21305] batch time: 0.060 trainign loss: 3.4922 avg training loss: 7.2415
batch: [11870/21305] batch time: 2.534 trainign loss: 6.3647 avg training loss: 7.2414
batch: [11880/21305] batch time: 0.052 trainign loss: 7.5245 avg training loss: 7.2412
batch: [11890/21305] batch time: 2.434 trainign loss: 7.7616 avg training loss: 7.2413
batch: [11900/21305] batch time: 0.062 trainign loss: 5.4217 avg training loss: 7.2412
batch: [11910/21305] batch time: 2.301 trainign loss: 6.9821 avg training loss: 7.2412
batch: [11920/21305] batch time: 0.056 trainign loss: 6.3458 avg training loss: 7.2411
batch: [11930/21305] batch time: 1.938 trainign loss: 5.2414 avg training loss: 7.2410
batch: [11940/21305] batch time: 0.053 trainign loss: 7.3654 avg training loss: 7.2410
batch: [11950/21305] batch time: 2.207 trainign loss: 6.6682 avg training loss: 7.2410
batch: [11960/21305] batch time: 0.058 trainign loss: 6.9823 avg training loss: 7.2409
batch: [11970/21305] batch time: 2.220 trainign loss: 6.2811 avg training loss: 7.2408
batch: [11980/21305] batch time: 0.056 trainign loss: 6.4235 avg training loss: 7.2408
batch: [11990/21305] batch time: 2.502 trainign loss: 5.6952 avg training loss: 7.2407
batch: [12000/21305] batch time: 0.063 trainign loss: 6.4884 avg training loss: 7.2406
batch: [12010/21305] batch time: 2.471 trainign loss: 7.4304 avg training loss: 7.2406
batch: [12020/21305] batch time: 0.056 trainign loss: 6.9910 avg training loss: 7.2406
batch: [12030/21305] batch time: 2.470 trainign loss: 5.9655 avg training loss: 7.2406
batch: [12040/21305] batch time: 0.057 trainign loss: 5.5762 avg training loss: 7.2405
batch: [12050/21305] batch time: 2.463 trainign loss: 6.1246 avg training loss: 7.2404
batch: [12060/21305] batch time: 0.055 trainign loss: 6.8288 avg training loss: 7.2403
batch: [12070/21305] batch time: 1.939 trainign loss: 3.2200 avg training loss: 7.2402
batch: [12080/21305] batch time: 0.056 trainign loss: 6.5827 avg training loss: 7.2400
batch: [12090/21305] batch time: 2.347 trainign loss: 5.5674 avg training loss: 7.2400
batch: [12100/21305] batch time: 0.057 trainign loss: 6.3367 avg training loss: 7.2399
batch: [12110/21305] batch time: 2.432 trainign loss: 6.9353 avg training loss: 7.2399
batch: [12120/21305] batch time: 0.058 trainign loss: 3.9435 avg training loss: 7.2397
batch: [12130/21305] batch time: 2.469 trainign loss: 8.6468 avg training loss: 7.2397
batch: [12140/21305] batch time: 0.062 trainign loss: 5.1213 avg training loss: 7.2396
batch: [12150/21305] batch time: 2.309 trainign loss: 7.4081 avg training loss: 7.2395
batch: [12160/21305] batch time: 0.055 trainign loss: 7.4750 avg training loss: 7.2396
batch: [12170/21305] batch time: 1.977 trainign loss: 6.7426 avg training loss: 7.2395
batch: [12180/21305] batch time: 0.059 trainign loss: 6.7671 avg training loss: 7.2395
batch: [12190/21305] batch time: 2.048 trainign loss: 7.1993 avg training loss: 7.2395
batch: [12200/21305] batch time: 0.053 trainign loss: 6.1511 avg training loss: 7.2395
batch: [12210/21305] batch time: 2.308 trainign loss: 7.0778 avg training loss: 7.2394
batch: [12220/21305] batch time: 0.060 trainign loss: 6.6053 avg training loss: 7.2394
batch: [12230/21305] batch time: 2.568 trainign loss: 7.0280 avg training loss: 7.2394
batch: [12240/21305] batch time: 0.056 trainign loss: 6.0812 avg training loss: 7.2393
batch: [12250/21305] batch time: 2.595 trainign loss: 6.0758 avg training loss: 7.2392
batch: [12260/21305] batch time: 0.062 trainign loss: 6.8833 avg training loss: 7.2392
batch: [12270/21305] batch time: 2.217 trainign loss: 5.4966 avg training loss: 7.2391
batch: [12280/21305] batch time: 0.056 trainign loss: 4.3330 avg training loss: 7.2390
batch: [12290/21305] batch time: 2.260 trainign loss: 7.2510 avg training loss: 7.2390
batch: [12300/21305] batch time: 0.056 trainign loss: 7.3190 avg training loss: 7.2389
batch: [12310/21305] batch time: 2.083 trainign loss: 6.5716 avg training loss: 7.2389
batch: [12320/21305] batch time: 0.056 trainign loss: 6.2292 avg training loss: 7.2388
batch: [12330/21305] batch time: 1.011 trainign loss: 6.8842 avg training loss: 7.2388
batch: [12340/21305] batch time: 0.056 trainign loss: 6.6880 avg training loss: 7.2388
batch: [12350/21305] batch time: 1.530 trainign loss: 7.2760 avg training loss: 7.2387
batch: [12360/21305] batch time: 0.062 trainign loss: 5.0839 avg training loss: 7.2387
batch: [12370/21305] batch time: 1.313 trainign loss: 5.0287 avg training loss: 7.2386
batch: [12380/21305] batch time: 0.057 trainign loss: 6.5961 avg training loss: 7.2385
batch: [12390/21305] batch time: 1.039 trainign loss: 7.6865 avg training loss: 7.2385
batch: [12400/21305] batch time: 0.057 trainign loss: 6.0130 avg training loss: 7.2385
batch: [12410/21305] batch time: 0.058 trainign loss: 7.2378 avg training loss: 7.2384
batch: [12420/21305] batch time: 0.056 trainign loss: 5.5929 avg training loss: 7.2384
batch: [12430/21305] batch time: 1.064 trainign loss: 5.2885 avg training loss: 7.2383
batch: [12440/21305] batch time: 0.056 trainign loss: 7.1213 avg training loss: 7.2382
batch: [12450/21305] batch time: 0.942 trainign loss: 7.7526 avg training loss: 7.2382
batch: [12460/21305] batch time: 0.056 trainign loss: 7.2037 avg training loss: 7.2382
batch: [12470/21305] batch time: 0.790 trainign loss: 5.7330 avg training loss: 7.2381
batch: [12480/21305] batch time: 0.060 trainign loss: 5.6228 avg training loss: 7.2381
batch: [12490/21305] batch time: 1.528 trainign loss: 6.0972 avg training loss: 7.2380
batch: [12500/21305] batch time: 0.056 trainign loss: 7.0487 avg training loss: 7.2379
batch: [12510/21305] batch time: 0.813 trainign loss: 7.5875 avg training loss: 7.2379
batch: [12520/21305] batch time: 0.060 trainign loss: 7.0752 avg training loss: 7.2379
batch: [12530/21305] batch time: 1.236 trainign loss: 6.6746 avg training loss: 7.2378
batch: [12540/21305] batch time: 0.056 trainign loss: 5.1216 avg training loss: 7.2377
batch: [12550/21305] batch time: 1.581 trainign loss: 6.6110 avg training loss: 7.2376
batch: [12560/21305] batch time: 0.063 trainign loss: 4.2501 avg training loss: 7.2375
batch: [12570/21305] batch time: 1.402 trainign loss: 0.0051 avg training loss: 7.2371
batch: [12580/21305] batch time: 0.062 trainign loss: 7.6289 avg training loss: 7.2372
batch: [12590/21305] batch time: 1.254 trainign loss: 3.8052 avg training loss: 7.2372
batch: [12600/21305] batch time: 0.056 trainign loss: 7.9253 avg training loss: 7.2371
batch: [12610/21305] batch time: 0.847 trainign loss: 4.8130 avg training loss: 7.2371
batch: [12620/21305] batch time: 0.063 trainign loss: 6.6221 avg training loss: 7.2371
batch: [12630/21305] batch time: 0.062 trainign loss: 6.3433 avg training loss: 7.2371
batch: [12640/21305] batch time: 0.062 trainign loss: 7.0756 avg training loss: 7.2371
batch: [12650/21305] batch time: 0.056 trainign loss: 5.5702 avg training loss: 7.2370
batch: [12660/21305] batch time: 0.214 trainign loss: 6.2182 avg training loss: 7.2369
batch: [12670/21305] batch time: 0.063 trainign loss: 7.3028 avg training loss: 7.2369
batch: [12680/21305] batch time: 0.122 trainign loss: 4.5440 avg training loss: 7.2368
batch: [12690/21305] batch time: 0.472 trainign loss: 7.3694 avg training loss: 7.2367
batch: [12700/21305] batch time: 0.056 trainign loss: 6.4856 avg training loss: 7.2367
batch: [12710/21305] batch time: 0.199 trainign loss: 5.8777 avg training loss: 7.2366
batch: [12720/21305] batch time: 0.063 trainign loss: 5.4293 avg training loss: 7.2365
batch: [12730/21305] batch time: 0.074 trainign loss: 5.6011 avg training loss: 7.2364
batch: [12740/21305] batch time: 0.056 trainign loss: 4.8109 avg training loss: 7.2364
batch: [12750/21305] batch time: 0.052 trainign loss: 3.5599 avg training loss: 7.2362
batch: [12760/21305] batch time: 0.063 trainign loss: 7.1088 avg training loss: 7.2362
batch: [12770/21305] batch time: 0.051 trainign loss: 5.8317 avg training loss: 7.2362
batch: [12780/21305] batch time: 0.061 trainign loss: 7.2812 avg training loss: 7.2361
batch: [12790/21305] batch time: 0.051 trainign loss: 5.2399 avg training loss: 7.2360
batch: [12800/21305] batch time: 0.057 trainign loss: 7.0004 avg training loss: 7.2360
batch: [12810/21305] batch time: 0.057 trainign loss: 6.9457 avg training loss: 7.2360
batch: [12820/21305] batch time: 0.056 trainign loss: 6.5078 avg training loss: 7.2359
batch: [12830/21305] batch time: 0.050 trainign loss: 4.7067 avg training loss: 7.2358
batch: [12840/21305] batch time: 0.062 trainign loss: 6.6011 avg training loss: 7.2358
batch: [12850/21305] batch time: 0.060 trainign loss: 7.3380 avg training loss: 7.2358
batch: [12860/21305] batch time: 0.056 trainign loss: 7.0279 avg training loss: 7.2357
batch: [12870/21305] batch time: 0.051 trainign loss: 6.3185 avg training loss: 7.2357
batch: [12880/21305] batch time: 0.056 trainign loss: 5.5548 avg training loss: 7.2356
batch: [12890/21305] batch time: 0.056 trainign loss: 6.1371 avg training loss: 7.2355
batch: [12900/21305] batch time: 0.056 trainign loss: 7.1122 avg training loss: 7.2355
batch: [12910/21305] batch time: 0.056 trainign loss: 6.1344 avg training loss: 7.2355
batch: [12920/21305] batch time: 0.056 trainign loss: 6.0846 avg training loss: 7.2354
batch: [12930/21305] batch time: 0.056 trainign loss: 5.8933 avg training loss: 7.2353
batch: [12940/21305] batch time: 0.058 trainign loss: 6.4059 avg training loss: 7.2353
batch: [12950/21305] batch time: 0.057 trainign loss: 4.8509 avg training loss: 7.2351
batch: [12960/21305] batch time: 0.061 trainign loss: 6.9232 avg training loss: 7.2351
batch: [12970/21305] batch time: 0.058 trainign loss: 5.1972 avg training loss: 7.2351
batch: [12980/21305] batch time: 0.056 trainign loss: 6.4644 avg training loss: 7.2349
batch: [12990/21305] batch time: 0.051 trainign loss: 6.2907 avg training loss: 7.2349
batch: [13000/21305] batch time: 0.057 trainign loss: 6.9303 avg training loss: 7.2349
batch: [13010/21305] batch time: 0.059 trainign loss: 6.4933 avg training loss: 7.2348
batch: [13020/21305] batch time: 0.058 trainign loss: 7.6232 avg training loss: 7.2347
batch: [13030/21305] batch time: 0.053 trainign loss: 6.8678 avg training loss: 7.2347
batch: [13040/21305] batch time: 0.056 trainign loss: 6.9151 avg training loss: 7.2346
batch: [13050/21305] batch time: 0.051 trainign loss: 2.4246 avg training loss: 7.2345
batch: [13060/21305] batch time: 0.056 trainign loss: 6.7166 avg training loss: 7.2344
batch: [13070/21305] batch time: 0.056 trainign loss: 6.7939 avg training loss: 7.2344
batch: [13080/21305] batch time: 0.062 trainign loss: 6.1413 avg training loss: 7.2343
batch: [13090/21305] batch time: 0.056 trainign loss: 4.8283 avg training loss: 7.2342
batch: [13100/21305] batch time: 0.056 trainign loss: 7.4461 avg training loss: 7.2341
batch: [13110/21305] batch time: 0.062 trainign loss: 6.7498 avg training loss: 7.2340
batch: [13120/21305] batch time: 0.056 trainign loss: 6.5521 avg training loss: 7.2340
batch: [13130/21305] batch time: 0.057 trainign loss: 5.5479 avg training loss: 7.2339
batch: [13140/21305] batch time: 0.058 trainign loss: 2.2598 avg training loss: 7.2338
batch: [13150/21305] batch time: 0.056 trainign loss: 7.1410 avg training loss: 7.2337
batch: [13160/21305] batch time: 0.057 trainign loss: 6.1116 avg training loss: 7.2337
batch: [13170/21305] batch time: 0.056 trainign loss: 6.8849 avg training loss: 7.2337
batch: [13180/21305] batch time: 0.056 trainign loss: 5.0396 avg training loss: 7.2336
batch: [13190/21305] batch time: 0.063 trainign loss: 6.3051 avg training loss: 7.2335
batch: [13200/21305] batch time: 0.056 trainign loss: 3.0444 avg training loss: 7.2334
batch: [13210/21305] batch time: 0.057 trainign loss: 0.0019 avg training loss: 7.2329
batch: [13220/21305] batch time: 0.051 trainign loss: 0.0001 avg training loss: 7.2325
batch: [13230/21305] batch time: 0.052 trainign loss: 0.0000 avg training loss: 7.2320
batch: [13240/21305] batch time: 0.057 trainign loss: 7.9773 avg training loss: 7.2321
batch: [13250/21305] batch time: 0.051 trainign loss: 7.6917 avg training loss: 7.2321
batch: [13260/21305] batch time: 0.056 trainign loss: 7.5809 avg training loss: 7.2321
batch: [13270/21305] batch time: 0.051 trainign loss: 6.6492 avg training loss: 7.2321
batch: [13280/21305] batch time: 0.057 trainign loss: 7.4077 avg training loss: 7.2320
batch: [13290/21305] batch time: 0.050 trainign loss: 6.7910 avg training loss: 7.2320
batch: [13300/21305] batch time: 0.058 trainign loss: 7.0170 avg training loss: 7.2319
batch: [13310/21305] batch time: 0.059 trainign loss: 4.9973 avg training loss: 7.2318
batch: [13320/21305] batch time: 0.056 trainign loss: 5.3300 avg training loss: 7.2316
batch: [13330/21305] batch time: 0.056 trainign loss: 7.1376 avg training loss: 7.2316
batch: [13340/21305] batch time: 0.056 trainign loss: 7.0662 avg training loss: 7.2316
batch: [13350/21305] batch time: 0.052 trainign loss: 5.7578 avg training loss: 7.2316
batch: [13360/21305] batch time: 0.859 trainign loss: 7.9087 avg training loss: 7.2315
batch: [13370/21305] batch time: 0.062 trainign loss: 5.8662 avg training loss: 7.2315
batch: [13380/21305] batch time: 0.056 trainign loss: 5.2308 avg training loss: 7.2314
batch: [13390/21305] batch time: 0.056 trainign loss: 6.0770 avg training loss: 7.2313
batch: [13400/21305] batch time: 0.056 trainign loss: 4.6073 avg training loss: 7.2313
batch: [13410/21305] batch time: 0.056 trainign loss: 7.3380 avg training loss: 7.2313
batch: [13420/21305] batch time: 0.056 trainign loss: 6.6053 avg training loss: 7.2313
batch: [13430/21305] batch time: 0.051 trainign loss: 6.6056 avg training loss: 7.2312
batch: [13440/21305] batch time: 0.057 trainign loss: 5.8439 avg training loss: 7.2312
batch: [13450/21305] batch time: 0.056 trainign loss: 7.0823 avg training loss: 7.2311
batch: [13460/21305] batch time: 0.056 trainign loss: 6.4291 avg training loss: 7.2311
batch: [13470/21305] batch time: 0.054 trainign loss: 3.3318 avg training loss: 7.2310
batch: [13480/21305] batch time: 0.062 trainign loss: 4.6230 avg training loss: 7.2309
batch: [13490/21305] batch time: 0.061 trainign loss: 6.9716 avg training loss: 7.2308
batch: [13500/21305] batch time: 0.058 trainign loss: 6.6447 avg training loss: 7.2308
batch: [13510/21305] batch time: 0.443 trainign loss: 5.3972 avg training loss: 7.2307
batch: [13520/21305] batch time: 0.058 trainign loss: 6.1643 avg training loss: 7.2307
batch: [13530/21305] batch time: 1.730 trainign loss: 5.8970 avg training loss: 7.2306
batch: [13540/21305] batch time: 0.063 trainign loss: 2.9771 avg training loss: 7.2305
batch: [13550/21305] batch time: 2.432 trainign loss: 7.2059 avg training loss: 7.2304
batch: [13560/21305] batch time: 0.062 trainign loss: 6.5841 avg training loss: 7.2304
batch: [13570/21305] batch time: 2.306 trainign loss: 6.6029 avg training loss: 7.2304
batch: [13580/21305] batch time: 0.055 trainign loss: 7.3580 avg training loss: 7.2303
batch: [13590/21305] batch time: 1.827 trainign loss: 4.9764 avg training loss: 7.2303
batch: [13600/21305] batch time: 0.062 trainign loss: 6.4573 avg training loss: 7.2302
batch: [13610/21305] batch time: 2.439 trainign loss: 7.4471 avg training loss: 7.2301
batch: [13620/21305] batch time: 0.056 trainign loss: 6.4301 avg training loss: 7.2301
batch: [13630/21305] batch time: 2.169 trainign loss: 6.3977 avg training loss: 7.2300
batch: [13640/21305] batch time: 0.055 trainign loss: 6.9116 avg training loss: 7.2300
batch: [13650/21305] batch time: 2.248 trainign loss: 6.1118 avg training loss: 7.2300
batch: [13660/21305] batch time: 0.455 trainign loss: 5.6018 avg training loss: 7.2299
batch: [13670/21305] batch time: 1.740 trainign loss: 6.5500 avg training loss: 7.2298
batch: [13680/21305] batch time: 0.343 trainign loss: 5.7012 avg training loss: 7.2298
batch: [13690/21305] batch time: 1.870 trainign loss: 3.6815 avg training loss: 7.2297
batch: [13700/21305] batch time: 0.063 trainign loss: 7.8230 avg training loss: 7.2296
batch: [13710/21305] batch time: 1.900 trainign loss: 6.8533 avg training loss: 7.2296
batch: [13720/21305] batch time: 0.057 trainign loss: 6.2387 avg training loss: 7.2296
batch: [13730/21305] batch time: 2.067 trainign loss: 7.0136 avg training loss: 7.2296
batch: [13740/21305] batch time: 0.056 trainign loss: 5.3754 avg training loss: 7.2295
batch: [13750/21305] batch time: 1.021 trainign loss: 7.1716 avg training loss: 7.2294
batch: [13760/21305] batch time: 0.060 trainign loss: 7.3074 avg training loss: 7.2294
batch: [13770/21305] batch time: 0.966 trainign loss: 6.9320 avg training loss: 7.2294
batch: [13780/21305] batch time: 0.058 trainign loss: 5.4701 avg training loss: 7.2292
batch: [13790/21305] batch time: 1.409 trainign loss: 7.2305 avg training loss: 7.2292
batch: [13800/21305] batch time: 0.061 trainign loss: 6.0869 avg training loss: 7.2292
batch: [13810/21305] batch time: 1.147 trainign loss: 6.4296 avg training loss: 7.2291
batch: [13820/21305] batch time: 0.056 trainign loss: 7.2284 avg training loss: 7.2291
batch: [13830/21305] batch time: 0.522 trainign loss: 5.9998 avg training loss: 7.2290
batch: [13840/21305] batch time: 0.056 trainign loss: 6.1299 avg training loss: 7.2289
batch: [13850/21305] batch time: 1.328 trainign loss: 0.9263 avg training loss: 7.2287
batch: [13860/21305] batch time: 0.056 trainign loss: 6.0860 avg training loss: 7.2287
batch: [13870/21305] batch time: 0.913 trainign loss: 7.8125 avg training loss: 7.2286
batch: [13880/21305] batch time: 0.061 trainign loss: 6.1746 avg training loss: 7.2286
batch: [13890/21305] batch time: 1.293 trainign loss: 7.0896 avg training loss: 7.2285
batch: [13900/21305] batch time: 0.062 trainign loss: 4.4425 avg training loss: 7.2285
batch: [13910/21305] batch time: 1.679 trainign loss: 7.7102 avg training loss: 7.2284
batch: [13920/21305] batch time: 0.056 trainign loss: 4.5262 avg training loss: 7.2283
batch: [13930/21305] batch time: 1.409 trainign loss: 6.4680 avg training loss: 7.2281
batch: [13940/21305] batch time: 0.910 trainign loss: 7.6513 avg training loss: 7.2282
batch: [13950/21305] batch time: 1.115 trainign loss: 5.1376 avg training loss: 7.2281
batch: [13960/21305] batch time: 0.948 trainign loss: 5.7381 avg training loss: 7.2281
batch: [13970/21305] batch time: 1.267 trainign loss: 7.1874 avg training loss: 7.2281
batch: [13980/21305] batch time: 1.300 trainign loss: 7.0663 avg training loss: 7.2280
batch: [13990/21305] batch time: 0.583 trainign loss: 6.5514 avg training loss: 7.2280
batch: [14000/21305] batch time: 1.583 trainign loss: 5.8968 avg training loss: 7.2279
batch: [14010/21305] batch time: 0.767 trainign loss: 4.8080 avg training loss: 7.2277
batch: [14020/21305] batch time: 1.350 trainign loss: 7.3328 avg training loss: 7.2276
batch: [14030/21305] batch time: 0.597 trainign loss: 6.9783 avg training loss: 7.2276
batch: [14040/21305] batch time: 1.106 trainign loss: 6.9741 avg training loss: 7.2275
batch: [14050/21305] batch time: 1.010 trainign loss: 7.1643 avg training loss: 7.2275
batch: [14060/21305] batch time: 0.735 trainign loss: 6.5219 avg training loss: 7.2275
batch: [14070/21305] batch time: 1.255 trainign loss: 6.7909 avg training loss: 7.2275
batch: [14080/21305] batch time: 0.455 trainign loss: 6.5502 avg training loss: 7.2275
batch: [14090/21305] batch time: 1.482 trainign loss: 6.8423 avg training loss: 7.2274
batch: [14100/21305] batch time: 0.513 trainign loss: 6.5275 avg training loss: 7.2274
batch: [14110/21305] batch time: 1.201 trainign loss: 6.3464 avg training loss: 7.2273
batch: [14120/21305] batch time: 0.203 trainign loss: 7.3622 avg training loss: 7.2272
batch: [14130/21305] batch time: 1.375 trainign loss: 6.0817 avg training loss: 7.2271
batch: [14140/21305] batch time: 0.060 trainign loss: 6.6271 avg training loss: 7.2270
batch: [14150/21305] batch time: 0.969 trainign loss: 7.0755 avg training loss: 7.2270
batch: [14160/21305] batch time: 0.168 trainign loss: 6.7165 avg training loss: 7.2270
batch: [14170/21305] batch time: 0.548 trainign loss: 6.3948 avg training loss: 7.2269
batch: [14180/21305] batch time: 1.119 trainign loss: 6.1760 avg training loss: 7.2269
batch: [14190/21305] batch time: 1.261 trainign loss: 2.1581 avg training loss: 7.2267
batch: [14200/21305] batch time: 1.012 trainign loss: 6.8100 avg training loss: 7.2266
batch: [14210/21305] batch time: 0.673 trainign loss: 5.8307 avg training loss: 7.2266
batch: [14220/21305] batch time: 1.220 trainign loss: 7.7921 avg training loss: 7.2264
batch: [14230/21305] batch time: 0.432 trainign loss: 6.6692 avg training loss: 7.2264
batch: [14240/21305] batch time: 2.287 trainign loss: 7.0535 avg training loss: 7.2264
batch: [14250/21305] batch time: 0.061 trainign loss: 6.7431 avg training loss: 7.2264
batch: [14260/21305] batch time: 1.804 trainign loss: 5.0797 avg training loss: 7.2263
batch: [14270/21305] batch time: 0.458 trainign loss: 4.9705 avg training loss: 7.2262
batch: [14280/21305] batch time: 1.089 trainign loss: 4.8672 avg training loss: 7.2261
batch: [14290/21305] batch time: 1.132 trainign loss: 6.4453 avg training loss: 7.2260
batch: [14300/21305] batch time: 0.825 trainign loss: 6.8708 avg training loss: 7.2260
batch: [14310/21305] batch time: 1.010 trainign loss: 6.4746 avg training loss: 7.2259
batch: [14320/21305] batch time: 0.058 trainign loss: 2.2010 avg training loss: 7.2258
batch: [14330/21305] batch time: 1.269 trainign loss: 6.4337 avg training loss: 7.2257
batch: [14340/21305] batch time: 1.370 trainign loss: 6.0534 avg training loss: 7.2257
batch: [14350/21305] batch time: 1.230 trainign loss: 4.7969 avg training loss: 7.2257
batch: [14360/21305] batch time: 1.045 trainign loss: 5.7041 avg training loss: 7.2256
batch: [14370/21305] batch time: 1.858 trainign loss: 5.7797 avg training loss: 7.2255
batch: [14380/21305] batch time: 1.077 trainign loss: 6.4512 avg training loss: 7.2255
batch: [14390/21305] batch time: 1.166 trainign loss: 6.6473 avg training loss: 7.2254
batch: [14400/21305] batch time: 1.129 trainign loss: 5.8856 avg training loss: 7.2254
batch: [14410/21305] batch time: 1.153 trainign loss: 6.2852 avg training loss: 7.2253
batch: [14420/21305] batch time: 1.423 trainign loss: 6.3210 avg training loss: 7.2252
batch: [14430/21305] batch time: 0.721 trainign loss: 6.4473 avg training loss: 7.2252
batch: [14440/21305] batch time: 1.307 trainign loss: 6.8899 avg training loss: 7.2251
batch: [14450/21305] batch time: 0.914 trainign loss: 5.5075 avg training loss: 7.2251
batch: [14460/21305] batch time: 0.480 trainign loss: 6.6665 avg training loss: 7.2250
batch: [14470/21305] batch time: 1.886 trainign loss: 5.5943 avg training loss: 7.2249
batch: [14480/21305] batch time: 0.054 trainign loss: 4.1622 avg training loss: 7.2249
batch: [14490/21305] batch time: 2.220 trainign loss: 5.1273 avg training loss: 7.2248
batch: [14500/21305] batch time: 0.062 trainign loss: 1.4861 avg training loss: 7.2246
batch: [14510/21305] batch time: 1.968 trainign loss: 0.0014 avg training loss: 7.2242
batch: [14520/21305] batch time: 0.062 trainign loss: 0.0002 avg training loss: 7.2237
batch: [14530/21305] batch time: 1.808 trainign loss: 0.0001 avg training loss: 7.2233
batch: [14540/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.2228
batch: [14550/21305] batch time: 1.787 trainign loss: 0.0001 avg training loss: 7.2224
batch: [14560/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.2220
batch: [14570/21305] batch time: 1.783 trainign loss: 0.0001 avg training loss: 7.2215
batch: [14580/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.2211
batch: [14590/21305] batch time: 2.124 trainign loss: 0.0000 avg training loss: 7.2206
batch: [14600/21305] batch time: 0.057 trainign loss: 8.5422 avg training loss: 7.2206
batch: [14610/21305] batch time: 2.398 trainign loss: 8.2645 avg training loss: 7.2206
batch: [14620/21305] batch time: 0.057 trainign loss: 6.7044 avg training loss: 7.2206
batch: [14630/21305] batch time: 2.236 trainign loss: 5.7954 avg training loss: 7.2205
batch: [14640/21305] batch time: 0.056 trainign loss: 5.5691 avg training loss: 7.2205
batch: [14650/21305] batch time: 2.346 trainign loss: 7.1343 avg training loss: 7.2205
batch: [14660/21305] batch time: 0.055 trainign loss: 6.1215 avg training loss: 7.2204
batch: [14670/21305] batch time: 2.592 trainign loss: 5.2543 avg training loss: 7.2204
batch: [14680/21305] batch time: 0.056 trainign loss: 1.5486 avg training loss: 7.2202
batch: [14690/21305] batch time: 2.191 trainign loss: 6.5613 avg training loss: 7.2201
batch: [14700/21305] batch time: 0.058 trainign loss: 3.7147 avg training loss: 7.2200
batch: [14710/21305] batch time: 1.849 trainign loss: 0.0036 avg training loss: 7.2196
batch: [14720/21305] batch time: 0.063 trainign loss: 7.5501 avg training loss: 7.2196
batch: [14730/21305] batch time: 2.253 trainign loss: 5.0459 avg training loss: 7.2195
batch: [14740/21305] batch time: 0.060 trainign loss: 5.0334 avg training loss: 7.2195
batch: [14750/21305] batch time: 2.158 trainign loss: 7.5494 avg training loss: 7.2194
batch: [14760/21305] batch time: 0.057 trainign loss: 7.2639 avg training loss: 7.2194
batch: [14770/21305] batch time: 1.675 trainign loss: 6.7660 avg training loss: 7.2194
batch: [14780/21305] batch time: 0.058 trainign loss: 6.3752 avg training loss: 7.2194
batch: [14790/21305] batch time: 1.037 trainign loss: 6.8836 avg training loss: 7.2194
batch: [14800/21305] batch time: 0.056 trainign loss: 6.4792 avg training loss: 7.2193
batch: [14810/21305] batch time: 1.500 trainign loss: 5.6204 avg training loss: 7.2192
batch: [14820/21305] batch time: 0.057 trainign loss: 6.6291 avg training loss: 7.2192
batch: [14830/21305] batch time: 1.998 trainign loss: 6.2619 avg training loss: 7.2192
batch: [14840/21305] batch time: 0.056 trainign loss: 5.3637 avg training loss: 7.2191
batch: [14850/21305] batch time: 1.400 trainign loss: 6.7881 avg training loss: 7.2191
batch: [14860/21305] batch time: 0.056 trainign loss: 7.2117 avg training loss: 7.2190
batch: [14870/21305] batch time: 1.332 trainign loss: 6.5122 avg training loss: 7.2190
batch: [14880/21305] batch time: 0.057 trainign loss: 6.6496 avg training loss: 7.2190
batch: [14890/21305] batch time: 0.883 trainign loss: 6.1140 avg training loss: 7.2189
batch: [14900/21305] batch time: 0.059 trainign loss: 6.4341 avg training loss: 7.2188
batch: [14910/21305] batch time: 0.898 trainign loss: 3.4316 avg training loss: 7.2188
batch: [14920/21305] batch time: 0.062 trainign loss: 6.2754 avg training loss: 7.2187
batch: [14930/21305] batch time: 0.517 trainign loss: 6.1079 avg training loss: 7.2187
batch: [14940/21305] batch time: 0.056 trainign loss: 6.5206 avg training loss: 7.2186
batch: [14950/21305] batch time: 0.057 trainign loss: 6.7768 avg training loss: 7.2185
batch: [14960/21305] batch time: 0.056 trainign loss: 6.6894 avg training loss: 7.2184
batch: [14970/21305] batch time: 0.063 trainign loss: 7.5172 avg training loss: 7.2183
batch: [14980/21305] batch time: 0.061 trainign loss: 7.6266 avg training loss: 7.2183
batch: [14990/21305] batch time: 0.056 trainign loss: 5.3364 avg training loss: 7.2182
batch: [15000/21305] batch time: 0.056 trainign loss: 5.9429 avg training loss: 7.2182
batch: [15010/21305] batch time: 0.056 trainign loss: 7.6868 avg training loss: 7.2181
batch: [15020/21305] batch time: 0.061 trainign loss: 5.8964 avg training loss: 7.2181
batch: [15030/21305] batch time: 0.056 trainign loss: 7.1858 avg training loss: 7.2180
batch: [15040/21305] batch time: 0.059 trainign loss: 2.5851 avg training loss: 7.2179
batch: [15050/21305] batch time: 0.051 trainign loss: 7.1344 avg training loss: 7.2179
batch: [15060/21305] batch time: 0.056 trainign loss: 6.5092 avg training loss: 7.2179
batch: [15070/21305] batch time: 0.056 trainign loss: 6.0281 avg training loss: 7.2178
batch: [15080/21305] batch time: 0.056 trainign loss: 3.7041 avg training loss: 7.2176
batch: [15090/21305] batch time: 0.052 trainign loss: 6.9395 avg training loss: 7.2177
batch: [15100/21305] batch time: 0.057 trainign loss: 5.9304 avg training loss: 7.2176
batch: [15110/21305] batch time: 0.057 trainign loss: 3.2855 avg training loss: 7.2176
batch: [15120/21305] batch time: 0.062 trainign loss: 7.2963 avg training loss: 7.2175
batch: [15130/21305] batch time: 0.063 trainign loss: 6.4525 avg training loss: 7.2174
batch: [15140/21305] batch time: 0.063 trainign loss: 7.7104 avg training loss: 7.2174
batch: [15150/21305] batch time: 0.056 trainign loss: 6.9039 avg training loss: 7.2174
batch: [15160/21305] batch time: 0.059 trainign loss: 6.6476 avg training loss: 7.2174
batch: [15170/21305] batch time: 0.053 trainign loss: 6.2022 avg training loss: 7.2173
batch: [15180/21305] batch time: 0.056 trainign loss: 5.4788 avg training loss: 7.2172
batch: [15190/21305] batch time: 0.054 trainign loss: 6.5223 avg training loss: 7.2172
batch: [15200/21305] batch time: 0.056 trainign loss: 6.8959 avg training loss: 7.2172
batch: [15210/21305] batch time: 0.056 trainign loss: 5.6012 avg training loss: 7.2170
batch: [15220/21305] batch time: 0.056 trainign loss: 6.5232 avg training loss: 7.2170
batch: [15230/21305] batch time: 0.056 trainign loss: 6.7855 avg training loss: 7.2169
batch: [15240/21305] batch time: 0.056 trainign loss: 6.4319 avg training loss: 7.2169
batch: [15250/21305] batch time: 0.058 trainign loss: 6.9431 avg training loss: 7.2168
batch: [15260/21305] batch time: 0.635 trainign loss: 5.6575 avg training loss: 7.2168
batch: [15270/21305] batch time: 0.056 trainign loss: 7.5857 avg training loss: 7.2167
batch: [15280/21305] batch time: 1.213 trainign loss: 6.1956 avg training loss: 7.2167
batch: [15290/21305] batch time: 0.396 trainign loss: 7.1102 avg training loss: 7.2166
batch: [15300/21305] batch time: 0.428 trainign loss: 6.4821 avg training loss: 7.2166
batch: [15310/21305] batch time: 1.675 trainign loss: 6.0134 avg training loss: 7.2164
batch: [15320/21305] batch time: 0.296 trainign loss: 7.3946 avg training loss: 7.2164
batch: [15330/21305] batch time: 1.326 trainign loss: 6.4274 avg training loss: 7.2164
batch: [15340/21305] batch time: 0.227 trainign loss: 5.4531 avg training loss: 7.2163
batch: [15350/21305] batch time: 1.362 trainign loss: 6.4440 avg training loss: 7.2163
batch: [15360/21305] batch time: 0.559 trainign loss: 7.1391 avg training loss: 7.2162
batch: [15370/21305] batch time: 1.498 trainign loss: 5.0407 avg training loss: 7.2161
batch: [15380/21305] batch time: 0.484 trainign loss: 5.9880 avg training loss: 7.2161
batch: [15390/21305] batch time: 1.809 trainign loss: 6.9095 avg training loss: 7.2161
batch: [15400/21305] batch time: 0.056 trainign loss: 7.1176 avg training loss: 7.2160
batch: [15410/21305] batch time: 2.300 trainign loss: 6.8747 avg training loss: 7.2160
batch: [15420/21305] batch time: 0.056 trainign loss: 6.7105 avg training loss: 7.2160
batch: [15430/21305] batch time: 2.627 trainign loss: 6.1468 avg training loss: 7.2159
batch: [15440/21305] batch time: 0.060 trainign loss: 7.2263 avg training loss: 7.2159
batch: [15450/21305] batch time: 2.124 trainign loss: 6.4597 avg training loss: 7.2159
batch: [15460/21305] batch time: 0.057 trainign loss: 6.8613 avg training loss: 7.2158
batch: [15470/21305] batch time: 2.255 trainign loss: 5.9969 avg training loss: 7.2157
batch: [15480/21305] batch time: 0.056 trainign loss: 6.9407 avg training loss: 7.2157
batch: [15490/21305] batch time: 2.209 trainign loss: 5.8708 avg training loss: 7.2157
batch: [15500/21305] batch time: 0.060 trainign loss: 4.6691 avg training loss: 7.2156
batch: [15510/21305] batch time: 2.867 trainign loss: 3.8664 avg training loss: 7.2155
batch: [15520/21305] batch time: 0.057 trainign loss: 8.1726 avg training loss: 7.2153
batch: [15530/21305] batch time: 2.065 trainign loss: 7.4720 avg training loss: 7.2153
batch: [15540/21305] batch time: 0.062 trainign loss: 7.2620 avg training loss: 7.2153
batch: [15550/21305] batch time: 2.399 trainign loss: 5.7057 avg training loss: 7.2153
batch: [15560/21305] batch time: 0.061 trainign loss: 7.7264 avg training loss: 7.2152
batch: [15570/21305] batch time: 2.154 trainign loss: 6.6763 avg training loss: 7.2152
batch: [15580/21305] batch time: 0.056 trainign loss: 6.4749 avg training loss: 7.2152
batch: [15590/21305] batch time: 2.218 trainign loss: 5.8463 avg training loss: 7.2151
batch: [15600/21305] batch time: 0.061 trainign loss: 6.3811 avg training loss: 7.2151
batch: [15610/21305] batch time: 1.146 trainign loss: 6.3565 avg training loss: 7.2150
batch: [15620/21305] batch time: 0.061 trainign loss: 6.3213 avg training loss: 7.2150
batch: [15630/21305] batch time: 0.761 trainign loss: 6.1403 avg training loss: 7.2149
batch: [15640/21305] batch time: 0.062 trainign loss: 7.0002 avg training loss: 7.2149
batch: [15650/21305] batch time: 0.503 trainign loss: 5.3079 avg training loss: 7.2148
batch: [15660/21305] batch time: 0.303 trainign loss: 5.0828 avg training loss: 7.2148
batch: [15670/21305] batch time: 0.061 trainign loss: 6.6016 avg training loss: 7.2147
batch: [15680/21305] batch time: 0.665 trainign loss: 7.0661 avg training loss: 7.2146
batch: [15690/21305] batch time: 0.237 trainign loss: 6.8310 avg training loss: 7.2146
batch: [15700/21305] batch time: 0.056 trainign loss: 6.3474 avg training loss: 7.2146
batch: [15710/21305] batch time: 1.260 trainign loss: 6.2306 avg training loss: 7.2145
batch: [15720/21305] batch time: 0.056 trainign loss: 4.7810 avg training loss: 7.2144
batch: [15730/21305] batch time: 1.670 trainign loss: 0.2070 avg training loss: 7.2141
batch: [15740/21305] batch time: 0.051 trainign loss: 11.8356 avg training loss: 7.2138
batch: [15750/21305] batch time: 2.568 trainign loss: 7.0817 avg training loss: 7.2139
batch: [15760/21305] batch time: 0.061 trainign loss: 6.4588 avg training loss: 7.2139
batch: [15770/21305] batch time: 1.583 trainign loss: 6.4037 avg training loss: 7.2139
batch: [15780/21305] batch time: 0.062 trainign loss: 6.2649 avg training loss: 7.2139
batch: [15790/21305] batch time: 1.393 trainign loss: 6.1662 avg training loss: 7.2138
batch: [15800/21305] batch time: 0.056 trainign loss: 4.8706 avg training loss: 7.2138
batch: [15810/21305] batch time: 1.492 trainign loss: 7.7683 avg training loss: 7.2137
batch: [15820/21305] batch time: 0.056 trainign loss: 6.3685 avg training loss: 7.2137
batch: [15830/21305] batch time: 2.551 trainign loss: 6.5610 avg training loss: 7.2137
batch: [15840/21305] batch time: 0.054 trainign loss: 6.3965 avg training loss: 7.2137
batch: [15850/21305] batch time: 2.114 trainign loss: 4.6882 avg training loss: 7.2136
batch: [15860/21305] batch time: 0.055 trainign loss: 7.9731 avg training loss: 7.2136
batch: [15870/21305] batch time: 1.796 trainign loss: 6.0389 avg training loss: 7.2136
batch: [15880/21305] batch time: 0.062 trainign loss: 6.8075 avg training loss: 7.2136
batch: [15890/21305] batch time: 2.344 trainign loss: 7.0547 avg training loss: 7.2135
batch: [15900/21305] batch time: 0.052 trainign loss: 5.6558 avg training loss: 7.2135
batch: [15910/21305] batch time: 1.746 trainign loss: 6.3177 avg training loss: 7.2134
batch: [15920/21305] batch time: 0.056 trainign loss: 6.6048 avg training loss: 7.2134
batch: [15930/21305] batch time: 2.432 trainign loss: 7.0565 avg training loss: 7.2133
batch: [15940/21305] batch time: 0.056 trainign loss: 6.3748 avg training loss: 7.2132
batch: [15950/21305] batch time: 2.096 trainign loss: 7.0668 avg training loss: 7.2132
batch: [15960/21305] batch time: 0.055 trainign loss: 4.9184 avg training loss: 7.2131
batch: [15970/21305] batch time: 2.273 trainign loss: 5.2117 avg training loss: 7.2130
batch: [15980/21305] batch time: 0.063 trainign loss: 5.8816 avg training loss: 7.2130
batch: [15990/21305] batch time: 2.441 trainign loss: 7.0214 avg training loss: 7.2130
batch: [16000/21305] batch time: 0.056 trainign loss: 6.6338 avg training loss: 7.2129
batch: [16010/21305] batch time: 2.130 trainign loss: 0.9654 avg training loss: 7.2127
batch: [16020/21305] batch time: 0.062 trainign loss: 6.6190 avg training loss: 7.2127
batch: [16030/21305] batch time: 2.226 trainign loss: 7.6975 avg training loss: 7.2127
batch: [16040/21305] batch time: 0.056 trainign loss: 5.2991 avg training loss: 7.2126
batch: [16050/21305] batch time: 2.219 trainign loss: 6.6739 avg training loss: 7.2126
batch: [16060/21305] batch time: 0.054 trainign loss: 5.6579 avg training loss: 7.2126
batch: [16070/21305] batch time: 2.170 trainign loss: 7.7213 avg training loss: 7.2125
batch: [16080/21305] batch time: 0.062 trainign loss: 7.0456 avg training loss: 7.2125
batch: [16090/21305] batch time: 2.425 trainign loss: 6.0502 avg training loss: 7.2125
batch: [16100/21305] batch time: 0.056 trainign loss: 5.7183 avg training loss: 7.2124
batch: [16110/21305] batch time: 2.063 trainign loss: 5.4526 avg training loss: 7.2124
batch: [16120/21305] batch time: 0.056 trainign loss: 4.5358 avg training loss: 7.2123
batch: [16130/21305] batch time: 2.228 trainign loss: 6.1360 avg training loss: 7.2122
batch: [16140/21305] batch time: 0.057 trainign loss: 6.7370 avg training loss: 7.2122
batch: [16150/21305] batch time: 2.297 trainign loss: 6.9285 avg training loss: 7.2121
batch: [16160/21305] batch time: 0.061 trainign loss: 4.5838 avg training loss: 7.2121
batch: [16170/21305] batch time: 2.055 trainign loss: 7.3437 avg training loss: 7.2120
batch: [16180/21305] batch time: 0.056 trainign loss: 7.1166 avg training loss: 7.2120
batch: [16190/21305] batch time: 2.078 trainign loss: 7.0013 avg training loss: 7.2120
batch: [16200/21305] batch time: 0.057 trainign loss: 5.3837 avg training loss: 7.2120
batch: [16210/21305] batch time: 2.063 trainign loss: 6.7040 avg training loss: 7.2119
batch: [16220/21305] batch time: 0.053 trainign loss: 6.4011 avg training loss: 7.2119
batch: [16230/21305] batch time: 2.026 trainign loss: 6.6262 avg training loss: 7.2118
batch: [16240/21305] batch time: 0.062 trainign loss: 7.0984 avg training loss: 7.2118
batch: [16250/21305] batch time: 2.658 trainign loss: 6.5027 avg training loss: 7.2117
batch: [16260/21305] batch time: 0.057 trainign loss: 6.6544 avg training loss: 7.2116
batch: [16270/21305] batch time: 1.933 trainign loss: 7.2259 avg training loss: 7.2116
batch: [16280/21305] batch time: 0.061 trainign loss: 7.2399 avg training loss: 7.2116
batch: [16290/21305] batch time: 2.296 trainign loss: 6.6557 avg training loss: 7.2115
batch: [16300/21305] batch time: 0.056 trainign loss: 6.5969 avg training loss: 7.2115
batch: [16310/21305] batch time: 2.150 trainign loss: 6.6314 avg training loss: 7.2114
batch: [16320/21305] batch time: 0.399 trainign loss: 6.9076 avg training loss: 7.2114
batch: [16330/21305] batch time: 0.701 trainign loss: 5.4679 avg training loss: 7.2113
batch: [16340/21305] batch time: 0.461 trainign loss: 4.8410 avg training loss: 7.2112
batch: [16350/21305] batch time: 0.054 trainign loss: 7.7179 avg training loss: 7.2111
batch: [16360/21305] batch time: 0.306 trainign loss: 6.1451 avg training loss: 7.2110
batch: [16370/21305] batch time: 0.060 trainign loss: 5.9871 avg training loss: 7.2110
batch: [16380/21305] batch time: 0.592 trainign loss: 6.9373 avg training loss: 7.2109
batch: [16390/21305] batch time: 0.062 trainign loss: 6.4687 avg training loss: 7.2109
batch: [16400/21305] batch time: 1.135 trainign loss: 7.4882 avg training loss: 7.2109
batch: [16410/21305] batch time: 0.060 trainign loss: 7.3003 avg training loss: 7.2109
batch: [16420/21305] batch time: 1.524 trainign loss: 6.7334 avg training loss: 7.2108
batch: [16430/21305] batch time: 0.052 trainign loss: 5.8131 avg training loss: 7.2107
batch: [16440/21305] batch time: 1.921 trainign loss: 6.9620 avg training loss: 7.2107
batch: [16450/21305] batch time: 0.056 trainign loss: 7.3434 avg training loss: 7.2107
batch: [16460/21305] batch time: 1.977 trainign loss: 6.8800 avg training loss: 7.2106
batch: [16470/21305] batch time: 0.056 trainign loss: 6.6784 avg training loss: 7.2106
batch: [16480/21305] batch time: 1.793 trainign loss: 6.0116 avg training loss: 7.2105
batch: [16490/21305] batch time: 0.062 trainign loss: 5.6113 avg training loss: 7.2105
batch: [16500/21305] batch time: 1.857 trainign loss: 5.6403 avg training loss: 7.2104
batch: [16510/21305] batch time: 0.059 trainign loss: 6.7554 avg training loss: 7.2104
batch: [16520/21305] batch time: 1.274 trainign loss: 6.4075 avg training loss: 7.2103
batch: [16530/21305] batch time: 0.063 trainign loss: 6.7470 avg training loss: 7.2103
batch: [16540/21305] batch time: 1.666 trainign loss: 5.7940 avg training loss: 7.2101
batch: [16550/21305] batch time: 0.057 trainign loss: 7.2314 avg training loss: 7.2101
batch: [16560/21305] batch time: 2.130 trainign loss: 7.1689 avg training loss: 7.2101
batch: [16570/21305] batch time: 0.056 trainign loss: 5.7861 avg training loss: 7.2100
batch: [16580/21305] batch time: 2.276 trainign loss: 6.9717 avg training loss: 7.2100
batch: [16590/21305] batch time: 0.055 trainign loss: 6.8654 avg training loss: 7.2099
batch: [16600/21305] batch time: 2.256 trainign loss: 6.5884 avg training loss: 7.2098
batch: [16610/21305] batch time: 0.052 trainign loss: 4.9587 avg training loss: 7.2097
batch: [16620/21305] batch time: 1.501 trainign loss: 6.2812 avg training loss: 7.2096
batch: [16630/21305] batch time: 0.053 trainign loss: 5.5489 avg training loss: 7.2095
batch: [16640/21305] batch time: 1.842 trainign loss: 7.4909 avg training loss: 7.2094
batch: [16650/21305] batch time: 0.054 trainign loss: 7.0478 avg training loss: 7.2094
batch: [16660/21305] batch time: 1.065 trainign loss: 6.9745 avg training loss: 7.2094
batch: [16670/21305] batch time: 0.063 trainign loss: 6.8315 avg training loss: 7.2094
batch: [16680/21305] batch time: 0.826 trainign loss: 7.3566 avg training loss: 7.2093
batch: [16690/21305] batch time: 0.057 trainign loss: 7.0941 avg training loss: 7.2093
batch: [16700/21305] batch time: 0.397 trainign loss: 7.0291 avg training loss: 7.2093
batch: [16710/21305] batch time: 0.057 trainign loss: 6.9322 avg training loss: 7.2093
batch: [16720/21305] batch time: 0.761 trainign loss: 7.2585 avg training loss: 7.2093
batch: [16730/21305] batch time: 0.061 trainign loss: 5.3328 avg training loss: 7.2092
batch: [16740/21305] batch time: 1.092 trainign loss: 6.4220 avg training loss: 7.2092
batch: [16750/21305] batch time: 0.061 trainign loss: 6.6436 avg training loss: 7.2091
batch: [16760/21305] batch time: 1.241 trainign loss: 6.0275 avg training loss: 7.2091
batch: [16770/21305] batch time: 0.056 trainign loss: 6.9408 avg training loss: 7.2090
batch: [16780/21305] batch time: 0.556 trainign loss: 6.2622 avg training loss: 7.2090
batch: [16790/21305] batch time: 0.329 trainign loss: 6.1422 avg training loss: 7.2089
batch: [16800/21305] batch time: 0.063 trainign loss: 2.6290 avg training loss: 7.2087
batch: [16810/21305] batch time: 0.053 trainign loss: 7.7533 avg training loss: 7.2087
batch: [16820/21305] batch time: 0.058 trainign loss: 6.1896 avg training loss: 7.2087
batch: [16830/21305] batch time: 0.056 trainign loss: 6.5972 avg training loss: 7.2087
batch: [16840/21305] batch time: 0.056 trainign loss: 7.0855 avg training loss: 7.2086
batch: [16850/21305] batch time: 0.056 trainign loss: 6.6264 avg training loss: 7.2086
batch: [16860/21305] batch time: 0.063 trainign loss: 7.0069 avg training loss: 7.2085
batch: [16870/21305] batch time: 0.056 trainign loss: 6.7905 avg training loss: 7.2084
batch: [16880/21305] batch time: 0.056 trainign loss: 6.6558 avg training loss: 7.2084
batch: [16890/21305] batch time: 0.054 trainign loss: 6.4412 avg training loss: 7.2083
batch: [16900/21305] batch time: 0.062 trainign loss: 4.4899 avg training loss: 7.2082
batch: [16910/21305] batch time: 0.053 trainign loss: 0.0074 avg training loss: 7.2079
batch: [16920/21305] batch time: 0.061 trainign loss: 8.2013 avg training loss: 7.2079
batch: [16930/21305] batch time: 0.056 trainign loss: 4.9622 avg training loss: 7.2079
batch: [16940/21305] batch time: 0.056 trainign loss: 6.9107 avg training loss: 7.2078
batch: [16950/21305] batch time: 0.056 trainign loss: 7.0638 avg training loss: 7.2078
batch: [16960/21305] batch time: 0.062 trainign loss: 4.9085 avg training loss: 7.2078
batch: [16970/21305] batch time: 0.051 trainign loss: 5.4473 avg training loss: 7.2077
batch: [16980/21305] batch time: 0.056 trainign loss: 3.7083 avg training loss: 7.2076
batch: [16990/21305] batch time: 0.060 trainign loss: 3.1520 avg training loss: 7.2074
batch: [17000/21305] batch time: 0.509 trainign loss: 1.8902 avg training loss: 7.2072
batch: [17010/21305] batch time: 0.056 trainign loss: 7.4315 avg training loss: 7.2073
batch: [17020/21305] batch time: 0.542 trainign loss: 0.4958 avg training loss: 7.2071
batch: [17030/21305] batch time: 0.056 trainign loss: 8.2941 avg training loss: 7.2070
batch: [17040/21305] batch time: 0.697 trainign loss: 7.4675 avg training loss: 7.2070
batch: [17050/21305] batch time: 0.056 trainign loss: 6.8055 avg training loss: 7.2070
batch: [17060/21305] batch time: 0.062 trainign loss: 6.9068 avg training loss: 7.2069
batch: [17070/21305] batch time: 0.052 trainign loss: 7.5319 avg training loss: 7.2068
batch: [17080/21305] batch time: 0.057 trainign loss: 7.5077 avg training loss: 7.2068
batch: [17090/21305] batch time: 0.056 trainign loss: 6.6215 avg training loss: 7.2068
batch: [17100/21305] batch time: 0.064 trainign loss: 2.1080 avg training loss: 7.2067
batch: [17110/21305] batch time: 0.055 trainign loss: 7.7855 avg training loss: 7.2067
batch: [17120/21305] batch time: 0.058 trainign loss: 7.0909 avg training loss: 7.2067
batch: [17130/21305] batch time: 0.154 trainign loss: 6.0342 avg training loss: 7.2066
batch: [17140/21305] batch time: 0.057 trainign loss: 5.6114 avg training loss: 7.2066
batch: [17150/21305] batch time: 0.056 trainign loss: 6.2527 avg training loss: 7.2065
batch: [17160/21305] batch time: 0.063 trainign loss: 6.4555 avg training loss: 7.2064
batch: [17170/21305] batch time: 0.052 trainign loss: 5.1249 avg training loss: 7.2062
batch: [17180/21305] batch time: 0.062 trainign loss: 7.3158 avg training loss: 7.2062
batch: [17190/21305] batch time: 0.063 trainign loss: 5.3807 avg training loss: 7.2062
batch: [17200/21305] batch time: 0.056 trainign loss: 6.1732 avg training loss: 7.2061
batch: [17210/21305] batch time: 0.488 trainign loss: 6.3326 avg training loss: 7.2060
batch: [17220/21305] batch time: 0.058 trainign loss: 6.7917 avg training loss: 7.2060
batch: [17230/21305] batch time: 0.051 trainign loss: 4.9007 avg training loss: 7.2059
batch: [17240/21305] batch time: 0.056 trainign loss: 6.4923 avg training loss: 7.2058
batch: [17250/21305] batch time: 0.051 trainign loss: 7.9059 avg training loss: 7.2057
batch: [17260/21305] batch time: 0.056 trainign loss: 7.1720 avg training loss: 7.2057
batch: [17270/21305] batch time: 0.056 trainign loss: 6.3444 avg training loss: 7.2057
batch: [17280/21305] batch time: 0.056 trainign loss: 7.0847 avg training loss: 7.2056
batch: [17290/21305] batch time: 0.053 trainign loss: 5.0595 avg training loss: 7.2056
batch: [17300/21305] batch time: 0.056 trainign loss: 6.1165 avg training loss: 7.2055
batch: [17310/21305] batch time: 0.052 trainign loss: 3.4249 avg training loss: 7.2054
batch: [17320/21305] batch time: 0.056 trainign loss: 11.1658 avg training loss: 7.2051
batch: [17330/21305] batch time: 0.055 trainign loss: 7.9872 avg training loss: 7.2052
batch: [17340/21305] batch time: 0.057 trainign loss: 7.1637 avg training loss: 7.2052
batch: [17350/21305] batch time: 0.057 trainign loss: 8.0783 avg training loss: 7.2050
batch: [17360/21305] batch time: 0.056 trainign loss: 6.2084 avg training loss: 7.2050
batch: [17370/21305] batch time: 0.059 trainign loss: 6.1296 avg training loss: 7.2049
batch: [17380/21305] batch time: 0.056 trainign loss: 7.1294 avg training loss: 7.2049
batch: [17390/21305] batch time: 0.579 trainign loss: 7.3169 avg training loss: 7.2049
batch: [17400/21305] batch time: 0.056 trainign loss: 7.0010 avg training loss: 7.2049
batch: [17410/21305] batch time: 0.342 trainign loss: 6.7687 avg training loss: 7.2048
batch: [17420/21305] batch time: 0.059 trainign loss: 5.6557 avg training loss: 7.2048
batch: [17430/21305] batch time: 0.207 trainign loss: 5.3454 avg training loss: 7.2047
batch: [17440/21305] batch time: 0.059 trainign loss: 6.4494 avg training loss: 7.2047
batch: [17450/21305] batch time: 1.146 trainign loss: 6.3822 avg training loss: 7.2046
batch: [17460/21305] batch time: 0.056 trainign loss: 4.6649 avg training loss: 7.2045
batch: [17470/21305] batch time: 0.308 trainign loss: 6.9061 avg training loss: 7.2045
batch: [17480/21305] batch time: 0.061 trainign loss: 6.1905 avg training loss: 7.2044
batch: [17490/21305] batch time: 0.054 trainign loss: 6.5404 avg training loss: 7.2043
batch: [17500/21305] batch time: 0.062 trainign loss: 6.1630 avg training loss: 7.2043
batch: [17510/21305] batch time: 0.057 trainign loss: 6.2358 avg training loss: 7.2043
batch: [17520/21305] batch time: 0.058 trainign loss: 5.1338 avg training loss: 7.2042
batch: [17530/21305] batch time: 0.056 trainign loss: 5.2155 avg training loss: 7.2041
batch: [17540/21305] batch time: 0.056 trainign loss: 6.4288 avg training loss: 7.2040
batch: [17550/21305] batch time: 0.327 trainign loss: 6.1955 avg training loss: 7.2040
batch: [17560/21305] batch time: 0.056 trainign loss: 5.6695 avg training loss: 7.2039
batch: [17570/21305] batch time: 0.554 trainign loss: 6.7731 avg training loss: 7.2038
batch: [17580/21305] batch time: 0.056 trainign loss: 1.5388 avg training loss: 7.2036
batch: [17590/21305] batch time: 0.056 trainign loss: 10.9796 avg training loss: 7.2033
batch: [17600/21305] batch time: 0.058 trainign loss: 8.3853 avg training loss: 7.2032
batch: [17610/21305] batch time: 0.053 trainign loss: 8.3205 avg training loss: 7.2033
batch: [17620/21305] batch time: 0.060 trainign loss: 5.6366 avg training loss: 7.2033
batch: [17630/21305] batch time: 0.056 trainign loss: 6.6340 avg training loss: 7.2033
batch: [17640/21305] batch time: 0.056 trainign loss: 5.3626 avg training loss: 7.2032
batch: [17650/21305] batch time: 0.052 trainign loss: 5.6818 avg training loss: 7.2032
batch: [17660/21305] batch time: 0.056 trainign loss: 6.2436 avg training loss: 7.2032
batch: [17670/21305] batch time: 0.054 trainign loss: 5.7949 avg training loss: 7.2031
batch: [17680/21305] batch time: 0.058 trainign loss: 6.8046 avg training loss: 7.2030
batch: [17690/21305] batch time: 0.056 trainign loss: 5.4071 avg training loss: 7.2030
batch: [17700/21305] batch time: 0.056 trainign loss: 6.2736 avg training loss: 7.2029
batch: [17710/21305] batch time: 0.581 trainign loss: 6.5598 avg training loss: 7.2028
batch: [17720/21305] batch time: 0.057 trainign loss: 6.4634 avg training loss: 7.2028
batch: [17730/21305] batch time: 0.462 trainign loss: 6.4222 avg training loss: 7.2027
batch: [17740/21305] batch time: 0.056 trainign loss: 5.3045 avg training loss: 7.2026
batch: [17750/21305] batch time: 0.808 trainign loss: 6.1648 avg training loss: 7.2026
batch: [17760/21305] batch time: 0.056 trainign loss: 6.0620 avg training loss: 7.2026
batch: [17770/21305] batch time: 0.760 trainign loss: 6.7277 avg training loss: 7.2025
batch: [17780/21305] batch time: 0.058 trainign loss: 5.7725 avg training loss: 7.2025
batch: [17790/21305] batch time: 0.602 trainign loss: 6.8683 avg training loss: 7.2024
batch: [17800/21305] batch time: 0.056 trainign loss: 5.8919 avg training loss: 7.2024
batch: [17810/21305] batch time: 0.205 trainign loss: 6.7986 avg training loss: 7.2023
batch: [17820/21305] batch time: 0.056 trainign loss: 6.4734 avg training loss: 7.2023
batch: [17830/21305] batch time: 0.057 trainign loss: 5.3883 avg training loss: 7.2022
batch: [17840/21305] batch time: 0.285 trainign loss: 7.1009 avg training loss: 7.2022
batch: [17850/21305] batch time: 0.350 trainign loss: 6.7700 avg training loss: 7.2022
batch: [17860/21305] batch time: 0.062 trainign loss: 6.4821 avg training loss: 7.2021
batch: [17870/21305] batch time: 0.053 trainign loss: 5.7100 avg training loss: 7.2020
batch: [17880/21305] batch time: 0.061 trainign loss: 6.7872 avg training loss: 7.2020
batch: [17890/21305] batch time: 0.063 trainign loss: 6.6596 avg training loss: 7.2020
batch: [17900/21305] batch time: 0.056 trainign loss: 6.2393 avg training loss: 7.2019
batch: [17910/21305] batch time: 0.056 trainign loss: 3.3970 avg training loss: 7.2017
batch: [17920/21305] batch time: 0.058 trainign loss: 7.4827 avg training loss: 7.2017
batch: [17930/21305] batch time: 0.052 trainign loss: 6.2105 avg training loss: 7.2017
batch: [17940/21305] batch time: 0.056 trainign loss: 6.2936 avg training loss: 7.2017
batch: [17950/21305] batch time: 0.051 trainign loss: 6.3008 avg training loss: 7.2016
batch: [17960/21305] batch time: 0.056 trainign loss: 6.7079 avg training loss: 7.2015
batch: [17970/21305] batch time: 0.056 trainign loss: 6.0086 avg training loss: 7.2015
batch: [17980/21305] batch time: 0.060 trainign loss: 1.4339 avg training loss: 7.2013
batch: [17990/21305] batch time: 0.056 trainign loss: 5.1277 avg training loss: 7.2012
batch: [18000/21305] batch time: 0.063 trainign loss: 6.9496 avg training loss: 7.2012
batch: [18010/21305] batch time: 0.052 trainign loss: 7.8079 avg training loss: 7.2011
batch: [18020/21305] batch time: 0.062 trainign loss: 4.0635 avg training loss: 7.2011
batch: [18030/21305] batch time: 0.709 trainign loss: 6.7433 avg training loss: 7.2010
batch: [18040/21305] batch time: 0.062 trainign loss: 6.2526 avg training loss: 7.2010
batch: [18050/21305] batch time: 1.356 trainign loss: 6.6545 avg training loss: 7.2010
batch: [18060/21305] batch time: 0.056 trainign loss: 7.3015 avg training loss: 7.2009
batch: [18070/21305] batch time: 1.819 trainign loss: 7.4421 avg training loss: 7.2009
batch: [18080/21305] batch time: 0.062 trainign loss: 6.2462 avg training loss: 7.2009
batch: [18090/21305] batch time: 1.959 trainign loss: 5.0575 avg training loss: 7.2008
batch: [18100/21305] batch time: 0.056 trainign loss: 5.6512 avg training loss: 7.2007
batch: [18110/21305] batch time: 2.196 trainign loss: 5.2980 avg training loss: 7.2006
batch: [18120/21305] batch time: 0.499 trainign loss: 6.5576 avg training loss: 7.2005
batch: [18130/21305] batch time: 2.207 trainign loss: 7.2137 avg training loss: 7.2005
batch: [18140/21305] batch time: 0.057 trainign loss: 7.1128 avg training loss: 7.2004
batch: [18150/21305] batch time: 2.251 trainign loss: 6.9330 avg training loss: 7.2004
batch: [18160/21305] batch time: 0.056 trainign loss: 5.0963 avg training loss: 7.2003
batch: [18170/21305] batch time: 2.448 trainign loss: 5.8362 avg training loss: 7.2002
batch: [18180/21305] batch time: 0.062 trainign loss: 0.0778 avg training loss: 7.2000
batch: [18190/21305] batch time: 2.017 trainign loss: 7.3164 avg training loss: 7.1999
batch: [18200/21305] batch time: 0.056 trainign loss: 6.8197 avg training loss: 7.1999
batch: [18210/21305] batch time: 1.883 trainign loss: 6.7630 avg training loss: 7.1999
batch: [18220/21305] batch time: 0.059 trainign loss: 3.9691 avg training loss: 7.1998
batch: [18230/21305] batch time: 2.264 trainign loss: 7.8132 avg training loss: 7.1997
batch: [18240/21305] batch time: 0.056 trainign loss: 5.9897 avg training loss: 7.1997
batch: [18250/21305] batch time: 2.031 trainign loss: 5.7313 avg training loss: 7.1997
batch: [18260/21305] batch time: 0.056 trainign loss: 1.9647 avg training loss: 7.1995
batch: [18270/21305] batch time: 2.156 trainign loss: 5.9265 avg training loss: 7.1995
batch: [18280/21305] batch time: 0.056 trainign loss: 6.5965 avg training loss: 7.1994
batch: [18290/21305] batch time: 2.030 trainign loss: 7.3793 avg training loss: 7.1994
batch: [18300/21305] batch time: 0.053 trainign loss: 6.3792 avg training loss: 7.1994
batch: [18310/21305] batch time: 2.403 trainign loss: 6.7019 avg training loss: 7.1993
batch: [18320/21305] batch time: 0.054 trainign loss: 5.2030 avg training loss: 7.1993
batch: [18330/21305] batch time: 2.232 trainign loss: 5.7759 avg training loss: 7.1992
batch: [18340/21305] batch time: 0.061 trainign loss: 7.2717 avg training loss: 7.1992
batch: [18350/21305] batch time: 1.728 trainign loss: 6.9181 avg training loss: 7.1991
batch: [18360/21305] batch time: 0.056 trainign loss: 5.4444 avg training loss: 7.1991
batch: [18370/21305] batch time: 0.816 trainign loss: 6.9055 avg training loss: 7.1990
batch: [18380/21305] batch time: 0.056 trainign loss: 6.0522 avg training loss: 7.1989
batch: [18390/21305] batch time: 0.860 trainign loss: 6.0462 avg training loss: 7.1988
batch: [18400/21305] batch time: 0.057 trainign loss: 7.2915 avg training loss: 7.1988
batch: [18410/21305] batch time: 1.058 trainign loss: 5.1055 avg training loss: 7.1988
batch: [18420/21305] batch time: 0.057 trainign loss: 6.4880 avg training loss: 7.1988
batch: [18430/21305] batch time: 0.063 trainign loss: 6.1957 avg training loss: 7.1987
batch: [18440/21305] batch time: 0.063 trainign loss: 0.0609 avg training loss: 7.1984
batch: [18450/21305] batch time: 0.052 trainign loss: 7.6146 avg training loss: 7.1983
batch: [18460/21305] batch time: 0.056 trainign loss: 6.8822 avg training loss: 7.1983
batch: [18470/21305] batch time: 0.407 trainign loss: 7.0074 avg training loss: 7.1983
batch: [18480/21305] batch time: 0.056 trainign loss: 6.4892 avg training loss: 7.1983
batch: [18490/21305] batch time: 0.053 trainign loss: 3.4777 avg training loss: 7.1981
batch: [18500/21305] batch time: 0.056 trainign loss: 0.0053 avg training loss: 7.1978
batch: [18510/21305] batch time: 0.056 trainign loss: 8.4502 avg training loss: 7.1977
batch: [18520/21305] batch time: 0.056 trainign loss: 8.1010 avg training loss: 7.1978
batch: [18530/21305] batch time: 0.056 trainign loss: 6.3940 avg training loss: 7.1978
batch: [18540/21305] batch time: 0.056 trainign loss: 5.9626 avg training loss: 7.1977
batch: [18550/21305] batch time: 0.050 trainign loss: 4.3190 avg training loss: 7.1976
batch: [18560/21305] batch time: 0.056 trainign loss: 7.4295 avg training loss: 7.1975
batch: [18570/21305] batch time: 0.056 trainign loss: 7.5307 avg training loss: 7.1976
batch: [18580/21305] batch time: 0.169 trainign loss: 6.6658 avg training loss: 7.1975
batch: [18590/21305] batch time: 0.056 trainign loss: 6.7371 avg training loss: 7.1974
batch: [18600/21305] batch time: 0.056 trainign loss: 5.6505 avg training loss: 7.1974
batch: [18610/21305] batch time: 0.054 trainign loss: 6.1675 avg training loss: 7.1972
batch: [18620/21305] batch time: 0.062 trainign loss: 7.9473 avg training loss: 7.1972
batch: [18630/21305] batch time: 0.054 trainign loss: 6.4832 avg training loss: 7.1972
batch: [18640/21305] batch time: 0.062 trainign loss: 5.0269 avg training loss: 7.1972
batch: [18650/21305] batch time: 0.063 trainign loss: 3.4859 avg training loss: 7.1971
batch: [18660/21305] batch time: 0.053 trainign loss: 0.0047 avg training loss: 7.1967
batch: [18670/21305] batch time: 0.675 trainign loss: 7.6327 avg training loss: 7.1966
batch: [18680/21305] batch time: 0.058 trainign loss: 7.1469 avg training loss: 7.1966
batch: [18690/21305] batch time: 0.145 trainign loss: 7.5666 avg training loss: 7.1966
batch: [18700/21305] batch time: 0.057 trainign loss: 6.9723 avg training loss: 7.1966
batch: [18710/21305] batch time: 0.467 trainign loss: 6.8737 avg training loss: 7.1966
batch: [18720/21305] batch time: 0.062 trainign loss: 5.9606 avg training loss: 7.1966
batch: [18730/21305] batch time: 0.387 trainign loss: 5.3711 avg training loss: 7.1965
batch: [18740/21305] batch time: 0.115 trainign loss: 7.4588 avg training loss: 7.1964
batch: [18750/21305] batch time: 0.057 trainign loss: 5.7951 avg training loss: 7.1964
batch: [18760/21305] batch time: 1.044 trainign loss: 4.9065 avg training loss: 7.1963
batch: [18770/21305] batch time: 0.380 trainign loss: 7.6406 avg training loss: 7.1962
batch: [18780/21305] batch time: 0.874 trainign loss: 6.9785 avg training loss: 7.1962
batch: [18790/21305] batch time: 0.054 trainign loss: 5.6387 avg training loss: 7.1961
batch: [18800/21305] batch time: 0.996 trainign loss: 7.1662 avg training loss: 7.1961
batch: [18810/21305] batch time: 0.061 trainign loss: 6.8621 avg training loss: 7.1961
batch: [18820/21305] batch time: 1.480 trainign loss: 4.2297 avg training loss: 7.1960
batch: [18830/21305] batch time: 0.375 trainign loss: 7.5162 avg training loss: 7.1960
batch: [18840/21305] batch time: 1.980 trainign loss: 6.2935 avg training loss: 7.1959
batch: [18850/21305] batch time: 0.062 trainign loss: 7.1819 avg training loss: 7.1958
batch: [18860/21305] batch time: 1.577 trainign loss: 6.8994 avg training loss: 7.1958
batch: [18870/21305] batch time: 0.056 trainign loss: 6.4525 avg training loss: 7.1957
batch: [18880/21305] batch time: 0.296 trainign loss: 4.3141 avg training loss: 7.1956
batch: [18890/21305] batch time: 0.062 trainign loss: 7.2097 avg training loss: 7.1956
batch: [18900/21305] batch time: 0.595 trainign loss: 6.0144 avg training loss: 7.1956
batch: [18910/21305] batch time: 0.062 trainign loss: 6.6578 avg training loss: 7.1955
batch: [18920/21305] batch time: 0.120 trainign loss: 6.8390 avg training loss: 7.1955
batch: [18930/21305] batch time: 0.052 trainign loss: 7.2685 avg training loss: 7.1954
batch: [18940/21305] batch time: 1.494 trainign loss: 6.3764 avg training loss: 7.1954
batch: [18950/21305] batch time: 0.061 trainign loss: 6.8467 avg training loss: 7.1954
batch: [18960/21305] batch time: 1.908 trainign loss: 6.8375 avg training loss: 7.1953
batch: [18970/21305] batch time: 0.055 trainign loss: 6.4983 avg training loss: 7.1952
batch: [18980/21305] batch time: 1.987 trainign loss: 8.4979 avg training loss: 7.1951
batch: [18990/21305] batch time: 0.052 trainign loss: 7.4092 avg training loss: 7.1951
batch: [19000/21305] batch time: 1.276 trainign loss: 5.7694 avg training loss: 7.1950
batch: [19010/21305] batch time: 0.060 trainign loss: 7.2370 avg training loss: 7.1950
batch: [19020/21305] batch time: 1.059 trainign loss: 6.6965 avg training loss: 7.1949
batch: [19030/21305] batch time: 0.591 trainign loss: 7.0802 avg training loss: 7.1949
batch: [19040/21305] batch time: 1.044 trainign loss: 6.6825 avg training loss: 7.1948
batch: [19050/21305] batch time: 1.148 trainign loss: 6.9029 avg training loss: 7.1948
batch: [19060/21305] batch time: 0.262 trainign loss: 5.3859 avg training loss: 7.1946
batch: [19070/21305] batch time: 0.404 trainign loss: 6.5230 avg training loss: 7.1945
batch: [19080/21305] batch time: 0.085 trainign loss: 6.7335 avg training loss: 7.1945
batch: [19090/21305] batch time: 0.063 trainign loss: 7.6775 avg training loss: 7.1945
batch: [19100/21305] batch time: 0.433 trainign loss: 7.1851 avg training loss: 7.1945
batch: [19110/21305] batch time: 0.328 trainign loss: 3.6928 avg training loss: 7.1944
batch: [19120/21305] batch time: 0.056 trainign loss: 6.3195 avg training loss: 7.1944
batch: [19130/21305] batch time: 0.424 trainign loss: 7.1330 avg training loss: 7.1943
batch: [19140/21305] batch time: 0.061 trainign loss: 6.1540 avg training loss: 7.1943
batch: [19150/21305] batch time: 0.053 trainign loss: 6.3569 avg training loss: 7.1942
batch: [19160/21305] batch time: 0.276 trainign loss: 6.5145 avg training loss: 7.1942
batch: [19170/21305] batch time: 0.835 trainign loss: 6.3327 avg training loss: 7.1941
batch: [19180/21305] batch time: 0.057 trainign loss: 4.8011 avg training loss: 7.1941
batch: [19190/21305] batch time: 0.877 trainign loss: 6.4433 avg training loss: 7.1940
batch: [19200/21305] batch time: 0.527 trainign loss: 3.5259 avg training loss: 7.1939
batch: [19210/21305] batch time: 1.781 trainign loss: 6.8014 avg training loss: 7.1938
batch: [19220/21305] batch time: 0.599 trainign loss: 7.2256 avg training loss: 7.1938
batch: [19230/21305] batch time: 1.724 trainign loss: 6.3535 avg training loss: 7.1938
batch: [19240/21305] batch time: 0.403 trainign loss: 5.9150 avg training loss: 7.1937
batch: [19250/21305] batch time: 1.472 trainign loss: 3.1122 avg training loss: 7.1936
batch: [19260/21305] batch time: 0.062 trainign loss: 5.2880 avg training loss: 7.1935
batch: [19270/21305] batch time: 2.082 trainign loss: 7.2631 avg training loss: 7.1934
batch: [19280/21305] batch time: 0.056 trainign loss: 8.1316 avg training loss: 7.1934
batch: [19290/21305] batch time: 2.121 trainign loss: 7.0272 avg training loss: 7.1934
batch: [19300/21305] batch time: 0.062 trainign loss: 7.0277 avg training loss: 7.1934
batch: [19310/21305] batch time: 2.157 trainign loss: 7.1806 avg training loss: 7.1934
batch: [19320/21305] batch time: 0.057 trainign loss: 7.3289 avg training loss: 7.1933
batch: [19330/21305] batch time: 1.946 trainign loss: 6.5506 avg training loss: 7.1933
batch: [19340/21305] batch time: 0.056 trainign loss: 5.1189 avg training loss: 7.1932
batch: [19350/21305] batch time: 1.735 trainign loss: 6.8140 avg training loss: 7.1932
batch: [19360/21305] batch time: 0.363 trainign loss: 5.9214 avg training loss: 7.1932
batch: [19370/21305] batch time: 0.503 trainign loss: 6.4046 avg training loss: 7.1931
batch: [19380/21305] batch time: 1.578 trainign loss: 6.1350 avg training loss: 7.1930
batch: [19390/21305] batch time: 0.169 trainign loss: 7.6247 avg training loss: 7.1930
batch: [19400/21305] batch time: 0.966 trainign loss: 6.9049 avg training loss: 7.1930
batch: [19410/21305] batch time: 1.246 trainign loss: 5.0085 avg training loss: 7.1929
batch: [19420/21305] batch time: 0.463 trainign loss: 6.4371 avg training loss: 7.1928
batch: [19430/21305] batch time: 1.224 trainign loss: 6.5511 avg training loss: 7.1928
batch: [19440/21305] batch time: 0.440 trainign loss: 6.9960 avg training loss: 7.1927
batch: [19450/21305] batch time: 1.438 trainign loss: 6.7427 avg training loss: 7.1927
batch: [19460/21305] batch time: 0.057 trainign loss: 6.2990 avg training loss: 7.1927
batch: [19470/21305] batch time: 0.822 trainign loss: 6.9768 avg training loss: 7.1927
batch: [19480/21305] batch time: 0.216 trainign loss: 5.8610 avg training loss: 7.1926
batch: [19490/21305] batch time: 0.254 trainign loss: 5.0532 avg training loss: 7.1925
batch: [19500/21305] batch time: 0.056 trainign loss: 6.7202 avg training loss: 7.1925
batch: [19510/21305] batch time: 0.061 trainign loss: 6.0478 avg training loss: 7.1925
batch: [19520/21305] batch time: 0.295 trainign loss: 6.8726 avg training loss: 7.1925
batch: [19530/21305] batch time: 0.056 trainign loss: 6.7752 avg training loss: 7.1924
batch: [19540/21305] batch time: 0.424 trainign loss: 6.2441 avg training loss: 7.1923
batch: [19550/21305] batch time: 0.057 trainign loss: 6.0766 avg training loss: 7.1922
batch: [19560/21305] batch time: 0.436 trainign loss: 7.0980 avg training loss: 7.1922
batch: [19570/21305] batch time: 0.063 trainign loss: 6.8262 avg training loss: 7.1921
batch: [19580/21305] batch time: 0.933 trainign loss: 5.4235 avg training loss: 7.1921
batch: [19590/21305] batch time: 0.056 trainign loss: 5.5228 avg training loss: 7.1919
batch: [19600/21305] batch time: 1.257 trainign loss: 6.2804 avg training loss: 7.1919
batch: [19610/21305] batch time: 0.056 trainign loss: 6.4467 avg training loss: 7.1919
batch: [19620/21305] batch time: 0.480 trainign loss: 6.6463 avg training loss: 7.1918
batch: [19630/21305] batch time: 0.051 trainign loss: 6.7554 avg training loss: 7.1918
batch: [19640/21305] batch time: 0.056 trainign loss: 7.1679 avg training loss: 7.1918
batch: [19650/21305] batch time: 0.062 trainign loss: 6.3505 avg training loss: 7.1917
batch: [19660/21305] batch time: 1.093 trainign loss: 6.1748 avg training loss: 7.1916
batch: [19670/21305] batch time: 0.059 trainign loss: 6.2458 avg training loss: 7.1915
batch: [19680/21305] batch time: 0.519 trainign loss: 0.0381 avg training loss: 7.1912
batch: [19690/21305] batch time: 0.063 trainign loss: 7.3818 avg training loss: 7.1913
batch: [19700/21305] batch time: 1.036 trainign loss: 7.5908 avg training loss: 7.1913
batch: [19710/21305] batch time: 0.057 trainign loss: 6.3587 avg training loss: 7.1912
batch: [19720/21305] batch time: 1.241 trainign loss: 6.1166 avg training loss: 7.1912
batch: [19730/21305] batch time: 0.062 trainign loss: 5.7461 avg training loss: 7.1911
batch: [19740/21305] batch time: 1.160 trainign loss: 5.5114 avg training loss: 7.1911
batch: [19750/21305] batch time: 0.058 trainign loss: 7.2086 avg training loss: 7.1910
batch: [19760/21305] batch time: 0.057 trainign loss: 6.5027 avg training loss: 7.1910
batch: [19770/21305] batch time: 0.056 trainign loss: 5.8928 avg training loss: 7.1909
batch: [19780/21305] batch time: 0.056 trainign loss: 4.8939 avg training loss: 7.1909
batch: [19790/21305] batch time: 0.056 trainign loss: 6.0678 avg training loss: 7.1908
batch: [19800/21305] batch time: 0.060 trainign loss: 5.7583 avg training loss: 7.1907
batch: [19810/21305] batch time: 0.056 trainign loss: 5.2906 avg training loss: 7.1906
batch: [19820/21305] batch time: 0.062 trainign loss: 6.7487 avg training loss: 7.1905
batch: [19830/21305] batch time: 0.056 trainign loss: 7.1201 avg training loss: 7.1905
batch: [19840/21305] batch time: 0.158 trainign loss: 6.2854 avg training loss: 7.1905
batch: [19850/21305] batch time: 0.051 trainign loss: 6.2944 avg training loss: 7.1904
batch: [19860/21305] batch time: 0.057 trainign loss: 7.3416 avg training loss: 7.1904
batch: [19870/21305] batch time: 0.050 trainign loss: 6.8233 avg training loss: 7.1903
batch: [19880/21305] batch time: 0.056 trainign loss: 5.0425 avg training loss: 7.1903
batch: [19890/21305] batch time: 0.058 trainign loss: 4.5424 avg training loss: 7.1902
batch: [19900/21305] batch time: 0.051 trainign loss: 8.7747 avg training loss: 7.1900
batch: [19910/21305] batch time: 0.053 trainign loss: 1.5937 avg training loss: 7.1897
batch: [19920/21305] batch time: 0.056 trainign loss: 7.2505 avg training loss: 7.1898
batch: [19930/21305] batch time: 0.056 trainign loss: 7.7092 avg training loss: 7.1898
batch: [19940/21305] batch time: 0.062 trainign loss: 5.9187 avg training loss: 7.1898
batch: [19950/21305] batch time: 0.058 trainign loss: 6.3776 avg training loss: 7.1897
batch: [19960/21305] batch time: 0.057 trainign loss: 7.1178 avg training loss: 7.1896
batch: [19970/21305] batch time: 1.049 trainign loss: 7.0290 avg training loss: 7.1895
batch: [19980/21305] batch time: 0.058 trainign loss: 7.5050 avg training loss: 7.1895
batch: [19990/21305] batch time: 0.762 trainign loss: 6.1031 avg training loss: 7.1895
batch: [20000/21305] batch time: 0.056 trainign loss: 6.5461 avg training loss: 7.1894
batch: [20010/21305] batch time: 0.052 trainign loss: 5.7663 avg training loss: 7.1894
batch: [20020/21305] batch time: 0.057 trainign loss: 1.0721 avg training loss: 7.1892
batch: [20030/21305] batch time: 0.057 trainign loss: 6.1947 avg training loss: 7.1891
batch: [20040/21305] batch time: 0.056 trainign loss: 5.3120 avg training loss: 7.1890
batch: [20050/21305] batch time: 0.056 trainign loss: 2.0673 avg training loss: 7.1888
batch: [20060/21305] batch time: 0.056 trainign loss: 4.4571 avg training loss: 7.1887
batch: [20070/21305] batch time: 0.053 trainign loss: 6.9478 avg training loss: 7.1887
batch: [20080/21305] batch time: 0.057 trainign loss: 6.6976 avg training loss: 7.1887
batch: [20090/21305] batch time: 0.053 trainign loss: 4.4708 avg training loss: 7.1886
batch: [20100/21305] batch time: 0.056 trainign loss: 10.1322 avg training loss: 7.1884
batch: [20110/21305] batch time: 0.063 trainign loss: 6.8236 avg training loss: 7.1884
batch: [20120/21305] batch time: 1.234 trainign loss: 7.6363 avg training loss: 7.1884
batch: [20130/21305] batch time: 0.056 trainign loss: 6.6573 avg training loss: 7.1884
batch: [20140/21305] batch time: 0.759 trainign loss: 6.7917 avg training loss: 7.1884
batch: [20150/21305] batch time: 0.054 trainign loss: 5.9304 avg training loss: 7.1884
batch: [20160/21305] batch time: 0.448 trainign loss: 7.0655 avg training loss: 7.1883
batch: [20170/21305] batch time: 0.053 trainign loss: 7.4101 avg training loss: 7.1882
batch: [20180/21305] batch time: 0.516 trainign loss: 6.7179 avg training loss: 7.1882
batch: [20190/21305] batch time: 0.055 trainign loss: 3.8710 avg training loss: 7.1882
batch: [20200/21305] batch time: 0.056 trainign loss: 4.8039 avg training loss: 7.1881
batch: [20210/21305] batch time: 0.058 trainign loss: 7.5665 avg training loss: 7.1880
batch: [20220/21305] batch time: 0.056 trainign loss: 7.1171 avg training loss: 7.1880
batch: [20230/21305] batch time: 0.055 trainign loss: 6.6286 avg training loss: 7.1880
batch: [20240/21305] batch time: 0.057 trainign loss: 5.1799 avg training loss: 7.1879
batch: [20250/21305] batch time: 0.051 trainign loss: 7.1911 avg training loss: 7.1878
batch: [20260/21305] batch time: 0.062 trainign loss: 6.6574 avg training loss: 7.1878
batch: [20270/21305] batch time: 0.051 trainign loss: 1.2057 avg training loss: 7.1876
batch: [20280/21305] batch time: 0.056 trainign loss: 6.8941 avg training loss: 7.1875
batch: [20290/21305] batch time: 0.054 trainign loss: 7.0114 avg training loss: 7.1875
batch: [20300/21305] batch time: 0.061 trainign loss: 7.1544 avg training loss: 7.1875
batch: [20310/21305] batch time: 0.053 trainign loss: 6.6163 avg training loss: 7.1874
batch: [20320/21305] batch time: 0.062 trainign loss: 6.2668 avg training loss: 7.1874
batch: [20330/21305] batch time: 0.052 trainign loss: 4.8475 avg training loss: 7.1873
batch: [20340/21305] batch time: 0.056 trainign loss: 6.1015 avg training loss: 7.1872
batch: [20350/21305] batch time: 0.051 trainign loss: 6.8524 avg training loss: 7.1872
batch: [20360/21305] batch time: 0.056 trainign loss: 6.8409 avg training loss: 7.1871
batch: [20370/21305] batch time: 0.056 trainign loss: 6.5897 avg training loss: 7.1870
batch: [20380/21305] batch time: 0.063 trainign loss: 6.0628 avg training loss: 7.1870
batch: [20390/21305] batch time: 0.056 trainign loss: 7.0197 avg training loss: 7.1869
batch: [20400/21305] batch time: 0.062 trainign loss: 6.4001 avg training loss: 7.1869
batch: [20410/21305] batch time: 0.052 trainign loss: 6.5029 avg training loss: 7.1868
batch: [20420/21305] batch time: 0.061 trainign loss: 5.8056 avg training loss: 7.1868
batch: [20430/21305] batch time: 0.052 trainign loss: 7.1033 avg training loss: 7.1866
batch: [20440/21305] batch time: 0.056 trainign loss: 6.1996 avg training loss: 7.1865
batch: [20450/21305] batch time: 0.057 trainign loss: 5.1317 avg training loss: 7.1865
batch: [20460/21305] batch time: 0.055 trainign loss: 6.9750 avg training loss: 7.1865
batch: [20470/21305] batch time: 0.051 trainign loss: 5.1080 avg training loss: 7.1864
batch: [20480/21305] batch time: 0.056 trainign loss: 0.0517 avg training loss: 7.1861
batch: [20490/21305] batch time: 0.056 trainign loss: 13.3630 avg training loss: 7.1858
batch: [20500/21305] batch time: 0.062 trainign loss: 7.9844 avg training loss: 7.1859
batch: [20510/21305] batch time: 0.051 trainign loss: 7.2108 avg training loss: 7.1859
batch: [20520/21305] batch time: 0.056 trainign loss: 6.4842 avg training loss: 7.1859
batch: [20530/21305] batch time: 0.055 trainign loss: 5.8686 avg training loss: 7.1858
batch: [20540/21305] batch time: 0.056 trainign loss: 7.9567 avg training loss: 7.1857
batch: [20550/21305] batch time: 0.051 trainign loss: 7.0124 avg training loss: 7.1856
batch: [20560/21305] batch time: 0.055 trainign loss: 6.3990 avg training loss: 7.1855
batch: [20570/21305] batch time: 0.055 trainign loss: 7.4308 avg training loss: 7.1855
batch: [20580/21305] batch time: 0.058 trainign loss: 6.6287 avg training loss: 7.1855
batch: [20590/21305] batch time: 0.060 trainign loss: 6.0229 avg training loss: 7.1855
batch: [20600/21305] batch time: 0.063 trainign loss: 6.3423 avg training loss: 7.1854
batch: [20610/21305] batch time: 0.057 trainign loss: 7.1389 avg training loss: 7.1854
batch: [20620/21305] batch time: 0.063 trainign loss: 6.9117 avg training loss: 7.1854
batch: [20630/21305] batch time: 0.056 trainign loss: 6.5646 avg training loss: 7.1853
batch: [20640/21305] batch time: 0.061 trainign loss: 6.6236 avg training loss: 7.1853
batch: [20650/21305] batch time: 0.051 trainign loss: 6.6536 avg training loss: 7.1853
batch: [20660/21305] batch time: 0.058 trainign loss: 5.5947 avg training loss: 7.1852
batch: [20670/21305] batch time: 0.052 trainign loss: 7.1973 avg training loss: 7.1851
batch: [20680/21305] batch time: 0.056 trainign loss: 5.2831 avg training loss: 7.1851
batch: [20690/21305] batch time: 0.053 trainign loss: 6.5684 avg training loss: 7.1850
batch: [20700/21305] batch time: 0.062 trainign loss: 6.8501 avg training loss: 7.1850
batch: [20710/21305] batch time: 0.056 trainign loss: 6.6232 avg training loss: 7.1849
batch: [20720/21305] batch time: 0.056 trainign loss: 6.8432 avg training loss: 7.1849
batch: [20730/21305] batch time: 0.053 trainign loss: 6.5174 avg training loss: 7.1849
batch: [20740/21305] batch time: 0.062 trainign loss: 6.5010 avg training loss: 7.1848
batch: [20750/21305] batch time: 0.056 trainign loss: 5.4908 avg training loss: 7.1848
batch: [20760/21305] batch time: 0.062 trainign loss: 7.3137 avg training loss: 7.1848
batch: [20770/21305] batch time: 0.051 trainign loss: 6.6618 avg training loss: 7.1848
batch: [20780/21305] batch time: 0.062 trainign loss: 6.9641 avg training loss: 7.1847
batch: [20790/21305] batch time: 0.054 trainign loss: 6.1827 avg training loss: 7.1847
batch: [20800/21305] batch time: 0.275 trainign loss: 6.0830 avg training loss: 7.1846
batch: [20810/21305] batch time: 0.062 trainign loss: 7.2474 avg training loss: 7.1846
batch: [20820/21305] batch time: 0.061 trainign loss: 6.2157 avg training loss: 7.1846
batch: [20830/21305] batch time: 0.054 trainign loss: 6.2777 avg training loss: 7.1846
batch: [20840/21305] batch time: 0.077 trainign loss: 5.5171 avg training loss: 7.1845
batch: [20850/21305] batch time: 0.062 trainign loss: 5.6476 avg training loss: 7.1844
batch: [20860/21305] batch time: 1.317 trainign loss: 6.3359 avg training loss: 7.1843
batch: [20870/21305] batch time: 0.056 trainign loss: 6.5148 avg training loss: 7.1843
batch: [20880/21305] batch time: 1.117 trainign loss: 5.7684 avg training loss: 7.1842
batch: [20890/21305] batch time: 0.055 trainign loss: 7.0306 avg training loss: 7.1842
batch: [20900/21305] batch time: 1.351 trainign loss: 4.2327 avg training loss: 7.1841
batch: [20910/21305] batch time: 0.056 trainign loss: 6.2716 avg training loss: 7.1840
batch: [20920/21305] batch time: 0.351 trainign loss: 4.0163 avg training loss: 7.1839
batch: [20930/21305] batch time: 0.062 trainign loss: 7.9537 avg training loss: 7.1838
batch: [20940/21305] batch time: 1.086 trainign loss: 1.9155 avg training loss: 7.1836
batch: [20950/21305] batch time: 0.056 trainign loss: 7.1609 avg training loss: 7.1836
batch: [20960/21305] batch time: 0.539 trainign loss: 7.1187 avg training loss: 7.1837
batch: [20970/21305] batch time: 0.056 trainign loss: 5.4486 avg training loss: 7.1836
batch: [20980/21305] batch time: 0.494 trainign loss: 7.1668 avg training loss: 7.1836
batch: [20990/21305] batch time: 0.057 trainign loss: 7.3818 avg training loss: 7.1836
batch: [21000/21305] batch time: 0.651 trainign loss: 6.3189 avg training loss: 7.1836
batch: [21010/21305] batch time: 0.970 trainign loss: 6.6839 avg training loss: 7.1835
batch: [21020/21305] batch time: 0.056 trainign loss: 6.1898 avg training loss: 7.1835
batch: [21030/21305] batch time: 1.055 trainign loss: 6.8662 avg training loss: 7.1834
batch: [21040/21305] batch time: 0.063 trainign loss: 7.2211 avg training loss: 7.1834
batch: [21050/21305] batch time: 0.406 trainign loss: 6.2491 avg training loss: 7.1833
batch: [21060/21305] batch time: 0.057 trainign loss: 6.2512 avg training loss: 7.1833
batch: [21070/21305] batch time: 0.373 trainign loss: 5.8867 avg training loss: 7.1832
batch: [21080/21305] batch time: 0.062 trainign loss: 6.3483 avg training loss: 7.1831
batch: [21090/21305] batch time: 0.056 trainign loss: 7.1704 avg training loss: 7.1831
batch: [21100/21305] batch time: 0.062 trainign loss: 5.0562 avg training loss: 7.1830
batch: [21110/21305] batch time: 0.054 trainign loss: 7.4346 avg training loss: 7.1829
batch: [21120/21305] batch time: 0.056 trainign loss: 7.5104 avg training loss: 7.1829
batch: [21130/21305] batch time: 0.051 trainign loss: 7.4308 avg training loss: 7.1829
batch: [21140/21305] batch time: 0.056 trainign loss: 6.7098 avg training loss: 7.1828
batch: [21150/21305] batch time: 0.453 trainign loss: 6.7076 avg training loss: 7.1828
batch: [21160/21305] batch time: 0.056 trainign loss: 6.9882 avg training loss: 7.1827
batch: [21170/21305] batch time: 0.489 trainign loss: 4.7599 avg training loss: 7.1825
batch: [21180/21305] batch time: 0.058 trainign loss: 7.4121 avg training loss: 7.1826
batch: [21190/21305] batch time: 0.052 trainign loss: 7.0781 avg training loss: 7.1826
batch: [21200/21305] batch time: 0.056 trainign loss: 6.3013 avg training loss: 7.1825
batch: [21210/21305] batch time: 0.056 trainign loss: 6.4145 avg training loss: 7.1825
batch: [21220/21305] batch time: 0.057 trainign loss: 5.6712 avg training loss: 7.1824
batch: [21230/21305] batch time: 0.061 trainign loss: 6.6035 avg training loss: 7.1824
batch: [21240/21305] batch time: 0.063 trainign loss: 5.2374 avg training loss: 7.1823
batch: [21250/21305] batch time: 0.051 trainign loss: 6.9200 avg training loss: 7.1823
batch: [21260/21305] batch time: 0.063 trainign loss: 7.0401 avg training loss: 7.1822
batch: [21270/21305] batch time: 0.060 trainign loss: 3.4056 avg training loss: 7.1822
batch: [21280/21305] batch time: 0.056 trainign loss: 5.8426 avg training loss: 7.1821
batch: [21290/21305] batch time: 0.056 trainign loss: 4.6251 avg training loss: 7.1819
batch: [21300/21305] batch time: 0.056 trainign loss: 7.0741 avg training loss: 7.1819
Epoch: 9
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.317 trainign loss: 6.6264 avg training loss: 7.1819
batch: [10/21305] batch time: 0.056 trainign loss: 8.2476 avg training loss: 7.1817
batch: [20/21305] batch time: 2.457 trainign loss: 7.4035 avg training loss: 7.1817
batch: [30/21305] batch time: 0.056 trainign loss: 6.7419 avg training loss: 7.1817
batch: [40/21305] batch time: 2.226 trainign loss: 7.0744 avg training loss: 7.1816
batch: [50/21305] batch time: 0.056 trainign loss: 3.1988 avg training loss: 7.1815
batch: [60/21305] batch time: 2.091 trainign loss: 7.2676 avg training loss: 7.1815
batch: [70/21305] batch time: 0.054 trainign loss: 7.2258 avg training loss: 7.1814
batch: [80/21305] batch time: 2.477 trainign loss: 6.9399 avg training loss: 7.1814
batch: [90/21305] batch time: 0.055 trainign loss: 6.8949 avg training loss: 7.1814
batch: [100/21305] batch time: 2.369 trainign loss: 6.9674 avg training loss: 7.1813
batch: [110/21305] batch time: 0.054 trainign loss: 6.3536 avg training loss: 7.1813
batch: [120/21305] batch time: 2.352 trainign loss: 6.7672 avg training loss: 7.1812
batch: [130/21305] batch time: 0.052 trainign loss: 7.1694 avg training loss: 7.1812
batch: [140/21305] batch time: 2.191 trainign loss: 5.1400 avg training loss: 7.1811
batch: [150/21305] batch time: 0.063 trainign loss: 7.7020 avg training loss: 7.1810
batch: [160/21305] batch time: 1.938 trainign loss: 6.9306 avg training loss: 7.1810
batch: [170/21305] batch time: 0.355 trainign loss: 6.2827 avg training loss: 7.1810
batch: [180/21305] batch time: 2.174 trainign loss: 6.0350 avg training loss: 7.1809
batch: [190/21305] batch time: 0.063 trainign loss: 5.2745 avg training loss: 7.1808
batch: [200/21305] batch time: 1.923 trainign loss: 0.0102 avg training loss: 7.1805
batch: [210/21305] batch time: 0.060 trainign loss: 7.9494 avg training loss: 7.1804
batch: [220/21305] batch time: 1.795 trainign loss: 7.3208 avg training loss: 7.1805
batch: [230/21305] batch time: 0.100 trainign loss: 5.7379 avg training loss: 7.1804
batch: [240/21305] batch time: 2.095 trainign loss: 7.2859 avg training loss: 7.1804
batch: [250/21305] batch time: 1.004 trainign loss: 7.3414 avg training loss: 7.1804
batch: [260/21305] batch time: 1.438 trainign loss: 6.7534 avg training loss: 7.1803
batch: [270/21305] batch time: 0.825 trainign loss: 4.6279 avg training loss: 7.1803
batch: [280/21305] batch time: 1.421 trainign loss: 6.4613 avg training loss: 7.1803
batch: [290/21305] batch time: 0.471 trainign loss: 6.8276 avg training loss: 7.1802
batch: [300/21305] batch time: 1.509 trainign loss: 6.5687 avg training loss: 7.1801
batch: [310/21305] batch time: 1.434 trainign loss: 5.8161 avg training loss: 7.1801
batch: [320/21305] batch time: 0.627 trainign loss: 6.7831 avg training loss: 7.1800
batch: [330/21305] batch time: 1.393 trainign loss: 6.5351 avg training loss: 7.1800
batch: [340/21305] batch time: 0.716 trainign loss: 7.4349 avg training loss: 7.1799
batch: [350/21305] batch time: 1.904 trainign loss: 7.7375 avg training loss: 7.1799
batch: [360/21305] batch time: 1.131 trainign loss: 7.1930 avg training loss: 7.1799
batch: [370/21305] batch time: 1.655 trainign loss: 5.9062 avg training loss: 7.1798
batch: [380/21305] batch time: 0.813 trainign loss: 6.4122 avg training loss: 7.1798
batch: [390/21305] batch time: 1.628 trainign loss: 3.9079 avg training loss: 7.1797
batch: [400/21305] batch time: 0.986 trainign loss: 6.5417 avg training loss: 7.1796
batch: [410/21305] batch time: 0.945 trainign loss: 7.3356 avg training loss: 7.1796
batch: [420/21305] batch time: 1.841 trainign loss: 7.0460 avg training loss: 7.1795
batch: [430/21305] batch time: 0.850 trainign loss: 7.2175 avg training loss: 7.1796
batch: [440/21305] batch time: 1.690 trainign loss: 6.1572 avg training loss: 7.1795
batch: [450/21305] batch time: 0.347 trainign loss: 5.7033 avg training loss: 7.1794
batch: [460/21305] batch time: 2.573 trainign loss: 7.3010 avg training loss: 7.1794
batch: [470/21305] batch time: 0.056 trainign loss: 6.1390 avg training loss: 7.1794
batch: [480/21305] batch time: 2.352 trainign loss: 7.3143 avg training loss: 7.1793
batch: [490/21305] batch time: 0.578 trainign loss: 6.8645 avg training loss: 7.1793
batch: [500/21305] batch time: 0.987 trainign loss: 3.7953 avg training loss: 7.1793
batch: [510/21305] batch time: 1.081 trainign loss: 5.9920 avg training loss: 7.1793
batch: [520/21305] batch time: 0.879 trainign loss: 3.9704 avg training loss: 7.1792
batch: [530/21305] batch time: 1.047 trainign loss: 6.4127 avg training loss: 7.1791
batch: [540/21305] batch time: 1.336 trainign loss: 6.2911 avg training loss: 7.1791
batch: [550/21305] batch time: 0.845 trainign loss: 4.6137 avg training loss: 7.1789
batch: [560/21305] batch time: 1.838 trainign loss: 6.2046 avg training loss: 7.1790
batch: [570/21305] batch time: 0.150 trainign loss: 7.0330 avg training loss: 7.1789
batch: [580/21305] batch time: 1.744 trainign loss: 6.6978 avg training loss: 7.1788
batch: [590/21305] batch time: 0.492 trainign loss: 7.0774 avg training loss: 7.1788
batch: [600/21305] batch time: 2.211 trainign loss: 6.4395 avg training loss: 7.1788
batch: [610/21305] batch time: 0.299 trainign loss: 7.4800 avg training loss: 7.1788
batch: [620/21305] batch time: 2.032 trainign loss: 6.8153 avg training loss: 7.1788
batch: [630/21305] batch time: 0.640 trainign loss: 6.4697 avg training loss: 7.1787
batch: [640/21305] batch time: 1.535 trainign loss: 7.0672 avg training loss: 7.1787
batch: [650/21305] batch time: 0.855 trainign loss: 6.1917 avg training loss: 7.1787
batch: [660/21305] batch time: 0.518 trainign loss: 5.7700 avg training loss: 7.1786
batch: [670/21305] batch time: 2.236 trainign loss: 8.2475 avg training loss: 7.1785
batch: [680/21305] batch time: 0.053 trainign loss: 7.9841 avg training loss: 7.1785
batch: [690/21305] batch time: 1.928 trainign loss: 6.2670 avg training loss: 7.1785
batch: [700/21305] batch time: 0.088 trainign loss: 7.4849 avg training loss: 7.1784
batch: [710/21305] batch time: 0.848 trainign loss: 6.4538 avg training loss: 7.1783
batch: [720/21305] batch time: 1.137 trainign loss: 6.6059 avg training loss: 7.1783
batch: [730/21305] batch time: 0.228 trainign loss: 7.5542 avg training loss: 7.1783
batch: [740/21305] batch time: 1.100 trainign loss: 7.6777 avg training loss: 7.1783
batch: [750/21305] batch time: 0.056 trainign loss: 7.0365 avg training loss: 7.1782
batch: [760/21305] batch time: 0.213 trainign loss: 6.5131 avg training loss: 7.1782
batch: [770/21305] batch time: 0.058 trainign loss: 4.8481 avg training loss: 7.1781
batch: [780/21305] batch time: 0.541 trainign loss: 7.7330 avg training loss: 7.1781
batch: [790/21305] batch time: 0.056 trainign loss: 7.1389 avg training loss: 7.1781
batch: [800/21305] batch time: 0.051 trainign loss: 7.2234 avg training loss: 7.1781
batch: [810/21305] batch time: 0.056 trainign loss: 6.6512 avg training loss: 7.1781
batch: [820/21305] batch time: 0.056 trainign loss: 7.0355 avg training loss: 7.1781
batch: [830/21305] batch time: 0.059 trainign loss: 6.5998 avg training loss: 7.1780
batch: [840/21305] batch time: 0.056 trainign loss: 7.0270 avg training loss: 7.1780
batch: [850/21305] batch time: 0.056 trainign loss: 5.5088 avg training loss: 7.1779
batch: [860/21305] batch time: 0.053 trainign loss: 5.6233 avg training loss: 7.1779
batch: [870/21305] batch time: 0.056 trainign loss: 5.2695 avg training loss: 7.1778
batch: [880/21305] batch time: 0.405 trainign loss: 6.9663 avg training loss: 7.1777
batch: [890/21305] batch time: 0.056 trainign loss: 8.2451 avg training loss: 7.1777
batch: [900/21305] batch time: 0.851 trainign loss: 6.5251 avg training loss: 7.1777
batch: [910/21305] batch time: 0.056 trainign loss: 6.6258 avg training loss: 7.1777
batch: [920/21305] batch time: 0.056 trainign loss: 4.0663 avg training loss: 7.1776
batch: [930/21305] batch time: 0.058 trainign loss: 7.0512 avg training loss: 7.1775
batch: [940/21305] batch time: 0.062 trainign loss: 4.2737 avg training loss: 7.1775
batch: [950/21305] batch time: 0.056 trainign loss: 0.0037 avg training loss: 7.1771
batch: [960/21305] batch time: 0.051 trainign loss: 9.1072 avg training loss: 7.1772
batch: [970/21305] batch time: 0.055 trainign loss: 7.8711 avg training loss: 7.1772
batch: [980/21305] batch time: 0.056 trainign loss: 7.1248 avg training loss: 7.1772
batch: [990/21305] batch time: 0.057 trainign loss: 6.7254 avg training loss: 7.1772
batch: [1000/21305] batch time: 0.056 trainign loss: 4.7276 avg training loss: 7.1771
batch: [1010/21305] batch time: 0.060 trainign loss: 7.2912 avg training loss: 7.1771
batch: [1020/21305] batch time: 0.056 trainign loss: 7.8880 avg training loss: 7.1771
batch: [1030/21305] batch time: 0.062 trainign loss: 7.0016 avg training loss: 7.1770
batch: [1040/21305] batch time: 0.056 trainign loss: 7.2996 avg training loss: 7.1770
batch: [1050/21305] batch time: 0.056 trainign loss: 6.3851 avg training loss: 7.1770
batch: [1060/21305] batch time: 0.051 trainign loss: 5.6835 avg training loss: 7.1769
batch: [1070/21305] batch time: 0.056 trainign loss: 4.7638 avg training loss: 7.1768
batch: [1080/21305] batch time: 0.051 trainign loss: 0.0534 avg training loss: 7.1766
batch: [1090/21305] batch time: 0.056 trainign loss: 7.8570 avg training loss: 7.1764
batch: [1100/21305] batch time: 0.056 trainign loss: 6.2439 avg training loss: 7.1764
batch: [1110/21305] batch time: 0.059 trainign loss: 6.6766 avg training loss: 7.1764
batch: [1120/21305] batch time: 0.063 trainign loss: 7.6341 avg training loss: 7.1764
batch: [1130/21305] batch time: 0.061 trainign loss: 6.4771 avg training loss: 7.1764
batch: [1140/21305] batch time: 0.052 trainign loss: 7.3842 avg training loss: 7.1764
batch: [1150/21305] batch time: 0.192 trainign loss: 7.0146 avg training loss: 7.1763
batch: [1160/21305] batch time: 0.057 trainign loss: 7.1439 avg training loss: 7.1763
batch: [1170/21305] batch time: 1.081 trainign loss: 6.8945 avg training loss: 7.1763
batch: [1180/21305] batch time: 0.054 trainign loss: 6.2838 avg training loss: 7.1762
batch: [1190/21305] batch time: 1.221 trainign loss: 7.2217 avg training loss: 7.1762
batch: [1200/21305] batch time: 0.056 trainign loss: 5.1447 avg training loss: 7.1761
batch: [1210/21305] batch time: 1.250 trainign loss: 6.8437 avg training loss: 7.1760
batch: [1220/21305] batch time: 0.061 trainign loss: 7.5980 avg training loss: 7.1760
batch: [1230/21305] batch time: 1.112 trainign loss: 6.3738 avg training loss: 7.1760
batch: [1240/21305] batch time: 0.057 trainign loss: 6.4110 avg training loss: 7.1759
batch: [1250/21305] batch time: 0.704 trainign loss: 7.0010 avg training loss: 7.1759
batch: [1260/21305] batch time: 0.058 trainign loss: 5.8380 avg training loss: 7.1758
batch: [1270/21305] batch time: 0.372 trainign loss: 6.6200 avg training loss: 7.1758
batch: [1280/21305] batch time: 0.057 trainign loss: 7.0239 avg training loss: 7.1758
batch: [1290/21305] batch time: 0.140 trainign loss: 6.2677 avg training loss: 7.1757
batch: [1300/21305] batch time: 0.061 trainign loss: 6.5277 avg training loss: 7.1757
batch: [1310/21305] batch time: 0.080 trainign loss: 6.9884 avg training loss: 7.1757
batch: [1320/21305] batch time: 0.051 trainign loss: 5.9317 avg training loss: 7.1756
batch: [1330/21305] batch time: 0.062 trainign loss: 6.9239 avg training loss: 7.1756
batch: [1340/21305] batch time: 0.052 trainign loss: 7.1187 avg training loss: 7.1756
batch: [1350/21305] batch time: 0.373 trainign loss: 6.8434 avg training loss: 7.1755
batch: [1360/21305] batch time: 0.056 trainign loss: 4.3756 avg training loss: 7.1755
batch: [1370/21305] batch time: 0.425 trainign loss: 7.5957 avg training loss: 7.1754
batch: [1380/21305] batch time: 0.061 trainign loss: 5.8856 avg training loss: 7.1753
batch: [1390/21305] batch time: 0.056 trainign loss: 6.8601 avg training loss: 7.1753
batch: [1400/21305] batch time: 0.052 trainign loss: 6.2613 avg training loss: 7.1752
batch: [1410/21305] batch time: 0.058 trainign loss: 6.9071 avg training loss: 7.1751
batch: [1420/21305] batch time: 0.057 trainign loss: 6.2072 avg training loss: 7.1750
batch: [1430/21305] batch time: 0.056 trainign loss: 6.2901 avg training loss: 7.1750
batch: [1440/21305] batch time: 0.053 trainign loss: 5.4079 avg training loss: 7.1750
batch: [1450/21305] batch time: 0.057 trainign loss: 6.3307 avg training loss: 7.1749
batch: [1460/21305] batch time: 0.262 trainign loss: 6.5094 avg training loss: 7.1749
batch: [1470/21305] batch time: 0.206 trainign loss: 5.8018 avg training loss: 7.1748
batch: [1480/21305] batch time: 0.188 trainign loss: 7.0549 avg training loss: 7.1747
batch: [1490/21305] batch time: 0.109 trainign loss: 7.0793 avg training loss: 7.1747
batch: [1500/21305] batch time: 0.537 trainign loss: 6.5441 avg training loss: 7.1746
batch: [1510/21305] batch time: 0.056 trainign loss: 6.7387 avg training loss: 7.1746
batch: [1520/21305] batch time: 0.063 trainign loss: 2.3586 avg training loss: 7.1745
batch: [1530/21305] batch time: 0.058 trainign loss: 11.4833 avg training loss: 7.1742
batch: [1540/21305] batch time: 0.059 trainign loss: 6.9126 avg training loss: 7.1742
batch: [1550/21305] batch time: 0.062 trainign loss: 4.4483 avg training loss: 7.1742
batch: [1560/21305] batch time: 0.062 trainign loss: 6.1872 avg training loss: 7.1741
batch: [1570/21305] batch time: 0.225 trainign loss: 5.4074 avg training loss: 7.1741
batch: [1580/21305] batch time: 0.057 trainign loss: 5.9513 avg training loss: 7.1740
batch: [1590/21305] batch time: 0.745 trainign loss: 7.9225 avg training loss: 7.1738
batch: [1600/21305] batch time: 0.056 trainign loss: 7.3418 avg training loss: 7.1738
batch: [1610/21305] batch time: 1.420 trainign loss: 6.8744 avg training loss: 7.1738
batch: [1620/21305] batch time: 0.053 trainign loss: 6.2568 avg training loss: 7.1737
batch: [1630/21305] batch time: 1.686 trainign loss: 7.2746 avg training loss: 7.1737
batch: [1640/21305] batch time: 0.056 trainign loss: 6.6282 avg training loss: 7.1736
batch: [1650/21305] batch time: 1.900 trainign loss: 5.2967 avg training loss: 7.1736
batch: [1660/21305] batch time: 0.056 trainign loss: 6.1025 avg training loss: 7.1735
batch: [1670/21305] batch time: 2.247 trainign loss: 6.3935 avg training loss: 7.1735
batch: [1680/21305] batch time: 0.062 trainign loss: 5.6477 avg training loss: 7.1734
batch: [1690/21305] batch time: 2.102 trainign loss: 6.3132 avg training loss: 7.1734
batch: [1700/21305] batch time: 0.058 trainign loss: 6.3917 avg training loss: 7.1734
batch: [1710/21305] batch time: 1.479 trainign loss: 4.7837 avg training loss: 7.1733
batch: [1720/21305] batch time: 0.056 trainign loss: 7.4032 avg training loss: 7.1732
batch: [1730/21305] batch time: 1.301 trainign loss: 6.2087 avg training loss: 7.1732
batch: [1740/21305] batch time: 0.056 trainign loss: 5.9814 avg training loss: 7.1731
batch: [1750/21305] batch time: 0.493 trainign loss: 6.5981 avg training loss: 7.1731
batch: [1760/21305] batch time: 0.056 trainign loss: 7.1176 avg training loss: 7.1731
batch: [1770/21305] batch time: 0.645 trainign loss: 6.1947 avg training loss: 7.1730
batch: [1780/21305] batch time: 0.056 trainign loss: 6.5239 avg training loss: 7.1730
batch: [1790/21305] batch time: 0.969 trainign loss: 6.2880 avg training loss: 7.1729
batch: [1800/21305] batch time: 0.056 trainign loss: 6.2641 avg training loss: 7.1729
batch: [1810/21305] batch time: 1.507 trainign loss: 6.3876 avg training loss: 7.1729
batch: [1820/21305] batch time: 0.062 trainign loss: 4.6857 avg training loss: 7.1728
batch: [1830/21305] batch time: 0.950 trainign loss: 7.4725 avg training loss: 7.1727
batch: [1840/21305] batch time: 0.060 trainign loss: 7.0109 avg training loss: 7.1727
batch: [1850/21305] batch time: 0.784 trainign loss: 6.7238 avg training loss: 7.1727
batch: [1860/21305] batch time: 0.061 trainign loss: 6.2710 avg training loss: 7.1727
batch: [1870/21305] batch time: 0.516 trainign loss: 6.3988 avg training loss: 7.1726
batch: [1880/21305] batch time: 0.062 trainign loss: 4.4073 avg training loss: 7.1726
batch: [1890/21305] batch time: 0.051 trainign loss: 4.4587 avg training loss: 7.1724
batch: [1900/21305] batch time: 0.053 trainign loss: 7.5494 avg training loss: 7.1724
batch: [1910/21305] batch time: 0.056 trainign loss: 7.1553 avg training loss: 7.1724
batch: [1920/21305] batch time: 0.053 trainign loss: 6.2969 avg training loss: 7.1723
batch: [1930/21305] batch time: 0.739 trainign loss: 2.4757 avg training loss: 7.1722
batch: [1940/21305] batch time: 0.061 trainign loss: 6.3915 avg training loss: 7.1722
batch: [1950/21305] batch time: 1.575 trainign loss: 6.0999 avg training loss: 7.1721
batch: [1960/21305] batch time: 0.058 trainign loss: 4.5605 avg training loss: 7.1721
batch: [1970/21305] batch time: 2.366 trainign loss: 6.7591 avg training loss: 7.1720
batch: [1980/21305] batch time: 0.055 trainign loss: 4.8987 avg training loss: 7.1719
batch: [1990/21305] batch time: 2.400 trainign loss: 6.4568 avg training loss: 7.1719
batch: [2000/21305] batch time: 0.056 trainign loss: 5.3647 avg training loss: 7.1718
batch: [2010/21305] batch time: 2.722 trainign loss: 7.0853 avg training loss: 7.1718
batch: [2020/21305] batch time: 0.056 trainign loss: 3.4411 avg training loss: 7.1717
batch: [2030/21305] batch time: 2.483 trainign loss: 5.5467 avg training loss: 7.1716
batch: [2040/21305] batch time: 0.056 trainign loss: 6.9108 avg training loss: 7.1716
batch: [2050/21305] batch time: 2.558 trainign loss: 5.6740 avg training loss: 7.1715
batch: [2060/21305] batch time: 0.057 trainign loss: 2.4158 avg training loss: 7.1714
batch: [2070/21305] batch time: 2.566 trainign loss: 1.4003 avg training loss: 7.1712
batch: [2080/21305] batch time: 0.063 trainign loss: 0.0157 avg training loss: 7.1710
batch: [2090/21305] batch time: 2.079 trainign loss: 8.9142 avg training loss: 7.1708
batch: [2100/21305] batch time: 0.056 trainign loss: 8.0812 avg training loss: 7.1709
batch: [2110/21305] batch time: 2.964 trainign loss: 6.3928 avg training loss: 7.1709
batch: [2120/21305] batch time: 0.056 trainign loss: 0.4626 avg training loss: 7.1707
batch: [2130/21305] batch time: 2.217 trainign loss: 0.0005 avg training loss: 7.1703
batch: [2140/21305] batch time: 0.057 trainign loss: 0.0001 avg training loss: 7.1698
batch: [2150/21305] batch time: 2.070 trainign loss: 7.6623 avg training loss: 7.1700
batch: [2160/21305] batch time: 0.057 trainign loss: 7.8201 avg training loss: 7.1700
batch: [2170/21305] batch time: 2.333 trainign loss: 7.5308 avg training loss: 7.1700
batch: [2180/21305] batch time: 0.057 trainign loss: 7.2027 avg training loss: 7.1700
batch: [2190/21305] batch time: 2.072 trainign loss: 6.6805 avg training loss: 7.1700
batch: [2200/21305] batch time: 0.056 trainign loss: 6.8340 avg training loss: 7.1700
batch: [2210/21305] batch time: 2.316 trainign loss: 5.9802 avg training loss: 7.1699
batch: [2220/21305] batch time: 0.058 trainign loss: 7.0446 avg training loss: 7.1699
batch: [2230/21305] batch time: 2.403 trainign loss: 6.8265 avg training loss: 7.1699
batch: [2240/21305] batch time: 0.061 trainign loss: 6.6044 avg training loss: 7.1698
batch: [2250/21305] batch time: 2.231 trainign loss: 6.1314 avg training loss: 7.1697
batch: [2260/21305] batch time: 0.061 trainign loss: 5.4955 avg training loss: 7.1696
batch: [2270/21305] batch time: 2.548 trainign loss: 4.1321 avg training loss: 7.1695
batch: [2280/21305] batch time: 0.062 trainign loss: 6.1500 avg training loss: 7.1694
batch: [2290/21305] batch time: 1.976 trainign loss: 3.7968 avg training loss: 7.1692
batch: [2300/21305] batch time: 0.055 trainign loss: 5.5333 avg training loss: 7.1692
batch: [2310/21305] batch time: 2.323 trainign loss: 7.6919 avg training loss: 7.1691
batch: [2320/21305] batch time: 0.054 trainign loss: 8.0241 avg training loss: 7.1691
batch: [2330/21305] batch time: 2.386 trainign loss: 7.1977 avg training loss: 7.1691
batch: [2340/21305] batch time: 0.052 trainign loss: 4.6697 avg training loss: 7.1691
batch: [2350/21305] batch time: 2.203 trainign loss: 5.2721 avg training loss: 7.1690
batch: [2360/21305] batch time: 0.062 trainign loss: 7.1136 avg training loss: 7.1690
batch: [2370/21305] batch time: 2.160 trainign loss: 5.0505 avg training loss: 7.1689
batch: [2380/21305] batch time: 0.056 trainign loss: 6.1590 avg training loss: 7.1688
batch: [2390/21305] batch time: 2.174 trainign loss: 6.9274 avg training loss: 7.1688
batch: [2400/21305] batch time: 0.056 trainign loss: 5.7938 avg training loss: 7.1688
batch: [2410/21305] batch time: 2.512 trainign loss: 7.5349 avg training loss: 7.1688
batch: [2420/21305] batch time: 0.056 trainign loss: 6.8297 avg training loss: 7.1687
batch: [2430/21305] batch time: 1.985 trainign loss: 4.9429 avg training loss: 7.1687
batch: [2440/21305] batch time: 0.056 trainign loss: 6.6949 avg training loss: 7.1686
batch: [2450/21305] batch time: 2.159 trainign loss: 6.8639 avg training loss: 7.1686
batch: [2460/21305] batch time: 0.056 trainign loss: 6.6178 avg training loss: 7.1685
batch: [2470/21305] batch time: 2.072 trainign loss: 6.4933 avg training loss: 7.1684
batch: [2480/21305] batch time: 0.057 trainign loss: 7.1420 avg training loss: 7.1684
batch: [2490/21305] batch time: 2.440 trainign loss: 6.8212 avg training loss: 7.1683
batch: [2500/21305] batch time: 0.056 trainign loss: 7.4280 avg training loss: 7.1683
batch: [2510/21305] batch time: 1.716 trainign loss: 5.5683 avg training loss: 7.1683
batch: [2520/21305] batch time: 0.090 trainign loss: 5.4209 avg training loss: 7.1682
batch: [2530/21305] batch time: 1.630 trainign loss: 5.3304 avg training loss: 7.1682
batch: [2540/21305] batch time: 0.057 trainign loss: 6.8782 avg training loss: 7.1681
batch: [2550/21305] batch time: 2.028 trainign loss: 5.8695 avg training loss: 7.1681
batch: [2560/21305] batch time: 0.062 trainign loss: 5.8661 avg training loss: 7.1681
batch: [2570/21305] batch time: 1.455 trainign loss: 6.7009 avg training loss: 7.1680
batch: [2580/21305] batch time: 0.059 trainign loss: 5.1100 avg training loss: 7.1679
batch: [2590/21305] batch time: 0.693 trainign loss: 5.7102 avg training loss: 7.1679
batch: [2600/21305] batch time: 0.056 trainign loss: 0.2693 avg training loss: 7.1676
batch: [2610/21305] batch time: 0.523 trainign loss: 6.8341 avg training loss: 7.1676
batch: [2620/21305] batch time: 0.057 trainign loss: 7.8622 avg training loss: 7.1676
batch: [2630/21305] batch time: 0.361 trainign loss: 6.5522 avg training loss: 7.1676
batch: [2640/21305] batch time: 0.056 trainign loss: 3.9785 avg training loss: 7.1675
batch: [2650/21305] batch time: 0.052 trainign loss: 7.6274 avg training loss: 7.1674
batch: [2660/21305] batch time: 0.057 trainign loss: 7.1825 avg training loss: 7.1674
batch: [2670/21305] batch time: 0.057 trainign loss: 6.4621 avg training loss: 7.1673
batch: [2680/21305] batch time: 0.062 trainign loss: 5.2122 avg training loss: 7.1673
batch: [2690/21305] batch time: 0.517 trainign loss: 7.2525 avg training loss: 7.1672
batch: [2700/21305] batch time: 0.056 trainign loss: 6.6751 avg training loss: 7.1672
batch: [2710/21305] batch time: 0.585 trainign loss: 5.0607 avg training loss: 7.1671
batch: [2720/21305] batch time: 0.056 trainign loss: 2.4261 avg training loss: 7.1669
batch: [2730/21305] batch time: 0.319 trainign loss: 6.8101 avg training loss: 7.1669
batch: [2740/21305] batch time: 0.641 trainign loss: 7.5132 avg training loss: 7.1669
batch: [2750/21305] batch time: 1.211 trainign loss: 6.8601 avg training loss: 7.1668
batch: [2760/21305] batch time: 0.987 trainign loss: 6.7833 avg training loss: 7.1668
batch: [2770/21305] batch time: 0.073 trainign loss: 6.8586 avg training loss: 7.1667
batch: [2780/21305] batch time: 1.664 trainign loss: 7.3442 avg training loss: 7.1667
batch: [2790/21305] batch time: 0.956 trainign loss: 6.2649 avg training loss: 7.1667
batch: [2800/21305] batch time: 1.543 trainign loss: 6.8068 avg training loss: 7.1666
batch: [2810/21305] batch time: 0.847 trainign loss: 6.0888 avg training loss: 7.1666
batch: [2820/21305] batch time: 0.899 trainign loss: 6.7566 avg training loss: 7.1664
batch: [2830/21305] batch time: 1.425 trainign loss: 7.1263 avg training loss: 7.1664
batch: [2840/21305] batch time: 0.724 trainign loss: 1.2550 avg training loss: 7.1662
batch: [2850/21305] batch time: 1.817 trainign loss: 5.0165 avg training loss: 7.1662
batch: [2860/21305] batch time: 1.283 trainign loss: 6.8254 avg training loss: 7.1661
batch: [2870/21305] batch time: 0.309 trainign loss: 7.4145 avg training loss: 7.1660
batch: [2880/21305] batch time: 2.275 trainign loss: 6.5109 avg training loss: 7.1660
batch: [2890/21305] batch time: 0.307 trainign loss: 6.9991 avg training loss: 7.1659
batch: [2900/21305] batch time: 1.880 trainign loss: 6.6848 avg training loss: 7.1659
batch: [2910/21305] batch time: 0.254 trainign loss: 7.0882 avg training loss: 7.1659
batch: [2920/21305] batch time: 2.296 trainign loss: 6.1763 avg training loss: 7.1658
batch: [2930/21305] batch time: 0.062 trainign loss: 5.2331 avg training loss: 7.1658
batch: [2940/21305] batch time: 2.234 trainign loss: 4.8004 avg training loss: 7.1657
batch: [2950/21305] batch time: 0.056 trainign loss: 0.3656 avg training loss: 7.1654
batch: [2960/21305] batch time: 2.362 trainign loss: 6.1290 avg training loss: 7.1654
batch: [2970/21305] batch time: 0.056 trainign loss: 6.4542 avg training loss: 7.1654
batch: [2980/21305] batch time: 2.453 trainign loss: 6.6247 avg training loss: 7.1654
batch: [2990/21305] batch time: 0.054 trainign loss: 6.2912 avg training loss: 7.1653
batch: [3000/21305] batch time: 2.531 trainign loss: 4.9414 avg training loss: 7.1653
batch: [3010/21305] batch time: 0.057 trainign loss: 7.7017 avg training loss: 7.1652
batch: [3020/21305] batch time: 2.360 trainign loss: 4.7394 avg training loss: 7.1652
batch: [3030/21305] batch time: 0.054 trainign loss: 5.7621 avg training loss: 7.1651
batch: [3040/21305] batch time: 2.245 trainign loss: 6.2638 avg training loss: 7.1651
batch: [3050/21305] batch time: 0.062 trainign loss: 6.8331 avg training loss: 7.1650
batch: [3060/21305] batch time: 1.946 trainign loss: 6.3448 avg training loss: 7.1650
batch: [3070/21305] batch time: 0.083 trainign loss: 6.0694 avg training loss: 7.1649
batch: [3080/21305] batch time: 1.035 trainign loss: 6.6435 avg training loss: 7.1648
batch: [3090/21305] batch time: 1.502 trainign loss: 6.0621 avg training loss: 7.1648
batch: [3100/21305] batch time: 0.621 trainign loss: 6.7345 avg training loss: 7.1647
batch: [3110/21305] batch time: 1.770 trainign loss: 7.2511 avg training loss: 7.1647
batch: [3120/21305] batch time: 0.432 trainign loss: 5.7376 avg training loss: 7.1646
batch: [3130/21305] batch time: 2.220 trainign loss: 0.4277 avg training loss: 7.1644
batch: [3140/21305] batch time: 0.919 trainign loss: 7.4821 avg training loss: 7.1644
batch: [3150/21305] batch time: 1.435 trainign loss: 6.8604 avg training loss: 7.1644
batch: [3160/21305] batch time: 1.231 trainign loss: 6.6086 avg training loss: 7.1643
batch: [3170/21305] batch time: 1.744 trainign loss: 5.9315 avg training loss: 7.1643
batch: [3180/21305] batch time: 0.925 trainign loss: 5.8387 avg training loss: 7.1643
batch: [3190/21305] batch time: 0.532 trainign loss: 6.3169 avg training loss: 7.1642
batch: [3200/21305] batch time: 2.075 trainign loss: 6.7671 avg training loss: 7.1642
batch: [3210/21305] batch time: 0.061 trainign loss: 6.9395 avg training loss: 7.1641
batch: [3220/21305] batch time: 2.042 trainign loss: 6.6681 avg training loss: 7.1641
batch: [3230/21305] batch time: 0.062 trainign loss: 6.8157 avg training loss: 7.1640
batch: [3240/21305] batch time: 2.375 trainign loss: 5.4532 avg training loss: 7.1640
batch: [3250/21305] batch time: 0.056 trainign loss: 6.3198 avg training loss: 7.1639
batch: [3260/21305] batch time: 2.479 trainign loss: 7.1760 avg training loss: 7.1639
batch: [3270/21305] batch time: 0.059 trainign loss: 5.3079 avg training loss: 7.1638
batch: [3280/21305] batch time: 2.373 trainign loss: 2.8718 avg training loss: 7.1637
batch: [3290/21305] batch time: 0.059 trainign loss: 6.1053 avg training loss: 7.1635
batch: [3300/21305] batch time: 2.579 trainign loss: 5.8363 avg training loss: 7.1635
batch: [3310/21305] batch time: 0.052 trainign loss: 0.4277 avg training loss: 7.1633
batch: [3320/21305] batch time: 2.663 trainign loss: 7.5143 avg training loss: 7.1633
batch: [3330/21305] batch time: 0.056 trainign loss: 5.9053 avg training loss: 7.1633
batch: [3340/21305] batch time: 2.157 trainign loss: 3.8912 avg training loss: 7.1632
batch: [3350/21305] batch time: 0.056 trainign loss: 5.5489 avg training loss: 7.1631
batch: [3360/21305] batch time: 2.335 trainign loss: 7.1127 avg training loss: 7.1630
batch: [3370/21305] batch time: 0.061 trainign loss: 3.2630 avg training loss: 7.1628
batch: [3380/21305] batch time: 2.362 trainign loss: 8.0138 avg training loss: 7.1628
batch: [3390/21305] batch time: 0.056 trainign loss: 7.0367 avg training loss: 7.1628
batch: [3400/21305] batch time: 2.572 trainign loss: 6.3123 avg training loss: 7.1628
batch: [3410/21305] batch time: 0.062 trainign loss: 5.6998 avg training loss: 7.1627
batch: [3420/21305] batch time: 2.163 trainign loss: 6.6078 avg training loss: 7.1626
batch: [3430/21305] batch time: 0.056 trainign loss: 6.4844 avg training loss: 7.1626
batch: [3440/21305] batch time: 2.291 trainign loss: 7.0103 avg training loss: 7.1625
batch: [3450/21305] batch time: 0.054 trainign loss: 5.6699 avg training loss: 7.1625
batch: [3460/21305] batch time: 2.536 trainign loss: 5.9356 avg training loss: 7.1624
batch: [3470/21305] batch time: 0.055 trainign loss: 7.4463 avg training loss: 7.1624
batch: [3480/21305] batch time: 2.143 trainign loss: 7.3713 avg training loss: 7.1624
batch: [3490/21305] batch time: 0.061 trainign loss: 6.8743 avg training loss: 7.1623
batch: [3500/21305] batch time: 2.232 trainign loss: 6.3329 avg training loss: 7.1623
batch: [3510/21305] batch time: 0.052 trainign loss: 6.4136 avg training loss: 7.1623
batch: [3520/21305] batch time: 2.314 trainign loss: 5.9278 avg training loss: 7.1622
batch: [3530/21305] batch time: 0.056 trainign loss: 7.1371 avg training loss: 7.1622
batch: [3540/21305] batch time: 2.412 trainign loss: 5.5098 avg training loss: 7.1621
batch: [3550/21305] batch time: 0.056 trainign loss: 6.6440 avg training loss: 7.1620
batch: [3560/21305] batch time: 2.329 trainign loss: 6.4532 avg training loss: 7.1619
batch: [3570/21305] batch time: 0.062 trainign loss: 7.8118 avg training loss: 7.1619
batch: [3580/21305] batch time: 2.284 trainign loss: 6.2546 avg training loss: 7.1619
batch: [3590/21305] batch time: 0.060 trainign loss: 4.3504 avg training loss: 7.1618
batch: [3600/21305] batch time: 2.302 trainign loss: 7.3687 avg training loss: 7.1617
batch: [3610/21305] batch time: 0.056 trainign loss: 7.6609 avg training loss: 7.1617
batch: [3620/21305] batch time: 2.093 trainign loss: 7.0223 avg training loss: 7.1617
batch: [3630/21305] batch time: 0.056 trainign loss: 6.8392 avg training loss: 7.1617
batch: [3640/21305] batch time: 2.274 trainign loss: 6.5552 avg training loss: 7.1616
batch: [3650/21305] batch time: 0.056 trainign loss: 5.7524 avg training loss: 7.1615
batch: [3660/21305] batch time: 2.228 trainign loss: 5.9496 avg training loss: 7.1615
batch: [3670/21305] batch time: 0.051 trainign loss: 5.3793 avg training loss: 7.1614
batch: [3680/21305] batch time: 2.198 trainign loss: 7.3786 avg training loss: 7.1614
batch: [3690/21305] batch time: 0.057 trainign loss: 6.6335 avg training loss: 7.1614
batch: [3700/21305] batch time: 2.174 trainign loss: 5.1770 avg training loss: 7.1614
batch: [3710/21305] batch time: 0.056 trainign loss: 5.3199 avg training loss: 7.1613
batch: [3720/21305] batch time: 2.281 trainign loss: 6.1955 avg training loss: 7.1612
batch: [3730/21305] batch time: 0.062 trainign loss: 5.5063 avg training loss: 7.1612
batch: [3740/21305] batch time: 2.397 trainign loss: 7.3115 avg training loss: 7.1612
batch: [3750/21305] batch time: 0.061 trainign loss: 6.8764 avg training loss: 7.1611
batch: [3760/21305] batch time: 2.041 trainign loss: 6.5293 avg training loss: 7.1610
batch: [3770/21305] batch time: 0.056 trainign loss: 4.4771 avg training loss: 7.1609
batch: [3780/21305] batch time: 1.113 trainign loss: 6.8137 avg training loss: 7.1609
batch: [3790/21305] batch time: 0.057 trainign loss: 4.4883 avg training loss: 7.1609
batch: [3800/21305] batch time: 1.807 trainign loss: 5.8355 avg training loss: 7.1608
batch: [3810/21305] batch time: 0.054 trainign loss: 7.2698 avg training loss: 7.1608
batch: [3820/21305] batch time: 2.247 trainign loss: 4.8788 avg training loss: 7.1607
batch: [3830/21305] batch time: 0.053 trainign loss: 6.1932 avg training loss: 7.1607
batch: [3840/21305] batch time: 2.272 trainign loss: 6.0151 avg training loss: 7.1606
batch: [3850/21305] batch time: 0.063 trainign loss: 6.1230 avg training loss: 7.1606
batch: [3860/21305] batch time: 2.236 trainign loss: 6.2956 avg training loss: 7.1605
batch: [3870/21305] batch time: 0.061 trainign loss: 5.0771 avg training loss: 7.1605
batch: [3880/21305] batch time: 2.234 trainign loss: 0.1012 avg training loss: 7.1602
batch: [3890/21305] batch time: 0.059 trainign loss: 8.6541 avg training loss: 7.1601
batch: [3900/21305] batch time: 2.202 trainign loss: 7.8741 avg training loss: 7.1601
batch: [3910/21305] batch time: 0.063 trainign loss: 6.8829 avg training loss: 7.1601
batch: [3920/21305] batch time: 2.597 trainign loss: 4.4128 avg training loss: 7.1600
batch: [3930/21305] batch time: 0.052 trainign loss: 6.5335 avg training loss: 7.1600
batch: [3940/21305] batch time: 2.390 trainign loss: 5.8821 avg training loss: 7.1600
batch: [3950/21305] batch time: 0.056 trainign loss: 6.4701 avg training loss: 7.1599
batch: [3960/21305] batch time: 2.275 trainign loss: 7.3766 avg training loss: 7.1599
batch: [3970/21305] batch time: 0.057 trainign loss: 6.9267 avg training loss: 7.1599
batch: [3980/21305] batch time: 2.360 trainign loss: 5.7191 avg training loss: 7.1598
batch: [3990/21305] batch time: 0.063 trainign loss: 5.2748 avg training loss: 7.1597
batch: [4000/21305] batch time: 2.740 trainign loss: 7.5103 avg training loss: 7.1597
batch: [4010/21305] batch time: 0.051 trainign loss: 6.9360 avg training loss: 7.1596
batch: [4020/21305] batch time: 2.226 trainign loss: 4.4955 avg training loss: 7.1596
batch: [4030/21305] batch time: 0.057 trainign loss: 6.6691 avg training loss: 7.1595
batch: [4040/21305] batch time: 2.280 trainign loss: 6.7529 avg training loss: 7.1595
batch: [4050/21305] batch time: 0.056 trainign loss: 4.3323 avg training loss: 7.1594
batch: [4060/21305] batch time: 2.199 trainign loss: 9.6611 avg training loss: 7.1591
batch: [4070/21305] batch time: 0.055 trainign loss: 8.6886 avg training loss: 7.1590
batch: [4080/21305] batch time: 1.927 trainign loss: 7.9420 avg training loss: 7.1591
batch: [4090/21305] batch time: 0.056 trainign loss: 7.5014 avg training loss: 7.1590
batch: [4100/21305] batch time: 2.080 trainign loss: 6.1085 avg training loss: 7.1590
batch: [4110/21305] batch time: 0.057 trainign loss: 5.2326 avg training loss: 7.1590
batch: [4120/21305] batch time: 2.252 trainign loss: 5.5149 avg training loss: 7.1589
batch: [4130/21305] batch time: 0.056 trainign loss: 6.1907 avg training loss: 7.1589
batch: [4140/21305] batch time: 2.155 trainign loss: 5.8480 avg training loss: 7.1587
batch: [4150/21305] batch time: 0.056 trainign loss: 7.8411 avg training loss: 7.1587
batch: [4160/21305] batch time: 0.905 trainign loss: 7.1751 avg training loss: 7.1587
batch: [4170/21305] batch time: 0.056 trainign loss: 6.0360 avg training loss: 7.1586
batch: [4180/21305] batch time: 2.250 trainign loss: 6.6405 avg training loss: 7.1586
batch: [4190/21305] batch time: 0.058 trainign loss: 6.6755 avg training loss: 7.1585
batch: [4200/21305] batch time: 2.263 trainign loss: 5.7865 avg training loss: 7.1584
batch: [4210/21305] batch time: 0.056 trainign loss: 1.3905 avg training loss: 7.1583
batch: [4220/21305] batch time: 2.362 trainign loss: 6.6690 avg training loss: 7.1582
batch: [4230/21305] batch time: 0.054 trainign loss: 6.6355 avg training loss: 7.1581
batch: [4240/21305] batch time: 2.299 trainign loss: 4.7898 avg training loss: 7.1581
batch: [4250/21305] batch time: 0.062 trainign loss: 0.5002 avg training loss: 7.1577
batch: [4260/21305] batch time: 2.030 trainign loss: 8.4130 avg training loss: 7.1578
batch: [4270/21305] batch time: 0.058 trainign loss: 8.4299 avg training loss: 7.1578
batch: [4280/21305] batch time: 2.023 trainign loss: 7.2314 avg training loss: 7.1579
batch: [4290/21305] batch time: 0.056 trainign loss: 6.7775 avg training loss: 7.1578
batch: [4300/21305] batch time: 0.489 trainign loss: 6.5120 avg training loss: 7.1578
batch: [4310/21305] batch time: 0.056 trainign loss: 6.7746 avg training loss: 7.1578
batch: [4320/21305] batch time: 1.093 trainign loss: 5.5873 avg training loss: 7.1577
batch: [4330/21305] batch time: 0.056 trainign loss: 4.3262 avg training loss: 7.1576
batch: [4340/21305] batch time: 1.253 trainign loss: 5.4229 avg training loss: 7.1576
batch: [4350/21305] batch time: 0.057 trainign loss: 6.2562 avg training loss: 7.1575
batch: [4360/21305] batch time: 1.799 trainign loss: 4.9179 avg training loss: 7.1574
batch: [4370/21305] batch time: 0.061 trainign loss: 7.4957 avg training loss: 7.1573
batch: [4380/21305] batch time: 2.365 trainign loss: 6.2156 avg training loss: 7.1573
batch: [4390/21305] batch time: 0.052 trainign loss: 6.0994 avg training loss: 7.1572
batch: [4400/21305] batch time: 2.236 trainign loss: 6.9722 avg training loss: 7.1572
batch: [4410/21305] batch time: 0.056 trainign loss: 6.5270 avg training loss: 7.1572
batch: [4420/21305] batch time: 2.398 trainign loss: 5.6077 avg training loss: 7.1571
batch: [4430/21305] batch time: 0.056 trainign loss: 0.9384 avg training loss: 7.1569
batch: [4440/21305] batch time: 1.951 trainign loss: 9.0002 avg training loss: 7.1568
batch: [4450/21305] batch time: 0.056 trainign loss: 6.5186 avg training loss: 7.1567
batch: [4460/21305] batch time: 2.298 trainign loss: 3.8487 avg training loss: 7.1566
batch: [4470/21305] batch time: 0.057 trainign loss: 8.8968 avg training loss: 7.1565
batch: [4480/21305] batch time: 2.179 trainign loss: 7.0163 avg training loss: 7.1565
batch: [4490/21305] batch time: 0.062 trainign loss: 3.8808 avg training loss: 7.1564
batch: [4500/21305] batch time: 1.551 trainign loss: 8.1991 avg training loss: 7.1564
batch: [4510/21305] batch time: 0.063 trainign loss: 6.9975 avg training loss: 7.1563
batch: [4520/21305] batch time: 1.523 trainign loss: 7.7192 avg training loss: 7.1563
batch: [4530/21305] batch time: 0.058 trainign loss: 7.5082 avg training loss: 7.1563
batch: [4540/21305] batch time: 1.899 trainign loss: 6.7695 avg training loss: 7.1563
batch: [4550/21305] batch time: 0.056 trainign loss: 6.2380 avg training loss: 7.1562
batch: [4560/21305] batch time: 2.456 trainign loss: 7.1168 avg training loss: 7.1562
batch: [4570/21305] batch time: 0.063 trainign loss: 6.7987 avg training loss: 7.1561
batch: [4580/21305] batch time: 1.493 trainign loss: 6.9140 avg training loss: 7.1561
batch: [4590/21305] batch time: 0.057 trainign loss: 5.1034 avg training loss: 7.1560
batch: [4600/21305] batch time: 0.748 trainign loss: 6.4634 avg training loss: 7.1560
batch: [4610/21305] batch time: 0.061 trainign loss: 6.1167 avg training loss: 7.1560
batch: [4620/21305] batch time: 0.051 trainign loss: 6.1735 avg training loss: 7.1559
batch: [4630/21305] batch time: 0.055 trainign loss: 6.4932 avg training loss: 7.1558
batch: [4640/21305] batch time: 0.914 trainign loss: 5.5900 avg training loss: 7.1558
batch: [4650/21305] batch time: 0.059 trainign loss: 6.3680 avg training loss: 7.1558
batch: [4660/21305] batch time: 0.687 trainign loss: 6.9120 avg training loss: 7.1558
batch: [4670/21305] batch time: 0.056 trainign loss: 6.7242 avg training loss: 7.1556
batch: [4680/21305] batch time: 0.053 trainign loss: 7.5172 avg training loss: 7.1556
batch: [4690/21305] batch time: 0.056 trainign loss: 5.2652 avg training loss: 7.1556
batch: [4700/21305] batch time: 0.057 trainign loss: 6.2285 avg training loss: 7.1555
batch: [4710/21305] batch time: 0.062 trainign loss: 7.3415 avg training loss: 7.1555
batch: [4720/21305] batch time: 0.051 trainign loss: 6.6176 avg training loss: 7.1555
batch: [4730/21305] batch time: 0.063 trainign loss: 6.2173 avg training loss: 7.1554
batch: [4740/21305] batch time: 0.056 trainign loss: 7.1820 avg training loss: 7.1553
batch: [4750/21305] batch time: 0.056 trainign loss: 6.9022 avg training loss: 7.1553
batch: [4760/21305] batch time: 0.310 trainign loss: 4.1888 avg training loss: 7.1552
batch: [4770/21305] batch time: 0.062 trainign loss: 6.3800 avg training loss: 7.1551
batch: [4780/21305] batch time: 0.058 trainign loss: 3.6843 avg training loss: 7.1551
batch: [4790/21305] batch time: 0.061 trainign loss: 5.8714 avg training loss: 7.1550
batch: [4800/21305] batch time: 0.548 trainign loss: 6.9901 avg training loss: 7.1549
batch: [4810/21305] batch time: 0.056 trainign loss: 5.2422 avg training loss: 7.1549
batch: [4820/21305] batch time: 0.610 trainign loss: 6.7022 avg training loss: 7.1548
batch: [4830/21305] batch time: 0.056 trainign loss: 5.8699 avg training loss: 7.1548
batch: [4840/21305] batch time: 0.552 trainign loss: 1.0642 avg training loss: 7.1546
batch: [4850/21305] batch time: 0.056 trainign loss: 9.2651 avg training loss: 7.1544
batch: [4860/21305] batch time: 0.747 trainign loss: 6.2677 avg training loss: 7.1544
batch: [4870/21305] batch time: 0.056 trainign loss: 8.2931 avg training loss: 7.1545
batch: [4880/21305] batch time: 0.276 trainign loss: 7.3805 avg training loss: 7.1545
batch: [4890/21305] batch time: 0.062 trainign loss: 6.6827 avg training loss: 7.1544
batch: [4900/21305] batch time: 0.095 trainign loss: 6.1362 avg training loss: 7.1544
batch: [4910/21305] batch time: 0.056 trainign loss: 4.5844 avg training loss: 7.1543
batch: [4920/21305] batch time: 0.051 trainign loss: 7.8399 avg training loss: 7.1543
batch: [4930/21305] batch time: 0.063 trainign loss: 5.5003 avg training loss: 7.1542
batch: [4940/21305] batch time: 0.057 trainign loss: 7.1477 avg training loss: 7.1541
batch: [4950/21305] batch time: 0.061 trainign loss: 5.1838 avg training loss: 7.1541
batch: [4960/21305] batch time: 0.056 trainign loss: 6.6452 avg training loss: 7.1540
batch: [4970/21305] batch time: 0.061 trainign loss: 6.4441 avg training loss: 7.1539
batch: [4980/21305] batch time: 0.063 trainign loss: 4.3313 avg training loss: 7.1538
batch: [4990/21305] batch time: 0.057 trainign loss: 0.0086 avg training loss: 7.1535
batch: [5000/21305] batch time: 0.057 trainign loss: 6.4795 avg training loss: 7.1534
batch: [5010/21305] batch time: 0.058 trainign loss: 7.7966 avg training loss: 7.1535
batch: [5020/21305] batch time: 0.053 trainign loss: 6.5335 avg training loss: 7.1534
batch: [5030/21305] batch time: 0.056 trainign loss: 6.0971 avg training loss: 7.1534
batch: [5040/21305] batch time: 0.051 trainign loss: 3.8211 avg training loss: 7.1533
batch: [5050/21305] batch time: 0.056 trainign loss: 6.0913 avg training loss: 7.1532
batch: [5060/21305] batch time: 0.051 trainign loss: 7.4296 avg training loss: 7.1531
batch: [5070/21305] batch time: 0.056 trainign loss: 4.2880 avg training loss: 7.1531
batch: [5080/21305] batch time: 0.050 trainign loss: 0.7540 avg training loss: 7.1527
batch: [5090/21305] batch time: 0.058 trainign loss: 6.4488 avg training loss: 7.1528
batch: [5100/21305] batch time: 0.056 trainign loss: 7.0373 avg training loss: 7.1528
batch: [5110/21305] batch time: 0.056 trainign loss: 6.3004 avg training loss: 7.1527
batch: [5120/21305] batch time: 0.050 trainign loss: 6.5256 avg training loss: 7.1527
batch: [5130/21305] batch time: 0.056 trainign loss: 6.6804 avg training loss: 7.1527
batch: [5140/21305] batch time: 0.050 trainign loss: 6.3067 avg training loss: 7.1525
batch: [5150/21305] batch time: 0.059 trainign loss: 7.6938 avg training loss: 7.1525
batch: [5160/21305] batch time: 0.056 trainign loss: 7.4226 avg training loss: 7.1525
batch: [5170/21305] batch time: 0.058 trainign loss: 6.9505 avg training loss: 7.1525
batch: [5180/21305] batch time: 0.056 trainign loss: 6.2399 avg training loss: 7.1525
batch: [5190/21305] batch time: 0.057 trainign loss: 6.3224 avg training loss: 7.1524
batch: [5200/21305] batch time: 0.060 trainign loss: 5.9802 avg training loss: 7.1524
batch: [5210/21305] batch time: 0.058 trainign loss: 5.6748 avg training loss: 7.1523
batch: [5220/21305] batch time: 0.057 trainign loss: 6.3849 avg training loss: 7.1523
batch: [5230/21305] batch time: 0.056 trainign loss: 6.4985 avg training loss: 7.1522
batch: [5240/21305] batch time: 0.052 trainign loss: 4.6195 avg training loss: 7.1522
batch: [5250/21305] batch time: 0.057 trainign loss: 5.6283 avg training loss: 7.1521
batch: [5260/21305] batch time: 0.054 trainign loss: 7.0657 avg training loss: 7.1521
batch: [5270/21305] batch time: 0.058 trainign loss: 7.0939 avg training loss: 7.1521
batch: [5280/21305] batch time: 0.052 trainign loss: 6.8531 avg training loss: 7.1521
batch: [5290/21305] batch time: 1.050 trainign loss: 5.4092 avg training loss: 7.1520
batch: [5300/21305] batch time: 0.057 trainign loss: 6.5515 avg training loss: 7.1519
batch: [5310/21305] batch time: 1.142 trainign loss: 6.5970 avg training loss: 7.1519
batch: [5320/21305] batch time: 0.057 trainign loss: 7.0989 avg training loss: 7.1518
batch: [5330/21305] batch time: 1.701 trainign loss: 6.4204 avg training loss: 7.1518
batch: [5340/21305] batch time: 0.056 trainign loss: 6.2379 avg training loss: 7.1517
batch: [5350/21305] batch time: 0.832 trainign loss: 5.6784 avg training loss: 7.1516
batch: [5360/21305] batch time: 0.058 trainign loss: 7.4456 avg training loss: 7.1515
batch: [5370/21305] batch time: 1.593 trainign loss: 7.2672 avg training loss: 7.1515
batch: [5380/21305] batch time: 0.060 trainign loss: 7.1514 avg training loss: 7.1515
batch: [5390/21305] batch time: 1.221 trainign loss: 5.1015 avg training loss: 7.1515
batch: [5400/21305] batch time: 0.056 trainign loss: 7.0736 avg training loss: 7.1514
batch: [5410/21305] batch time: 1.074 trainign loss: 6.5726 avg training loss: 7.1514
batch: [5420/21305] batch time: 0.063 trainign loss: 7.1258 avg training loss: 7.1513
batch: [5430/21305] batch time: 0.412 trainign loss: 6.3036 avg training loss: 7.1513
batch: [5440/21305] batch time: 0.062 trainign loss: 6.2813 avg training loss: 7.1512
batch: [5450/21305] batch time: 0.063 trainign loss: 5.3039 avg training loss: 7.1511
batch: [5460/21305] batch time: 0.051 trainign loss: 7.3432 avg training loss: 7.1511
batch: [5470/21305] batch time: 0.057 trainign loss: 5.9510 avg training loss: 7.1510
batch: [5480/21305] batch time: 0.056 trainign loss: 7.3263 avg training loss: 7.1510
batch: [5490/21305] batch time: 0.063 trainign loss: 5.4842 avg training loss: 7.1510
batch: [5500/21305] batch time: 0.053 trainign loss: 5.6204 avg training loss: 7.1509
batch: [5510/21305] batch time: 0.057 trainign loss: 2.3481 avg training loss: 7.1508
batch: [5520/21305] batch time: 0.057 trainign loss: 7.0587 avg training loss: 7.1507
batch: [5530/21305] batch time: 0.056 trainign loss: 7.0934 avg training loss: 7.1507
batch: [5540/21305] batch time: 0.052 trainign loss: 6.7607 avg training loss: 7.1506
batch: [5550/21305] batch time: 0.056 trainign loss: 7.5346 avg training loss: 7.1506
batch: [5560/21305] batch time: 0.059 trainign loss: 5.1213 avg training loss: 7.1505
batch: [5570/21305] batch time: 0.056 trainign loss: 2.5684 avg training loss: 7.1504
batch: [5580/21305] batch time: 0.057 trainign loss: 7.8278 avg training loss: 7.1503
batch: [5590/21305] batch time: 0.056 trainign loss: 7.0177 avg training loss: 7.1503
batch: [5600/21305] batch time: 0.058 trainign loss: 6.7317 avg training loss: 7.1503
batch: [5610/21305] batch time: 0.056 trainign loss: 7.2410 avg training loss: 7.1502
batch: [5620/21305] batch time: 0.051 trainign loss: 7.3290 avg training loss: 7.1502
batch: [5630/21305] batch time: 0.056 trainign loss: 6.8100 avg training loss: 7.1502
batch: [5640/21305] batch time: 0.052 trainign loss: 6.0361 avg training loss: 7.1502
batch: [5650/21305] batch time: 0.058 trainign loss: 7.4721 avg training loss: 7.1501
batch: [5660/21305] batch time: 0.056 trainign loss: 4.3508 avg training loss: 7.1501
batch: [5670/21305] batch time: 0.056 trainign loss: 6.8878 avg training loss: 7.1500
batch: [5680/21305] batch time: 0.056 trainign loss: 6.2589 avg training loss: 7.1500
batch: [5690/21305] batch time: 0.889 trainign loss: 5.9502 avg training loss: 7.1499
batch: [5700/21305] batch time: 0.054 trainign loss: 5.9884 avg training loss: 7.1499
batch: [5710/21305] batch time: 1.345 trainign loss: 4.7849 avg training loss: 7.1498
batch: [5720/21305] batch time: 0.056 trainign loss: 6.5091 avg training loss: 7.1496
batch: [5730/21305] batch time: 0.056 trainign loss: 7.5031 avg training loss: 7.1496
batch: [5740/21305] batch time: 0.055 trainign loss: 7.1806 avg training loss: 7.1497
batch: [5750/21305] batch time: 0.059 trainign loss: 6.3951 avg training loss: 7.1496
batch: [5760/21305] batch time: 0.062 trainign loss: 5.5052 avg training loss: 7.1496
batch: [5770/21305] batch time: 0.062 trainign loss: 6.3315 avg training loss: 7.1496
batch: [5780/21305] batch time: 0.056 trainign loss: 6.0742 avg training loss: 7.1495
batch: [5790/21305] batch time: 0.056 trainign loss: 7.3002 avg training loss: 7.1495
batch: [5800/21305] batch time: 0.056 trainign loss: 5.9682 avg training loss: 7.1494
batch: [5810/21305] batch time: 0.056 trainign loss: 7.5136 avg training loss: 7.1494
batch: [5820/21305] batch time: 0.056 trainign loss: 7.1342 avg training loss: 7.1494
batch: [5830/21305] batch time: 0.057 trainign loss: 6.0182 avg training loss: 7.1493
batch: [5840/21305] batch time: 0.057 trainign loss: 6.7850 avg training loss: 7.1493
batch: [5850/21305] batch time: 0.061 trainign loss: 6.4664 avg training loss: 7.1493
batch: [5860/21305] batch time: 0.056 trainign loss: 7.2875 avg training loss: 7.1493
batch: [5870/21305] batch time: 0.052 trainign loss: 6.6555 avg training loss: 7.1493
batch: [5880/21305] batch time: 0.056 trainign loss: 6.1071 avg training loss: 7.1492
batch: [5890/21305] batch time: 0.055 trainign loss: 6.4054 avg training loss: 7.1492
batch: [5900/21305] batch time: 0.057 trainign loss: 5.0337 avg training loss: 7.1491
batch: [5910/21305] batch time: 0.050 trainign loss: 5.4180 avg training loss: 7.1490
batch: [5920/21305] batch time: 0.051 trainign loss: 5.2225 avg training loss: 7.1489
batch: [5930/21305] batch time: 0.054 trainign loss: 6.1787 avg training loss: 7.1489
batch: [5940/21305] batch time: 0.063 trainign loss: 5.6395 avg training loss: 7.1488
batch: [5950/21305] batch time: 0.052 trainign loss: 5.3218 avg training loss: 7.1487
batch: [5960/21305] batch time: 0.057 trainign loss: 4.3920 avg training loss: 7.1486
batch: [5970/21305] batch time: 0.051 trainign loss: 7.9305 avg training loss: 7.1486
batch: [5980/21305] batch time: 0.056 trainign loss: 6.5586 avg training loss: 7.1486
batch: [5990/21305] batch time: 0.057 trainign loss: 7.3848 avg training loss: 7.1487
batch: [6000/21305] batch time: 0.052 trainign loss: 6.9135 avg training loss: 7.1487
batch: [6010/21305] batch time: 0.063 trainign loss: 5.5643 avg training loss: 7.1486
batch: [6020/21305] batch time: 0.287 trainign loss: 6.6755 avg training loss: 7.1486
batch: [6030/21305] batch time: 0.059 trainign loss: 5.7938 avg training loss: 7.1485
batch: [6040/21305] batch time: 0.052 trainign loss: 2.5024 avg training loss: 7.1483
batch: [6050/21305] batch time: 0.062 trainign loss: 7.2541 avg training loss: 7.1483
batch: [6060/21305] batch time: 0.053 trainign loss: 7.0896 avg training loss: 7.1482
batch: [6070/21305] batch time: 0.056 trainign loss: 3.5096 avg training loss: 7.1481
batch: [6080/21305] batch time: 0.050 trainign loss: 4.2008 avg training loss: 7.1480
batch: [6090/21305] batch time: 0.056 trainign loss: 7.0143 avg training loss: 7.1479
batch: [6100/21305] batch time: 0.054 trainign loss: 7.9668 avg training loss: 7.1480
batch: [6110/21305] batch time: 0.057 trainign loss: 5.2274 avg training loss: 7.1479
batch: [6120/21305] batch time: 0.056 trainign loss: 6.5075 avg training loss: 7.1478
batch: [6130/21305] batch time: 0.063 trainign loss: 6.9580 avg training loss: 7.1477
batch: [6140/21305] batch time: 0.242 trainign loss: 6.0676 avg training loss: 7.1477
batch: [6150/21305] batch time: 0.056 trainign loss: 6.4392 avg training loss: 7.1477
batch: [6160/21305] batch time: 0.050 trainign loss: 5.6147 avg training loss: 7.1476
batch: [6170/21305] batch time: 0.057 trainign loss: 4.4796 avg training loss: 7.1476
batch: [6180/21305] batch time: 0.051 trainign loss: 6.7992 avg training loss: 7.1475
batch: [6190/21305] batch time: 0.056 trainign loss: 7.8367 avg training loss: 7.1475
batch: [6200/21305] batch time: 0.056 trainign loss: 6.8144 avg training loss: 7.1475
batch: [6210/21305] batch time: 0.056 trainign loss: 6.2110 avg training loss: 7.1474
batch: [6220/21305] batch time: 0.051 trainign loss: 6.9886 avg training loss: 7.1474
batch: [6230/21305] batch time: 0.056 trainign loss: 6.3817 avg training loss: 7.1474
batch: [6240/21305] batch time: 0.063 trainign loss: 7.1258 avg training loss: 7.1473
batch: [6250/21305] batch time: 0.057 trainign loss: 6.7014 avg training loss: 7.1473
batch: [6260/21305] batch time: 0.055 trainign loss: 7.5610 avg training loss: 7.1472
batch: [6270/21305] batch time: 0.056 trainign loss: 6.6203 avg training loss: 7.1472
batch: [6280/21305] batch time: 0.051 trainign loss: 6.8715 avg training loss: 7.1472
batch: [6290/21305] batch time: 0.057 trainign loss: 4.0576 avg training loss: 7.1471
batch: [6300/21305] batch time: 0.058 trainign loss: 7.2915 avg training loss: 7.1471
batch: [6310/21305] batch time: 0.061 trainign loss: 6.6732 avg training loss: 7.1470
batch: [6320/21305] batch time: 0.056 trainign loss: 6.8421 avg training loss: 7.1470
batch: [6330/21305] batch time: 0.056 trainign loss: 6.3666 avg training loss: 7.1469
batch: [6340/21305] batch time: 0.053 trainign loss: 4.2397 avg training loss: 7.1468
batch: [6350/21305] batch time: 0.056 trainign loss: 6.7669 avg training loss: 7.1468
batch: [6360/21305] batch time: 0.053 trainign loss: 7.5254 avg training loss: 7.1468
batch: [6370/21305] batch time: 0.057 trainign loss: 5.9294 avg training loss: 7.1468
batch: [6380/21305] batch time: 0.051 trainign loss: 6.5342 avg training loss: 7.1467
batch: [6390/21305] batch time: 0.056 trainign loss: 3.9017 avg training loss: 7.1466
batch: [6400/21305] batch time: 0.056 trainign loss: 6.6782 avg training loss: 7.1465
batch: [6410/21305] batch time: 0.056 trainign loss: 7.1203 avg training loss: 7.1465
batch: [6420/21305] batch time: 0.055 trainign loss: 6.1446 avg training loss: 7.1464
batch: [6430/21305] batch time: 0.057 trainign loss: 4.9886 avg training loss: 7.1463
batch: [6440/21305] batch time: 0.056 trainign loss: 6.1496 avg training loss: 7.1462
batch: [6450/21305] batch time: 0.058 trainign loss: 7.1284 avg training loss: 7.1461
batch: [6460/21305] batch time: 0.054 trainign loss: 6.9435 avg training loss: 7.1461
batch: [6470/21305] batch time: 0.056 trainign loss: 7.1373 avg training loss: 7.1461
batch: [6480/21305] batch time: 0.055 trainign loss: 6.2659 avg training loss: 7.1461
batch: [6490/21305] batch time: 0.056 trainign loss: 6.0054 avg training loss: 7.1460
batch: [6500/21305] batch time: 0.054 trainign loss: 5.2204 avg training loss: 7.1460
batch: [6510/21305] batch time: 0.061 trainign loss: 6.5093 avg training loss: 7.1459
batch: [6520/21305] batch time: 0.057 trainign loss: 6.0777 avg training loss: 7.1459
batch: [6530/21305] batch time: 0.063 trainign loss: 5.2396 avg training loss: 7.1458
batch: [6540/21305] batch time: 0.051 trainign loss: 6.8591 avg training loss: 7.1457
batch: [6550/21305] batch time: 0.056 trainign loss: 5.3582 avg training loss: 7.1457
batch: [6560/21305] batch time: 0.052 trainign loss: 6.3406 avg training loss: 7.1456
batch: [6570/21305] batch time: 0.059 trainign loss: 6.0364 avg training loss: 7.1456
batch: [6580/21305] batch time: 0.056 trainign loss: 4.4630 avg training loss: 7.1455
batch: [6590/21305] batch time: 0.056 trainign loss: 6.1478 avg training loss: 7.1455
batch: [6600/21305] batch time: 0.056 trainign loss: 7.2164 avg training loss: 7.1454
batch: [6610/21305] batch time: 0.056 trainign loss: 7.5473 avg training loss: 7.1454
batch: [6620/21305] batch time: 0.055 trainign loss: 6.5050 avg training loss: 7.1454
batch: [6630/21305] batch time: 0.056 trainign loss: 6.2388 avg training loss: 7.1453
batch: [6640/21305] batch time: 0.056 trainign loss: 4.8087 avg training loss: 7.1452
batch: [6650/21305] batch time: 0.058 trainign loss: 6.2035 avg training loss: 7.1452
batch: [6660/21305] batch time: 0.062 trainign loss: 5.8150 avg training loss: 7.1451
batch: [6670/21305] batch time: 0.063 trainign loss: 6.3197 avg training loss: 7.1450
batch: [6680/21305] batch time: 0.056 trainign loss: 7.3169 avg training loss: 7.1451
batch: [6690/21305] batch time: 0.063 trainign loss: 7.1173 avg training loss: 7.1451
batch: [6700/21305] batch time: 0.056 trainign loss: 4.9346 avg training loss: 7.1450
batch: [6710/21305] batch time: 0.056 trainign loss: 5.1824 avg training loss: 7.1449
batch: [6720/21305] batch time: 0.054 trainign loss: 6.5750 avg training loss: 7.1449
batch: [6730/21305] batch time: 0.056 trainign loss: 5.7937 avg training loss: 7.1448
batch: [6740/21305] batch time: 0.051 trainign loss: 7.5754 avg training loss: 7.1448
batch: [6750/21305] batch time: 0.061 trainign loss: 7.2286 avg training loss: 7.1448
batch: [6760/21305] batch time: 0.056 trainign loss: 6.4792 avg training loss: 7.1447
batch: [6770/21305] batch time: 0.056 trainign loss: 5.3344 avg training loss: 7.1447
batch: [6780/21305] batch time: 0.050 trainign loss: 5.8046 avg training loss: 7.1446
batch: [6790/21305] batch time: 0.056 trainign loss: 7.1400 avg training loss: 7.1446
batch: [6800/21305] batch time: 0.341 trainign loss: 6.0000 avg training loss: 7.1446
batch: [6810/21305] batch time: 0.058 trainign loss: 6.0567 avg training loss: 7.1445
batch: [6820/21305] batch time: 0.070 trainign loss: 5.3315 avg training loss: 7.1445
batch: [6830/21305] batch time: 0.056 trainign loss: 6.6021 avg training loss: 7.1444
batch: [6840/21305] batch time: 0.058 trainign loss: 7.6891 avg training loss: 7.1442
batch: [6850/21305] batch time: 0.058 trainign loss: 5.8041 avg training loss: 7.1442
batch: [6860/21305] batch time: 0.392 trainign loss: 5.9958 avg training loss: 7.1442
batch: [6870/21305] batch time: 0.058 trainign loss: 6.3904 avg training loss: 7.1441
batch: [6880/21305] batch time: 0.062 trainign loss: 5.8286 avg training loss: 7.1440
batch: [6890/21305] batch time: 0.056 trainign loss: 7.2719 avg training loss: 7.1439
batch: [6900/21305] batch time: 0.517 trainign loss: 6.2694 avg training loss: 7.1439
batch: [6910/21305] batch time: 0.063 trainign loss: 6.2376 avg training loss: 7.1439
batch: [6920/21305] batch time: 0.801 trainign loss: 6.8525 avg training loss: 7.1438
batch: [6930/21305] batch time: 0.063 trainign loss: 5.2632 avg training loss: 7.1438
batch: [6940/21305] batch time: 1.476 trainign loss: 5.1315 avg training loss: 7.1436
batch: [6950/21305] batch time: 0.062 trainign loss: 6.3865 avg training loss: 7.1436
batch: [6960/21305] batch time: 1.830 trainign loss: 4.3012 avg training loss: 7.1435
batch: [6970/21305] batch time: 0.062 trainign loss: 3.7881 avg training loss: 7.1434
batch: [6980/21305] batch time: 0.417 trainign loss: 6.8470 avg training loss: 7.1434
batch: [6990/21305] batch time: 0.056 trainign loss: 7.0534 avg training loss: 7.1434
batch: [7000/21305] batch time: 0.626 trainign loss: 5.5443 avg training loss: 7.1434
batch: [7010/21305] batch time: 0.056 trainign loss: 6.0817 avg training loss: 7.1433
batch: [7020/21305] batch time: 0.410 trainign loss: 7.5765 avg training loss: 7.1433
batch: [7030/21305] batch time: 0.056 trainign loss: 6.3074 avg training loss: 7.1433
batch: [7040/21305] batch time: 0.056 trainign loss: 6.8577 avg training loss: 7.1432
batch: [7050/21305] batch time: 0.056 trainign loss: 5.7372 avg training loss: 7.1431
batch: [7060/21305] batch time: 0.054 trainign loss: 5.6617 avg training loss: 7.1431
batch: [7070/21305] batch time: 0.056 trainign loss: 7.0276 avg training loss: 7.1430
batch: [7080/21305] batch time: 0.058 trainign loss: 6.4021 avg training loss: 7.1430
batch: [7090/21305] batch time: 0.060 trainign loss: 5.8521 avg training loss: 7.1430
batch: [7100/21305] batch time: 0.056 trainign loss: 6.0834 avg training loss: 7.1429
batch: [7110/21305] batch time: 0.056 trainign loss: 6.3969 avg training loss: 7.1428
batch: [7120/21305] batch time: 0.056 trainign loss: 4.2345 avg training loss: 7.1426
batch: [7130/21305] batch time: 0.057 trainign loss: 7.8588 avg training loss: 7.1426
batch: [7140/21305] batch time: 0.055 trainign loss: 7.4332 avg training loss: 7.1426
batch: [7150/21305] batch time: 0.062 trainign loss: 5.3434 avg training loss: 7.1426
batch: [7160/21305] batch time: 0.056 trainign loss: 7.6970 avg training loss: 7.1425
batch: [7170/21305] batch time: 0.056 trainign loss: 6.0751 avg training loss: 7.1425
batch: [7180/21305] batch time: 0.057 trainign loss: 6.4693 avg training loss: 7.1425
batch: [7190/21305] batch time: 0.056 trainign loss: 4.2599 avg training loss: 7.1424
batch: [7200/21305] batch time: 0.058 trainign loss: 5.8035 avg training loss: 7.1423
batch: [7210/21305] batch time: 0.056 trainign loss: 6.1410 avg training loss: 7.1423
batch: [7220/21305] batch time: 0.056 trainign loss: 5.3967 avg training loss: 7.1422
batch: [7230/21305] batch time: 0.056 trainign loss: 5.2962 avg training loss: 7.1421
batch: [7240/21305] batch time: 0.056 trainign loss: 7.2010 avg training loss: 7.1421
batch: [7250/21305] batch time: 0.056 trainign loss: 5.9620 avg training loss: 7.1421
batch: [7260/21305] batch time: 0.051 trainign loss: 5.0318 avg training loss: 7.1420
batch: [7270/21305] batch time: 0.056 trainign loss: 7.0251 avg training loss: 7.1420
batch: [7280/21305] batch time: 0.062 trainign loss: 5.9660 avg training loss: 7.1420
batch: [7290/21305] batch time: 0.056 trainign loss: 5.0242 avg training loss: 7.1419
batch: [7300/21305] batch time: 0.056 trainign loss: 6.5880 avg training loss: 7.1418
batch: [7310/21305] batch time: 0.056 trainign loss: 7.0385 avg training loss: 7.1418
batch: [7320/21305] batch time: 0.052 trainign loss: 6.8859 avg training loss: 7.1418
batch: [7330/21305] batch time: 0.056 trainign loss: 5.3279 avg training loss: 7.1417
batch: [7340/21305] batch time: 0.054 trainign loss: 7.0860 avg training loss: 7.1417
batch: [7350/21305] batch time: 0.054 trainign loss: 7.1028 avg training loss: 7.1416
batch: [7360/21305] batch time: 0.052 trainign loss: 6.8322 avg training loss: 7.1416
batch: [7370/21305] batch time: 0.052 trainign loss: 2.8801 avg training loss: 7.1415
batch: [7380/21305] batch time: 0.052 trainign loss: 5.8579 avg training loss: 7.1414
batch: [7390/21305] batch time: 0.056 trainign loss: 7.2731 avg training loss: 7.1413
batch: [7400/21305] batch time: 0.054 trainign loss: 4.7777 avg training loss: 7.1412
batch: [7410/21305] batch time: 0.056 trainign loss: 7.3299 avg training loss: 7.1411
batch: [7420/21305] batch time: 0.056 trainign loss: 7.3738 avg training loss: 7.1411
batch: [7430/21305] batch time: 0.056 trainign loss: 6.8574 avg training loss: 7.1411
batch: [7440/21305] batch time: 0.058 trainign loss: 6.3976 avg training loss: 7.1411
batch: [7450/21305] batch time: 0.063 trainign loss: 6.6867 avg training loss: 7.1411
batch: [7460/21305] batch time: 0.058 trainign loss: 3.1281 avg training loss: 7.1409
batch: [7470/21305] batch time: 0.056 trainign loss: 3.8113 avg training loss: 7.1409
batch: [7480/21305] batch time: 0.056 trainign loss: 6.6479 avg training loss: 7.1408
batch: [7490/21305] batch time: 0.056 trainign loss: 6.5280 avg training loss: 7.1408
batch: [7500/21305] batch time: 0.052 trainign loss: 7.2250 avg training loss: 7.1408
batch: [7510/21305] batch time: 0.057 trainign loss: 5.8751 avg training loss: 7.1407
batch: [7520/21305] batch time: 0.056 trainign loss: 7.3309 avg training loss: 7.1407
batch: [7530/21305] batch time: 0.056 trainign loss: 5.4184 avg training loss: 7.1407
batch: [7540/21305] batch time: 0.057 trainign loss: 7.0372 avg training loss: 7.1406
batch: [7550/21305] batch time: 0.062 trainign loss: 7.5579 avg training loss: 7.1406
batch: [7560/21305] batch time: 0.051 trainign loss: 6.7376 avg training loss: 7.1406
batch: [7570/21305] batch time: 0.059 trainign loss: 6.7329 avg training loss: 7.1405
batch: [7580/21305] batch time: 0.056 trainign loss: 6.1698 avg training loss: 7.1405
batch: [7590/21305] batch time: 0.056 trainign loss: 7.4007 avg training loss: 7.1405
batch: [7600/21305] batch time: 0.056 trainign loss: 6.3304 avg training loss: 7.1404
batch: [7610/21305] batch time: 0.057 trainign loss: 5.8900 avg training loss: 7.1404
batch: [7620/21305] batch time: 0.051 trainign loss: 4.6581 avg training loss: 7.1403
batch: [7630/21305] batch time: 0.057 trainign loss: 7.4378 avg training loss: 7.1402
batch: [7640/21305] batch time: 0.056 trainign loss: 6.6910 avg training loss: 7.1402
batch: [7650/21305] batch time: 0.061 trainign loss: 5.2861 avg training loss: 7.1401
batch: [7660/21305] batch time: 0.053 trainign loss: 7.0892 avg training loss: 7.1400
batch: [7670/21305] batch time: 0.055 trainign loss: 7.1688 avg training loss: 7.1400
batch: [7680/21305] batch time: 0.063 trainign loss: 5.6263 avg training loss: 7.1400
batch: [7690/21305] batch time: 0.051 trainign loss: 5.1252 avg training loss: 7.1399
batch: [7700/21305] batch time: 0.053 trainign loss: 6.6993 avg training loss: 7.1398
batch: [7710/21305] batch time: 0.056 trainign loss: 5.3869 avg training loss: 7.1397
batch: [7720/21305] batch time: 0.058 trainign loss: 5.8860 avg training loss: 7.1396
batch: [7730/21305] batch time: 0.057 trainign loss: 4.8009 avg training loss: 7.1394
batch: [7740/21305] batch time: 0.063 trainign loss: 7.6117 avg training loss: 7.1394
batch: [7750/21305] batch time: 0.052 trainign loss: 7.6141 avg training loss: 7.1394
batch: [7760/21305] batch time: 0.052 trainign loss: 6.7929 avg training loss: 7.1394
batch: [7770/21305] batch time: 0.054 trainign loss: 4.4122 avg training loss: 7.1393
batch: [7780/21305] batch time: 0.051 trainign loss: 6.9084 avg training loss: 7.1393
batch: [7790/21305] batch time: 0.056 trainign loss: 7.2727 avg training loss: 7.1393
batch: [7800/21305] batch time: 0.057 trainign loss: 6.9194 avg training loss: 7.1393
batch: [7810/21305] batch time: 0.056 trainign loss: 5.8630 avg training loss: 7.1392
batch: [7820/21305] batch time: 0.573 trainign loss: 6.6374 avg training loss: 7.1391
batch: [7830/21305] batch time: 0.056 trainign loss: 6.1820 avg training loss: 7.1391
batch: [7840/21305] batch time: 0.489 trainign loss: 6.9472 avg training loss: 7.1391
batch: [7850/21305] batch time: 0.058 trainign loss: 5.0757 avg training loss: 7.1391
batch: [7860/21305] batch time: 0.485 trainign loss: 5.2769 avg training loss: 7.1390
batch: [7870/21305] batch time: 0.056 trainign loss: 6.0401 avg training loss: 7.1389
batch: [7880/21305] batch time: 0.256 trainign loss: 4.3854 avg training loss: 7.1389
batch: [7890/21305] batch time: 0.056 trainign loss: 5.7975 avg training loss: 7.1388
batch: [7900/21305] batch time: 0.056 trainign loss: 5.8326 avg training loss: 7.1387
batch: [7910/21305] batch time: 0.058 trainign loss: 7.2873 avg training loss: 7.1387
batch: [7920/21305] batch time: 0.082 trainign loss: 5.8386 avg training loss: 7.1387
batch: [7930/21305] batch time: 0.056 trainign loss: 6.1478 avg training loss: 7.1386
batch: [7940/21305] batch time: 0.054 trainign loss: 7.3234 avg training loss: 7.1386
batch: [7950/21305] batch time: 0.054 trainign loss: 7.0626 avg training loss: 7.1386
batch: [7960/21305] batch time: 0.056 trainign loss: 6.0248 avg training loss: 7.1385
batch: [7970/21305] batch time: 0.052 trainign loss: 6.2678 avg training loss: 7.1385
batch: [7980/21305] batch time: 0.051 trainign loss: 7.0892 avg training loss: 7.1384
batch: [7990/21305] batch time: 0.060 trainign loss: 7.2739 avg training loss: 7.1384
batch: [8000/21305] batch time: 0.052 trainign loss: 6.8647 avg training loss: 7.1384
batch: [8010/21305] batch time: 0.054 trainign loss: 5.6722 avg training loss: 7.1383
batch: [8020/21305] batch time: 0.052 trainign loss: 1.9842 avg training loss: 7.1381
batch: [8030/21305] batch time: 0.056 trainign loss: 5.5407 avg training loss: 7.1380
batch: [8040/21305] batch time: 0.059 trainign loss: 7.2547 avg training loss: 7.1380
batch: [8050/21305] batch time: 0.057 trainign loss: 8.0418 avg training loss: 7.1380
batch: [8060/21305] batch time: 0.051 trainign loss: 7.0249 avg training loss: 7.1380
batch: [8070/21305] batch time: 0.056 trainign loss: 6.1470 avg training loss: 7.1380
batch: [8080/21305] batch time: 0.056 trainign loss: 6.9249 avg training loss: 7.1380
batch: [8090/21305] batch time: 0.056 trainign loss: 6.7988 avg training loss: 7.1379
batch: [8100/21305] batch time: 0.054 trainign loss: 6.1958 avg training loss: 7.1379
batch: [8110/21305] batch time: 0.056 trainign loss: 7.0835 avg training loss: 7.1378
batch: [8120/21305] batch time: 0.053 trainign loss: 6.4169 avg training loss: 7.1378
batch: [8130/21305] batch time: 0.406 trainign loss: 6.1863 avg training loss: 7.1377
batch: [8140/21305] batch time: 0.058 trainign loss: 6.2137 avg training loss: 7.1377
batch: [8150/21305] batch time: 0.053 trainign loss: 6.0844 avg training loss: 7.1376
batch: [8160/21305] batch time: 0.056 trainign loss: 6.6675 avg training loss: 7.1376
batch: [8170/21305] batch time: 0.062 trainign loss: 6.3760 avg training loss: 7.1375
batch: [8180/21305] batch time: 0.056 trainign loss: 5.7277 avg training loss: 7.1375
batch: [8190/21305] batch time: 0.785 trainign loss: 7.1641 avg training loss: 7.1375
batch: [8200/21305] batch time: 0.056 trainign loss: 6.8635 avg training loss: 7.1375
batch: [8210/21305] batch time: 0.180 trainign loss: 5.9726 avg training loss: 7.1374
batch: [8220/21305] batch time: 0.056 trainign loss: 6.3019 avg training loss: 7.1374
batch: [8230/21305] batch time: 0.062 trainign loss: 6.8455 avg training loss: 7.1373
batch: [8240/21305] batch time: 0.056 trainign loss: 7.2958 avg training loss: 7.1373
batch: [8250/21305] batch time: 0.250 trainign loss: 6.3194 avg training loss: 7.1373
batch: [8260/21305] batch time: 0.062 trainign loss: 6.5659 avg training loss: 7.1372
batch: [8270/21305] batch time: 0.143 trainign loss: 7.5701 avg training loss: 7.1372
batch: [8280/21305] batch time: 0.056 trainign loss: 6.5866 avg training loss: 7.1372
batch: [8290/21305] batch time: 0.053 trainign loss: 5.9321 avg training loss: 7.1371
batch: [8300/21305] batch time: 0.054 trainign loss: 5.8674 avg training loss: 7.1371
batch: [8310/21305] batch time: 0.063 trainign loss: 7.3587 avg training loss: 7.1370
batch: [8320/21305] batch time: 0.062 trainign loss: 6.4969 avg training loss: 7.1370
batch: [8330/21305] batch time: 0.056 trainign loss: 7.1531 avg training loss: 7.1370
batch: [8340/21305] batch time: 0.053 trainign loss: 5.8895 avg training loss: 7.1369
batch: [8350/21305] batch time: 0.062 trainign loss: 4.4075 avg training loss: 7.1368
batch: [8360/21305] batch time: 0.062 trainign loss: 5.6128 avg training loss: 7.1368
batch: [8370/21305] batch time: 0.056 trainign loss: 7.2410 avg training loss: 7.1367
batch: [8380/21305] batch time: 0.056 trainign loss: 5.1455 avg training loss: 7.1367
batch: [8390/21305] batch time: 0.056 trainign loss: 4.0463 avg training loss: 7.1365
batch: [8400/21305] batch time: 0.056 trainign loss: 8.9671 avg training loss: 7.1363
batch: [8410/21305] batch time: 0.056 trainign loss: 5.9243 avg training loss: 7.1363
batch: [8420/21305] batch time: 0.058 trainign loss: 6.9968 avg training loss: 7.1361
batch: [8430/21305] batch time: 0.062 trainign loss: 7.7521 avg training loss: 7.1361
batch: [8440/21305] batch time: 0.052 trainign loss: 7.5314 avg training loss: 7.1362
batch: [8450/21305] batch time: 0.055 trainign loss: 7.0960 avg training loss: 7.1361
batch: [8460/21305] batch time: 0.050 trainign loss: 7.0761 avg training loss: 7.1361
batch: [8470/21305] batch time: 0.058 trainign loss: 7.5761 avg training loss: 7.1361
batch: [8480/21305] batch time: 0.062 trainign loss: 6.8581 avg training loss: 7.1361
batch: [8490/21305] batch time: 0.056 trainign loss: 6.4749 avg training loss: 7.1360
batch: [8500/21305] batch time: 0.059 trainign loss: 5.4183 avg training loss: 7.1359
batch: [8510/21305] batch time: 0.057 trainign loss: 5.3735 avg training loss: 7.1358
batch: [8520/21305] batch time: 0.061 trainign loss: 7.1331 avg training loss: 7.1357
batch: [8530/21305] batch time: 1.144 trainign loss: 6.9448 avg training loss: 7.1357
batch: [8540/21305] batch time: 0.060 trainign loss: 7.0772 avg training loss: 7.1357
batch: [8550/21305] batch time: 1.366 trainign loss: 7.6287 avg training loss: 7.1357
batch: [8560/21305] batch time: 0.057 trainign loss: 6.8707 avg training loss: 7.1356
batch: [8570/21305] batch time: 0.089 trainign loss: 6.0416 avg training loss: 7.1356
batch: [8580/21305] batch time: 0.053 trainign loss: 6.4365 avg training loss: 7.1356
batch: [8590/21305] batch time: 0.063 trainign loss: 7.4695 avg training loss: 7.1355
batch: [8600/21305] batch time: 0.059 trainign loss: 6.3904 avg training loss: 7.1355
batch: [8610/21305] batch time: 0.134 trainign loss: 5.7170 avg training loss: 7.1354
batch: [8620/21305] batch time: 0.057 trainign loss: 6.6315 avg training loss: 7.1354
batch: [8630/21305] batch time: 0.257 trainign loss: 5.0840 avg training loss: 7.1353
batch: [8640/21305] batch time: 0.056 trainign loss: 6.7960 avg training loss: 7.1353
batch: [8650/21305] batch time: 0.552 trainign loss: 3.9763 avg training loss: 7.1352
batch: [8660/21305] batch time: 0.057 trainign loss: 4.8910 avg training loss: 7.1351
batch: [8670/21305] batch time: 0.673 trainign loss: 5.5998 avg training loss: 7.1351
batch: [8680/21305] batch time: 0.056 trainign loss: 6.8431 avg training loss: 7.1351
batch: [8690/21305] batch time: 0.054 trainign loss: 5.7105 avg training loss: 7.1350
batch: [8700/21305] batch time: 0.056 trainign loss: 5.1065 avg training loss: 7.1349
batch: [8710/21305] batch time: 0.848 trainign loss: 1.2853 avg training loss: 7.1348
batch: [8720/21305] batch time: 0.061 trainign loss: 0.0016 avg training loss: 7.1344
batch: [8730/21305] batch time: 0.960 trainign loss: 0.0001 avg training loss: 7.1340
batch: [8740/21305] batch time: 0.062 trainign loss: 8.2480 avg training loss: 7.1340
batch: [8750/21305] batch time: 1.241 trainign loss: 7.9552 avg training loss: 7.1341
batch: [8760/21305] batch time: 0.056 trainign loss: 7.5009 avg training loss: 7.1340
batch: [8770/21305] batch time: 0.412 trainign loss: 6.6412 avg training loss: 7.1340
batch: [8780/21305] batch time: 0.062 trainign loss: 5.9876 avg training loss: 7.1340
batch: [8790/21305] batch time: 1.277 trainign loss: 6.2927 avg training loss: 7.1340
batch: [8800/21305] batch time: 0.061 trainign loss: 5.8222 avg training loss: 7.1339
batch: [8810/21305] batch time: 1.393 trainign loss: 7.2948 avg training loss: 7.1338
batch: [8820/21305] batch time: 0.057 trainign loss: 6.5366 avg training loss: 7.1338
batch: [8830/21305] batch time: 1.887 trainign loss: 6.8323 avg training loss: 7.1338
batch: [8840/21305] batch time: 0.055 trainign loss: 6.3597 avg training loss: 7.1338
batch: [8850/21305] batch time: 0.842 trainign loss: 6.3351 avg training loss: 7.1337
batch: [8860/21305] batch time: 0.054 trainign loss: 7.2148 avg training loss: 7.1337
batch: [8870/21305] batch time: 0.235 trainign loss: 7.1694 avg training loss: 7.1337
batch: [8880/21305] batch time: 0.051 trainign loss: 5.1085 avg training loss: 7.1337
batch: [8890/21305] batch time: 0.056 trainign loss: 7.1259 avg training loss: 7.1336
batch: [8900/21305] batch time: 0.052 trainign loss: 5.6933 avg training loss: 7.1336
batch: [8910/21305] batch time: 0.057 trainign loss: 5.3483 avg training loss: 7.1335
batch: [8920/21305] batch time: 0.056 trainign loss: 2.1234 avg training loss: 7.1334
batch: [8930/21305] batch time: 0.057 trainign loss: 7.5830 avg training loss: 7.1333
batch: [8940/21305] batch time: 0.056 trainign loss: 6.6078 avg training loss: 7.1333
batch: [8950/21305] batch time: 0.058 trainign loss: 5.8398 avg training loss: 7.1333
batch: [8960/21305] batch time: 0.050 trainign loss: 6.3509 avg training loss: 7.1332
batch: [8970/21305] batch time: 0.058 trainign loss: 6.4487 avg training loss: 7.1332
batch: [8980/21305] batch time: 0.050 trainign loss: 5.4702 avg training loss: 7.1331
batch: [8990/21305] batch time: 0.056 trainign loss: 6.8982 avg training loss: 7.1330
batch: [9000/21305] batch time: 0.056 trainign loss: 0.7367 avg training loss: 7.1329
batch: [9010/21305] batch time: 0.056 trainign loss: 8.3494 avg training loss: 7.1328
batch: [9020/21305] batch time: 0.052 trainign loss: 6.1532 avg training loss: 7.1328
batch: [9030/21305] batch time: 0.057 trainign loss: 5.5757 avg training loss: 7.1327
batch: [9040/21305] batch time: 0.060 trainign loss: 6.9151 avg training loss: 7.1327
batch: [9050/21305] batch time: 0.058 trainign loss: 6.1407 avg training loss: 7.1327
batch: [9060/21305] batch time: 0.056 trainign loss: 5.0885 avg training loss: 7.1326
batch: [9070/21305] batch time: 0.056 trainign loss: 3.2497 avg training loss: 7.1325
batch: [9080/21305] batch time: 0.063 trainign loss: 8.0644 avg training loss: 7.1325
batch: [9090/21305] batch time: 0.056 trainign loss: 7.0490 avg training loss: 7.1325
batch: [9100/21305] batch time: 0.056 trainign loss: 7.1615 avg training loss: 7.1325
batch: [9110/21305] batch time: 0.059 trainign loss: 5.8660 avg training loss: 7.1324
batch: [9120/21305] batch time: 0.051 trainign loss: 6.6794 avg training loss: 7.1324
batch: [9130/21305] batch time: 0.062 trainign loss: 4.9201 avg training loss: 7.1322
batch: [9140/21305] batch time: 0.056 trainign loss: 6.9275 avg training loss: 7.1321
batch: [9150/21305] batch time: 0.062 trainign loss: 6.4895 avg training loss: 7.1321
batch: [9160/21305] batch time: 0.056 trainign loss: 6.9646 avg training loss: 7.1321
batch: [9170/21305] batch time: 0.058 trainign loss: 5.8786 avg training loss: 7.1320
batch: [9180/21305] batch time: 0.058 trainign loss: 7.1160 avg training loss: 7.1320
batch: [9190/21305] batch time: 0.056 trainign loss: 6.5617 avg training loss: 7.1320
batch: [9200/21305] batch time: 0.054 trainign loss: 6.6980 avg training loss: 7.1319
batch: [9210/21305] batch time: 0.056 trainign loss: 5.7161 avg training loss: 7.1318
batch: [9220/21305] batch time: 0.062 trainign loss: 4.9105 avg training loss: 7.1318
batch: [9230/21305] batch time: 0.904 trainign loss: 6.4118 avg training loss: 7.1317
batch: [9240/21305] batch time: 0.063 trainign loss: 5.2228 avg training loss: 7.1317
batch: [9250/21305] batch time: 1.658 trainign loss: 6.9538 avg training loss: 7.1316
batch: [9260/21305] batch time: 0.056 trainign loss: 6.8945 avg training loss: 7.1316
batch: [9270/21305] batch time: 2.359 trainign loss: 6.8519 avg training loss: 7.1316
batch: [9280/21305] batch time: 0.062 trainign loss: 6.9221 avg training loss: 7.1315
batch: [9290/21305] batch time: 2.404 trainign loss: 5.0472 avg training loss: 7.1315
batch: [9300/21305] batch time: 0.058 trainign loss: 7.7702 avg training loss: 7.1314
batch: [9310/21305] batch time: 2.137 trainign loss: 7.6237 avg training loss: 7.1314
batch: [9320/21305] batch time: 0.062 trainign loss: 5.2892 avg training loss: 7.1314
batch: [9330/21305] batch time: 2.321 trainign loss: 4.9080 avg training loss: 7.1313
batch: [9340/21305] batch time: 0.057 trainign loss: 7.0898 avg training loss: 7.1312
batch: [9350/21305] batch time: 2.755 trainign loss: 7.0985 avg training loss: 7.1312
batch: [9360/21305] batch time: 0.058 trainign loss: 4.8807 avg training loss: 7.1312
batch: [9370/21305] batch time: 2.556 trainign loss: 5.5237 avg training loss: 7.1311
batch: [9380/21305] batch time: 0.056 trainign loss: 5.8792 avg training loss: 7.1311
batch: [9390/21305] batch time: 2.359 trainign loss: 6.1485 avg training loss: 7.1310
batch: [9400/21305] batch time: 0.060 trainign loss: 6.6898 avg training loss: 7.1310
batch: [9410/21305] batch time: 2.233 trainign loss: 5.6921 avg training loss: 7.1309
batch: [9420/21305] batch time: 0.056 trainign loss: 3.6048 avg training loss: 7.1308
batch: [9430/21305] batch time: 2.672 trainign loss: 6.6686 avg training loss: 7.1308
batch: [9440/21305] batch time: 0.056 trainign loss: 7.2420 avg training loss: 7.1307
batch: [9450/21305] batch time: 2.220 trainign loss: 4.9759 avg training loss: 7.1306
batch: [9460/21305] batch time: 0.062 trainign loss: 6.8426 avg training loss: 7.1306
batch: [9470/21305] batch time: 2.297 trainign loss: 7.4908 avg training loss: 7.1305
batch: [9480/21305] batch time: 0.057 trainign loss: 5.4717 avg training loss: 7.1305
batch: [9490/21305] batch time: 2.236 trainign loss: 7.5209 avg training loss: 7.1304
batch: [9500/21305] batch time: 0.053 trainign loss: 7.3100 avg training loss: 7.1304
batch: [9510/21305] batch time: 2.345 trainign loss: 5.7976 avg training loss: 7.1303
batch: [9520/21305] batch time: 0.056 trainign loss: 7.2168 avg training loss: 7.1303
batch: [9530/21305] batch time: 2.399 trainign loss: 6.7791 avg training loss: 7.1303
batch: [9540/21305] batch time: 0.061 trainign loss: 5.8626 avg training loss: 7.1302
batch: [9550/21305] batch time: 2.157 trainign loss: 4.7542 avg training loss: 7.1301
batch: [9560/21305] batch time: 0.057 trainign loss: 5.2757 avg training loss: 7.1300
batch: [9570/21305] batch time: 2.150 trainign loss: 6.6276 avg training loss: 7.1300
batch: [9580/21305] batch time: 0.056 trainign loss: 8.1201 avg training loss: 7.1299
batch: [9590/21305] batch time: 2.592 trainign loss: 7.1638 avg training loss: 7.1299
batch: [9600/21305] batch time: 0.058 trainign loss: 6.8852 avg training loss: 7.1299
batch: [9610/21305] batch time: 2.507 trainign loss: 6.3517 avg training loss: 7.1298
batch: [9620/21305] batch time: 0.053 trainign loss: 6.7105 avg training loss: 7.1297
batch: [9630/21305] batch time: 2.096 trainign loss: 7.4085 avg training loss: 7.1297
batch: [9640/21305] batch time: 0.056 trainign loss: 7.1616 avg training loss: 7.1297
batch: [9650/21305] batch time: 2.131 trainign loss: 5.8945 avg training loss: 7.1296
batch: [9660/21305] batch time: 0.056 trainign loss: 3.9372 avg training loss: 7.1295
batch: [9670/21305] batch time: 2.086 trainign loss: 6.8413 avg training loss: 7.1295
batch: [9680/21305] batch time: 0.056 trainign loss: 7.0325 avg training loss: 7.1295
batch: [9690/21305] batch time: 2.294 trainign loss: 6.4672 avg training loss: 7.1295
batch: [9700/21305] batch time: 0.056 trainign loss: 6.4232 avg training loss: 7.1294
batch: [9710/21305] batch time: 2.187 trainign loss: 6.1519 avg training loss: 7.1294
batch: [9720/21305] batch time: 0.052 trainign loss: 6.9259 avg training loss: 7.1293
batch: [9730/21305] batch time: 2.141 trainign loss: 6.7637 avg training loss: 7.1293
batch: [9740/21305] batch time: 0.056 trainign loss: 6.3206 avg training loss: 7.1293
batch: [9750/21305] batch time: 1.830 trainign loss: 7.0369 avg training loss: 7.1292
batch: [9760/21305] batch time: 1.000 trainign loss: 6.3488 avg training loss: 7.1292
batch: [9770/21305] batch time: 1.057 trainign loss: 6.7171 avg training loss: 7.1291
batch: [9780/21305] batch time: 1.154 trainign loss: 7.4660 avg training loss: 7.1291
batch: [9790/21305] batch time: 0.800 trainign loss: 6.3870 avg training loss: 7.1290
batch: [9800/21305] batch time: 1.372 trainign loss: 6.3587 avg training loss: 7.1290
batch: [9810/21305] batch time: 1.023 trainign loss: 7.7113 avg training loss: 7.1289
batch: [9820/21305] batch time: 0.565 trainign loss: 5.2635 avg training loss: 7.1289
batch: [9830/21305] batch time: 1.776 trainign loss: 5.3202 avg training loss: 7.1288
batch: [9840/21305] batch time: 1.345 trainign loss: 4.9612 avg training loss: 7.1287
batch: [9850/21305] batch time: 1.181 trainign loss: 6.7967 avg training loss: 7.1287
batch: [9860/21305] batch time: 1.250 trainign loss: 2.8497 avg training loss: 7.1286
batch: [9870/21305] batch time: 1.301 trainign loss: 4.8272 avg training loss: 7.1286
batch: [9880/21305] batch time: 0.249 trainign loss: 7.6964 avg training loss: 7.1285
batch: [9890/21305] batch time: 1.876 trainign loss: 6.6608 avg training loss: 7.1285
batch: [9900/21305] batch time: 0.785 trainign loss: 5.9227 avg training loss: 7.1284
batch: [9910/21305] batch time: 1.132 trainign loss: 6.4098 avg training loss: 7.1284
batch: [9920/21305] batch time: 0.060 trainign loss: 6.3898 avg training loss: 7.1283
batch: [9930/21305] batch time: 0.086 trainign loss: 5.7806 avg training loss: 7.1283
batch: [9940/21305] batch time: 0.063 trainign loss: 7.6182 avg training loss: 7.1283
batch: [9950/21305] batch time: 0.777 trainign loss: 6.4058 avg training loss: 7.1282
batch: [9960/21305] batch time: 0.104 trainign loss: 7.3498 avg training loss: 7.1282
batch: [9970/21305] batch time: 0.754 trainign loss: 6.7274 avg training loss: 7.1282
batch: [9980/21305] batch time: 0.296 trainign loss: 5.8241 avg training loss: 7.1281
batch: [9990/21305] batch time: 1.145 trainign loss: 6.7597 avg training loss: 7.1281
batch: [10000/21305] batch time: 0.386 trainign loss: 5.3526 avg training loss: 7.1281
batch: [10010/21305] batch time: 1.346 trainign loss: 6.6319 avg training loss: 7.1280
batch: [10020/21305] batch time: 0.061 trainign loss: 7.2121 avg training loss: 7.1280
batch: [10030/21305] batch time: 0.843 trainign loss: 6.0033 avg training loss: 7.1280
batch: [10040/21305] batch time: 1.020 trainign loss: 6.0513 avg training loss: 7.1280
batch: [10050/21305] batch time: 0.281 trainign loss: 5.6832 avg training loss: 7.1279
batch: [10060/21305] batch time: 0.888 trainign loss: 4.7820 avg training loss: 7.1278
batch: [10070/21305] batch time: 0.482 trainign loss: 6.4157 avg training loss: 7.1277
batch: [10080/21305] batch time: 0.497 trainign loss: 5.4569 avg training loss: 7.1277
batch: [10090/21305] batch time: 0.968 trainign loss: 8.1209 avg training loss: 7.1276
batch: [10100/21305] batch time: 0.377 trainign loss: 8.0376 avg training loss: 7.1275
batch: [10110/21305] batch time: 1.629 trainign loss: 6.5116 avg training loss: 7.1275
batch: [10120/21305] batch time: 0.396 trainign loss: 6.9324 avg training loss: 7.1275
batch: [10130/21305] batch time: 0.889 trainign loss: 5.9430 avg training loss: 7.1275
batch: [10140/21305] batch time: 0.650 trainign loss: 7.1252 avg training loss: 7.1274
batch: [10150/21305] batch time: 1.453 trainign loss: 6.5262 avg training loss: 7.1274
batch: [10160/21305] batch time: 0.061 trainign loss: 5.7644 avg training loss: 7.1273
batch: [10170/21305] batch time: 1.016 trainign loss: 6.9644 avg training loss: 7.1273
batch: [10180/21305] batch time: 0.413 trainign loss: 6.6024 avg training loss: 7.1273
batch: [10190/21305] batch time: 1.858 trainign loss: 6.6937 avg training loss: 7.1272
batch: [10200/21305] batch time: 0.417 trainign loss: 5.3749 avg training loss: 7.1272
batch: [10210/21305] batch time: 1.495 trainign loss: 6.4859 avg training loss: 7.1271
batch: [10220/21305] batch time: 0.864 trainign loss: 5.8251 avg training loss: 7.1270
batch: [10230/21305] batch time: 1.692 trainign loss: 5.1216 avg training loss: 7.1270
batch: [10240/21305] batch time: 0.056 trainign loss: 6.9694 avg training loss: 7.1269
batch: [10250/21305] batch time: 1.624 trainign loss: 6.7115 avg training loss: 7.1268
batch: [10260/21305] batch time: 0.056 trainign loss: 7.5187 avg training loss: 7.1268
batch: [10270/21305] batch time: 1.467 trainign loss: 7.1045 avg training loss: 7.1267
batch: [10280/21305] batch time: 0.061 trainign loss: 7.0840 avg training loss: 7.1267
batch: [10290/21305] batch time: 1.500 trainign loss: 6.3315 avg training loss: 7.1267
batch: [10300/21305] batch time: 0.057 trainign loss: 5.8189 avg training loss: 7.1266
batch: [10310/21305] batch time: 0.571 trainign loss: 5.4868 avg training loss: 7.1265
batch: [10320/21305] batch time: 0.055 trainign loss: 1.9370 avg training loss: 7.1264
batch: [10330/21305] batch time: 1.733 trainign loss: 7.0555 avg training loss: 7.1264
batch: [10340/21305] batch time: 0.062 trainign loss: 7.1425 avg training loss: 7.1264
batch: [10350/21305] batch time: 1.965 trainign loss: 7.2796 avg training loss: 7.1263
batch: [10360/21305] batch time: 0.057 trainign loss: 6.3635 avg training loss: 7.1263
batch: [10370/21305] batch time: 0.871 trainign loss: 6.1171 avg training loss: 7.1263
batch: [10380/21305] batch time: 0.056 trainign loss: 6.6509 avg training loss: 7.1262
batch: [10390/21305] batch time: 2.151 trainign loss: 6.3104 avg training loss: 7.1261
batch: [10400/21305] batch time: 0.056 trainign loss: 6.2193 avg training loss: 7.1261
batch: [10410/21305] batch time: 2.116 trainign loss: 6.4805 avg training loss: 7.1260
batch: [10420/21305] batch time: 0.058 trainign loss: 7.6045 avg training loss: 7.1260
batch: [10430/21305] batch time: 0.763 trainign loss: 5.4979 avg training loss: 7.1260
batch: [10440/21305] batch time: 0.056 trainign loss: 6.4087 avg training loss: 7.1258
batch: [10450/21305] batch time: 1.210 trainign loss: 6.3325 avg training loss: 7.1258
batch: [10460/21305] batch time: 0.056 trainign loss: 6.8401 avg training loss: 7.1258
batch: [10470/21305] batch time: 1.719 trainign loss: 5.4654 avg training loss: 7.1258
batch: [10480/21305] batch time: 0.063 trainign loss: 7.5945 avg training loss: 7.1256
batch: [10490/21305] batch time: 1.755 trainign loss: 6.8089 avg training loss: 7.1256
batch: [10500/21305] batch time: 0.056 trainign loss: 5.0512 avg training loss: 7.1256
batch: [10510/21305] batch time: 1.705 trainign loss: 4.5208 avg training loss: 7.1255
batch: [10520/21305] batch time: 0.058 trainign loss: 6.7227 avg training loss: 7.1254
batch: [10530/21305] batch time: 0.871 trainign loss: 6.8882 avg training loss: 7.1254
batch: [10540/21305] batch time: 0.056 trainign loss: 5.6472 avg training loss: 7.1254
batch: [10550/21305] batch time: 0.057 trainign loss: 4.4574 avg training loss: 7.1253
batch: [10560/21305] batch time: 0.062 trainign loss: 8.8827 avg training loss: 7.1251
batch: [10570/21305] batch time: 0.055 trainign loss: 6.8045 avg training loss: 7.1251
batch: [10580/21305] batch time: 0.062 trainign loss: 7.4292 avg training loss: 7.1251
batch: [10590/21305] batch time: 0.056 trainign loss: 6.6100 avg training loss: 7.1250
batch: [10600/21305] batch time: 0.056 trainign loss: 6.4786 avg training loss: 7.1250
batch: [10610/21305] batch time: 0.496 trainign loss: 4.2828 avg training loss: 7.1249
batch: [10620/21305] batch time: 0.056 trainign loss: 6.9129 avg training loss: 7.1249
batch: [10630/21305] batch time: 0.243 trainign loss: 6.0777 avg training loss: 7.1249
batch: [10640/21305] batch time: 0.055 trainign loss: 5.5165 avg training loss: 7.1248
batch: [10650/21305] batch time: 0.360 trainign loss: 7.7298 avg training loss: 7.1247
batch: [10660/21305] batch time: 0.057 trainign loss: 7.1724 avg training loss: 7.1247
batch: [10670/21305] batch time: 0.062 trainign loss: 4.4214 avg training loss: 7.1246
batch: [10680/21305] batch time: 0.058 trainign loss: 6.8406 avg training loss: 7.1246
batch: [10690/21305] batch time: 0.779 trainign loss: 6.1696 avg training loss: 7.1245
batch: [10700/21305] batch time: 0.056 trainign loss: 7.2804 avg training loss: 7.1244
batch: [10710/21305] batch time: 0.536 trainign loss: 6.3887 avg training loss: 7.1243
batch: [10720/21305] batch time: 0.056 trainign loss: 7.3085 avg training loss: 7.1242
batch: [10730/21305] batch time: 0.427 trainign loss: 7.6019 avg training loss: 7.1242
batch: [10740/21305] batch time: 0.062 trainign loss: 7.3766 avg training loss: 7.1242
batch: [10750/21305] batch time: 0.772 trainign loss: 6.7299 avg training loss: 7.1242
batch: [10760/21305] batch time: 0.056 trainign loss: 4.5760 avg training loss: 7.1241
batch: [10770/21305] batch time: 1.136 trainign loss: 6.1709 avg training loss: 7.1240
batch: [10780/21305] batch time: 0.058 trainign loss: 7.4537 avg training loss: 7.1240
batch: [10790/21305] batch time: 0.913 trainign loss: 6.8207 avg training loss: 7.1240
batch: [10800/21305] batch time: 0.394 trainign loss: 5.5201 avg training loss: 7.1240
batch: [10810/21305] batch time: 0.986 trainign loss: 6.8002 avg training loss: 7.1239
batch: [10820/21305] batch time: 0.123 trainign loss: 4.0628 avg training loss: 7.1238
batch: [10830/21305] batch time: 0.794 trainign loss: 1.5030 avg training loss: 7.1236
batch: [10840/21305] batch time: 0.056 trainign loss: 6.7662 avg training loss: 7.1236
batch: [10850/21305] batch time: 0.606 trainign loss: 7.1083 avg training loss: 7.1236
batch: [10860/21305] batch time: 0.056 trainign loss: 6.1874 avg training loss: 7.1236
batch: [10870/21305] batch time: 0.238 trainign loss: 5.8886 avg training loss: 7.1235
batch: [10880/21305] batch time: 0.056 trainign loss: 5.2838 avg training loss: 7.1235
batch: [10890/21305] batch time: 0.056 trainign loss: 6.7258 avg training loss: 7.1234
batch: [10900/21305] batch time: 0.056 trainign loss: 7.4240 avg training loss: 7.1234
batch: [10910/21305] batch time: 0.056 trainign loss: 7.0548 avg training loss: 7.1234
batch: [10920/21305] batch time: 0.050 trainign loss: 6.8747 avg training loss: 7.1233
batch: [10930/21305] batch time: 0.506 trainign loss: 5.0028 avg training loss: 7.1232
batch: [10940/21305] batch time: 0.057 trainign loss: 6.5248 avg training loss: 7.1232
batch: [10950/21305] batch time: 0.651 trainign loss: 5.8076 avg training loss: 7.1231
batch: [10960/21305] batch time: 0.057 trainign loss: 6.6684 avg training loss: 7.1231
batch: [10970/21305] batch time: 1.770 trainign loss: 6.3778 avg training loss: 7.1230
batch: [10980/21305] batch time: 0.062 trainign loss: 5.6317 avg training loss: 7.1229
batch: [10990/21305] batch time: 0.918 trainign loss: 6.8068 avg training loss: 7.1229
batch: [11000/21305] batch time: 0.056 trainign loss: 6.4031 avg training loss: 7.1228
batch: [11010/21305] batch time: 0.056 trainign loss: 5.5242 avg training loss: 7.1227
batch: [11020/21305] batch time: 0.056 trainign loss: 7.3149 avg training loss: 7.1227
batch: [11030/21305] batch time: 0.171 trainign loss: 5.4662 avg training loss: 7.1227
batch: [11040/21305] batch time: 0.056 trainign loss: 6.6804 avg training loss: 7.1226
batch: [11050/21305] batch time: 0.369 trainign loss: 6.1393 avg training loss: 7.1225
batch: [11060/21305] batch time: 0.059 trainign loss: 7.0225 avg training loss: 7.1225
batch: [11070/21305] batch time: 0.057 trainign loss: 5.6480 avg training loss: 7.1225
batch: [11080/21305] batch time: 0.058 trainign loss: 6.8689 avg training loss: 7.1224
batch: [11090/21305] batch time: 0.051 trainign loss: 7.5077 avg training loss: 7.1224
batch: [11100/21305] batch time: 0.056 trainign loss: 6.1579 avg training loss: 7.1223
batch: [11110/21305] batch time: 0.538 trainign loss: 6.4393 avg training loss: 7.1223
batch: [11120/21305] batch time: 0.058 trainign loss: 6.3278 avg training loss: 7.1223
batch: [11130/21305] batch time: 0.056 trainign loss: 4.7417 avg training loss: 7.1222
batch: [11140/21305] batch time: 0.056 trainign loss: 3.7909 avg training loss: 7.1221
batch: [11150/21305] batch time: 0.057 trainign loss: 6.3817 avg training loss: 7.1220
batch: [11160/21305] batch time: 0.056 trainign loss: 6.6454 avg training loss: 7.1220
batch: [11170/21305] batch time: 0.430 trainign loss: 3.9452 avg training loss: 7.1219
batch: [11180/21305] batch time: 0.057 trainign loss: 8.7391 avg training loss: 7.1218
batch: [11190/21305] batch time: 1.287 trainign loss: 7.3746 avg training loss: 7.1217
batch: [11200/21305] batch time: 0.056 trainign loss: 7.4595 avg training loss: 7.1217
batch: [11210/21305] batch time: 0.086 trainign loss: 5.0426 avg training loss: 7.1217
batch: [11220/21305] batch time: 0.062 trainign loss: 6.3345 avg training loss: 7.1216
batch: [11230/21305] batch time: 0.708 trainign loss: 5.4515 avg training loss: 7.1216
batch: [11240/21305] batch time: 0.056 trainign loss: 6.2000 avg training loss: 7.1215
batch: [11250/21305] batch time: 0.061 trainign loss: 0.6206 avg training loss: 7.1214
batch: [11260/21305] batch time: 0.057 trainign loss: 7.9036 avg training loss: 7.1214
batch: [11270/21305] batch time: 0.190 trainign loss: 6.4514 avg training loss: 7.1214
batch: [11280/21305] batch time: 0.136 trainign loss: 5.3538 avg training loss: 7.1213
batch: [11290/21305] batch time: 0.367 trainign loss: 6.5442 avg training loss: 7.1213
batch: [11300/21305] batch time: 0.180 trainign loss: 1.4925 avg training loss: 7.1211
batch: [11310/21305] batch time: 0.788 trainign loss: 6.6089 avg training loss: 7.1211
batch: [11320/21305] batch time: 0.239 trainign loss: 6.8979 avg training loss: 7.1210
batch: [11330/21305] batch time: 1.838 trainign loss: 7.6597 avg training loss: 7.1210
batch: [11340/21305] batch time: 0.062 trainign loss: 7.7362 avg training loss: 7.1210
batch: [11350/21305] batch time: 1.778 trainign loss: 7.0633 avg training loss: 7.1210
batch: [11360/21305] batch time: 0.056 trainign loss: 6.0850 avg training loss: 7.1209
batch: [11370/21305] batch time: 1.837 trainign loss: 5.8912 avg training loss: 7.1209
batch: [11380/21305] batch time: 0.056 trainign loss: 4.2764 avg training loss: 7.1208
batch: [11390/21305] batch time: 1.239 trainign loss: 6.9049 avg training loss: 7.1205
batch: [11400/21305] batch time: 0.056 trainign loss: 8.0979 avg training loss: 7.1205
batch: [11410/21305] batch time: 1.468 trainign loss: 6.4956 avg training loss: 7.1205
batch: [11420/21305] batch time: 0.061 trainign loss: 5.8413 avg training loss: 7.1205
batch: [11430/21305] batch time: 2.535 trainign loss: 6.6934 avg training loss: 7.1205
batch: [11440/21305] batch time: 0.056 trainign loss: 6.5450 avg training loss: 7.1205
batch: [11450/21305] batch time: 1.412 trainign loss: 6.1441 avg training loss: 7.1204
batch: [11460/21305] batch time: 0.056 trainign loss: 6.2425 avg training loss: 7.1204
batch: [11470/21305] batch time: 0.051 trainign loss: 6.6571 avg training loss: 7.1203
batch: [11480/21305] batch time: 0.059 trainign loss: 6.7706 avg training loss: 7.1203
batch: [11490/21305] batch time: 0.057 trainign loss: 6.2389 avg training loss: 7.1201
batch: [11500/21305] batch time: 0.058 trainign loss: 6.5984 avg training loss: 7.1201
batch: [11510/21305] batch time: 0.212 trainign loss: 5.5000 avg training loss: 7.1200
batch: [11520/21305] batch time: 0.056 trainign loss: 7.0241 avg training loss: 7.1200
batch: [11530/21305] batch time: 0.063 trainign loss: 5.7509 avg training loss: 7.1199
batch: [11540/21305] batch time: 0.063 trainign loss: 7.9079 avg training loss: 7.1199
batch: [11550/21305] batch time: 0.224 trainign loss: 5.1836 avg training loss: 7.1199
batch: [11560/21305] batch time: 0.056 trainign loss: 7.3048 avg training loss: 7.1199
batch: [11570/21305] batch time: 0.376 trainign loss: 5.2903 avg training loss: 7.1198
batch: [11580/21305] batch time: 0.062 trainign loss: 7.6328 avg training loss: 7.1198
batch: [11590/21305] batch time: 0.056 trainign loss: 6.8529 avg training loss: 7.1197
batch: [11600/21305] batch time: 0.057 trainign loss: 4.6833 avg training loss: 7.1197
batch: [11610/21305] batch time: 0.057 trainign loss: 5.7628 avg training loss: 7.1196
batch: [11620/21305] batch time: 0.056 trainign loss: 6.8635 avg training loss: 7.1196
batch: [11630/21305] batch time: 0.050 trainign loss: 7.2846 avg training loss: 7.1195
batch: [11640/21305] batch time: 0.056 trainign loss: 6.4789 avg training loss: 7.1195
batch: [11650/21305] batch time: 0.052 trainign loss: 7.2841 avg training loss: 7.1195
batch: [11660/21305] batch time: 0.056 trainign loss: 5.2516 avg training loss: 7.1194
batch: [11670/21305] batch time: 0.051 trainign loss: 6.8120 avg training loss: 7.1194
batch: [11680/21305] batch time: 0.056 trainign loss: 5.7503 avg training loss: 7.1193
batch: [11690/21305] batch time: 0.050 trainign loss: 5.8439 avg training loss: 7.1192
batch: [11700/21305] batch time: 0.061 trainign loss: 6.5246 avg training loss: 7.1191
batch: [11710/21305] batch time: 0.055 trainign loss: 3.8599 avg training loss: 7.1191
batch: [11720/21305] batch time: 0.056 trainign loss: 7.2790 avg training loss: 7.1190
batch: [11730/21305] batch time: 0.056 trainign loss: 5.4016 avg training loss: 7.1190
batch: [11740/21305] batch time: 0.056 trainign loss: 5.9589 avg training loss: 7.1189
batch: [11750/21305] batch time: 0.061 trainign loss: 6.7286 avg training loss: 7.1189
batch: [11760/21305] batch time: 0.058 trainign loss: 6.6894 avg training loss: 7.1189
batch: [11770/21305] batch time: 0.051 trainign loss: 6.3671 avg training loss: 7.1189
batch: [11780/21305] batch time: 0.058 trainign loss: 4.6776 avg training loss: 7.1188
batch: [11790/21305] batch time: 0.054 trainign loss: 1.0059 avg training loss: 7.1186
batch: [11800/21305] batch time: 0.062 trainign loss: 0.0011 avg training loss: 7.1182
batch: [11810/21305] batch time: 0.143 trainign loss: 6.4904 avg training loss: 7.1182
batch: [11820/21305] batch time: 0.056 trainign loss: 7.1544 avg training loss: 7.1182
batch: [11830/21305] batch time: 0.056 trainign loss: 7.3920 avg training loss: 7.1181
batch: [11840/21305] batch time: 0.057 trainign loss: 7.1006 avg training loss: 7.1181
batch: [11850/21305] batch time: 0.056 trainign loss: 5.7605 avg training loss: 7.1180
batch: [11860/21305] batch time: 0.056 trainign loss: 4.1357 avg training loss: 7.1179
batch: [11870/21305] batch time: 0.056 trainign loss: 6.7140 avg training loss: 7.1179
batch: [11880/21305] batch time: 0.062 trainign loss: 6.9432 avg training loss: 7.1177
batch: [11890/21305] batch time: 0.057 trainign loss: 7.6709 avg training loss: 7.1177
batch: [11900/21305] batch time: 0.056 trainign loss: 5.5009 avg training loss: 7.1177
batch: [11910/21305] batch time: 0.297 trainign loss: 7.0058 avg training loss: 7.1176
batch: [11920/21305] batch time: 0.058 trainign loss: 6.2271 avg training loss: 7.1176
batch: [11930/21305] batch time: 0.513 trainign loss: 5.4891 avg training loss: 7.1175
batch: [11940/21305] batch time: 0.056 trainign loss: 7.3549 avg training loss: 7.1175
batch: [11950/21305] batch time: 1.146 trainign loss: 6.6152 avg training loss: 7.1175
batch: [11960/21305] batch time: 0.056 trainign loss: 7.0183 avg training loss: 7.1174
batch: [11970/21305] batch time: 2.290 trainign loss: 6.4955 avg training loss: 7.1174
batch: [11980/21305] batch time: 0.056 trainign loss: 6.6379 avg training loss: 7.1174
batch: [11990/21305] batch time: 2.140 trainign loss: 5.8664 avg training loss: 7.1173
batch: [12000/21305] batch time: 0.061 trainign loss: 6.4061 avg training loss: 7.1173
batch: [12010/21305] batch time: 2.182 trainign loss: 7.4086 avg training loss: 7.1172
batch: [12020/21305] batch time: 0.062 trainign loss: 6.7506 avg training loss: 7.1172
batch: [12030/21305] batch time: 1.690 trainign loss: 5.8148 avg training loss: 7.1172
batch: [12040/21305] batch time: 0.057 trainign loss: 5.8256 avg training loss: 7.1171
batch: [12050/21305] batch time: 1.871 trainign loss: 6.0572 avg training loss: 7.1171
batch: [12060/21305] batch time: 0.056 trainign loss: 6.7283 avg training loss: 7.1170
batch: [12070/21305] batch time: 1.296 trainign loss: 3.2647 avg training loss: 7.1170
batch: [12080/21305] batch time: 0.063 trainign loss: 6.5690 avg training loss: 7.1167
batch: [12090/21305] batch time: 2.371 trainign loss: 5.8487 avg training loss: 7.1167
batch: [12100/21305] batch time: 0.056 trainign loss: 6.0302 avg training loss: 7.1167
batch: [12110/21305] batch time: 2.324 trainign loss: 6.8750 avg training loss: 7.1166
batch: [12120/21305] batch time: 0.063 trainign loss: 4.4772 avg training loss: 7.1165
batch: [12130/21305] batch time: 1.438 trainign loss: 7.5233 avg training loss: 7.1165
batch: [12140/21305] batch time: 0.191 trainign loss: 6.0342 avg training loss: 7.1164
batch: [12150/21305] batch time: 1.324 trainign loss: 7.1828 avg training loss: 7.1164
batch: [12160/21305] batch time: 0.587 trainign loss: 7.2459 avg training loss: 7.1164
batch: [12170/21305] batch time: 1.748 trainign loss: 6.7659 avg training loss: 7.1164
batch: [12180/21305] batch time: 0.489 trainign loss: 6.5791 avg training loss: 7.1164
batch: [12190/21305] batch time: 1.227 trainign loss: 6.9622 avg training loss: 7.1163
batch: [12200/21305] batch time: 0.059 trainign loss: 5.9556 avg training loss: 7.1163
batch: [12210/21305] batch time: 0.658 trainign loss: 7.0548 avg training loss: 7.1163
batch: [12220/21305] batch time: 0.057 trainign loss: 6.6831 avg training loss: 7.1163
batch: [12230/21305] batch time: 0.466 trainign loss: 7.2246 avg training loss: 7.1162
batch: [12240/21305] batch time: 0.056 trainign loss: 6.2089 avg training loss: 7.1162
batch: [12250/21305] batch time: 0.311 trainign loss: 6.1557 avg training loss: 7.1162
batch: [12260/21305] batch time: 0.056 trainign loss: 6.7409 avg training loss: 7.1161
batch: [12270/21305] batch time: 1.224 trainign loss: 5.1183 avg training loss: 7.1161
batch: [12280/21305] batch time: 0.056 trainign loss: 3.9656 avg training loss: 7.1159
batch: [12290/21305] batch time: 0.210 trainign loss: 7.2942 avg training loss: 7.1159
batch: [12300/21305] batch time: 0.058 trainign loss: 7.3890 avg training loss: 7.1159
batch: [12310/21305] batch time: 0.052 trainign loss: 6.7674 avg training loss: 7.1159
batch: [12320/21305] batch time: 0.057 trainign loss: 6.2467 avg training loss: 7.1159
batch: [12330/21305] batch time: 0.194 trainign loss: 6.6179 avg training loss: 7.1158
batch: [12340/21305] batch time: 0.056 trainign loss: 6.4980 avg training loss: 7.1158
batch: [12350/21305] batch time: 0.398 trainign loss: 6.9616 avg training loss: 7.1158
batch: [12360/21305] batch time: 0.062 trainign loss: 5.0846 avg training loss: 7.1157
batch: [12370/21305] batch time: 1.392 trainign loss: 5.1253 avg training loss: 7.1156
batch: [12380/21305] batch time: 0.056 trainign loss: 6.3173 avg training loss: 7.1156
batch: [12390/21305] batch time: 0.873 trainign loss: 7.4529 avg training loss: 7.1156
batch: [12400/21305] batch time: 0.056 trainign loss: 6.0171 avg training loss: 7.1155
batch: [12410/21305] batch time: 0.909 trainign loss: 6.9550 avg training loss: 7.1155
batch: [12420/21305] batch time: 0.058 trainign loss: 5.1707 avg training loss: 7.1154
batch: [12430/21305] batch time: 1.081 trainign loss: 5.2965 avg training loss: 7.1154
batch: [12440/21305] batch time: 0.056 trainign loss: 6.9314 avg training loss: 7.1153
batch: [12450/21305] batch time: 0.792 trainign loss: 7.6450 avg training loss: 7.1152
batch: [12460/21305] batch time: 0.058 trainign loss: 7.5168 avg training loss: 7.1153
batch: [12470/21305] batch time: 0.328 trainign loss: 5.8675 avg training loss: 7.1152
batch: [12480/21305] batch time: 0.057 trainign loss: 5.6762 avg training loss: 7.1152
batch: [12490/21305] batch time: 0.612 trainign loss: 5.7578 avg training loss: 7.1151
batch: [12500/21305] batch time: 0.060 trainign loss: 7.0010 avg training loss: 7.1150
batch: [12510/21305] batch time: 0.420 trainign loss: 7.2882 avg training loss: 7.1150
batch: [12520/21305] batch time: 0.056 trainign loss: 6.8542 avg training loss: 7.1150
batch: [12530/21305] batch time: 0.152 trainign loss: 6.5056 avg training loss: 7.1149
batch: [12540/21305] batch time: 0.056 trainign loss: 4.9851 avg training loss: 7.1148
batch: [12550/21305] batch time: 0.541 trainign loss: 6.3121 avg training loss: 7.1148
batch: [12560/21305] batch time: 0.063 trainign loss: 4.2360 avg training loss: 7.1147
batch: [12570/21305] batch time: 1.216 trainign loss: 0.0069 avg training loss: 7.1143
batch: [12580/21305] batch time: 0.055 trainign loss: 7.5838 avg training loss: 7.1144
batch: [12590/21305] batch time: 1.100 trainign loss: 4.1181 avg training loss: 7.1144
batch: [12600/21305] batch time: 0.056 trainign loss: 7.9486 avg training loss: 7.1143
batch: [12610/21305] batch time: 1.986 trainign loss: 4.5073 avg training loss: 7.1143
batch: [12620/21305] batch time: 0.054 trainign loss: 6.6040 avg training loss: 7.1143
batch: [12630/21305] batch time: 2.475 trainign loss: 6.3922 avg training loss: 7.1143
batch: [12640/21305] batch time: 0.056 trainign loss: 7.1593 avg training loss: 7.1143
batch: [12650/21305] batch time: 2.209 trainign loss: 5.4389 avg training loss: 7.1142
batch: [12660/21305] batch time: 0.056 trainign loss: 6.1790 avg training loss: 7.1142
batch: [12670/21305] batch time: 2.300 trainign loss: 7.4739 avg training loss: 7.1141
batch: [12680/21305] batch time: 0.062 trainign loss: 4.6547 avg training loss: 7.1141
batch: [12690/21305] batch time: 2.465 trainign loss: 7.5329 avg training loss: 7.1140
batch: [12700/21305] batch time: 0.055 trainign loss: 6.8320 avg training loss: 7.1140
batch: [12710/21305] batch time: 1.665 trainign loss: 5.8072 avg training loss: 7.1139
batch: [12720/21305] batch time: 0.057 trainign loss: 5.3387 avg training loss: 7.1139
batch: [12730/21305] batch time: 1.832 trainign loss: 5.8196 avg training loss: 7.1138
batch: [12740/21305] batch time: 0.060 trainign loss: 4.4892 avg training loss: 7.1137
batch: [12750/21305] batch time: 2.068 trainign loss: 3.3725 avg training loss: 7.1136
batch: [12760/21305] batch time: 0.056 trainign loss: 6.5263 avg training loss: 7.1136
batch: [12770/21305] batch time: 0.650 trainign loss: 5.6652 avg training loss: 7.1135
batch: [12780/21305] batch time: 0.061 trainign loss: 7.2952 avg training loss: 7.1135
batch: [12790/21305] batch time: 0.984 trainign loss: 5.3579 avg training loss: 7.1134
batch: [12800/21305] batch time: 0.056 trainign loss: 6.8882 avg training loss: 7.1134
batch: [12810/21305] batch time: 1.130 trainign loss: 6.9008 avg training loss: 7.1134
batch: [12820/21305] batch time: 0.058 trainign loss: 6.3126 avg training loss: 7.1133
batch: [12830/21305] batch time: 1.497 trainign loss: 4.5131 avg training loss: 7.1133
batch: [12840/21305] batch time: 0.063 trainign loss: 6.2368 avg training loss: 7.1132
batch: [12850/21305] batch time: 1.902 trainign loss: 6.8318 avg training loss: 7.1132
batch: [12860/21305] batch time: 0.056 trainign loss: 6.7146 avg training loss: 7.1131
batch: [12870/21305] batch time: 2.014 trainign loss: 6.2804 avg training loss: 7.1131
batch: [12880/21305] batch time: 0.057 trainign loss: 5.3759 avg training loss: 7.1130
batch: [12890/21305] batch time: 1.527 trainign loss: 6.6301 avg training loss: 7.1130
batch: [12900/21305] batch time: 0.058 trainign loss: 7.2128 avg training loss: 7.1130
batch: [12910/21305] batch time: 1.808 trainign loss: 6.3505 avg training loss: 7.1129
batch: [12920/21305] batch time: 0.062 trainign loss: 6.1433 avg training loss: 7.1129
batch: [12930/21305] batch time: 1.304 trainign loss: 5.9624 avg training loss: 7.1128
batch: [12940/21305] batch time: 0.057 trainign loss: 6.1107 avg training loss: 7.1128
batch: [12950/21305] batch time: 0.880 trainign loss: 4.9194 avg training loss: 7.1127
batch: [12960/21305] batch time: 0.056 trainign loss: 6.3842 avg training loss: 7.1126
batch: [12970/21305] batch time: 0.878 trainign loss: 4.6683 avg training loss: 7.1126
batch: [12980/21305] batch time: 0.056 trainign loss: 7.1735 avg training loss: 7.1125
batch: [12990/21305] batch time: 0.066 trainign loss: 6.7339 avg training loss: 7.1125
batch: [13000/21305] batch time: 0.057 trainign loss: 6.9975 avg training loss: 7.1124
batch: [13010/21305] batch time: 0.062 trainign loss: 6.1331 avg training loss: 7.1123
batch: [13020/21305] batch time: 0.056 trainign loss: 7.3443 avg training loss: 7.1123
batch: [13030/21305] batch time: 0.056 trainign loss: 6.8274 avg training loss: 7.1123
batch: [13040/21305] batch time: 0.056 trainign loss: 7.2551 avg training loss: 7.1122
batch: [13050/21305] batch time: 0.051 trainign loss: 2.5186 avg training loss: 7.1121
batch: [13060/21305] batch time: 0.055 trainign loss: 6.5835 avg training loss: 7.1121
batch: [13070/21305] batch time: 0.056 trainign loss: 6.8883 avg training loss: 7.1121
batch: [13080/21305] batch time: 0.063 trainign loss: 5.9321 avg training loss: 7.1120
batch: [13090/21305] batch time: 0.063 trainign loss: 4.8124 avg training loss: 7.1119
batch: [13100/21305] batch time: 0.051 trainign loss: 6.8654 avg training loss: 7.1118
batch: [13110/21305] batch time: 0.063 trainign loss: 6.9485 avg training loss: 7.1117
batch: [13120/21305] batch time: 0.056 trainign loss: 6.6447 avg training loss: 7.1117
batch: [13130/21305] batch time: 0.058 trainign loss: 5.6541 avg training loss: 7.1117
batch: [13140/21305] batch time: 0.056 trainign loss: 1.9781 avg training loss: 7.1116
batch: [13150/21305] batch time: 0.062 trainign loss: 7.2373 avg training loss: 7.1115
batch: [13160/21305] batch time: 0.051 trainign loss: 5.8143 avg training loss: 7.1115
batch: [13170/21305] batch time: 0.056 trainign loss: 6.8481 avg training loss: 7.1115
batch: [13180/21305] batch time: 0.053 trainign loss: 5.0026 avg training loss: 7.1114
batch: [13190/21305] batch time: 0.056 trainign loss: 6.0202 avg training loss: 7.1113
batch: [13200/21305] batch time: 0.051 trainign loss: 3.0910 avg training loss: 7.1112
batch: [13210/21305] batch time: 0.056 trainign loss: 0.0020 avg training loss: 7.1108
batch: [13220/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.1104
batch: [13230/21305] batch time: 0.062 trainign loss: 0.0001 avg training loss: 7.1100
batch: [13240/21305] batch time: 0.057 trainign loss: 8.1336 avg training loss: 7.1101
batch: [13250/21305] batch time: 0.062 trainign loss: 7.3344 avg training loss: 7.1101
batch: [13260/21305] batch time: 0.050 trainign loss: 7.5625 avg training loss: 7.1101
batch: [13270/21305] batch time: 0.055 trainign loss: 6.9246 avg training loss: 7.1101
batch: [13280/21305] batch time: 0.056 trainign loss: 7.6702 avg training loss: 7.1101
batch: [13290/21305] batch time: 0.063 trainign loss: 6.7618 avg training loss: 7.1101
batch: [13300/21305] batch time: 0.055 trainign loss: 7.2011 avg training loss: 7.1100
batch: [13310/21305] batch time: 0.062 trainign loss: 4.7954 avg training loss: 7.1100
batch: [13320/21305] batch time: 0.055 trainign loss: 5.3082 avg training loss: 7.1098
batch: [13330/21305] batch time: 0.058 trainign loss: 6.9072 avg training loss: 7.1098
batch: [13340/21305] batch time: 0.647 trainign loss: 7.3456 avg training loss: 7.1098
batch: [13350/21305] batch time: 0.056 trainign loss: 5.9832 avg training loss: 7.1098
batch: [13360/21305] batch time: 0.548 trainign loss: 7.8034 avg training loss: 7.1097
batch: [13370/21305] batch time: 0.056 trainign loss: 6.0351 avg training loss: 7.1097
batch: [13380/21305] batch time: 0.551 trainign loss: 4.3985 avg training loss: 7.1096
batch: [13390/21305] batch time: 0.056 trainign loss: 6.1502 avg training loss: 7.1095
batch: [13400/21305] batch time: 1.035 trainign loss: 4.6489 avg training loss: 7.1095
batch: [13410/21305] batch time: 0.062 trainign loss: 7.2874 avg training loss: 7.1095
batch: [13420/21305] batch time: 0.167 trainign loss: 6.5799 avg training loss: 7.1095
batch: [13430/21305] batch time: 0.057 trainign loss: 6.7577 avg training loss: 7.1095
batch: [13440/21305] batch time: 0.433 trainign loss: 5.7444 avg training loss: 7.1094
batch: [13450/21305] batch time: 0.056 trainign loss: 7.0643 avg training loss: 7.1094
batch: [13460/21305] batch time: 0.233 trainign loss: 6.5900 avg training loss: 7.1094
batch: [13470/21305] batch time: 0.062 trainign loss: 3.5258 avg training loss: 7.1093
batch: [13480/21305] batch time: 0.056 trainign loss: 3.8604 avg training loss: 7.1092
batch: [13490/21305] batch time: 0.057 trainign loss: 7.0133 avg training loss: 7.1092
batch: [13500/21305] batch time: 0.056 trainign loss: 6.6597 avg training loss: 7.1092
batch: [13510/21305] batch time: 0.057 trainign loss: 5.8132 avg training loss: 7.1091
batch: [13520/21305] batch time: 0.053 trainign loss: 6.5283 avg training loss: 7.1091
batch: [13530/21305] batch time: 0.058 trainign loss: 5.9848 avg training loss: 7.1090
batch: [13540/21305] batch time: 0.057 trainign loss: 3.0200 avg training loss: 7.1089
batch: [13550/21305] batch time: 0.286 trainign loss: 7.2957 avg training loss: 7.1089
batch: [13560/21305] batch time: 0.056 trainign loss: 6.4698 avg training loss: 7.1089
batch: [13570/21305] batch time: 0.469 trainign loss: 6.4892 avg training loss: 7.1088
batch: [13580/21305] batch time: 0.054 trainign loss: 6.9003 avg training loss: 7.1088
batch: [13590/21305] batch time: 1.230 trainign loss: 4.7142 avg training loss: 7.1087
batch: [13600/21305] batch time: 0.063 trainign loss: 6.3275 avg training loss: 7.1087
batch: [13610/21305] batch time: 0.624 trainign loss: 7.7121 avg training loss: 7.1086
batch: [13620/21305] batch time: 0.053 trainign loss: 6.6693 avg training loss: 7.1086
batch: [13630/21305] batch time: 0.621 trainign loss: 6.3959 avg training loss: 7.1086
batch: [13640/21305] batch time: 0.061 trainign loss: 6.8819 avg training loss: 7.1086
batch: [13650/21305] batch time: 0.124 trainign loss: 6.0108 avg training loss: 7.1085
batch: [13660/21305] batch time: 0.056 trainign loss: 5.6838 avg training loss: 7.1084
batch: [13670/21305] batch time: 0.165 trainign loss: 6.5664 avg training loss: 7.1084
batch: [13680/21305] batch time: 0.052 trainign loss: 5.4863 avg training loss: 7.1083
batch: [13690/21305] batch time: 0.059 trainign loss: 3.8120 avg training loss: 7.1083
batch: [13700/21305] batch time: 0.051 trainign loss: 7.7495 avg training loss: 7.1083
batch: [13710/21305] batch time: 0.063 trainign loss: 6.7449 avg training loss: 7.1083
batch: [13720/21305] batch time: 0.059 trainign loss: 6.4954 avg training loss: 7.1082
batch: [13730/21305] batch time: 0.057 trainign loss: 7.1448 avg training loss: 7.1082
batch: [13740/21305] batch time: 0.051 trainign loss: 5.3878 avg training loss: 7.1081
batch: [13750/21305] batch time: 0.056 trainign loss: 7.1934 avg training loss: 7.1081
batch: [13760/21305] batch time: 0.056 trainign loss: 7.2362 avg training loss: 7.1081
batch: [13770/21305] batch time: 0.056 trainign loss: 6.8765 avg training loss: 7.1081
batch: [13780/21305] batch time: 0.052 trainign loss: 5.4963 avg training loss: 7.1080
batch: [13790/21305] batch time: 0.063 trainign loss: 7.1838 avg training loss: 7.1080
batch: [13800/21305] batch time: 0.056 trainign loss: 6.1142 avg training loss: 7.1080
batch: [13810/21305] batch time: 0.061 trainign loss: 6.4866 avg training loss: 7.1079
batch: [13820/21305] batch time: 0.053 trainign loss: 7.1485 avg training loss: 7.1079
batch: [13830/21305] batch time: 0.056 trainign loss: 6.0317 avg training loss: 7.1078
batch: [13840/21305] batch time: 0.056 trainign loss: 6.0342 avg training loss: 7.1077
batch: [13850/21305] batch time: 0.056 trainign loss: 0.8753 avg training loss: 7.1076
batch: [13860/21305] batch time: 0.056 trainign loss: 6.5725 avg training loss: 7.1075
batch: [13870/21305] batch time: 0.058 trainign loss: 7.4912 avg training loss: 7.1075
batch: [13880/21305] batch time: 0.051 trainign loss: 6.0722 avg training loss: 7.1075
batch: [13890/21305] batch time: 0.056 trainign loss: 6.8059 avg training loss: 7.1074
batch: [13900/21305] batch time: 0.054 trainign loss: 5.2876 avg training loss: 7.1074
batch: [13910/21305] batch time: 0.056 trainign loss: 7.3847 avg training loss: 7.1073
batch: [13920/21305] batch time: 0.057 trainign loss: 5.2399 avg training loss: 7.1073
batch: [13930/21305] batch time: 0.057 trainign loss: 5.8678 avg training loss: 7.1072
batch: [13940/21305] batch time: 0.051 trainign loss: 7.5441 avg training loss: 7.1071
batch: [13950/21305] batch time: 0.056 trainign loss: 4.8008 avg training loss: 7.1071
batch: [13960/21305] batch time: 0.055 trainign loss: 5.7780 avg training loss: 7.1071
batch: [13970/21305] batch time: 0.057 trainign loss: 7.0178 avg training loss: 7.1070
batch: [13980/21305] batch time: 0.051 trainign loss: 6.9827 avg training loss: 7.1070
batch: [13990/21305] batch time: 0.057 trainign loss: 6.6255 avg training loss: 7.1070
batch: [14000/21305] batch time: 0.053 trainign loss: 5.9926 avg training loss: 7.1069
batch: [14010/21305] batch time: 0.061 trainign loss: 4.5671 avg training loss: 7.1068
batch: [14020/21305] batch time: 0.058 trainign loss: 7.1653 avg training loss: 7.1066
batch: [14030/21305] batch time: 0.492 trainign loss: 7.0967 avg training loss: 7.1066
batch: [14040/21305] batch time: 0.061 trainign loss: 6.8121 avg training loss: 7.1066
batch: [14050/21305] batch time: 1.033 trainign loss: 6.9582 avg training loss: 7.1066
batch: [14060/21305] batch time: 0.056 trainign loss: 6.4418 avg training loss: 7.1066
batch: [14070/21305] batch time: 0.062 trainign loss: 6.6079 avg training loss: 7.1065
batch: [14080/21305] batch time: 0.056 trainign loss: 6.4885 avg training loss: 7.1065
batch: [14090/21305] batch time: 0.465 trainign loss: 6.8036 avg training loss: 7.1065
batch: [14100/21305] batch time: 0.056 trainign loss: 6.6748 avg training loss: 7.1064
batch: [14110/21305] batch time: 0.393 trainign loss: 6.2097 avg training loss: 7.1064
batch: [14120/21305] batch time: 0.061 trainign loss: 7.0912 avg training loss: 7.1063
batch: [14130/21305] batch time: 0.601 trainign loss: 5.9135 avg training loss: 7.1062
batch: [14140/21305] batch time: 0.054 trainign loss: 6.2562 avg training loss: 7.1062
batch: [14150/21305] batch time: 0.056 trainign loss: 6.8293 avg training loss: 7.1061
batch: [14160/21305] batch time: 0.063 trainign loss: 6.4075 avg training loss: 7.1061
batch: [14170/21305] batch time: 0.056 trainign loss: 6.0488 avg training loss: 7.1060
batch: [14180/21305] batch time: 0.056 trainign loss: 6.0500 avg training loss: 7.1060
batch: [14190/21305] batch time: 0.056 trainign loss: 1.8567 avg training loss: 7.1058
batch: [14200/21305] batch time: 0.054 trainign loss: 7.0003 avg training loss: 7.1058
batch: [14210/21305] batch time: 0.059 trainign loss: 6.1294 avg training loss: 7.1057
batch: [14220/21305] batch time: 0.053 trainign loss: 7.2102 avg training loss: 7.1056
batch: [14230/21305] batch time: 0.056 trainign loss: 6.4167 avg training loss: 7.1056
batch: [14240/21305] batch time: 0.051 trainign loss: 6.8724 avg training loss: 7.1056
batch: [14250/21305] batch time: 0.052 trainign loss: 6.6245 avg training loss: 7.1056
batch: [14260/21305] batch time: 0.053 trainign loss: 5.1887 avg training loss: 7.1055
batch: [14270/21305] batch time: 0.057 trainign loss: 4.6749 avg training loss: 7.1054
batch: [14280/21305] batch time: 0.056 trainign loss: 4.8231 avg training loss: 7.1053
batch: [14290/21305] batch time: 0.058 trainign loss: 6.3430 avg training loss: 7.1053
batch: [14300/21305] batch time: 0.052 trainign loss: 6.8110 avg training loss: 7.1053
batch: [14310/21305] batch time: 0.056 trainign loss: 6.1796 avg training loss: 7.1052
batch: [14320/21305] batch time: 0.056 trainign loss: 2.1666 avg training loss: 7.1051
batch: [14330/21305] batch time: 0.056 trainign loss: 6.6556 avg training loss: 7.1050
batch: [14340/21305] batch time: 0.390 trainign loss: 5.8778 avg training loss: 7.1050
batch: [14350/21305] batch time: 0.328 trainign loss: 5.7126 avg training loss: 7.1050
batch: [14360/21305] batch time: 0.854 trainign loss: 5.6484 avg training loss: 7.1049
batch: [14370/21305] batch time: 0.816 trainign loss: 6.0032 avg training loss: 7.1049
batch: [14380/21305] batch time: 0.094 trainign loss: 6.1495 avg training loss: 7.1049
batch: [14390/21305] batch time: 1.129 trainign loss: 6.6098 avg training loss: 7.1048
batch: [14400/21305] batch time: 1.171 trainign loss: 5.8764 avg training loss: 7.1048
batch: [14410/21305] batch time: 0.920 trainign loss: 6.2308 avg training loss: 7.1047
batch: [14420/21305] batch time: 1.116 trainign loss: 6.1431 avg training loss: 7.1046
batch: [14430/21305] batch time: 0.852 trainign loss: 6.3255 avg training loss: 7.1046
batch: [14440/21305] batch time: 0.389 trainign loss: 7.0235 avg training loss: 7.1046
batch: [14450/21305] batch time: 0.062 trainign loss: 5.4850 avg training loss: 7.1046
batch: [14460/21305] batch time: 0.057 trainign loss: 6.8953 avg training loss: 7.1045
batch: [14470/21305] batch time: 0.277 trainign loss: 5.5977 avg training loss: 7.1045
batch: [14480/21305] batch time: 0.056 trainign loss: 4.0161 avg training loss: 7.1044
batch: [14490/21305] batch time: 0.220 trainign loss: 5.1477 avg training loss: 7.1043
batch: [14500/21305] batch time: 0.063 trainign loss: 1.9595 avg training loss: 7.1042
batch: [14510/21305] batch time: 0.062 trainign loss: 0.0017 avg training loss: 7.1038
batch: [14520/21305] batch time: 0.052 trainign loss: 0.0002 avg training loss: 7.1034
batch: [14530/21305] batch time: 0.058 trainign loss: 0.0001 avg training loss: 7.1030
batch: [14540/21305] batch time: 0.054 trainign loss: 0.0001 avg training loss: 7.1026
batch: [14550/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.1023
batch: [14560/21305] batch time: 0.062 trainign loss: 0.0001 avg training loss: 7.1019
batch: [14570/21305] batch time: 0.057 trainign loss: 0.0001 avg training loss: 7.1015
batch: [14580/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.1011
batch: [14590/21305] batch time: 0.057 trainign loss: 0.0001 avg training loss: 7.1007
batch: [14600/21305] batch time: 0.056 trainign loss: 7.1311 avg training loss: 7.1007
batch: [14610/21305] batch time: 0.056 trainign loss: 7.9082 avg training loss: 7.1007
batch: [14620/21305] batch time: 0.051 trainign loss: 6.8244 avg training loss: 7.1007
batch: [14630/21305] batch time: 0.056 trainign loss: 5.9737 avg training loss: 7.1006
batch: [14640/21305] batch time: 0.059 trainign loss: 5.8674 avg training loss: 7.1006
batch: [14650/21305] batch time: 0.058 trainign loss: 7.0140 avg training loss: 7.1006
batch: [14660/21305] batch time: 0.051 trainign loss: 6.0658 avg training loss: 7.1005
batch: [14670/21305] batch time: 0.060 trainign loss: 4.8340 avg training loss: 7.1005
batch: [14680/21305] batch time: 0.055 trainign loss: 1.8195 avg training loss: 7.1003
batch: [14690/21305] batch time: 0.056 trainign loss: 6.9435 avg training loss: 7.1003
batch: [14700/21305] batch time: 0.052 trainign loss: 4.1613 avg training loss: 7.1003
batch: [14710/21305] batch time: 0.058 trainign loss: 0.0042 avg training loss: 7.0999
batch: [14720/21305] batch time: 0.055 trainign loss: 7.8905 avg training loss: 7.0999
batch: [14730/21305] batch time: 0.056 trainign loss: 4.7626 avg training loss: 7.0998
batch: [14740/21305] batch time: 0.056 trainign loss: 5.3642 avg training loss: 7.0998
batch: [14750/21305] batch time: 0.062 trainign loss: 7.5405 avg training loss: 7.0998
batch: [14760/21305] batch time: 0.061 trainign loss: 7.0050 avg training loss: 7.0998
batch: [14770/21305] batch time: 0.057 trainign loss: 6.7686 avg training loss: 7.0997
batch: [14780/21305] batch time: 0.053 trainign loss: 6.5116 avg training loss: 7.0997
batch: [14790/21305] batch time: 0.056 trainign loss: 7.0614 avg training loss: 7.0997
batch: [14800/21305] batch time: 0.056 trainign loss: 6.3810 avg training loss: 7.0997
batch: [14810/21305] batch time: 0.056 trainign loss: 5.5647 avg training loss: 7.0996
batch: [14820/21305] batch time: 0.057 trainign loss: 6.3251 avg training loss: 7.0996
batch: [14830/21305] batch time: 0.058 trainign loss: 6.5768 avg training loss: 7.0996
batch: [14840/21305] batch time: 0.053 trainign loss: 5.4615 avg training loss: 7.0995
batch: [14850/21305] batch time: 0.057 trainign loss: 6.6104 avg training loss: 7.0995
batch: [14860/21305] batch time: 0.056 trainign loss: 7.0278 avg training loss: 7.0994
batch: [14870/21305] batch time: 0.056 trainign loss: 6.2049 avg training loss: 7.0994
batch: [14880/21305] batch time: 0.054 trainign loss: 6.8752 avg training loss: 7.0994
batch: [14890/21305] batch time: 0.059 trainign loss: 6.1560 avg training loss: 7.0993
batch: [14900/21305] batch time: 0.056 trainign loss: 6.2960 avg training loss: 7.0993
batch: [14910/21305] batch time: 0.057 trainign loss: 3.2438 avg training loss: 7.0992
batch: [14920/21305] batch time: 0.056 trainign loss: 5.9163 avg training loss: 7.0992
batch: [14930/21305] batch time: 0.062 trainign loss: 5.9243 avg training loss: 7.0991
batch: [14940/21305] batch time: 0.051 trainign loss: 6.5841 avg training loss: 7.0991
batch: [14950/21305] batch time: 0.056 trainign loss: 6.5746 avg training loss: 7.0990
batch: [14960/21305] batch time: 0.051 trainign loss: 6.8393 avg training loss: 7.0989
batch: [14970/21305] batch time: 0.056 trainign loss: 7.4681 avg training loss: 7.0988
batch: [14980/21305] batch time: 0.055 trainign loss: 7.5122 avg training loss: 7.0988
batch: [14990/21305] batch time: 0.056 trainign loss: 5.1734 avg training loss: 7.0988
batch: [15000/21305] batch time: 0.050 trainign loss: 5.7168 avg training loss: 7.0987
batch: [15010/21305] batch time: 0.062 trainign loss: 7.6907 avg training loss: 7.0987
batch: [15020/21305] batch time: 0.051 trainign loss: 5.7776 avg training loss: 7.0987
batch: [15030/21305] batch time: 0.056 trainign loss: 6.9595 avg training loss: 7.0986
batch: [15040/21305] batch time: 0.062 trainign loss: 2.2357 avg training loss: 7.0985
batch: [15050/21305] batch time: 0.061 trainign loss: 6.9732 avg training loss: 7.0985
batch: [15060/21305] batch time: 0.056 trainign loss: 6.2704 avg training loss: 7.0985
batch: [15070/21305] batch time: 0.056 trainign loss: 5.8587 avg training loss: 7.0984
batch: [15080/21305] batch time: 0.056 trainign loss: 3.4742 avg training loss: 7.0982
batch: [15090/21305] batch time: 0.056 trainign loss: 6.4747 avg training loss: 7.0983
batch: [15100/21305] batch time: 0.308 trainign loss: 6.0036 avg training loss: 7.0983
batch: [15110/21305] batch time: 0.056 trainign loss: 3.0058 avg training loss: 7.0982
batch: [15120/21305] batch time: 0.057 trainign loss: 7.2394 avg training loss: 7.0981
batch: [15130/21305] batch time: 0.057 trainign loss: 6.3701 avg training loss: 7.0980
batch: [15140/21305] batch time: 0.056 trainign loss: 7.8432 avg training loss: 7.0981
batch: [15150/21305] batch time: 0.058 trainign loss: 6.9651 avg training loss: 7.0981
batch: [15160/21305] batch time: 0.056 trainign loss: 6.5331 avg training loss: 7.0980
batch: [15170/21305] batch time: 0.061 trainign loss: 6.4910 avg training loss: 7.0980
batch: [15180/21305] batch time: 0.053 trainign loss: 5.2008 avg training loss: 7.0979
batch: [15190/21305] batch time: 0.056 trainign loss: 6.8593 avg training loss: 7.0979
batch: [15200/21305] batch time: 0.056 trainign loss: 7.0392 avg training loss: 7.0979
batch: [15210/21305] batch time: 0.061 trainign loss: 5.6176 avg training loss: 7.0978
batch: [15220/21305] batch time: 0.058 trainign loss: 6.5337 avg training loss: 7.0978
batch: [15230/21305] batch time: 0.061 trainign loss: 6.8997 avg training loss: 7.0977
batch: [15240/21305] batch time: 0.056 trainign loss: 6.5041 avg training loss: 7.0977
batch: [15250/21305] batch time: 0.058 trainign loss: 6.9956 avg training loss: 7.0977
batch: [15260/21305] batch time: 0.057 trainign loss: 5.7089 avg training loss: 7.0976
batch: [15270/21305] batch time: 0.063 trainign loss: 7.4734 avg training loss: 7.0975
batch: [15280/21305] batch time: 0.052 trainign loss: 5.8925 avg training loss: 7.0975
batch: [15290/21305] batch time: 0.051 trainign loss: 7.2362 avg training loss: 7.0974
batch: [15300/21305] batch time: 0.055 trainign loss: 6.5629 avg training loss: 7.0974
batch: [15310/21305] batch time: 0.056 trainign loss: 5.8305 avg training loss: 7.0973
batch: [15320/21305] batch time: 0.050 trainign loss: 7.4856 avg training loss: 7.0973
batch: [15330/21305] batch time: 0.057 trainign loss: 6.3130 avg training loss: 7.0973
batch: [15340/21305] batch time: 0.053 trainign loss: 5.2190 avg training loss: 7.0972
batch: [15350/21305] batch time: 0.056 trainign loss: 6.2719 avg training loss: 7.0972
batch: [15360/21305] batch time: 0.056 trainign loss: 7.2027 avg training loss: 7.0971
batch: [15370/21305] batch time: 0.058 trainign loss: 5.2861 avg training loss: 7.0971
batch: [15380/21305] batch time: 0.059 trainign loss: 5.9767 avg training loss: 7.0970
batch: [15390/21305] batch time: 0.051 trainign loss: 6.5883 avg training loss: 7.0970
batch: [15400/21305] batch time: 0.052 trainign loss: 6.7830 avg training loss: 7.0970
batch: [15410/21305] batch time: 0.061 trainign loss: 6.6218 avg training loss: 7.0970
batch: [15420/21305] batch time: 0.131 trainign loss: 6.5953 avg training loss: 7.0970
batch: [15430/21305] batch time: 0.182 trainign loss: 5.8123 avg training loss: 7.0969
batch: [15440/21305] batch time: 0.057 trainign loss: 7.1724 avg training loss: 7.0969
batch: [15450/21305] batch time: 0.226 trainign loss: 6.7306 avg training loss: 7.0968
batch: [15460/21305] batch time: 0.056 trainign loss: 6.4586 avg training loss: 7.0968
batch: [15470/21305] batch time: 0.053 trainign loss: 5.9785 avg training loss: 7.0967
batch: [15480/21305] batch time: 0.055 trainign loss: 6.9007 avg training loss: 7.0967
batch: [15490/21305] batch time: 0.056 trainign loss: 5.8036 avg training loss: 7.0967
batch: [15500/21305] batch time: 0.051 trainign loss: 4.6560 avg training loss: 7.0966
batch: [15510/21305] batch time: 0.051 trainign loss: 4.1422 avg training loss: 7.0965
batch: [15520/21305] batch time: 0.057 trainign loss: 7.6393 avg training loss: 7.0964
batch: [15530/21305] batch time: 0.063 trainign loss: 7.3468 avg training loss: 7.0963
batch: [15540/21305] batch time: 0.051 trainign loss: 7.0781 avg training loss: 7.0963
batch: [15550/21305] batch time: 0.056 trainign loss: 5.2280 avg training loss: 7.0963
batch: [15560/21305] batch time: 0.056 trainign loss: 7.9177 avg training loss: 7.0962
batch: [15570/21305] batch time: 0.111 trainign loss: 6.8590 avg training loss: 7.0962
batch: [15580/21305] batch time: 0.462 trainign loss: 6.4619 avg training loss: 7.0962
batch: [15590/21305] batch time: 0.063 trainign loss: 5.9228 avg training loss: 7.0962
batch: [15600/21305] batch time: 0.214 trainign loss: 6.2620 avg training loss: 7.0961
batch: [15610/21305] batch time: 0.056 trainign loss: 6.2831 avg training loss: 7.0961
batch: [15620/21305] batch time: 0.817 trainign loss: 6.2358 avg training loss: 7.0961
batch: [15630/21305] batch time: 0.056 trainign loss: 5.9300 avg training loss: 7.0960
batch: [15640/21305] batch time: 0.530 trainign loss: 6.9958 avg training loss: 7.0960
batch: [15650/21305] batch time: 0.058 trainign loss: 5.5778 avg training loss: 7.0960
batch: [15660/21305] batch time: 0.424 trainign loss: 5.2832 avg training loss: 7.0959
batch: [15670/21305] batch time: 0.056 trainign loss: 6.5406 avg training loss: 7.0958
batch: [15680/21305] batch time: 0.582 trainign loss: 6.9114 avg training loss: 7.0958
batch: [15690/21305] batch time: 0.056 trainign loss: 6.6046 avg training loss: 7.0958
batch: [15700/21305] batch time: 0.645 trainign loss: 6.2406 avg training loss: 7.0957
batch: [15710/21305] batch time: 0.056 trainign loss: 6.2271 avg training loss: 7.0956
batch: [15720/21305] batch time: 0.471 trainign loss: 4.5988 avg training loss: 7.0956
batch: [15730/21305] batch time: 0.554 trainign loss: 0.2869 avg training loss: 7.0954
batch: [15740/21305] batch time: 0.062 trainign loss: 11.3318 avg training loss: 7.0951
batch: [15750/21305] batch time: 0.608 trainign loss: 7.3097 avg training loss: 7.0952
batch: [15760/21305] batch time: 1.104 trainign loss: 6.3467 avg training loss: 7.0952
batch: [15770/21305] batch time: 0.663 trainign loss: 6.0409 avg training loss: 7.0951
batch: [15780/21305] batch time: 0.440 trainign loss: 6.2311 avg training loss: 7.0951
batch: [15790/21305] batch time: 0.772 trainign loss: 6.1531 avg training loss: 7.0951
batch: [15800/21305] batch time: 0.058 trainign loss: 4.9342 avg training loss: 7.0951
batch: [15810/21305] batch time: 0.051 trainign loss: 7.2353 avg training loss: 7.0950
batch: [15820/21305] batch time: 0.062 trainign loss: 6.2295 avg training loss: 7.0950
batch: [15830/21305] batch time: 0.056 trainign loss: 6.1578 avg training loss: 7.0949
batch: [15840/21305] batch time: 0.056 trainign loss: 5.9401 avg training loss: 7.0949
batch: [15850/21305] batch time: 0.056 trainign loss: 5.1619 avg training loss: 7.0949
batch: [15860/21305] batch time: 0.480 trainign loss: 7.6753 avg training loss: 7.0948
batch: [15870/21305] batch time: 0.408 trainign loss: 6.0720 avg training loss: 7.0948
batch: [15880/21305] batch time: 0.301 trainign loss: 6.9289 avg training loss: 7.0948
batch: [15890/21305] batch time: 0.056 trainign loss: 6.9171 avg training loss: 7.0948
batch: [15900/21305] batch time: 0.061 trainign loss: 5.2707 avg training loss: 7.0948
batch: [15910/21305] batch time: 0.056 trainign loss: 6.2762 avg training loss: 7.0947
batch: [15920/21305] batch time: 0.164 trainign loss: 6.3088 avg training loss: 7.0947
batch: [15930/21305] batch time: 0.056 trainign loss: 6.7623 avg training loss: 7.0946
batch: [15940/21305] batch time: 0.058 trainign loss: 6.1449 avg training loss: 7.0945
batch: [15950/21305] batch time: 0.055 trainign loss: 7.0196 avg training loss: 7.0945
batch: [15960/21305] batch time: 0.056 trainign loss: 4.8551 avg training loss: 7.0944
batch: [15970/21305] batch time: 0.052 trainign loss: 4.8980 avg training loss: 7.0943
batch: [15980/21305] batch time: 0.056 trainign loss: 5.7156 avg training loss: 7.0943
batch: [15990/21305] batch time: 0.062 trainign loss: 6.9276 avg training loss: 7.0943
batch: [16000/21305] batch time: 0.059 trainign loss: 6.4977 avg training loss: 7.0942
batch: [16010/21305] batch time: 0.063 trainign loss: 0.9084 avg training loss: 7.0941
batch: [16020/21305] batch time: 0.054 trainign loss: 6.4155 avg training loss: 7.0940
batch: [16030/21305] batch time: 0.051 trainign loss: 7.4768 avg training loss: 7.0940
batch: [16040/21305] batch time: 0.056 trainign loss: 4.7647 avg training loss: 7.0940
batch: [16050/21305] batch time: 0.062 trainign loss: 6.3922 avg training loss: 7.0939
batch: [16060/21305] batch time: 0.052 trainign loss: 5.5753 avg training loss: 7.0939
batch: [16070/21305] batch time: 0.052 trainign loss: 7.3308 avg training loss: 7.0939
batch: [16080/21305] batch time: 0.056 trainign loss: 6.7022 avg training loss: 7.0938
batch: [16090/21305] batch time: 0.057 trainign loss: 6.0855 avg training loss: 7.0938
batch: [16100/21305] batch time: 0.061 trainign loss: 5.3692 avg training loss: 7.0937
batch: [16110/21305] batch time: 0.051 trainign loss: 5.4825 avg training loss: 7.0937
batch: [16120/21305] batch time: 0.062 trainign loss: 4.4965 avg training loss: 7.0936
batch: [16130/21305] batch time: 0.057 trainign loss: 6.0918 avg training loss: 7.0935
batch: [16140/21305] batch time: 0.056 trainign loss: 6.7806 avg training loss: 7.0935
batch: [16150/21305] batch time: 0.051 trainign loss: 6.9493 avg training loss: 7.0935
batch: [16160/21305] batch time: 0.056 trainign loss: 4.0754 avg training loss: 7.0934
batch: [16170/21305] batch time: 0.057 trainign loss: 7.7076 avg training loss: 7.0934
batch: [16180/21305] batch time: 0.062 trainign loss: 7.3859 avg training loss: 7.0934
batch: [16190/21305] batch time: 0.062 trainign loss: 7.0122 avg training loss: 7.0934
batch: [16200/21305] batch time: 0.060 trainign loss: 5.2571 avg training loss: 7.0934
batch: [16210/21305] batch time: 0.063 trainign loss: 6.7341 avg training loss: 7.0933
batch: [16220/21305] batch time: 0.057 trainign loss: 6.5605 avg training loss: 7.0933
batch: [16230/21305] batch time: 0.057 trainign loss: 6.6660 avg training loss: 7.0933
batch: [16240/21305] batch time: 0.056 trainign loss: 7.1907 avg training loss: 7.0932
batch: [16250/21305] batch time: 0.051 trainign loss: 6.4988 avg training loss: 7.0932
batch: [16260/21305] batch time: 0.056 trainign loss: 6.7126 avg training loss: 7.0931
batch: [16270/21305] batch time: 0.057 trainign loss: 7.1111 avg training loss: 7.0931
batch: [16280/21305] batch time: 0.056 trainign loss: 7.0814 avg training loss: 7.0931
batch: [16290/21305] batch time: 0.051 trainign loss: 6.5708 avg training loss: 7.0931
batch: [16300/21305] batch time: 0.061 trainign loss: 6.7164 avg training loss: 7.0930
batch: [16310/21305] batch time: 0.060 trainign loss: 6.6469 avg training loss: 7.0930
batch: [16320/21305] batch time: 0.054 trainign loss: 6.8763 avg training loss: 7.0929
batch: [16330/21305] batch time: 0.056 trainign loss: 5.4008 avg training loss: 7.0929
batch: [16340/21305] batch time: 0.050 trainign loss: 5.0103 avg training loss: 7.0928
batch: [16350/21305] batch time: 0.059 trainign loss: 7.2867 avg training loss: 7.0927
batch: [16360/21305] batch time: 0.058 trainign loss: 6.3094 avg training loss: 7.0926
batch: [16370/21305] batch time: 0.053 trainign loss: 5.8987 avg training loss: 7.0926
batch: [16380/21305] batch time: 0.060 trainign loss: 7.1022 avg training loss: 7.0925
batch: [16390/21305] batch time: 0.062 trainign loss: 6.5353 avg training loss: 7.0925
batch: [16400/21305] batch time: 0.063 trainign loss: 7.6048 avg training loss: 7.0925
batch: [16410/21305] batch time: 0.056 trainign loss: 7.3647 avg training loss: 7.0925
batch: [16420/21305] batch time: 0.053 trainign loss: 6.6731 avg training loss: 7.0925
batch: [16430/21305] batch time: 0.056 trainign loss: 5.9249 avg training loss: 7.0924
batch: [16440/21305] batch time: 0.061 trainign loss: 7.0561 avg training loss: 7.0924
batch: [16450/21305] batch time: 0.056 trainign loss: 7.4745 avg training loss: 7.0924
batch: [16460/21305] batch time: 0.056 trainign loss: 6.9379 avg training loss: 7.0923
batch: [16470/21305] batch time: 0.051 trainign loss: 6.7256 avg training loss: 7.0923
batch: [16480/21305] batch time: 0.062 trainign loss: 5.7484 avg training loss: 7.0923
batch: [16490/21305] batch time: 0.062 trainign loss: 5.5788 avg training loss: 7.0922
batch: [16500/21305] batch time: 0.056 trainign loss: 5.8171 avg training loss: 7.0922
batch: [16510/21305] batch time: 0.056 trainign loss: 6.5340 avg training loss: 7.0922
batch: [16520/21305] batch time: 0.061 trainign loss: 6.2891 avg training loss: 7.0921
batch: [16530/21305] batch time: 0.054 trainign loss: 6.7328 avg training loss: 7.0921
batch: [16540/21305] batch time: 0.063 trainign loss: 5.8697 avg training loss: 7.0920
batch: [16550/21305] batch time: 0.055 trainign loss: 7.0494 avg training loss: 7.0919
batch: [16560/21305] batch time: 0.053 trainign loss: 7.2603 avg training loss: 7.0919
batch: [16570/21305] batch time: 0.061 trainign loss: 6.3798 avg training loss: 7.0919
batch: [16580/21305] batch time: 0.062 trainign loss: 7.0443 avg training loss: 7.0919
batch: [16590/21305] batch time: 0.062 trainign loss: 7.1684 avg training loss: 7.0918
batch: [16600/21305] batch time: 0.056 trainign loss: 6.7296 avg training loss: 7.0918
batch: [16610/21305] batch time: 0.062 trainign loss: 5.1537 avg training loss: 7.0917
batch: [16620/21305] batch time: 0.055 trainign loss: 6.3281 avg training loss: 7.0916
batch: [16630/21305] batch time: 0.061 trainign loss: 5.4994 avg training loss: 7.0915
batch: [16640/21305] batch time: 0.056 trainign loss: 7.2489 avg training loss: 7.0915
batch: [16650/21305] batch time: 0.053 trainign loss: 6.8110 avg training loss: 7.0915
batch: [16660/21305] batch time: 0.058 trainign loss: 6.6579 avg training loss: 7.0914
batch: [16670/21305] batch time: 0.057 trainign loss: 6.3525 avg training loss: 7.0914
batch: [16680/21305] batch time: 0.058 trainign loss: 6.9120 avg training loss: 7.0913
batch: [16690/21305] batch time: 0.053 trainign loss: 6.8227 avg training loss: 7.0913
batch: [16700/21305] batch time: 0.058 trainign loss: 6.6510 avg training loss: 7.0912
batch: [16710/21305] batch time: 0.053 trainign loss: 6.7231 avg training loss: 7.0912
batch: [16720/21305] batch time: 0.056 trainign loss: 6.8937 avg training loss: 7.0912
batch: [16730/21305] batch time: 0.299 trainign loss: 4.9630 avg training loss: 7.0912
batch: [16740/21305] batch time: 0.056 trainign loss: 6.2990 avg training loss: 7.0911
batch: [16750/21305] batch time: 0.061 trainign loss: 6.4032 avg training loss: 7.0911
batch: [16760/21305] batch time: 0.057 trainign loss: 6.0604 avg training loss: 7.0910
batch: [16770/21305] batch time: 0.058 trainign loss: 6.9724 avg training loss: 7.0910
batch: [16780/21305] batch time: 0.337 trainign loss: 6.2379 avg training loss: 7.0910
batch: [16790/21305] batch time: 0.542 trainign loss: 6.3585 avg training loss: 7.0909
batch: [16800/21305] batch time: 0.058 trainign loss: 1.8861 avg training loss: 7.0907
batch: [16810/21305] batch time: 0.575 trainign loss: 7.3275 avg training loss: 7.0908
batch: [16820/21305] batch time: 0.086 trainign loss: 5.6243 avg training loss: 7.0907
batch: [16830/21305] batch time: 0.074 trainign loss: 6.2924 avg training loss: 7.0907
batch: [16840/21305] batch time: 0.056 trainign loss: 6.7558 avg training loss: 7.0906
batch: [16850/21305] batch time: 0.192 trainign loss: 6.6005 avg training loss: 7.0906
batch: [16860/21305] batch time: 0.056 trainign loss: 6.9344 avg training loss: 7.0905
batch: [16870/21305] batch time: 0.051 trainign loss: 6.7212 avg training loss: 7.0904
batch: [16880/21305] batch time: 0.056 trainign loss: 6.3871 avg training loss: 7.0904
batch: [16890/21305] batch time: 0.061 trainign loss: 6.6278 avg training loss: 7.0904
batch: [16900/21305] batch time: 0.239 trainign loss: 4.2758 avg training loss: 7.0903
batch: [16910/21305] batch time: 0.051 trainign loss: 0.0070 avg training loss: 7.0900
batch: [16920/21305] batch time: 0.059 trainign loss: 8.4256 avg training loss: 7.0900
batch: [16930/21305] batch time: 0.051 trainign loss: 4.5543 avg training loss: 7.0900
batch: [16940/21305] batch time: 0.653 trainign loss: 6.5667 avg training loss: 7.0899
batch: [16950/21305] batch time: 0.056 trainign loss: 6.9938 avg training loss: 7.0899
batch: [16960/21305] batch time: 1.187 trainign loss: 4.4148 avg training loss: 7.0899
batch: [16970/21305] batch time: 0.056 trainign loss: 5.6754 avg training loss: 7.0898
batch: [16980/21305] batch time: 1.010 trainign loss: 3.5970 avg training loss: 7.0897
batch: [16990/21305] batch time: 0.058 trainign loss: 3.4163 avg training loss: 7.0896
batch: [17000/21305] batch time: 1.597 trainign loss: 1.9746 avg training loss: 7.0894
batch: [17010/21305] batch time: 0.061 trainign loss: 7.2570 avg training loss: 7.0895
batch: [17020/21305] batch time: 1.516 trainign loss: 0.8693 avg training loss: 7.0893
batch: [17030/21305] batch time: 0.056 trainign loss: 7.1192 avg training loss: 7.0892
batch: [17040/21305] batch time: 1.774 trainign loss: 7.4300 avg training loss: 7.0892
batch: [17050/21305] batch time: 0.056 trainign loss: 6.3412 avg training loss: 7.0892
batch: [17060/21305] batch time: 2.495 trainign loss: 6.8280 avg training loss: 7.0891
batch: [17070/21305] batch time: 0.056 trainign loss: 7.1701 avg training loss: 7.0890
batch: [17080/21305] batch time: 2.264 trainign loss: 7.2429 avg training loss: 7.0890
batch: [17090/21305] batch time: 0.063 trainign loss: 6.2743 avg training loss: 7.0890
batch: [17100/21305] batch time: 2.073 trainign loss: 2.0501 avg training loss: 7.0889
batch: [17110/21305] batch time: 0.056 trainign loss: 7.6143 avg training loss: 7.0889
batch: [17120/21305] batch time: 2.033 trainign loss: 7.0664 avg training loss: 7.0889
batch: [17130/21305] batch time: 0.056 trainign loss: 5.7628 avg training loss: 7.0888
batch: [17140/21305] batch time: 2.312 trainign loss: 5.5197 avg training loss: 7.0888
batch: [17150/21305] batch time: 0.062 trainign loss: 6.2740 avg training loss: 7.0887
batch: [17160/21305] batch time: 1.107 trainign loss: 6.7369 avg training loss: 7.0886
batch: [17170/21305] batch time: 0.057 trainign loss: 4.8898 avg training loss: 7.0885
batch: [17180/21305] batch time: 0.605 trainign loss: 7.4910 avg training loss: 7.0885
batch: [17190/21305] batch time: 0.056 trainign loss: 5.3629 avg training loss: 7.0885
batch: [17200/21305] batch time: 0.804 trainign loss: 6.0177 avg training loss: 7.0884
batch: [17210/21305] batch time: 0.056 trainign loss: 6.0929 avg training loss: 7.0883
batch: [17220/21305] batch time: 0.050 trainign loss: 6.4237 avg training loss: 7.0883
batch: [17230/21305] batch time: 0.056 trainign loss: 4.8057 avg training loss: 7.0882
batch: [17240/21305] batch time: 0.192 trainign loss: 6.3465 avg training loss: 7.0881
batch: [17250/21305] batch time: 0.063 trainign loss: 8.0481 avg training loss: 7.0880
batch: [17260/21305] batch time: 0.411 trainign loss: 7.2369 avg training loss: 7.0880
batch: [17270/21305] batch time: 0.063 trainign loss: 6.2195 avg training loss: 7.0880
batch: [17280/21305] batch time: 0.335 trainign loss: 6.8068 avg training loss: 7.0880
batch: [17290/21305] batch time: 0.288 trainign loss: 4.9119 avg training loss: 7.0880
batch: [17300/21305] batch time: 0.334 trainign loss: 6.1777 avg training loss: 7.0879
batch: [17310/21305] batch time: 0.061 trainign loss: 3.4432 avg training loss: 7.0878
batch: [17320/21305] batch time: 0.052 trainign loss: 10.8984 avg training loss: 7.0875
batch: [17330/21305] batch time: 0.060 trainign loss: 7.9162 avg training loss: 7.0876
batch: [17340/21305] batch time: 0.052 trainign loss: 7.0789 avg training loss: 7.0876
batch: [17350/21305] batch time: 0.055 trainign loss: 7.4872 avg training loss: 7.0874
batch: [17360/21305] batch time: 0.051 trainign loss: 6.7953 avg training loss: 7.0874
batch: [17370/21305] batch time: 0.062 trainign loss: 6.1861 avg training loss: 7.0874
batch: [17380/21305] batch time: 0.057 trainign loss: 7.0135 avg training loss: 7.0874
batch: [17390/21305] batch time: 0.056 trainign loss: 7.2256 avg training loss: 7.0874
batch: [17400/21305] batch time: 0.054 trainign loss: 6.9093 avg training loss: 7.0873
batch: [17410/21305] batch time: 0.057 trainign loss: 6.8373 avg training loss: 7.0873
batch: [17420/21305] batch time: 0.051 trainign loss: 5.9412 avg training loss: 7.0873
batch: [17430/21305] batch time: 0.056 trainign loss: 5.4004 avg training loss: 7.0873
batch: [17440/21305] batch time: 0.051 trainign loss: 6.1774 avg training loss: 7.0872
batch: [17450/21305] batch time: 0.057 trainign loss: 6.2425 avg training loss: 7.0871
batch: [17460/21305] batch time: 0.057 trainign loss: 4.8535 avg training loss: 7.0871
batch: [17470/21305] batch time: 0.439 trainign loss: 6.5346 avg training loss: 7.0870
batch: [17480/21305] batch time: 0.060 trainign loss: 5.7797 avg training loss: 7.0869
batch: [17490/21305] batch time: 0.121 trainign loss: 6.1591 avg training loss: 7.0869
batch: [17500/21305] batch time: 0.063 trainign loss: 5.8284 avg training loss: 7.0868
batch: [17510/21305] batch time: 0.699 trainign loss: 6.2571 avg training loss: 7.0868
batch: [17520/21305] batch time: 0.063 trainign loss: 5.5630 avg training loss: 7.0867
batch: [17530/21305] batch time: 0.056 trainign loss: 5.0544 avg training loss: 7.0867
batch: [17540/21305] batch time: 0.058 trainign loss: 6.2863 avg training loss: 7.0866
batch: [17550/21305] batch time: 0.057 trainign loss: 6.5187 avg training loss: 7.0866
batch: [17560/21305] batch time: 0.053 trainign loss: 5.8135 avg training loss: 7.0865
batch: [17570/21305] batch time: 0.057 trainign loss: 6.7137 avg training loss: 7.0864
batch: [17580/21305] batch time: 0.057 trainign loss: 1.4834 avg training loss: 7.0863
batch: [17590/21305] batch time: 0.057 trainign loss: 10.7658 avg training loss: 7.0860
batch: [17600/21305] batch time: 0.056 trainign loss: 8.0350 avg training loss: 7.0859
batch: [17610/21305] batch time: 0.063 trainign loss: 8.1607 avg training loss: 7.0860
batch: [17620/21305] batch time: 0.056 trainign loss: 5.2777 avg training loss: 7.0860
batch: [17630/21305] batch time: 0.056 trainign loss: 6.6303 avg training loss: 7.0859
batch: [17640/21305] batch time: 0.051 trainign loss: 5.2999 avg training loss: 7.0859
batch: [17650/21305] batch time: 0.056 trainign loss: 5.5292 avg training loss: 7.0859
batch: [17660/21305] batch time: 0.058 trainign loss: 6.2071 avg training loss: 7.0858
batch: [17670/21305] batch time: 0.063 trainign loss: 5.4131 avg training loss: 7.0858
batch: [17680/21305] batch time: 0.054 trainign loss: 6.5752 avg training loss: 7.0857
batch: [17690/21305] batch time: 0.058 trainign loss: 5.1344 avg training loss: 7.0857
batch: [17700/21305] batch time: 0.061 trainign loss: 6.3475 avg training loss: 7.0856
batch: [17710/21305] batch time: 0.056 trainign loss: 6.5390 avg training loss: 7.0855
batch: [17720/21305] batch time: 0.058 trainign loss: 6.5668 avg training loss: 7.0855
batch: [17730/21305] batch time: 0.058 trainign loss: 6.6504 avg training loss: 7.0855
batch: [17740/21305] batch time: 0.051 trainign loss: 5.5529 avg training loss: 7.0854
batch: [17750/21305] batch time: 0.057 trainign loss: 6.3398 avg training loss: 7.0854
batch: [17760/21305] batch time: 0.052 trainign loss: 6.2527 avg training loss: 7.0854
batch: [17770/21305] batch time: 0.058 trainign loss: 6.9440 avg training loss: 7.0854
batch: [17780/21305] batch time: 0.053 trainign loss: 6.0075 avg training loss: 7.0853
batch: [17790/21305] batch time: 0.056 trainign loss: 7.0043 avg training loss: 7.0853
batch: [17800/21305] batch time: 0.062 trainign loss: 5.8776 avg training loss: 7.0852
batch: [17810/21305] batch time: 0.056 trainign loss: 7.0116 avg training loss: 7.0852
batch: [17820/21305] batch time: 0.052 trainign loss: 6.4525 avg training loss: 7.0852
batch: [17830/21305] batch time: 0.156 trainign loss: 5.2233 avg training loss: 7.0851
batch: [17840/21305] batch time: 0.055 trainign loss: 7.1263 avg training loss: 7.0851
batch: [17850/21305] batch time: 0.061 trainign loss: 6.7256 avg training loss: 7.0851
batch: [17860/21305] batch time: 0.052 trainign loss: 6.5042 avg training loss: 7.0850
batch: [17870/21305] batch time: 0.057 trainign loss: 5.4861 avg training loss: 7.0850
batch: [17880/21305] batch time: 0.052 trainign loss: 6.5776 avg training loss: 7.0850
batch: [17890/21305] batch time: 0.056 trainign loss: 6.5451 avg training loss: 7.0849
batch: [17900/21305] batch time: 0.057 trainign loss: 6.1884 avg training loss: 7.0849
batch: [17910/21305] batch time: 0.590 trainign loss: 3.3481 avg training loss: 7.0847
batch: [17920/21305] batch time: 0.062 trainign loss: 7.4299 avg training loss: 7.0847
batch: [17930/21305] batch time: 0.638 trainign loss: 6.0196 avg training loss: 7.0847
batch: [17940/21305] batch time: 0.056 trainign loss: 6.3847 avg training loss: 7.0847
batch: [17950/21305] batch time: 0.188 trainign loss: 6.2507 avg training loss: 7.0846
batch: [17960/21305] batch time: 0.054 trainign loss: 6.7262 avg training loss: 7.0846
batch: [17970/21305] batch time: 0.058 trainign loss: 6.0899 avg training loss: 7.0845
batch: [17980/21305] batch time: 0.056 trainign loss: 1.3850 avg training loss: 7.0844
batch: [17990/21305] batch time: 0.056 trainign loss: 4.5363 avg training loss: 7.0843
batch: [18000/21305] batch time: 0.056 trainign loss: 7.1031 avg training loss: 7.0843
batch: [18010/21305] batch time: 0.125 trainign loss: 7.8689 avg training loss: 7.0843
batch: [18020/21305] batch time: 0.056 trainign loss: 4.1851 avg training loss: 7.0842
batch: [18030/21305] batch time: 0.060 trainign loss: 6.6157 avg training loss: 7.0842
batch: [18040/21305] batch time: 0.055 trainign loss: 6.3032 avg training loss: 7.0842
batch: [18050/21305] batch time: 0.344 trainign loss: 6.6303 avg training loss: 7.0841
batch: [18060/21305] batch time: 0.056 trainign loss: 7.3323 avg training loss: 7.0841
batch: [18070/21305] batch time: 0.063 trainign loss: 7.5273 avg training loss: 7.0841
batch: [18080/21305] batch time: 0.053 trainign loss: 6.3989 avg training loss: 7.0841
batch: [18090/21305] batch time: 0.052 trainign loss: 5.2335 avg training loss: 7.0840
batch: [18100/21305] batch time: 0.054 trainign loss: 5.7771 avg training loss: 7.0839
batch: [18110/21305] batch time: 0.056 trainign loss: 5.4487 avg training loss: 7.0839
batch: [18120/21305] batch time: 0.056 trainign loss: 7.0377 avg training loss: 7.0838
batch: [18130/21305] batch time: 0.057 trainign loss: 7.2941 avg training loss: 7.0838
batch: [18140/21305] batch time: 0.057 trainign loss: 7.4847 avg training loss: 7.0837
batch: [18150/21305] batch time: 0.063 trainign loss: 7.1383 avg training loss: 7.0837
batch: [18160/21305] batch time: 0.056 trainign loss: 5.0542 avg training loss: 7.0837
batch: [18170/21305] batch time: 0.661 trainign loss: 5.7485 avg training loss: 7.0836
batch: [18180/21305] batch time: 0.056 trainign loss: 0.1167 avg training loss: 7.0834
batch: [18190/21305] batch time: 0.926 trainign loss: 7.5646 avg training loss: 7.0834
batch: [18200/21305] batch time: 0.056 trainign loss: 7.1536 avg training loss: 7.0834
batch: [18210/21305] batch time: 0.387 trainign loss: 6.4562 avg training loss: 7.0834
batch: [18220/21305] batch time: 0.056 trainign loss: 4.0785 avg training loss: 7.0833
batch: [18230/21305] batch time: 0.519 trainign loss: 8.2910 avg training loss: 7.0833
batch: [18240/21305] batch time: 0.062 trainign loss: 6.3743 avg training loss: 7.0833
batch: [18250/21305] batch time: 0.560 trainign loss: 5.7385 avg training loss: 7.0833
batch: [18260/21305] batch time: 0.056 trainign loss: 2.4610 avg training loss: 7.0831
batch: [18270/21305] batch time: 0.053 trainign loss: 6.0380 avg training loss: 7.0831
batch: [18280/21305] batch time: 0.056 trainign loss: 6.1999 avg training loss: 7.0830
batch: [18290/21305] batch time: 0.051 trainign loss: 7.2792 avg training loss: 7.0830
batch: [18300/21305] batch time: 0.062 trainign loss: 6.0613 avg training loss: 7.0830
batch: [18310/21305] batch time: 0.054 trainign loss: 6.5503 avg training loss: 7.0830
batch: [18320/21305] batch time: 0.053 trainign loss: 5.1519 avg training loss: 7.0829
batch: [18330/21305] batch time: 0.054 trainign loss: 6.1478 avg training loss: 7.0829
batch: [18340/21305] batch time: 0.056 trainign loss: 7.2003 avg training loss: 7.0829
batch: [18350/21305] batch time: 0.063 trainign loss: 7.1992 avg training loss: 7.0828
batch: [18360/21305] batch time: 0.055 trainign loss: 5.8120 avg training loss: 7.0828
batch: [18370/21305] batch time: 0.059 trainign loss: 6.5218 avg training loss: 7.0827
batch: [18380/21305] batch time: 0.062 trainign loss: 6.2184 avg training loss: 7.0827
batch: [18390/21305] batch time: 0.062 trainign loss: 5.9814 avg training loss: 7.0826
batch: [18400/21305] batch time: 0.054 trainign loss: 7.0144 avg training loss: 7.0826
batch: [18410/21305] batch time: 0.057 trainign loss: 4.9258 avg training loss: 7.0826
batch: [18420/21305] batch time: 0.060 trainign loss: 6.3455 avg training loss: 7.0825
batch: [18430/21305] batch time: 0.059 trainign loss: 6.2984 avg training loss: 7.0825
batch: [18440/21305] batch time: 0.054 trainign loss: 0.0869 avg training loss: 7.0822
batch: [18450/21305] batch time: 0.056 trainign loss: 8.1824 avg training loss: 7.0822
batch: [18460/21305] batch time: 0.054 trainign loss: 7.0449 avg training loss: 7.0822
batch: [18470/21305] batch time: 0.056 trainign loss: 7.2482 avg training loss: 7.0822
batch: [18480/21305] batch time: 0.061 trainign loss: 6.0191 avg training loss: 7.0821
batch: [18490/21305] batch time: 0.056 trainign loss: 3.9409 avg training loss: 7.0820
batch: [18500/21305] batch time: 0.056 trainign loss: 0.0064 avg training loss: 7.0817
batch: [18510/21305] batch time: 0.057 trainign loss: 9.0309 avg training loss: 7.0817
batch: [18520/21305] batch time: 0.060 trainign loss: 7.9809 avg training loss: 7.0817
batch: [18530/21305] batch time: 0.056 trainign loss: 6.2162 avg training loss: 7.0817
batch: [18540/21305] batch time: 0.054 trainign loss: 6.1443 avg training loss: 7.0817
batch: [18550/21305] batch time: 0.056 trainign loss: 4.2280 avg training loss: 7.0816
batch: [18560/21305] batch time: 0.051 trainign loss: 7.1615 avg training loss: 7.0815
batch: [18570/21305] batch time: 0.060 trainign loss: 7.3020 avg training loss: 7.0815
batch: [18580/21305] batch time: 0.056 trainign loss: 6.7826 avg training loss: 7.0815
batch: [18590/21305] batch time: 0.051 trainign loss: 6.8353 avg training loss: 7.0815
batch: [18600/21305] batch time: 0.052 trainign loss: 5.7790 avg training loss: 7.0814
batch: [18610/21305] batch time: 0.056 trainign loss: 6.0333 avg training loss: 7.0813
batch: [18620/21305] batch time: 0.056 trainign loss: 7.8045 avg training loss: 7.0813
batch: [18630/21305] batch time: 0.056 trainign loss: 6.3682 avg training loss: 7.0813
batch: [18640/21305] batch time: 0.056 trainign loss: 5.1112 avg training loss: 7.0812
batch: [18650/21305] batch time: 0.062 trainign loss: 3.4947 avg training loss: 7.0811
batch: [18660/21305] batch time: 0.062 trainign loss: 0.0030 avg training loss: 7.0808
batch: [18670/21305] batch time: 0.056 trainign loss: 8.5799 avg training loss: 7.0807
batch: [18680/21305] batch time: 0.057 trainign loss: 7.0308 avg training loss: 7.0808
batch: [18690/21305] batch time: 0.056 trainign loss: 7.4445 avg training loss: 7.0808
batch: [18700/21305] batch time: 0.057 trainign loss: 6.6761 avg training loss: 7.0808
batch: [18710/21305] batch time: 0.062 trainign loss: 6.6324 avg training loss: 7.0807
batch: [18720/21305] batch time: 0.055 trainign loss: 5.9403 avg training loss: 7.0807
batch: [18730/21305] batch time: 0.057 trainign loss: 5.3281 avg training loss: 7.0807
batch: [18740/21305] batch time: 0.051 trainign loss: 7.0625 avg training loss: 7.0806
batch: [18750/21305] batch time: 0.056 trainign loss: 5.5916 avg training loss: 7.0806
batch: [18760/21305] batch time: 0.052 trainign loss: 5.1094 avg training loss: 7.0805
batch: [18770/21305] batch time: 0.062 trainign loss: 7.6188 avg training loss: 7.0804
batch: [18780/21305] batch time: 0.051 trainign loss: 7.1832 avg training loss: 7.0804
batch: [18790/21305] batch time: 0.061 trainign loss: 5.9135 avg training loss: 7.0804
batch: [18800/21305] batch time: 0.063 trainign loss: 7.2468 avg training loss: 7.0804
batch: [18810/21305] batch time: 0.056 trainign loss: 6.8836 avg training loss: 7.0804
batch: [18820/21305] batch time: 0.051 trainign loss: 4.5561 avg training loss: 7.0803
batch: [18830/21305] batch time: 0.056 trainign loss: 7.3931 avg training loss: 7.0803
batch: [18840/21305] batch time: 0.056 trainign loss: 6.2539 avg training loss: 7.0802
batch: [18850/21305] batch time: 0.058 trainign loss: 7.0866 avg training loss: 7.0801
batch: [18860/21305] batch time: 0.056 trainign loss: 6.8504 avg training loss: 7.0801
batch: [18870/21305] batch time: 0.057 trainign loss: 6.6019 avg training loss: 7.0801
batch: [18880/21305] batch time: 0.056 trainign loss: 4.1770 avg training loss: 7.0799
batch: [18890/21305] batch time: 0.057 trainign loss: 7.1186 avg training loss: 7.0799
batch: [18900/21305] batch time: 0.059 trainign loss: 6.0571 avg training loss: 7.0799
batch: [18910/21305] batch time: 0.056 trainign loss: 6.6643 avg training loss: 7.0799
batch: [18920/21305] batch time: 0.056 trainign loss: 6.8303 avg training loss: 7.0799
batch: [18930/21305] batch time: 0.056 trainign loss: 7.1893 avg training loss: 7.0798
batch: [18940/21305] batch time: 0.057 trainign loss: 6.2583 avg training loss: 7.0798
batch: [18950/21305] batch time: 0.057 trainign loss: 6.8923 avg training loss: 7.0798
batch: [18960/21305] batch time: 0.055 trainign loss: 6.6934 avg training loss: 7.0797
batch: [18970/21305] batch time: 0.062 trainign loss: 6.5258 avg training loss: 7.0796
batch: [18980/21305] batch time: 0.055 trainign loss: 8.1752 avg training loss: 7.0795
batch: [18990/21305] batch time: 0.063 trainign loss: 7.7124 avg training loss: 7.0795
batch: [19000/21305] batch time: 0.056 trainign loss: 5.9031 avg training loss: 7.0795
batch: [19010/21305] batch time: 0.059 trainign loss: 7.4665 avg training loss: 7.0795
batch: [19020/21305] batch time: 0.054 trainign loss: 6.8550 avg training loss: 7.0795
batch: [19030/21305] batch time: 0.056 trainign loss: 7.0570 avg training loss: 7.0794
batch: [19040/21305] batch time: 0.051 trainign loss: 6.8529 avg training loss: 7.0794
batch: [19050/21305] batch time: 0.056 trainign loss: 7.0930 avg training loss: 7.0793
batch: [19060/21305] batch time: 0.054 trainign loss: 5.1426 avg training loss: 7.0792
batch: [19070/21305] batch time: 0.057 trainign loss: 6.6048 avg training loss: 7.0791
batch: [19080/21305] batch time: 0.054 trainign loss: 6.4858 avg training loss: 7.0791
batch: [19090/21305] batch time: 0.056 trainign loss: 7.9037 avg training loss: 7.0791
batch: [19100/21305] batch time: 0.056 trainign loss: 7.3239 avg training loss: 7.0791
batch: [19110/21305] batch time: 0.063 trainign loss: 3.8141 avg training loss: 7.0791
batch: [19120/21305] batch time: 0.056 trainign loss: 6.1785 avg training loss: 7.0790
batch: [19130/21305] batch time: 0.059 trainign loss: 7.1850 avg training loss: 7.0790
batch: [19140/21305] batch time: 0.055 trainign loss: 5.9831 avg training loss: 7.0790
batch: [19150/21305] batch time: 0.059 trainign loss: 6.1335 avg training loss: 7.0789
batch: [19160/21305] batch time: 0.056 trainign loss: 6.9225 avg training loss: 7.0789
batch: [19170/21305] batch time: 0.063 trainign loss: 6.3646 avg training loss: 7.0789
batch: [19180/21305] batch time: 0.056 trainign loss: 4.5715 avg training loss: 7.0788
batch: [19190/21305] batch time: 0.057 trainign loss: 6.4451 avg training loss: 7.0788
batch: [19200/21305] batch time: 0.055 trainign loss: 4.2231 avg training loss: 7.0787
batch: [19210/21305] batch time: 0.058 trainign loss: 6.9766 avg training loss: 7.0786
batch: [19220/21305] batch time: 0.051 trainign loss: 6.8795 avg training loss: 7.0786
batch: [19230/21305] batch time: 0.057 trainign loss: 5.9621 avg training loss: 7.0786
batch: [19240/21305] batch time: 0.056 trainign loss: 6.1354 avg training loss: 7.0785
batch: [19250/21305] batch time: 0.056 trainign loss: 2.9113 avg training loss: 7.0785
batch: [19260/21305] batch time: 0.051 trainign loss: 4.9117 avg training loss: 7.0784
batch: [19270/21305] batch time: 0.175 trainign loss: 7.0602 avg training loss: 7.0783
batch: [19280/21305] batch time: 0.056 trainign loss: 7.7956 avg training loss: 7.0783
batch: [19290/21305] batch time: 0.195 trainign loss: 6.7343 avg training loss: 7.0783
batch: [19300/21305] batch time: 0.054 trainign loss: 7.0575 avg training loss: 7.0782
batch: [19310/21305] batch time: 0.055 trainign loss: 7.4440 avg training loss: 7.0782
batch: [19320/21305] batch time: 0.052 trainign loss: 7.4296 avg training loss: 7.0782
batch: [19330/21305] batch time: 0.119 trainign loss: 6.5439 avg training loss: 7.0782
batch: [19340/21305] batch time: 0.052 trainign loss: 5.1579 avg training loss: 7.0782
batch: [19350/21305] batch time: 0.264 trainign loss: 6.8867 avg training loss: 7.0781
batch: [19360/21305] batch time: 0.061 trainign loss: 6.0023 avg training loss: 7.0781
batch: [19370/21305] batch time: 0.056 trainign loss: 6.4612 avg training loss: 7.0781
batch: [19380/21305] batch time: 0.063 trainign loss: 5.8797 avg training loss: 7.0780
batch: [19390/21305] batch time: 0.056 trainign loss: 7.6336 avg training loss: 7.0780
batch: [19400/21305] batch time: 0.063 trainign loss: 6.9669 avg training loss: 7.0779
batch: [19410/21305] batch time: 0.057 trainign loss: 5.1728 avg training loss: 7.0779
batch: [19420/21305] batch time: 0.060 trainign loss: 6.6423 avg training loss: 7.0778
batch: [19430/21305] batch time: 0.063 trainign loss: 6.6735 avg training loss: 7.0778
batch: [19440/21305] batch time: 0.056 trainign loss: 6.8927 avg training loss: 7.0778
batch: [19450/21305] batch time: 0.058 trainign loss: 6.6715 avg training loss: 7.0778
batch: [19460/21305] batch time: 0.053 trainign loss: 6.2871 avg training loss: 7.0777
batch: [19470/21305] batch time: 0.057 trainign loss: 6.9638 avg training loss: 7.0777
batch: [19480/21305] batch time: 0.056 trainign loss: 5.6886 avg training loss: 7.0777
batch: [19490/21305] batch time: 0.057 trainign loss: 4.9046 avg training loss: 7.0776
batch: [19500/21305] batch time: 0.054 trainign loss: 6.6324 avg training loss: 7.0776
batch: [19510/21305] batch time: 0.057 trainign loss: 6.0335 avg training loss: 7.0776
batch: [19520/21305] batch time: 0.057 trainign loss: 6.6009 avg training loss: 7.0776
batch: [19530/21305] batch time: 0.059 trainign loss: 6.8104 avg training loss: 7.0775
batch: [19540/21305] batch time: 0.498 trainign loss: 6.1002 avg training loss: 7.0774
batch: [19550/21305] batch time: 0.056 trainign loss: 6.2094 avg training loss: 7.0773
batch: [19560/21305] batch time: 0.051 trainign loss: 7.2590 avg training loss: 7.0773
batch: [19570/21305] batch time: 0.061 trainign loss: 6.9845 avg training loss: 7.0773
batch: [19580/21305] batch time: 0.058 trainign loss: 5.5254 avg training loss: 7.0773
batch: [19590/21305] batch time: 1.240 trainign loss: 5.4082 avg training loss: 7.0772
batch: [19600/21305] batch time: 0.056 trainign loss: 6.2836 avg training loss: 7.0771
batch: [19610/21305] batch time: 0.954 trainign loss: 6.6685 avg training loss: 7.0771
batch: [19620/21305] batch time: 0.063 trainign loss: 6.6444 avg training loss: 7.0771
batch: [19630/21305] batch time: 0.739 trainign loss: 6.7161 avg training loss: 7.0770
batch: [19640/21305] batch time: 0.056 trainign loss: 7.4164 avg training loss: 7.0770
batch: [19650/21305] batch time: 0.911 trainign loss: 6.5740 avg training loss: 7.0770
batch: [19660/21305] batch time: 0.062 trainign loss: 6.2472 avg training loss: 7.0769
batch: [19670/21305] batch time: 0.062 trainign loss: 6.1399 avg training loss: 7.0769
batch: [19680/21305] batch time: 0.056 trainign loss: 0.0290 avg training loss: 7.0766
batch: [19690/21305] batch time: 1.054 trainign loss: 7.3117 avg training loss: 7.0766
batch: [19700/21305] batch time: 0.058 trainign loss: 7.5750 avg training loss: 7.0766
batch: [19710/21305] batch time: 1.093 trainign loss: 6.2359 avg training loss: 7.0766
batch: [19720/21305] batch time: 0.062 trainign loss: 6.0719 avg training loss: 7.0766
batch: [19730/21305] batch time: 1.634 trainign loss: 5.4539 avg training loss: 7.0765
batch: [19740/21305] batch time: 0.056 trainign loss: 5.4698 avg training loss: 7.0765
batch: [19750/21305] batch time: 0.973 trainign loss: 7.1913 avg training loss: 7.0764
batch: [19760/21305] batch time: 0.062 trainign loss: 6.1659 avg training loss: 7.0764
batch: [19770/21305] batch time: 0.337 trainign loss: 5.5968 avg training loss: 7.0764
batch: [19780/21305] batch time: 0.057 trainign loss: 4.8564 avg training loss: 7.0763
batch: [19790/21305] batch time: 0.191 trainign loss: 5.9688 avg training loss: 7.0762
batch: [19800/21305] batch time: 0.057 trainign loss: 5.8847 avg training loss: 7.0762
batch: [19810/21305] batch time: 0.056 trainign loss: 4.9005 avg training loss: 7.0761
batch: [19820/21305] batch time: 0.056 trainign loss: 6.3892 avg training loss: 7.0760
batch: [19830/21305] batch time: 0.216 trainign loss: 7.2080 avg training loss: 7.0760
batch: [19840/21305] batch time: 0.058 trainign loss: 6.2356 avg training loss: 7.0760
batch: [19850/21305] batch time: 0.056 trainign loss: 6.3200 avg training loss: 7.0759
batch: [19860/21305] batch time: 0.057 trainign loss: 7.4848 avg training loss: 7.0759
batch: [19870/21305] batch time: 0.415 trainign loss: 7.1168 avg training loss: 7.0759
batch: [19880/21305] batch time: 0.058 trainign loss: 4.9558 avg training loss: 7.0758
batch: [19890/21305] batch time: 0.056 trainign loss: 4.6550 avg training loss: 7.0757
batch: [19900/21305] batch time: 0.062 trainign loss: 8.8101 avg training loss: 7.0756
batch: [19910/21305] batch time: 0.055 trainign loss: 1.5280 avg training loss: 7.0753
batch: [19920/21305] batch time: 0.062 trainign loss: 6.7954 avg training loss: 7.0754
batch: [19930/21305] batch time: 0.058 trainign loss: 7.4116 avg training loss: 7.0754
batch: [19940/21305] batch time: 0.056 trainign loss: 5.7593 avg training loss: 7.0754
batch: [19950/21305] batch time: 0.056 trainign loss: 6.2906 avg training loss: 7.0752
batch: [19960/21305] batch time: 0.056 trainign loss: 6.6741 avg training loss: 7.0751
batch: [19970/21305] batch time: 0.053 trainign loss: 7.0843 avg training loss: 7.0751
batch: [19980/21305] batch time: 0.056 trainign loss: 7.5818 avg training loss: 7.0751
batch: [19990/21305] batch time: 0.051 trainign loss: 5.9626 avg training loss: 7.0751
batch: [20000/21305] batch time: 0.056 trainign loss: 6.7570 avg training loss: 7.0750
batch: [20010/21305] batch time: 0.057 trainign loss: 5.7151 avg training loss: 7.0750
batch: [20020/21305] batch time: 0.058 trainign loss: 0.5079 avg training loss: 7.0748
batch: [20030/21305] batch time: 0.056 trainign loss: 6.1549 avg training loss: 7.0747
batch: [20040/21305] batch time: 0.058 trainign loss: 4.9992 avg training loss: 7.0746
batch: [20050/21305] batch time: 0.055 trainign loss: 2.1939 avg training loss: 7.0745
batch: [20060/21305] batch time: 0.057 trainign loss: 4.1344 avg training loss: 7.0744
batch: [20070/21305] batch time: 0.054 trainign loss: 6.9701 avg training loss: 7.0743
batch: [20080/21305] batch time: 0.061 trainign loss: 6.7700 avg training loss: 7.0743
batch: [20090/21305] batch time: 0.060 trainign loss: 4.2619 avg training loss: 7.0743
batch: [20100/21305] batch time: 0.058 trainign loss: 10.0773 avg training loss: 7.0741
batch: [20110/21305] batch time: 0.061 trainign loss: 6.8519 avg training loss: 7.0741
batch: [20120/21305] batch time: 0.061 trainign loss: 7.7052 avg training loss: 7.0741
batch: [20130/21305] batch time: 0.051 trainign loss: 6.9853 avg training loss: 7.0741
batch: [20140/21305] batch time: 0.063 trainign loss: 6.5219 avg training loss: 7.0741
batch: [20150/21305] batch time: 0.056 trainign loss: 5.7058 avg training loss: 7.0741
batch: [20160/21305] batch time: 0.058 trainign loss: 6.9192 avg training loss: 7.0740
batch: [20170/21305] batch time: 0.051 trainign loss: 7.2623 avg training loss: 7.0740
batch: [20180/21305] batch time: 0.056 trainign loss: 6.4301 avg training loss: 7.0740
batch: [20190/21305] batch time: 0.986 trainign loss: 3.5593 avg training loss: 7.0739
batch: [20200/21305] batch time: 0.056 trainign loss: 5.0974 avg training loss: 7.0738
batch: [20210/21305] batch time: 0.499 trainign loss: 7.3559 avg training loss: 7.0738
batch: [20220/21305] batch time: 0.056 trainign loss: 7.0992 avg training loss: 7.0737
batch: [20230/21305] batch time: 0.056 trainign loss: 6.4472 avg training loss: 7.0737
batch: [20240/21305] batch time: 0.062 trainign loss: 5.1040 avg training loss: 7.0737
batch: [20250/21305] batch time: 0.056 trainign loss: 6.9687 avg training loss: 7.0736
batch: [20260/21305] batch time: 0.056 trainign loss: 6.5830 avg training loss: 7.0735
batch: [20270/21305] batch time: 0.051 trainign loss: 1.1285 avg training loss: 7.0734
batch: [20280/21305] batch time: 0.056 trainign loss: 7.1496 avg training loss: 7.0733
batch: [20290/21305] batch time: 0.056 trainign loss: 7.3445 avg training loss: 7.0734
batch: [20300/21305] batch time: 0.060 trainign loss: 6.8976 avg training loss: 7.0733
batch: [20310/21305] batch time: 0.055 trainign loss: 6.8226 avg training loss: 7.0733
batch: [20320/21305] batch time: 0.055 trainign loss: 6.3821 avg training loss: 7.0733
batch: [20330/21305] batch time: 0.052 trainign loss: 4.9505 avg training loss: 7.0732
batch: [20340/21305] batch time: 0.056 trainign loss: 7.0933 avg training loss: 7.0731
batch: [20350/21305] batch time: 0.057 trainign loss: 7.2491 avg training loss: 7.0731
batch: [20360/21305] batch time: 0.059 trainign loss: 7.0179 avg training loss: 7.0731
batch: [20370/21305] batch time: 0.053 trainign loss: 6.7408 avg training loss: 7.0731
batch: [20380/21305] batch time: 0.063 trainign loss: 6.1301 avg training loss: 7.0730
batch: [20390/21305] batch time: 0.052 trainign loss: 7.0982 avg training loss: 7.0730
batch: [20400/21305] batch time: 0.056 trainign loss: 6.4471 avg training loss: 7.0729
batch: [20410/21305] batch time: 0.053 trainign loss: 6.5415 avg training loss: 7.0729
batch: [20420/21305] batch time: 0.056 trainign loss: 6.2121 avg training loss: 7.0729
batch: [20430/21305] batch time: 0.051 trainign loss: 7.0711 avg training loss: 7.0727
batch: [20440/21305] batch time: 0.056 trainign loss: 6.6969 avg training loss: 7.0727
batch: [20450/21305] batch time: 0.062 trainign loss: 5.4157 avg training loss: 7.0727
batch: [20460/21305] batch time: 0.056 trainign loss: 7.1590 avg training loss: 7.0727
batch: [20470/21305] batch time: 0.051 trainign loss: 4.6559 avg training loss: 7.0726
batch: [20480/21305] batch time: 0.056 trainign loss: 0.0658 avg training loss: 7.0724
batch: [20490/21305] batch time: 0.056 trainign loss: 13.0532 avg training loss: 7.0721
batch: [20500/21305] batch time: 0.056 trainign loss: 8.1585 avg training loss: 7.0722
batch: [20510/21305] batch time: 0.056 trainign loss: 7.4570 avg training loss: 7.0723
batch: [20520/21305] batch time: 0.058 trainign loss: 6.6414 avg training loss: 7.0722
batch: [20530/21305] batch time: 0.056 trainign loss: 5.9988 avg training loss: 7.0721
batch: [20540/21305] batch time: 0.057 trainign loss: 8.2466 avg training loss: 7.0720
batch: [20550/21305] batch time: 0.056 trainign loss: 7.1754 avg training loss: 7.0720
batch: [20560/21305] batch time: 0.058 trainign loss: 6.4537 avg training loss: 7.0720
batch: [20570/21305] batch time: 0.056 trainign loss: 7.4807 avg training loss: 7.0720
batch: [20580/21305] batch time: 0.057 trainign loss: 6.5041 avg training loss: 7.0720
batch: [20590/21305] batch time: 0.052 trainign loss: 5.8936 avg training loss: 7.0719
batch: [20600/21305] batch time: 0.057 trainign loss: 6.3170 avg training loss: 7.0719
batch: [20610/21305] batch time: 0.056 trainign loss: 7.1389 avg training loss: 7.0719
batch: [20620/21305] batch time: 0.056 trainign loss: 6.8250 avg training loss: 7.0719
batch: [20630/21305] batch time: 0.057 trainign loss: 6.5285 avg training loss: 7.0718
batch: [20640/21305] batch time: 0.056 trainign loss: 6.7245 avg training loss: 7.0718
batch: [20650/21305] batch time: 0.051 trainign loss: 6.7344 avg training loss: 7.0718
batch: [20660/21305] batch time: 0.062 trainign loss: 5.4848 avg training loss: 7.0717
batch: [20670/21305] batch time: 0.055 trainign loss: 7.3518 avg training loss: 7.0717
batch: [20680/21305] batch time: 0.058 trainign loss: 5.0262 avg training loss: 7.0716
batch: [20690/21305] batch time: 0.057 trainign loss: 6.7758 avg training loss: 7.0716
batch: [20700/21305] batch time: 0.058 trainign loss: 6.8669 avg training loss: 7.0716
batch: [20710/21305] batch time: 0.055 trainign loss: 6.8482 avg training loss: 7.0715
batch: [20720/21305] batch time: 0.056 trainign loss: 6.9470 avg training loss: 7.0715
batch: [20730/21305] batch time: 0.057 trainign loss: 6.3601 avg training loss: 7.0715
batch: [20740/21305] batch time: 0.053 trainign loss: 6.7069 avg training loss: 7.0715
batch: [20750/21305] batch time: 0.055 trainign loss: 5.5352 avg training loss: 7.0714
batch: [20760/21305] batch time: 0.061 trainign loss: 7.2231 avg training loss: 7.0714
batch: [20770/21305] batch time: 0.052 trainign loss: 6.4420 avg training loss: 7.0714
batch: [20780/21305] batch time: 0.061 trainign loss: 6.7913 avg training loss: 7.0714
batch: [20790/21305] batch time: 0.060 trainign loss: 6.0705 avg training loss: 7.0714
batch: [20800/21305] batch time: 0.056 trainign loss: 5.9376 avg training loss: 7.0713
batch: [20810/21305] batch time: 0.054 trainign loss: 6.9150 avg training loss: 7.0713
batch: [20820/21305] batch time: 0.063 trainign loss: 6.1506 avg training loss: 7.0713
batch: [20830/21305] batch time: 0.051 trainign loss: 6.4307 avg training loss: 7.0713
batch: [20840/21305] batch time: 0.056 trainign loss: 5.8334 avg training loss: 7.0712
batch: [20850/21305] batch time: 0.055 trainign loss: 5.6780 avg training loss: 7.0711
batch: [20860/21305] batch time: 0.062 trainign loss: 6.4410 avg training loss: 7.0711
batch: [20870/21305] batch time: 0.061 trainign loss: 6.4397 avg training loss: 7.0710
batch: [20880/21305] batch time: 0.057 trainign loss: 5.5241 avg training loss: 7.0710
batch: [20890/21305] batch time: 0.056 trainign loss: 7.1435 avg training loss: 7.0710
batch: [20900/21305] batch time: 0.056 trainign loss: 4.2083 avg training loss: 7.0709
batch: [20910/21305] batch time: 0.056 trainign loss: 5.7054 avg training loss: 7.0708
batch: [20920/21305] batch time: 0.056 trainign loss: 4.4012 avg training loss: 7.0707
batch: [20930/21305] batch time: 0.056 trainign loss: 7.6883 avg training loss: 7.0706
batch: [20940/21305] batch time: 0.056 trainign loss: 2.3429 avg training loss: 7.0704
batch: [20950/21305] batch time: 0.056 trainign loss: 6.9980 avg training loss: 7.0704
batch: [20960/21305] batch time: 0.056 trainign loss: 6.8916 avg training loss: 7.0705
batch: [20970/21305] batch time: 0.053 trainign loss: 5.7162 avg training loss: 7.0705
batch: [20980/21305] batch time: 0.062 trainign loss: 7.1920 avg training loss: 7.0705
batch: [20990/21305] batch time: 0.054 trainign loss: 7.3317 avg training loss: 7.0704
batch: [21000/21305] batch time: 0.056 trainign loss: 6.1319 avg training loss: 7.0704
batch: [21010/21305] batch time: 0.054 trainign loss: 6.4638 avg training loss: 7.0704
batch: [21020/21305] batch time: 0.056 trainign loss: 6.1085 avg training loss: 7.0703
batch: [21030/21305] batch time: 0.052 trainign loss: 6.8809 avg training loss: 7.0703
batch: [21040/21305] batch time: 0.063 trainign loss: 7.1936 avg training loss: 7.0703
batch: [21050/21305] batch time: 0.054 trainign loss: 6.4445 avg training loss: 7.0703
batch: [21060/21305] batch time: 0.062 trainign loss: 6.0932 avg training loss: 7.0702
batch: [21070/21305] batch time: 0.051 trainign loss: 6.0104 avg training loss: 7.0702
batch: [21080/21305] batch time: 0.062 trainign loss: 6.0480 avg training loss: 7.0701
batch: [21090/21305] batch time: 0.056 trainign loss: 7.1919 avg training loss: 7.0700
batch: [21100/21305] batch time: 0.056 trainign loss: 4.7326 avg training loss: 7.0699
batch: [21110/21305] batch time: 0.059 trainign loss: 7.5895 avg training loss: 7.0699
batch: [21120/21305] batch time: 0.057 trainign loss: 7.4559 avg training loss: 7.0699
batch: [21130/21305] batch time: 0.053 trainign loss: 7.1300 avg training loss: 7.0698
batch: [21140/21305] batch time: 0.062 trainign loss: 6.7123 avg training loss: 7.0698
batch: [21150/21305] batch time: 0.058 trainign loss: 6.8059 avg training loss: 7.0698
batch: [21160/21305] batch time: 0.273 trainign loss: 6.7804 avg training loss: 7.0697
batch: [21170/21305] batch time: 0.057 trainign loss: 4.6787 avg training loss: 7.0696
batch: [21180/21305] batch time: 1.230 trainign loss: 7.0481 avg training loss: 7.0696
batch: [21190/21305] batch time: 0.057 trainign loss: 6.9629 avg training loss: 7.0696
batch: [21200/21305] batch time: 1.076 trainign loss: 6.4683 avg training loss: 7.0695
batch: [21210/21305] batch time: 0.062 trainign loss: 6.6157 avg training loss: 7.0695
batch: [21220/21305] batch time: 0.737 trainign loss: 6.1195 avg training loss: 7.0695
batch: [21230/21305] batch time: 0.056 trainign loss: 6.5884 avg training loss: 7.0694
batch: [21240/21305] batch time: 1.071 trainign loss: 5.4727 avg training loss: 7.0694
batch: [21250/21305] batch time: 0.061 trainign loss: 6.7000 avg training loss: 7.0694
batch: [21260/21305] batch time: 0.435 trainign loss: 7.1064 avg training loss: 7.0693
batch: [21270/21305] batch time: 0.056 trainign loss: 3.7967 avg training loss: 7.0693
batch: [21280/21305] batch time: 0.839 trainign loss: 5.8588 avg training loss: 7.0692
batch: [21290/21305] batch time: 0.056 trainign loss: 4.6384 avg training loss: 7.0691
batch: [21300/21305] batch time: 0.884 trainign loss: 7.2266 avg training loss: 7.0691
Epoch: 10
----------------------------------------------------------------------
batch: [0/21305] batch time: 2.696 trainign loss: 6.7258 avg training loss: 7.0691
batch: [10/21305] batch time: 0.195 trainign loss: 8.3492 avg training loss: 7.0689
batch: [20/21305] batch time: 1.930 trainign loss: 7.6374 avg training loss: 7.0690
batch: [30/21305] batch time: 0.474 trainign loss: 6.8878 avg training loss: 7.0689
batch: [40/21305] batch time: 1.666 trainign loss: 7.0767 avg training loss: 7.0689
batch: [50/21305] batch time: 0.699 trainign loss: 3.4015 avg training loss: 7.0689
batch: [60/21305] batch time: 0.572 trainign loss: 7.2761 avg training loss: 7.0688
batch: [70/21305] batch time: 0.466 trainign loss: 7.0467 avg training loss: 7.0688
batch: [80/21305] batch time: 0.330 trainign loss: 6.8912 avg training loss: 7.0688
batch: [90/21305] batch time: 0.061 trainign loss: 6.8981 avg training loss: 7.0687
batch: [100/21305] batch time: 0.809 trainign loss: 6.7664 avg training loss: 7.0687
batch: [110/21305] batch time: 0.131 trainign loss: 5.9424 avg training loss: 7.0687
batch: [120/21305] batch time: 0.907 trainign loss: 6.8135 avg training loss: 7.0686
batch: [130/21305] batch time: 0.393 trainign loss: 7.2078 avg training loss: 7.0685
batch: [140/21305] batch time: 0.311 trainign loss: 5.3828 avg training loss: 7.0685
batch: [150/21305] batch time: 0.585 trainign loss: 7.7754 avg training loss: 7.0685
batch: [160/21305] batch time: 1.333 trainign loss: 7.0619 avg training loss: 7.0685
batch: [170/21305] batch time: 0.246 trainign loss: 6.6386 avg training loss: 7.0684
batch: [180/21305] batch time: 1.246 trainign loss: 6.0027 avg training loss: 7.0684
batch: [190/21305] batch time: 0.057 trainign loss: 5.2743 avg training loss: 7.0683
batch: [200/21305] batch time: 0.520 trainign loss: 0.0125 avg training loss: 7.0680
batch: [210/21305] batch time: 0.056 trainign loss: 9.2284 avg training loss: 7.0680
batch: [220/21305] batch time: 1.239 trainign loss: 7.3243 avg training loss: 7.0680
batch: [230/21305] batch time: 0.056 trainign loss: 6.0128 avg training loss: 7.0680
batch: [240/21305] batch time: 0.902 trainign loss: 7.2942 avg training loss: 7.0680
batch: [250/21305] batch time: 0.056 trainign loss: 7.5759 avg training loss: 7.0680
batch: [260/21305] batch time: 0.514 trainign loss: 6.8593 avg training loss: 7.0680
batch: [270/21305] batch time: 0.057 trainign loss: 4.7607 avg training loss: 7.0680
batch: [280/21305] batch time: 0.726 trainign loss: 6.3084 avg training loss: 7.0679
batch: [290/21305] batch time: 0.056 trainign loss: 6.9000 avg training loss: 7.0679
batch: [300/21305] batch time: 1.228 trainign loss: 6.6635 avg training loss: 7.0678
batch: [310/21305] batch time: 0.058 trainign loss: 5.7014 avg training loss: 7.0678
batch: [320/21305] batch time: 1.544 trainign loss: 6.7729 avg training loss: 7.0678
batch: [330/21305] batch time: 0.062 trainign loss: 6.2531 avg training loss: 7.0677
batch: [340/21305] batch time: 0.586 trainign loss: 7.4986 avg training loss: 7.0677
batch: [350/21305] batch time: 0.056 trainign loss: 7.6569 avg training loss: 7.0677
batch: [360/21305] batch time: 1.054 trainign loss: 7.2031 avg training loss: 7.0676
batch: [370/21305] batch time: 0.056 trainign loss: 6.1190 avg training loss: 7.0676
batch: [380/21305] batch time: 1.688 trainign loss: 6.3474 avg training loss: 7.0676
batch: [390/21305] batch time: 0.058 trainign loss: 3.9967 avg training loss: 7.0675
batch: [400/21305] batch time: 2.342 trainign loss: 6.7937 avg training loss: 7.0674
batch: [410/21305] batch time: 0.058 trainign loss: 7.6144 avg training loss: 7.0674
batch: [420/21305] batch time: 2.383 trainign loss: 6.9723 avg training loss: 7.0674
batch: [430/21305] batch time: 0.057 trainign loss: 7.1000 avg training loss: 7.0674
batch: [440/21305] batch time: 2.443 trainign loss: 6.5515 avg training loss: 7.0674
batch: [450/21305] batch time: 0.061 trainign loss: 5.7063 avg training loss: 7.0673
batch: [460/21305] batch time: 2.194 trainign loss: 7.6388 avg training loss: 7.0673
batch: [470/21305] batch time: 0.056 trainign loss: 6.4163 avg training loss: 7.0673
batch: [480/21305] batch time: 2.266 trainign loss: 7.1463 avg training loss: 7.0673
batch: [490/21305] batch time: 0.063 trainign loss: 6.6645 avg training loss: 7.0673
batch: [500/21305] batch time: 1.411 trainign loss: 4.3030 avg training loss: 7.0672
batch: [510/21305] batch time: 0.056 trainign loss: 5.8011 avg training loss: 7.0672
batch: [520/21305] batch time: 1.419 trainign loss: 3.6878 avg training loss: 7.0672
batch: [530/21305] batch time: 0.056 trainign loss: 6.5416 avg training loss: 7.0671
batch: [540/21305] batch time: 0.995 trainign loss: 6.7363 avg training loss: 7.0671
batch: [550/21305] batch time: 0.056 trainign loss: 4.3476 avg training loss: 7.0670
batch: [560/21305] batch time: 1.239 trainign loss: 6.2671 avg training loss: 7.0670
batch: [570/21305] batch time: 0.056 trainign loss: 5.9939 avg training loss: 7.0669
batch: [580/21305] batch time: 1.998 trainign loss: 6.5842 avg training loss: 7.0668
batch: [590/21305] batch time: 0.063 trainign loss: 6.7867 avg training loss: 7.0668
batch: [600/21305] batch time: 2.012 trainign loss: 6.5303 avg training loss: 7.0667
batch: [610/21305] batch time: 0.063 trainign loss: 7.3023 avg training loss: 7.0668
batch: [620/21305] batch time: 0.940 trainign loss: 6.6041 avg training loss: 7.0667
batch: [630/21305] batch time: 0.056 trainign loss: 6.1018 avg training loss: 7.0667
batch: [640/21305] batch time: 0.185 trainign loss: 7.1221 avg training loss: 7.0667
batch: [650/21305] batch time: 0.057 trainign loss: 6.2684 avg training loss: 7.0666
batch: [660/21305] batch time: 0.056 trainign loss: 5.6263 avg training loss: 7.0665
batch: [670/21305] batch time: 0.056 trainign loss: 8.0923 avg training loss: 7.0664
batch: [680/21305] batch time: 0.051 trainign loss: 7.8652 avg training loss: 7.0664
batch: [690/21305] batch time: 0.057 trainign loss: 6.5871 avg training loss: 7.0664
batch: [700/21305] batch time: 0.051 trainign loss: 7.4423 avg training loss: 7.0664
batch: [710/21305] batch time: 0.056 trainign loss: 5.9102 avg training loss: 7.0664
batch: [720/21305] batch time: 0.580 trainign loss: 6.5754 avg training loss: 7.0663
batch: [730/21305] batch time: 0.058 trainign loss: 7.7013 avg training loss: 7.0663
batch: [740/21305] batch time: 0.990 trainign loss: 7.3672 avg training loss: 7.0663
batch: [750/21305] batch time: 0.056 trainign loss: 6.6318 avg training loss: 7.0663
batch: [760/21305] batch time: 0.064 trainign loss: 6.1974 avg training loss: 7.0662
batch: [770/21305] batch time: 0.061 trainign loss: 4.4535 avg training loss: 7.0661
batch: [780/21305] batch time: 0.056 trainign loss: 7.3316 avg training loss: 7.0661
batch: [790/21305] batch time: 0.056 trainign loss: 7.1166 avg training loss: 7.0661
batch: [800/21305] batch time: 0.058 trainign loss: 6.8316 avg training loss: 7.0661
batch: [810/21305] batch time: 0.058 trainign loss: 6.3361 avg training loss: 7.0661
batch: [820/21305] batch time: 0.056 trainign loss: 7.0056 avg training loss: 7.0660
batch: [830/21305] batch time: 0.051 trainign loss: 6.2896 avg training loss: 7.0660
batch: [840/21305] batch time: 0.062 trainign loss: 6.8049 avg training loss: 7.0660
batch: [850/21305] batch time: 0.056 trainign loss: 5.9319 avg training loss: 7.0659
batch: [860/21305] batch time: 0.130 trainign loss: 4.9590 avg training loss: 7.0658
batch: [870/21305] batch time: 0.061 trainign loss: 4.9619 avg training loss: 7.0658
batch: [880/21305] batch time: 0.054 trainign loss: 6.6610 avg training loss: 7.0656
batch: [890/21305] batch time: 0.063 trainign loss: 8.3028 avg training loss: 7.0656
batch: [900/21305] batch time: 0.058 trainign loss: 6.1673 avg training loss: 7.0657
batch: [910/21305] batch time: 0.063 trainign loss: 6.8378 avg training loss: 7.0656
batch: [920/21305] batch time: 0.058 trainign loss: 4.6752 avg training loss: 7.0656
batch: [930/21305] batch time: 0.053 trainign loss: 6.4232 avg training loss: 7.0655
batch: [940/21305] batch time: 0.057 trainign loss: 4.2587 avg training loss: 7.0654
batch: [950/21305] batch time: 0.056 trainign loss: 0.0040 avg training loss: 7.0651
batch: [960/21305] batch time: 0.062 trainign loss: 7.7222 avg training loss: 7.0651
batch: [970/21305] batch time: 0.060 trainign loss: 7.8662 avg training loss: 7.0652
batch: [980/21305] batch time: 0.051 trainign loss: 6.9386 avg training loss: 7.0652
batch: [990/21305] batch time: 0.051 trainign loss: 6.7592 avg training loss: 7.0652
batch: [1000/21305] batch time: 0.062 trainign loss: 4.6275 avg training loss: 7.0651
batch: [1010/21305] batch time: 0.050 trainign loss: 6.7790 avg training loss: 7.0650
batch: [1020/21305] batch time: 0.063 trainign loss: 7.5537 avg training loss: 7.0650
batch: [1030/21305] batch time: 0.062 trainign loss: 6.5155 avg training loss: 7.0650
batch: [1040/21305] batch time: 0.052 trainign loss: 7.1052 avg training loss: 7.0650
batch: [1050/21305] batch time: 0.051 trainign loss: 6.7332 avg training loss: 7.0650
batch: [1060/21305] batch time: 0.062 trainign loss: 5.7329 avg training loss: 7.0649
batch: [1070/21305] batch time: 0.197 trainign loss: 4.6272 avg training loss: 7.0648
batch: [1080/21305] batch time: 0.056 trainign loss: 0.0335 avg training loss: 7.0646
batch: [1090/21305] batch time: 0.056 trainign loss: 7.9919 avg training loss: 7.0645
batch: [1100/21305] batch time: 0.057 trainign loss: 6.1254 avg training loss: 7.0645
batch: [1110/21305] batch time: 0.053 trainign loss: 6.8418 avg training loss: 7.0645
batch: [1120/21305] batch time: 0.062 trainign loss: 7.6758 avg training loss: 7.0645
batch: [1130/21305] batch time: 0.253 trainign loss: 6.2742 avg training loss: 7.0645
batch: [1140/21305] batch time: 0.056 trainign loss: 7.2599 avg training loss: 7.0645
batch: [1150/21305] batch time: 0.297 trainign loss: 6.8166 avg training loss: 7.0644
batch: [1160/21305] batch time: 0.058 trainign loss: 7.1349 avg training loss: 7.0644
batch: [1170/21305] batch time: 0.625 trainign loss: 7.0043 avg training loss: 7.0644
batch: [1180/21305] batch time: 0.063 trainign loss: 6.5147 avg training loss: 7.0643
batch: [1190/21305] batch time: 0.334 trainign loss: 7.1174 avg training loss: 7.0643
batch: [1200/21305] batch time: 0.056 trainign loss: 4.9594 avg training loss: 7.0643
batch: [1210/21305] batch time: 0.402 trainign loss: 6.9819 avg training loss: 7.0642
batch: [1220/21305] batch time: 0.059 trainign loss: 7.6832 avg training loss: 7.0642
batch: [1230/21305] batch time: 1.567 trainign loss: 6.6825 avg training loss: 7.0642
batch: [1240/21305] batch time: 0.056 trainign loss: 6.5992 avg training loss: 7.0641
batch: [1250/21305] batch time: 1.450 trainign loss: 7.1384 avg training loss: 7.0641
batch: [1260/21305] batch time: 0.060 trainign loss: 5.6100 avg training loss: 7.0641
batch: [1270/21305] batch time: 1.262 trainign loss: 6.7500 avg training loss: 7.0641
batch: [1280/21305] batch time: 0.056 trainign loss: 6.9677 avg training loss: 7.0640
batch: [1290/21305] batch time: 1.092 trainign loss: 5.9574 avg training loss: 7.0640
batch: [1300/21305] batch time: 0.056 trainign loss: 6.1153 avg training loss: 7.0639
batch: [1310/21305] batch time: 0.536 trainign loss: 7.1267 avg training loss: 7.0639
batch: [1320/21305] batch time: 0.056 trainign loss: 5.3244 avg training loss: 7.0639
batch: [1330/21305] batch time: 0.771 trainign loss: 7.0868 avg training loss: 7.0638
batch: [1340/21305] batch time: 0.056 trainign loss: 7.0727 avg training loss: 7.0638
batch: [1350/21305] batch time: 0.717 trainign loss: 7.0327 avg training loss: 7.0638
batch: [1360/21305] batch time: 0.058 trainign loss: 4.2249 avg training loss: 7.0638
batch: [1370/21305] batch time: 0.763 trainign loss: 7.5373 avg training loss: 7.0637
batch: [1380/21305] batch time: 0.058 trainign loss: 5.7571 avg training loss: 7.0636
batch: [1390/21305] batch time: 0.056 trainign loss: 6.8426 avg training loss: 7.0636
batch: [1400/21305] batch time: 0.062 trainign loss: 6.1269 avg training loss: 7.0636
batch: [1410/21305] batch time: 0.063 trainign loss: 6.9720 avg training loss: 7.0634
batch: [1420/21305] batch time: 0.058 trainign loss: 5.8522 avg training loss: 7.0634
batch: [1430/21305] batch time: 0.056 trainign loss: 5.9383 avg training loss: 7.0633
batch: [1440/21305] batch time: 0.062 trainign loss: 5.3492 avg training loss: 7.0633
batch: [1450/21305] batch time: 0.054 trainign loss: 6.2752 avg training loss: 7.0632
batch: [1460/21305] batch time: 0.056 trainign loss: 6.5570 avg training loss: 7.0632
batch: [1470/21305] batch time: 0.053 trainign loss: 5.7397 avg training loss: 7.0631
batch: [1480/21305] batch time: 0.062 trainign loss: 7.1130 avg training loss: 7.0631
batch: [1490/21305] batch time: 0.057 trainign loss: 7.0946 avg training loss: 7.0631
batch: [1500/21305] batch time: 0.056 trainign loss: 6.5765 avg training loss: 7.0631
batch: [1510/21305] batch time: 0.058 trainign loss: 6.9807 avg training loss: 7.0630
batch: [1520/21305] batch time: 0.056 trainign loss: 1.9909 avg training loss: 7.0629
batch: [1530/21305] batch time: 0.053 trainign loss: 11.2304 avg training loss: 7.0626
batch: [1540/21305] batch time: 0.062 trainign loss: 6.9794 avg training loss: 7.0627
batch: [1550/21305] batch time: 0.056 trainign loss: 4.2566 avg training loss: 7.0627
batch: [1560/21305] batch time: 0.063 trainign loss: 6.6293 avg training loss: 7.0626
batch: [1570/21305] batch time: 0.056 trainign loss: 5.9952 avg training loss: 7.0626
batch: [1580/21305] batch time: 0.056 trainign loss: 6.2539 avg training loss: 7.0626
batch: [1590/21305] batch time: 0.051 trainign loss: 8.6389 avg training loss: 7.0624
batch: [1600/21305] batch time: 0.059 trainign loss: 7.5896 avg training loss: 7.0624
batch: [1610/21305] batch time: 0.056 trainign loss: 7.3696 avg training loss: 7.0625
batch: [1620/21305] batch time: 0.056 trainign loss: 6.3338 avg training loss: 7.0624
batch: [1630/21305] batch time: 0.056 trainign loss: 7.6436 avg training loss: 7.0624
batch: [1640/21305] batch time: 0.056 trainign loss: 7.1546 avg training loss: 7.0624
batch: [1650/21305] batch time: 0.214 trainign loss: 5.9392 avg training loss: 7.0624
batch: [1660/21305] batch time: 0.056 trainign loss: 6.0661 avg training loss: 7.0623
batch: [1670/21305] batch time: 0.337 trainign loss: 6.5513 avg training loss: 7.0623
batch: [1680/21305] batch time: 0.056 trainign loss: 5.6991 avg training loss: 7.0623
batch: [1690/21305] batch time: 0.051 trainign loss: 6.3845 avg training loss: 7.0623
batch: [1700/21305] batch time: 0.056 trainign loss: 6.7189 avg training loss: 7.0622
batch: [1710/21305] batch time: 0.510 trainign loss: 5.1019 avg training loss: 7.0622
batch: [1720/21305] batch time: 0.729 trainign loss: 7.6109 avg training loss: 7.0621
batch: [1730/21305] batch time: 0.063 trainign loss: 6.4291 avg training loss: 7.0621
batch: [1740/21305] batch time: 1.392 trainign loss: 5.9883 avg training loss: 7.0621
batch: [1750/21305] batch time: 0.062 trainign loss: 6.4917 avg training loss: 7.0621
batch: [1760/21305] batch time: 2.340 trainign loss: 6.8034 avg training loss: 7.0620
batch: [1770/21305] batch time: 0.061 trainign loss: 5.9976 avg training loss: 7.0620
batch: [1780/21305] batch time: 1.984 trainign loss: 6.3930 avg training loss: 7.0619
batch: [1790/21305] batch time: 0.358 trainign loss: 6.3910 avg training loss: 7.0619
batch: [1800/21305] batch time: 2.156 trainign loss: 6.2833 avg training loss: 7.0619
batch: [1810/21305] batch time: 0.062 trainign loss: 6.4290 avg training loss: 7.0619
batch: [1820/21305] batch time: 2.230 trainign loss: 4.5784 avg training loss: 7.0618
batch: [1830/21305] batch time: 0.063 trainign loss: 7.5563 avg training loss: 7.0617
batch: [1840/21305] batch time: 2.163 trainign loss: 7.1415 avg training loss: 7.0617
batch: [1850/21305] batch time: 0.056 trainign loss: 6.7134 avg training loss: 7.0617
batch: [1860/21305] batch time: 1.996 trainign loss: 6.3784 avg training loss: 7.0617
batch: [1870/21305] batch time: 0.056 trainign loss: 6.4614 avg training loss: 7.0617
batch: [1880/21305] batch time: 2.208 trainign loss: 4.6862 avg training loss: 7.0616
batch: [1890/21305] batch time: 0.056 trainign loss: 4.5316 avg training loss: 7.0615
batch: [1900/21305] batch time: 2.901 trainign loss: 7.3189 avg training loss: 7.0615
batch: [1910/21305] batch time: 0.057 trainign loss: 6.8794 avg training loss: 7.0615
batch: [1920/21305] batch time: 2.375 trainign loss: 6.4326 avg training loss: 7.0614
batch: [1930/21305] batch time: 0.056 trainign loss: 2.6132 avg training loss: 7.0613
batch: [1940/21305] batch time: 2.264 trainign loss: 6.3313 avg training loss: 7.0613
batch: [1950/21305] batch time: 0.055 trainign loss: 6.2576 avg training loss: 7.0613
batch: [1960/21305] batch time: 2.362 trainign loss: 4.5098 avg training loss: 7.0612
batch: [1970/21305] batch time: 0.056 trainign loss: 7.1304 avg training loss: 7.0612
batch: [1980/21305] batch time: 2.300 trainign loss: 4.8763 avg training loss: 7.0611
batch: [1990/21305] batch time: 0.062 trainign loss: 6.5022 avg training loss: 7.0611
batch: [2000/21305] batch time: 1.724 trainign loss: 5.5943 avg training loss: 7.0611
batch: [2010/21305] batch time: 0.056 trainign loss: 6.9949 avg training loss: 7.0610
batch: [2020/21305] batch time: 1.660 trainign loss: 3.0909 avg training loss: 7.0610
batch: [2030/21305] batch time: 0.058 trainign loss: 5.8226 avg training loss: 7.0608
batch: [2040/21305] batch time: 1.454 trainign loss: 6.8327 avg training loss: 7.0609
batch: [2050/21305] batch time: 0.062 trainign loss: 5.6137 avg training loss: 7.0608
batch: [2060/21305] batch time: 1.630 trainign loss: 2.9093 avg training loss: 7.0607
batch: [2070/21305] batch time: 0.056 trainign loss: 1.2329 avg training loss: 7.0606
batch: [2080/21305] batch time: 0.136 trainign loss: 0.0298 avg training loss: 7.0604
batch: [2090/21305] batch time: 0.056 trainign loss: 7.0176 avg training loss: 7.0602
batch: [2100/21305] batch time: 0.238 trainign loss: 7.3977 avg training loss: 7.0602
batch: [2110/21305] batch time: 0.055 trainign loss: 6.0026 avg training loss: 7.0602
batch: [2120/21305] batch time: 0.078 trainign loss: 0.3064 avg training loss: 7.0600
batch: [2130/21305] batch time: 0.056 trainign loss: 0.0005 avg training loss: 7.0597
batch: [2140/21305] batch time: 0.056 trainign loss: 0.0001 avg training loss: 7.0593
batch: [2150/21305] batch time: 0.056 trainign loss: 7.7821 avg training loss: 7.0594
batch: [2160/21305] batch time: 0.061 trainign loss: 7.4182 avg training loss: 7.0595
batch: [2170/21305] batch time: 0.056 trainign loss: 7.5132 avg training loss: 7.0595
batch: [2180/21305] batch time: 0.213 trainign loss: 7.1729 avg training loss: 7.0594
batch: [2190/21305] batch time: 0.056 trainign loss: 6.5554 avg training loss: 7.0594
batch: [2200/21305] batch time: 0.556 trainign loss: 6.8036 avg training loss: 7.0594
batch: [2210/21305] batch time: 0.062 trainign loss: 5.4839 avg training loss: 7.0594
batch: [2220/21305] batch time: 0.668 trainign loss: 6.8311 avg training loss: 7.0593
batch: [2230/21305] batch time: 0.056 trainign loss: 6.5653 avg training loss: 7.0593
batch: [2240/21305] batch time: 0.298 trainign loss: 6.6730 avg training loss: 7.0592
batch: [2250/21305] batch time: 0.071 trainign loss: 6.0818 avg training loss: 7.0592
batch: [2260/21305] batch time: 1.123 trainign loss: 5.4438 avg training loss: 7.0591
batch: [2270/21305] batch time: 0.060 trainign loss: 3.7998 avg training loss: 7.0590
batch: [2280/21305] batch time: 2.095 trainign loss: 5.8857 avg training loss: 7.0588
batch: [2290/21305] batch time: 0.059 trainign loss: 3.3366 avg training loss: 7.0587
batch: [2300/21305] batch time: 2.259 trainign loss: 5.1289 avg training loss: 7.0587
batch: [2310/21305] batch time: 0.052 trainign loss: 7.6035 avg training loss: 7.0586
batch: [2320/21305] batch time: 2.386 trainign loss: 7.8556 avg training loss: 7.0586
batch: [2330/21305] batch time: 0.052 trainign loss: 7.3415 avg training loss: 7.0586
batch: [2340/21305] batch time: 0.912 trainign loss: 4.9604 avg training loss: 7.0586
batch: [2350/21305] batch time: 0.056 trainign loss: 5.4592 avg training loss: 7.0585
batch: [2360/21305] batch time: 1.593 trainign loss: 7.3654 avg training loss: 7.0585
batch: [2370/21305] batch time: 0.427 trainign loss: 4.8346 avg training loss: 7.0585
batch: [2380/21305] batch time: 1.011 trainign loss: 5.5773 avg training loss: 7.0584
batch: [2390/21305] batch time: 0.607 trainign loss: 7.1063 avg training loss: 7.0584
batch: [2400/21305] batch time: 0.303 trainign loss: 5.6814 avg training loss: 7.0584
batch: [2410/21305] batch time: 0.863 trainign loss: 7.5652 avg training loss: 7.0583
batch: [2420/21305] batch time: 0.263 trainign loss: 6.8393 avg training loss: 7.0583
batch: [2430/21305] batch time: 0.649 trainign loss: 5.4558 avg training loss: 7.0583
batch: [2440/21305] batch time: 0.920 trainign loss: 6.5157 avg training loss: 7.0583
batch: [2450/21305] batch time: 0.684 trainign loss: 6.6145 avg training loss: 7.0582
batch: [2460/21305] batch time: 1.794 trainign loss: 6.4231 avg training loss: 7.0582
batch: [2470/21305] batch time: 0.056 trainign loss: 6.5832 avg training loss: 7.0581
batch: [2480/21305] batch time: 1.170 trainign loss: 7.1539 avg training loss: 7.0581
batch: [2490/21305] batch time: 0.056 trainign loss: 6.7593 avg training loss: 7.0580
batch: [2500/21305] batch time: 1.038 trainign loss: 7.4658 avg training loss: 7.0580
batch: [2510/21305] batch time: 0.056 trainign loss: 5.2060 avg training loss: 7.0580
batch: [2520/21305] batch time: 0.770 trainign loss: 5.2937 avg training loss: 7.0579
batch: [2530/21305] batch time: 0.062 trainign loss: 5.1378 avg training loss: 7.0579
batch: [2540/21305] batch time: 0.992 trainign loss: 6.3339 avg training loss: 7.0578
batch: [2550/21305] batch time: 0.056 trainign loss: 5.6917 avg training loss: 7.0578
batch: [2560/21305] batch time: 2.191 trainign loss: 6.0300 avg training loss: 7.0577
batch: [2570/21305] batch time: 0.055 trainign loss: 7.0016 avg training loss: 7.0577
batch: [2580/21305] batch time: 2.095 trainign loss: 5.2246 avg training loss: 7.0577
batch: [2590/21305] batch time: 0.060 trainign loss: 5.4498 avg training loss: 7.0576
batch: [2600/21305] batch time: 2.556 trainign loss: 0.1893 avg training loss: 7.0574
batch: [2610/21305] batch time: 0.057 trainign loss: 7.1498 avg training loss: 7.0573
batch: [2620/21305] batch time: 2.443 trainign loss: 7.7211 avg training loss: 7.0574
batch: [2630/21305] batch time: 0.061 trainign loss: 6.5186 avg training loss: 7.0574
batch: [2640/21305] batch time: 2.308 trainign loss: 3.5230 avg training loss: 7.0573
batch: [2650/21305] batch time: 0.057 trainign loss: 7.6558 avg training loss: 7.0572
batch: [2660/21305] batch time: 2.239 trainign loss: 7.4195 avg training loss: 7.0572
batch: [2670/21305] batch time: 0.056 trainign loss: 6.4231 avg training loss: 7.0572
batch: [2680/21305] batch time: 2.114 trainign loss: 4.3264 avg training loss: 7.0571
batch: [2690/21305] batch time: 0.056 trainign loss: 7.3621 avg training loss: 7.0571
batch: [2700/21305] batch time: 2.447 trainign loss: 7.0577 avg training loss: 7.0570
batch: [2710/21305] batch time: 0.056 trainign loss: 5.1551 avg training loss: 7.0569
batch: [2720/21305] batch time: 2.114 trainign loss: 2.3338 avg training loss: 7.0568
batch: [2730/21305] batch time: 0.056 trainign loss: 7.5296 avg training loss: 7.0568
batch: [2740/21305] batch time: 2.130 trainign loss: 7.6391 avg training loss: 7.0568
batch: [2750/21305] batch time: 0.054 trainign loss: 6.7946 avg training loss: 7.0568
batch: [2760/21305] batch time: 2.059 trainign loss: 6.8868 avg training loss: 7.0567
batch: [2770/21305] batch time: 0.056 trainign loss: 6.9468 avg training loss: 7.0567
batch: [2780/21305] batch time: 2.072 trainign loss: 7.3993 avg training loss: 7.0567
batch: [2790/21305] batch time: 0.056 trainign loss: 6.5871 avg training loss: 7.0567
batch: [2800/21305] batch time: 2.296 trainign loss: 6.9453 avg training loss: 7.0566
batch: [2810/21305] batch time: 0.060 trainign loss: 6.3206 avg training loss: 7.0566
batch: [2820/21305] batch time: 2.092 trainign loss: 6.9475 avg training loss: 7.0565
batch: [2830/21305] batch time: 0.056 trainign loss: 7.2603 avg training loss: 7.0565
batch: [2840/21305] batch time: 2.371 trainign loss: 1.3204 avg training loss: 7.0564
batch: [2850/21305] batch time: 0.057 trainign loss: 5.2291 avg training loss: 7.0563
batch: [2860/21305] batch time: 2.405 trainign loss: 6.4667 avg training loss: 7.0562
batch: [2870/21305] batch time: 0.056 trainign loss: 7.6682 avg training loss: 7.0562
batch: [2880/21305] batch time: 2.629 trainign loss: 6.4564 avg training loss: 7.0561
batch: [2890/21305] batch time: 0.062 trainign loss: 6.9209 avg training loss: 7.0561
batch: [2900/21305] batch time: 2.193 trainign loss: 6.4535 avg training loss: 7.0561
batch: [2910/21305] batch time: 0.056 trainign loss: 7.0775 avg training loss: 7.0561
batch: [2920/21305] batch time: 2.451 trainign loss: 6.0133 avg training loss: 7.0560
batch: [2930/21305] batch time: 0.060 trainign loss: 5.3125 avg training loss: 7.0560
batch: [2940/21305] batch time: 2.393 trainign loss: 4.8736 avg training loss: 7.0559
batch: [2950/21305] batch time: 0.056 trainign loss: 0.4801 avg training loss: 7.0557
batch: [2960/21305] batch time: 2.194 trainign loss: 6.3830 avg training loss: 7.0557
batch: [2970/21305] batch time: 0.056 trainign loss: 6.4764 avg training loss: 7.0557
batch: [2980/21305] batch time: 2.527 trainign loss: 6.6099 avg training loss: 7.0557
batch: [2990/21305] batch time: 0.059 trainign loss: 5.8995 avg training loss: 7.0556
batch: [3000/21305] batch time: 2.648 trainign loss: 4.8818 avg training loss: 7.0556
batch: [3010/21305] batch time: 0.056 trainign loss: 7.6888 avg training loss: 7.0555
batch: [3020/21305] batch time: 2.378 trainign loss: 4.8836 avg training loss: 7.0555
batch: [3030/21305] batch time: 0.062 trainign loss: 5.9683 avg training loss: 7.0554
batch: [3040/21305] batch time: 2.376 trainign loss: 5.8976 avg training loss: 7.0554
batch: [3050/21305] batch time: 0.056 trainign loss: 7.2137 avg training loss: 7.0554
batch: [3060/21305] batch time: 2.337 trainign loss: 6.7542 avg training loss: 7.0554
batch: [3070/21305] batch time: 0.055 trainign loss: 6.4591 avg training loss: 7.0553
batch: [3080/21305] batch time: 2.405 trainign loss: 6.8124 avg training loss: 7.0553
batch: [3090/21305] batch time: 0.061 trainign loss: 6.3278 avg training loss: 7.0552
batch: [3100/21305] batch time: 2.322 trainign loss: 6.8136 avg training loss: 7.0552
batch: [3110/21305] batch time: 0.056 trainign loss: 7.4226 avg training loss: 7.0552
batch: [3120/21305] batch time: 2.442 trainign loss: 6.1598 avg training loss: 7.0551
batch: [3130/21305] batch time: 0.056 trainign loss: 0.6209 avg training loss: 7.0549
batch: [3140/21305] batch time: 2.126 trainign loss: 7.1958 avg training loss: 7.0549
batch: [3150/21305] batch time: 0.053 trainign loss: 6.2181 avg training loss: 7.0549
batch: [3160/21305] batch time: 2.570 trainign loss: 7.0950 avg training loss: 7.0549
batch: [3170/21305] batch time: 0.061 trainign loss: 5.9717 avg training loss: 7.0549
batch: [3180/21305] batch time: 2.278 trainign loss: 5.9493 avg training loss: 7.0548
batch: [3190/21305] batch time: 0.062 trainign loss: 6.0474 avg training loss: 7.0548
batch: [3200/21305] batch time: 2.499 trainign loss: 6.7244 avg training loss: 7.0547
batch: [3210/21305] batch time: 0.054 trainign loss: 6.9547 avg training loss: 7.0547
batch: [3220/21305] batch time: 2.151 trainign loss: 6.5513 avg training loss: 7.0547
batch: [3230/21305] batch time: 0.056 trainign loss: 6.6635 avg training loss: 7.0546
batch: [3240/21305] batch time: 2.647 trainign loss: 5.5116 avg training loss: 7.0546
batch: [3250/21305] batch time: 0.056 trainign loss: 6.1256 avg training loss: 7.0545
batch: [3260/21305] batch time: 2.145 trainign loss: 7.2293 avg training loss: 7.0545
batch: [3270/21305] batch time: 0.056 trainign loss: 5.4525 avg training loss: 7.0545
batch: [3280/21305] batch time: 2.312 trainign loss: 2.9985 avg training loss: 7.0544
batch: [3290/21305] batch time: 0.052 trainign loss: 5.7129 avg training loss: 7.0542
batch: [3300/21305] batch time: 2.481 trainign loss: 5.7353 avg training loss: 7.0542
batch: [3310/21305] batch time: 0.056 trainign loss: 0.6851 avg training loss: 7.0540
batch: [3320/21305] batch time: 2.140 trainign loss: 7.4974 avg training loss: 7.0540
batch: [3330/21305] batch time: 0.054 trainign loss: 6.4788 avg training loss: 7.0540
batch: [3340/21305] batch time: 2.007 trainign loss: 3.8677 avg training loss: 7.0540
batch: [3350/21305] batch time: 0.056 trainign loss: 5.1446 avg training loss: 7.0539
batch: [3360/21305] batch time: 2.113 trainign loss: 6.9332 avg training loss: 7.0538
batch: [3370/21305] batch time: 0.062 trainign loss: 3.1954 avg training loss: 7.0537
batch: [3380/21305] batch time: 2.620 trainign loss: 7.8699 avg training loss: 7.0537
batch: [3390/21305] batch time: 0.058 trainign loss: 6.8587 avg training loss: 7.0536
batch: [3400/21305] batch time: 2.435 trainign loss: 6.3882 avg training loss: 7.0536
batch: [3410/21305] batch time: 0.056 trainign loss: 5.4928 avg training loss: 7.0536
batch: [3420/21305] batch time: 1.341 trainign loss: 6.4867 avg training loss: 7.0535
batch: [3430/21305] batch time: 0.063 trainign loss: 6.5127 avg training loss: 7.0535
batch: [3440/21305] batch time: 1.201 trainign loss: 7.0456 avg training loss: 7.0534
batch: [3450/21305] batch time: 0.062 trainign loss: 5.8916 avg training loss: 7.0534
batch: [3460/21305] batch time: 1.648 trainign loss: 5.8429 avg training loss: 7.0533
batch: [3470/21305] batch time: 0.053 trainign loss: 7.4535 avg training loss: 7.0533
batch: [3480/21305] batch time: 1.772 trainign loss: 6.9911 avg training loss: 7.0533
batch: [3490/21305] batch time: 0.061 trainign loss: 6.8197 avg training loss: 7.0532
batch: [3500/21305] batch time: 1.950 trainign loss: 6.3657 avg training loss: 7.0532
batch: [3510/21305] batch time: 0.057 trainign loss: 6.4851 avg training loss: 7.0532
batch: [3520/21305] batch time: 1.990 trainign loss: 6.1734 avg training loss: 7.0532
batch: [3530/21305] batch time: 0.057 trainign loss: 6.9259 avg training loss: 7.0532
batch: [3540/21305] batch time: 2.468 trainign loss: 5.2901 avg training loss: 7.0531
batch: [3550/21305] batch time: 0.056 trainign loss: 6.4379 avg training loss: 7.0530
batch: [3560/21305] batch time: 2.754 trainign loss: 6.4552 avg training loss: 7.0529
batch: [3570/21305] batch time: 0.060 trainign loss: 7.4680 avg training loss: 7.0529
batch: [3580/21305] batch time: 2.368 trainign loss: 6.0871 avg training loss: 7.0529
batch: [3590/21305] batch time: 0.056 trainign loss: 4.3544 avg training loss: 7.0528
batch: [3600/21305] batch time: 2.360 trainign loss: 7.4472 avg training loss: 7.0527
batch: [3610/21305] batch time: 0.055 trainign loss: 7.8032 avg training loss: 7.0527
batch: [3620/21305] batch time: 2.122 trainign loss: 7.3215 avg training loss: 7.0527
batch: [3630/21305] batch time: 0.060 trainign loss: 6.8817 avg training loss: 7.0527
batch: [3640/21305] batch time: 2.027 trainign loss: 6.6313 avg training loss: 7.0527
batch: [3650/21305] batch time: 0.056 trainign loss: 6.0596 avg training loss: 7.0526
batch: [3660/21305] batch time: 2.214 trainign loss: 5.7702 avg training loss: 7.0526
batch: [3670/21305] batch time: 0.055 trainign loss: 4.7999 avg training loss: 7.0525
batch: [3680/21305] batch time: 1.779 trainign loss: 7.3512 avg training loss: 7.0525
batch: [3690/21305] batch time: 0.052 trainign loss: 6.3922 avg training loss: 7.0525
batch: [3700/21305] batch time: 1.507 trainign loss: 5.1573 avg training loss: 7.0524
batch: [3710/21305] batch time: 0.060 trainign loss: 5.2723 avg training loss: 7.0524
batch: [3720/21305] batch time: 2.042 trainign loss: 6.2654 avg training loss: 7.0523
batch: [3730/21305] batch time: 0.056 trainign loss: 5.9217 avg training loss: 7.0523
batch: [3740/21305] batch time: 0.647 trainign loss: 7.3665 avg training loss: 7.0523
batch: [3750/21305] batch time: 0.063 trainign loss: 7.0474 avg training loss: 7.0523
batch: [3760/21305] batch time: 0.063 trainign loss: 6.4099 avg training loss: 7.0522
batch: [3770/21305] batch time: 0.061 trainign loss: 4.3680 avg training loss: 7.0521
batch: [3780/21305] batch time: 0.056 trainign loss: 6.7340 avg training loss: 7.0521
batch: [3790/21305] batch time: 0.056 trainign loss: 4.7674 avg training loss: 7.0521
batch: [3800/21305] batch time: 0.056 trainign loss: 6.6062 avg training loss: 7.0521
batch: [3810/21305] batch time: 0.057 trainign loss: 6.9477 avg training loss: 7.0521
batch: [3820/21305] batch time: 0.056 trainign loss: 4.2790 avg training loss: 7.0520
batch: [3830/21305] batch time: 0.062 trainign loss: 6.0427 avg training loss: 7.0519
batch: [3840/21305] batch time: 0.056 trainign loss: 6.2941 avg training loss: 7.0519
batch: [3850/21305] batch time: 0.058 trainign loss: 6.1368 avg training loss: 7.0519
batch: [3860/21305] batch time: 0.056 trainign loss: 6.2044 avg training loss: 7.0518
batch: [3870/21305] batch time: 0.056 trainign loss: 4.9467 avg training loss: 7.0518
batch: [3880/21305] batch time: 0.056 trainign loss: 0.1248 avg training loss: 7.0516
batch: [3890/21305] batch time: 0.056 trainign loss: 8.9029 avg training loss: 7.0515
batch: [3900/21305] batch time: 0.056 trainign loss: 7.5246 avg training loss: 7.0515
batch: [3910/21305] batch time: 0.056 trainign loss: 6.8967 avg training loss: 7.0515
batch: [3920/21305] batch time: 0.057 trainign loss: 4.5203 avg training loss: 7.0514
batch: [3930/21305] batch time: 0.056 trainign loss: 6.1140 avg training loss: 7.0514
batch: [3940/21305] batch time: 0.056 trainign loss: 5.7088 avg training loss: 7.0513
batch: [3950/21305] batch time: 0.056 trainign loss: 6.2787 avg training loss: 7.0513
batch: [3960/21305] batch time: 0.057 trainign loss: 7.4062 avg training loss: 7.0513
batch: [3970/21305] batch time: 0.051 trainign loss: 6.6785 avg training loss: 7.0513
batch: [3980/21305] batch time: 0.056 trainign loss: 5.7859 avg training loss: 7.0512
batch: [3990/21305] batch time: 0.053 trainign loss: 5.1521 avg training loss: 7.0511
batch: [4000/21305] batch time: 0.056 trainign loss: 7.3927 avg training loss: 7.0511
batch: [4010/21305] batch time: 0.056 trainign loss: 6.9801 avg training loss: 7.0511
batch: [4020/21305] batch time: 0.062 trainign loss: 4.5364 avg training loss: 7.0510
batch: [4030/21305] batch time: 0.057 trainign loss: 6.9114 avg training loss: 7.0510
batch: [4040/21305] batch time: 0.057 trainign loss: 6.7890 avg training loss: 7.0510
batch: [4050/21305] batch time: 0.056 trainign loss: 3.8775 avg training loss: 7.0509
batch: [4060/21305] batch time: 0.057 trainign loss: 10.0104 avg training loss: 7.0506
batch: [4070/21305] batch time: 0.051 trainign loss: 9.8590 avg training loss: 7.0506
batch: [4080/21305] batch time: 0.056 trainign loss: 8.0338 avg training loss: 7.0507
batch: [4090/21305] batch time: 0.056 trainign loss: 7.4448 avg training loss: 7.0506
batch: [4100/21305] batch time: 0.056 trainign loss: 5.7596 avg training loss: 7.0506
batch: [4110/21305] batch time: 0.056 trainign loss: 5.2275 avg training loss: 7.0506
batch: [4120/21305] batch time: 0.058 trainign loss: 5.3223 avg training loss: 7.0505
batch: [4130/21305] batch time: 0.051 trainign loss: 6.2671 avg training loss: 7.0505
batch: [4140/21305] batch time: 0.056 trainign loss: 5.8269 avg training loss: 7.0504
batch: [4150/21305] batch time: 0.055 trainign loss: 7.7020 avg training loss: 7.0504
batch: [4160/21305] batch time: 0.056 trainign loss: 7.0078 avg training loss: 7.0503
batch: [4170/21305] batch time: 0.056 trainign loss: 5.8758 avg training loss: 7.0503
batch: [4180/21305] batch time: 0.056 trainign loss: 6.9482 avg training loss: 7.0502
batch: [4190/21305] batch time: 0.056 trainign loss: 6.9825 avg training loss: 7.0502
batch: [4200/21305] batch time: 0.060 trainign loss: 6.0395 avg training loss: 7.0501
batch: [4210/21305] batch time: 0.057 trainign loss: 1.4018 avg training loss: 7.0500
batch: [4220/21305] batch time: 0.062 trainign loss: 6.9151 avg training loss: 7.0500
batch: [4230/21305] batch time: 0.051 trainign loss: 6.9274 avg training loss: 7.0499
batch: [4240/21305] batch time: 0.057 trainign loss: 4.7190 avg training loss: 7.0498
batch: [4250/21305] batch time: 0.051 trainign loss: 0.4919 avg training loss: 7.0496
batch: [4260/21305] batch time: 0.056 trainign loss: 7.4348 avg training loss: 7.0496
batch: [4270/21305] batch time: 0.056 trainign loss: 7.9882 avg training loss: 7.0496
batch: [4280/21305] batch time: 0.056 trainign loss: 7.1350 avg training loss: 7.0496
batch: [4290/21305] batch time: 0.056 trainign loss: 6.7463 avg training loss: 7.0496
batch: [4300/21305] batch time: 0.056 trainign loss: 6.3711 avg training loss: 7.0496
batch: [4310/21305] batch time: 0.051 trainign loss: 6.5898 avg training loss: 7.0495
batch: [4320/21305] batch time: 0.056 trainign loss: 5.4991 avg training loss: 7.0495
batch: [4330/21305] batch time: 0.060 trainign loss: 4.2946 avg training loss: 7.0494
batch: [4340/21305] batch time: 0.056 trainign loss: 5.3293 avg training loss: 7.0493
batch: [4350/21305] batch time: 0.054 trainign loss: 5.9815 avg training loss: 7.0493
batch: [4360/21305] batch time: 0.056 trainign loss: 5.0746 avg training loss: 7.0492
batch: [4370/21305] batch time: 0.052 trainign loss: 7.0596 avg training loss: 7.0491
batch: [4380/21305] batch time: 0.058 trainign loss: 6.0852 avg training loss: 7.0491
batch: [4390/21305] batch time: 0.058 trainign loss: 6.6664 avg training loss: 7.0490
batch: [4400/21305] batch time: 0.056 trainign loss: 6.8567 avg training loss: 7.0490
batch: [4410/21305] batch time: 0.056 trainign loss: 6.6011 avg training loss: 7.0490
batch: [4420/21305] batch time: 0.057 trainign loss: 5.6588 avg training loss: 7.0490
batch: [4430/21305] batch time: 0.057 trainign loss: 1.2829 avg training loss: 7.0488
batch: [4440/21305] batch time: 0.063 trainign loss: 7.9095 avg training loss: 7.0486
batch: [4450/21305] batch time: 0.057 trainign loss: 6.5604 avg training loss: 7.0486
batch: [4460/21305] batch time: 0.056 trainign loss: 3.6310 avg training loss: 7.0485
batch: [4470/21305] batch time: 0.056 trainign loss: 8.1253 avg training loss: 7.0484
batch: [4480/21305] batch time: 0.058 trainign loss: 6.4646 avg training loss: 7.0484
batch: [4490/21305] batch time: 0.053 trainign loss: 3.3755 avg training loss: 7.0483
batch: [4500/21305] batch time: 0.062 trainign loss: 7.9152 avg training loss: 7.0483
batch: [4510/21305] batch time: 0.057 trainign loss: 7.1847 avg training loss: 7.0482
batch: [4520/21305] batch time: 0.056 trainign loss: 7.6283 avg training loss: 7.0482
batch: [4530/21305] batch time: 0.056 trainign loss: 7.3446 avg training loss: 7.0482
batch: [4540/21305] batch time: 0.056 trainign loss: 6.3774 avg training loss: 7.0482
batch: [4550/21305] batch time: 0.056 trainign loss: 6.1315 avg training loss: 7.0481
batch: [4560/21305] batch time: 0.062 trainign loss: 6.9076 avg training loss: 7.0481
batch: [4570/21305] batch time: 0.057 trainign loss: 6.7069 avg training loss: 7.0480
batch: [4580/21305] batch time: 0.057 trainign loss: 6.7711 avg training loss: 7.0480
batch: [4590/21305] batch time: 0.050 trainign loss: 4.9296 avg training loss: 7.0479
batch: [4600/21305] batch time: 0.056 trainign loss: 6.4368 avg training loss: 7.0479
batch: [4610/21305] batch time: 0.057 trainign loss: 6.0239 avg training loss: 7.0479
batch: [4620/21305] batch time: 0.056 trainign loss: 6.1355 avg training loss: 7.0478
batch: [4630/21305] batch time: 0.056 trainign loss: 5.9702 avg training loss: 7.0477
batch: [4640/21305] batch time: 0.058 trainign loss: 5.4578 avg training loss: 7.0477
batch: [4650/21305] batch time: 0.051 trainign loss: 6.0375 avg training loss: 7.0477
batch: [4660/21305] batch time: 0.056 trainign loss: 6.9163 avg training loss: 7.0477
batch: [4670/21305] batch time: 0.051 trainign loss: 6.6509 avg training loss: 7.0476
batch: [4680/21305] batch time: 0.056 trainign loss: 7.5656 avg training loss: 7.0475
batch: [4690/21305] batch time: 0.055 trainign loss: 5.0983 avg training loss: 7.0475
batch: [4700/21305] batch time: 0.056 trainign loss: 6.0703 avg training loss: 7.0474
batch: [4710/21305] batch time: 0.058 trainign loss: 7.3067 avg training loss: 7.0474
batch: [4720/21305] batch time: 0.056 trainign loss: 6.5132 avg training loss: 7.0474
batch: [4730/21305] batch time: 0.053 trainign loss: 6.0086 avg training loss: 7.0474
batch: [4740/21305] batch time: 0.062 trainign loss: 7.2757 avg training loss: 7.0473
batch: [4750/21305] batch time: 0.061 trainign loss: 6.9521 avg training loss: 7.0473
batch: [4760/21305] batch time: 0.062 trainign loss: 4.3022 avg training loss: 7.0472
batch: [4770/21305] batch time: 0.057 trainign loss: 6.2771 avg training loss: 7.0471
batch: [4780/21305] batch time: 0.058 trainign loss: 3.0047 avg training loss: 7.0470
batch: [4790/21305] batch time: 0.056 trainign loss: 5.8986 avg training loss: 7.0470
batch: [4800/21305] batch time: 0.063 trainign loss: 7.0300 avg training loss: 7.0469
batch: [4810/21305] batch time: 0.051 trainign loss: 5.4752 avg training loss: 7.0469
batch: [4820/21305] batch time: 0.056 trainign loss: 6.9520 avg training loss: 7.0469
batch: [4830/21305] batch time: 0.053 trainign loss: 5.9323 avg training loss: 7.0468
batch: [4840/21305] batch time: 0.055 trainign loss: 0.8869 avg training loss: 7.0467
batch: [4850/21305] batch time: 0.056 trainign loss: 9.9921 avg training loss: 7.0465
batch: [4860/21305] batch time: 0.056 trainign loss: 6.0779 avg training loss: 7.0466
batch: [4870/21305] batch time: 0.056 trainign loss: 8.3084 avg training loss: 7.0466
batch: [4880/21305] batch time: 0.056 trainign loss: 7.5260 avg training loss: 7.0466
batch: [4890/21305] batch time: 0.056 trainign loss: 6.4876 avg training loss: 7.0466
batch: [4900/21305] batch time: 0.056 trainign loss: 6.0381 avg training loss: 7.0465
batch: [4910/21305] batch time: 0.052 trainign loss: 4.6628 avg training loss: 7.0464
batch: [4920/21305] batch time: 0.057 trainign loss: 7.8804 avg training loss: 7.0464
batch: [4930/21305] batch time: 0.056 trainign loss: 5.4073 avg training loss: 7.0464
batch: [4940/21305] batch time: 0.059 trainign loss: 7.3261 avg training loss: 7.0463
batch: [4950/21305] batch time: 0.052 trainign loss: 5.4135 avg training loss: 7.0463
batch: [4960/21305] batch time: 0.063 trainign loss: 6.3964 avg training loss: 7.0462
batch: [4970/21305] batch time: 0.054 trainign loss: 6.4122 avg training loss: 7.0461
batch: [4980/21305] batch time: 0.056 trainign loss: 5.1454 avg training loss: 7.0461
batch: [4990/21305] batch time: 0.051 trainign loss: 0.0132 avg training loss: 7.0458
batch: [5000/21305] batch time: 0.059 trainign loss: 7.0597 avg training loss: 7.0458
batch: [5010/21305] batch time: 0.052 trainign loss: 7.8460 avg training loss: 7.0458
batch: [5020/21305] batch time: 0.062 trainign loss: 6.5328 avg training loss: 7.0458
batch: [5030/21305] batch time: 0.063 trainign loss: 6.0899 avg training loss: 7.0457
batch: [5040/21305] batch time: 0.054 trainign loss: 3.9263 avg training loss: 7.0457
batch: [5050/21305] batch time: 0.051 trainign loss: 6.1625 avg training loss: 7.0456
batch: [5060/21305] batch time: 0.052 trainign loss: 7.1192 avg training loss: 7.0455
batch: [5070/21305] batch time: 0.061 trainign loss: 4.5453 avg training loss: 7.0455
batch: [5080/21305] batch time: 0.057 trainign loss: 0.7258 avg training loss: 7.0452
batch: [5090/21305] batch time: 0.056 trainign loss: 6.5281 avg training loss: 7.0452
batch: [5100/21305] batch time: 0.056 trainign loss: 6.8028 avg training loss: 7.0452
batch: [5110/21305] batch time: 0.060 trainign loss: 6.0273 avg training loss: 7.0452
batch: [5120/21305] batch time: 0.057 trainign loss: 6.4045 avg training loss: 7.0452
batch: [5130/21305] batch time: 0.062 trainign loss: 6.5735 avg training loss: 7.0451
batch: [5140/21305] batch time: 0.056 trainign loss: 6.4542 avg training loss: 7.0450
batch: [5150/21305] batch time: 0.053 trainign loss: 7.2792 avg training loss: 7.0450
batch: [5160/21305] batch time: 0.056 trainign loss: 7.1216 avg training loss: 7.0450
batch: [5170/21305] batch time: 0.053 trainign loss: 7.0343 avg training loss: 7.0450
batch: [5180/21305] batch time: 0.062 trainign loss: 6.3464 avg training loss: 7.0450
batch: [5190/21305] batch time: 0.053 trainign loss: 6.2325 avg training loss: 7.0449
batch: [5200/21305] batch time: 0.063 trainign loss: 5.6207 avg training loss: 7.0449
batch: [5210/21305] batch time: 0.055 trainign loss: 5.6231 avg training loss: 7.0448
batch: [5220/21305] batch time: 0.056 trainign loss: 5.9791 avg training loss: 7.0448
batch: [5230/21305] batch time: 0.063 trainign loss: 6.1261 avg training loss: 7.0447
batch: [5240/21305] batch time: 0.056 trainign loss: 4.8139 avg training loss: 7.0447
batch: [5250/21305] batch time: 0.061 trainign loss: 4.8362 avg training loss: 7.0446
batch: [5260/21305] batch time: 0.056 trainign loss: 6.9914 avg training loss: 7.0446
batch: [5270/21305] batch time: 0.051 trainign loss: 7.2355 avg training loss: 7.0446
batch: [5280/21305] batch time: 0.053 trainign loss: 7.1319 avg training loss: 7.0446
batch: [5290/21305] batch time: 0.440 trainign loss: 5.7229 avg training loss: 7.0445
batch: [5300/21305] batch time: 0.059 trainign loss: 6.6847 avg training loss: 7.0445
batch: [5310/21305] batch time: 0.486 trainign loss: 6.6862 avg training loss: 7.0444
batch: [5320/21305] batch time: 0.057 trainign loss: 7.0547 avg training loss: 7.0444
batch: [5330/21305] batch time: 0.789 trainign loss: 6.5045 avg training loss: 7.0444
batch: [5340/21305] batch time: 0.058 trainign loss: 6.1149 avg training loss: 7.0443
batch: [5350/21305] batch time: 0.739 trainign loss: 5.8278 avg training loss: 7.0442
batch: [5360/21305] batch time: 0.056 trainign loss: 7.5761 avg training loss: 7.0442
batch: [5370/21305] batch time: 0.539 trainign loss: 7.2573 avg training loss: 7.0442
batch: [5380/21305] batch time: 0.056 trainign loss: 6.7165 avg training loss: 7.0441
batch: [5390/21305] batch time: 0.463 trainign loss: 4.7803 avg training loss: 7.0441
batch: [5400/21305] batch time: 0.056 trainign loss: 6.8049 avg training loss: 7.0441
batch: [5410/21305] batch time: 0.584 trainign loss: 6.7552 avg training loss: 7.0440
batch: [5420/21305] batch time: 0.062 trainign loss: 7.1336 avg training loss: 7.0440
batch: [5430/21305] batch time: 0.363 trainign loss: 6.0563 avg training loss: 7.0440
batch: [5440/21305] batch time: 0.057 trainign loss: 5.9235 avg training loss: 7.0439
batch: [5450/21305] batch time: 1.007 trainign loss: 5.2445 avg training loss: 7.0438
batch: [5460/21305] batch time: 0.056 trainign loss: 7.1534 avg training loss: 7.0437
batch: [5470/21305] batch time: 1.042 trainign loss: 5.6829 avg training loss: 7.0437
batch: [5480/21305] batch time: 0.056 trainign loss: 7.1708 avg training loss: 7.0437
batch: [5490/21305] batch time: 1.244 trainign loss: 5.5511 avg training loss: 7.0436
batch: [5500/21305] batch time: 0.063 trainign loss: 5.5419 avg training loss: 7.0436
batch: [5510/21305] batch time: 0.827 trainign loss: 1.9293 avg training loss: 7.0435
batch: [5520/21305] batch time: 0.056 trainign loss: 7.1925 avg training loss: 7.0434
batch: [5530/21305] batch time: 0.456 trainign loss: 7.0233 avg training loss: 7.0434
batch: [5540/21305] batch time: 0.056 trainign loss: 6.5197 avg training loss: 7.0433
batch: [5550/21305] batch time: 0.377 trainign loss: 7.5180 avg training loss: 7.0433
batch: [5560/21305] batch time: 0.056 trainign loss: 4.9145 avg training loss: 7.0432
batch: [5570/21305] batch time: 0.059 trainign loss: 2.9852 avg training loss: 7.0432
batch: [5580/21305] batch time: 0.057 trainign loss: 8.1076 avg training loss: 7.0431
batch: [5590/21305] batch time: 0.406 trainign loss: 6.8837 avg training loss: 7.0431
batch: [5600/21305] batch time: 0.054 trainign loss: 6.5893 avg training loss: 7.0431
batch: [5610/21305] batch time: 0.200 trainign loss: 7.0314 avg training loss: 7.0430
batch: [5620/21305] batch time: 0.056 trainign loss: 6.8192 avg training loss: 7.0430
batch: [5630/21305] batch time: 0.060 trainign loss: 6.4447 avg training loss: 7.0430
batch: [5640/21305] batch time: 0.056 trainign loss: 5.7978 avg training loss: 7.0429
batch: [5650/21305] batch time: 0.224 trainign loss: 7.5106 avg training loss: 7.0429
batch: [5660/21305] batch time: 0.053 trainign loss: 4.6923 avg training loss: 7.0428
batch: [5670/21305] batch time: 0.062 trainign loss: 6.7174 avg training loss: 7.0428
batch: [5680/21305] batch time: 0.055 trainign loss: 5.8888 avg training loss: 7.0428
batch: [5690/21305] batch time: 1.146 trainign loss: 6.1696 avg training loss: 7.0428
batch: [5700/21305] batch time: 0.051 trainign loss: 5.8701 avg training loss: 7.0427
batch: [5710/21305] batch time: 1.080 trainign loss: 4.6840 avg training loss: 7.0426
batch: [5720/21305] batch time: 0.055 trainign loss: 6.1691 avg training loss: 7.0425
batch: [5730/21305] batch time: 0.182 trainign loss: 7.1826 avg training loss: 7.0425
batch: [5740/21305] batch time: 0.051 trainign loss: 7.0721 avg training loss: 7.0425
batch: [5750/21305] batch time: 0.056 trainign loss: 6.4164 avg training loss: 7.0425
batch: [5760/21305] batch time: 0.054 trainign loss: 5.3373 avg training loss: 7.0425
batch: [5770/21305] batch time: 0.056 trainign loss: 6.4814 avg training loss: 7.0424
batch: [5780/21305] batch time: 0.056 trainign loss: 6.1007 avg training loss: 7.0424
batch: [5790/21305] batch time: 0.062 trainign loss: 7.3240 avg training loss: 7.0423
batch: [5800/21305] batch time: 0.058 trainign loss: 5.5512 avg training loss: 7.0423
batch: [5810/21305] batch time: 0.060 trainign loss: 7.4969 avg training loss: 7.0423
batch: [5820/21305] batch time: 0.056 trainign loss: 7.2797 avg training loss: 7.0423
batch: [5830/21305] batch time: 0.056 trainign loss: 6.1397 avg training loss: 7.0422
batch: [5840/21305] batch time: 0.055 trainign loss: 6.4017 avg training loss: 7.0422
batch: [5850/21305] batch time: 0.057 trainign loss: 6.3164 avg training loss: 7.0422
batch: [5860/21305] batch time: 0.053 trainign loss: 7.2181 avg training loss: 7.0422
batch: [5870/21305] batch time: 0.056 trainign loss: 6.5878 avg training loss: 7.0422
batch: [5880/21305] batch time: 0.051 trainign loss: 5.8723 avg training loss: 7.0421
batch: [5890/21305] batch time: 0.056 trainign loss: 6.2550 avg training loss: 7.0421
batch: [5900/21305] batch time: 0.056 trainign loss: 4.9743 avg training loss: 7.0420
batch: [5910/21305] batch time: 0.051 trainign loss: 5.1390 avg training loss: 7.0419
batch: [5920/21305] batch time: 0.345 trainign loss: 4.8956 avg training loss: 7.0418
batch: [5930/21305] batch time: 0.059 trainign loss: 6.3667 avg training loss: 7.0418
batch: [5940/21305] batch time: 0.550 trainign loss: 5.7006 avg training loss: 7.0418
batch: [5950/21305] batch time: 0.056 trainign loss: 5.2885 avg training loss: 7.0417
batch: [5960/21305] batch time: 0.054 trainign loss: 4.6603 avg training loss: 7.0416
batch: [5970/21305] batch time: 0.056 trainign loss: 7.7459 avg training loss: 7.0416
batch: [5980/21305] batch time: 0.054 trainign loss: 6.1158 avg training loss: 7.0416
batch: [5990/21305] batch time: 0.058 trainign loss: 7.1994 avg training loss: 7.0416
batch: [6000/21305] batch time: 0.056 trainign loss: 6.6419 avg training loss: 7.0416
batch: [6010/21305] batch time: 0.063 trainign loss: 5.4988 avg training loss: 7.0416
batch: [6020/21305] batch time: 0.051 trainign loss: 6.5617 avg training loss: 7.0415
batch: [6030/21305] batch time: 0.056 trainign loss: 5.6155 avg training loss: 7.0414
batch: [6040/21305] batch time: 0.052 trainign loss: 2.0268 avg training loss: 7.0413
batch: [6050/21305] batch time: 0.056 trainign loss: 6.8063 avg training loss: 7.0412
batch: [6060/21305] batch time: 0.056 trainign loss: 7.2950 avg training loss: 7.0412
batch: [6070/21305] batch time: 0.057 trainign loss: 3.5066 avg training loss: 7.0411
batch: [6080/21305] batch time: 0.056 trainign loss: 4.2039 avg training loss: 7.0410
batch: [6090/21305] batch time: 0.056 trainign loss: 6.9440 avg training loss: 7.0410
batch: [6100/21305] batch time: 0.053 trainign loss: 7.7546 avg training loss: 7.0410
batch: [6110/21305] batch time: 0.058 trainign loss: 5.0794 avg training loss: 7.0409
batch: [6120/21305] batch time: 0.056 trainign loss: 6.3710 avg training loss: 7.0409
batch: [6130/21305] batch time: 0.057 trainign loss: 6.7954 avg training loss: 7.0408
batch: [6140/21305] batch time: 0.055 trainign loss: 5.7598 avg training loss: 7.0407
batch: [6150/21305] batch time: 0.058 trainign loss: 6.1485 avg training loss: 7.0407
batch: [6160/21305] batch time: 0.059 trainign loss: 5.7455 avg training loss: 7.0407
batch: [6170/21305] batch time: 0.056 trainign loss: 4.2485 avg training loss: 7.0406
batch: [6180/21305] batch time: 0.056 trainign loss: 6.9133 avg training loss: 7.0405
batch: [6190/21305] batch time: 0.055 trainign loss: 7.4052 avg training loss: 7.0405
batch: [6200/21305] batch time: 0.051 trainign loss: 6.3591 avg training loss: 7.0405
batch: [6210/21305] batch time: 0.056 trainign loss: 6.1249 avg training loss: 7.0405
batch: [6220/21305] batch time: 0.056 trainign loss: 6.8799 avg training loss: 7.0404
batch: [6230/21305] batch time: 0.056 trainign loss: 6.5735 avg training loss: 7.0404
batch: [6240/21305] batch time: 0.051 trainign loss: 7.1344 avg training loss: 7.0404
batch: [6250/21305] batch time: 0.056 trainign loss: 6.5015 avg training loss: 7.0404
batch: [6260/21305] batch time: 0.050 trainign loss: 7.5297 avg training loss: 7.0403
batch: [6270/21305] batch time: 0.062 trainign loss: 6.6767 avg training loss: 7.0403
batch: [6280/21305] batch time: 0.051 trainign loss: 6.6393 avg training loss: 7.0403
batch: [6290/21305] batch time: 0.058 trainign loss: 3.7072 avg training loss: 7.0402
batch: [6300/21305] batch time: 0.965 trainign loss: 7.3412 avg training loss: 7.0402
batch: [6310/21305] batch time: 0.056 trainign loss: 6.6005 avg training loss: 7.0402
batch: [6320/21305] batch time: 0.477 trainign loss: 6.7846 avg training loss: 7.0401
batch: [6330/21305] batch time: 0.056 trainign loss: 6.4265 avg training loss: 7.0401
batch: [6340/21305] batch time: 0.431 trainign loss: 3.8061 avg training loss: 7.0400
batch: [6350/21305] batch time: 0.056 trainign loss: 6.6257 avg training loss: 7.0399
batch: [6360/21305] batch time: 0.122 trainign loss: 7.3861 avg training loss: 7.0399
batch: [6370/21305] batch time: 0.057 trainign loss: 5.9667 avg training loss: 7.0399
batch: [6380/21305] batch time: 0.057 trainign loss: 6.4286 avg training loss: 7.0399
batch: [6390/21305] batch time: 0.057 trainign loss: 4.5500 avg training loss: 7.0398
batch: [6400/21305] batch time: 0.440 trainign loss: 6.7393 avg training loss: 7.0398
batch: [6410/21305] batch time: 0.061 trainign loss: 7.3871 avg training loss: 7.0397
batch: [6420/21305] batch time: 0.527 trainign loss: 6.2941 avg training loss: 7.0397
batch: [6430/21305] batch time: 0.062 trainign loss: 4.5535 avg training loss: 7.0396
batch: [6440/21305] batch time: 0.694 trainign loss: 6.2874 avg training loss: 7.0395
batch: [6450/21305] batch time: 0.062 trainign loss: 6.7869 avg training loss: 7.0394
batch: [6460/21305] batch time: 0.533 trainign loss: 6.4640 avg training loss: 7.0394
batch: [6470/21305] batch time: 0.056 trainign loss: 7.2708 avg training loss: 7.0394
batch: [6480/21305] batch time: 1.357 trainign loss: 6.4418 avg training loss: 7.0393
batch: [6490/21305] batch time: 0.056 trainign loss: 5.8560 avg training loss: 7.0393
batch: [6500/21305] batch time: 1.195 trainign loss: 5.1708 avg training loss: 7.0393
batch: [6510/21305] batch time: 0.057 trainign loss: 6.2266 avg training loss: 7.0392
batch: [6520/21305] batch time: 1.943 trainign loss: 6.0757 avg training loss: 7.0392
batch: [6530/21305] batch time: 0.056 trainign loss: 4.9381 avg training loss: 7.0391
batch: [6540/21305] batch time: 2.082 trainign loss: 7.3557 avg training loss: 7.0390
batch: [6550/21305] batch time: 0.062 trainign loss: 5.4120 avg training loss: 7.0390
batch: [6560/21305] batch time: 2.094 trainign loss: 6.3383 avg training loss: 7.0389
batch: [6570/21305] batch time: 0.063 trainign loss: 5.8993 avg training loss: 7.0389
batch: [6580/21305] batch time: 1.933 trainign loss: 4.0876 avg training loss: 7.0388
batch: [6590/21305] batch time: 0.063 trainign loss: 6.1316 avg training loss: 7.0388
batch: [6600/21305] batch time: 2.822 trainign loss: 7.0724 avg training loss: 7.0388
batch: [6610/21305] batch time: 0.054 trainign loss: 7.2769 avg training loss: 7.0387
batch: [6620/21305] batch time: 2.489 trainign loss: 6.4542 avg training loss: 7.0387
batch: [6630/21305] batch time: 0.056 trainign loss: 6.1991 avg training loss: 7.0386
batch: [6640/21305] batch time: 1.838 trainign loss: 4.6315 avg training loss: 7.0386
batch: [6650/21305] batch time: 0.057 trainign loss: 5.8457 avg training loss: 7.0385
batch: [6660/21305] batch time: 0.960 trainign loss: 5.7674 avg training loss: 7.0385
batch: [6670/21305] batch time: 0.062 trainign loss: 5.8855 avg training loss: 7.0384
batch: [6680/21305] batch time: 0.630 trainign loss: 7.1015 avg training loss: 7.0384
batch: [6690/21305] batch time: 0.056 trainign loss: 6.8404 avg training loss: 7.0384
batch: [6700/21305] batch time: 1.268 trainign loss: 4.9707 avg training loss: 7.0383
batch: [6710/21305] batch time: 0.329 trainign loss: 5.1440 avg training loss: 7.0383
batch: [6720/21305] batch time: 1.310 trainign loss: 6.7604 avg training loss: 7.0382
batch: [6730/21305] batch time: 0.062 trainign loss: 5.6171 avg training loss: 7.0382
batch: [6740/21305] batch time: 1.528 trainign loss: 7.4045 avg training loss: 7.0382
batch: [6750/21305] batch time: 0.187 trainign loss: 7.1795 avg training loss: 7.0382
batch: [6760/21305] batch time: 0.487 trainign loss: 6.5107 avg training loss: 7.0381
batch: [6770/21305] batch time: 0.055 trainign loss: 5.2483 avg training loss: 7.0381
batch: [6780/21305] batch time: 0.946 trainign loss: 5.5240 avg training loss: 7.0380
batch: [6790/21305] batch time: 0.056 trainign loss: 6.8312 avg training loss: 7.0380
batch: [6800/21305] batch time: 0.076 trainign loss: 5.5221 avg training loss: 7.0379
batch: [6810/21305] batch time: 0.055 trainign loss: 6.0718 avg training loss: 7.0379
batch: [6820/21305] batch time: 0.052 trainign loss: 5.1522 avg training loss: 7.0378
batch: [6830/21305] batch time: 0.057 trainign loss: 6.3551 avg training loss: 7.0378
batch: [6840/21305] batch time: 0.059 trainign loss: 7.8181 avg training loss: 7.0376
batch: [6850/21305] batch time: 0.058 trainign loss: 5.6919 avg training loss: 7.0376
batch: [6860/21305] batch time: 0.053 trainign loss: 5.8401 avg training loss: 7.0376
batch: [6870/21305] batch time: 0.056 trainign loss: 6.1929 avg training loss: 7.0375
batch: [6880/21305] batch time: 0.056 trainign loss: 5.7676 avg training loss: 7.0374
batch: [6890/21305] batch time: 0.056 trainign loss: 7.0373 avg training loss: 7.0374
batch: [6900/21305] batch time: 0.056 trainign loss: 6.6706 avg training loss: 7.0373
batch: [6910/21305] batch time: 0.059 trainign loss: 5.7428 avg training loss: 7.0373
batch: [6920/21305] batch time: 0.051 trainign loss: 6.7934 avg training loss: 7.0373
batch: [6930/21305] batch time: 0.057 trainign loss: 5.4362 avg training loss: 7.0372
batch: [6940/21305] batch time: 0.053 trainign loss: 4.9530 avg training loss: 7.0371
batch: [6950/21305] batch time: 0.063 trainign loss: 7.2566 avg training loss: 7.0371
batch: [6960/21305] batch time: 0.056 trainign loss: 4.6023 avg training loss: 7.0370
batch: [6970/21305] batch time: 0.056 trainign loss: 3.5891 avg training loss: 7.0370
batch: [6980/21305] batch time: 0.052 trainign loss: 6.7537 avg training loss: 7.0370
batch: [6990/21305] batch time: 0.058 trainign loss: 7.2160 avg training loss: 7.0370
batch: [7000/21305] batch time: 0.055 trainign loss: 5.8285 avg training loss: 7.0370
batch: [7010/21305] batch time: 0.062 trainign loss: 6.3365 avg training loss: 7.0369
batch: [7020/21305] batch time: 0.051 trainign loss: 7.7375 avg training loss: 7.0369
batch: [7030/21305] batch time: 0.056 trainign loss: 6.3990 avg training loss: 7.0369
batch: [7040/21305] batch time: 0.057 trainign loss: 6.9358 avg training loss: 7.0369
batch: [7050/21305] batch time: 0.056 trainign loss: 5.5452 avg training loss: 7.0368
batch: [7060/21305] batch time: 0.061 trainign loss: 5.5553 avg training loss: 7.0368
batch: [7070/21305] batch time: 0.062 trainign loss: 6.9966 avg training loss: 7.0367
batch: [7080/21305] batch time: 0.053 trainign loss: 6.1418 avg training loss: 7.0367
batch: [7090/21305] batch time: 0.056 trainign loss: 5.7162 avg training loss: 7.0367
batch: [7100/21305] batch time: 0.062 trainign loss: 6.0279 avg training loss: 7.0366
batch: [7110/21305] batch time: 0.062 trainign loss: 6.1078 avg training loss: 7.0365
batch: [7120/21305] batch time: 0.057 trainign loss: 4.0566 avg training loss: 7.0364
batch: [7130/21305] batch time: 0.059 trainign loss: 7.1298 avg training loss: 7.0364
batch: [7140/21305] batch time: 0.054 trainign loss: 7.2243 avg training loss: 7.0363
batch: [7150/21305] batch time: 0.058 trainign loss: 5.2138 avg training loss: 7.0363
batch: [7160/21305] batch time: 0.062 trainign loss: 7.6289 avg training loss: 7.0362
batch: [7170/21305] batch time: 0.052 trainign loss: 5.7743 avg training loss: 7.0362
batch: [7180/21305] batch time: 0.054 trainign loss: 6.4639 avg training loss: 7.0362
batch: [7190/21305] batch time: 0.051 trainign loss: 4.8471 avg training loss: 7.0361
batch: [7200/21305] batch time: 0.056 trainign loss: 5.2450 avg training loss: 7.0361
batch: [7210/21305] batch time: 0.056 trainign loss: 6.1417 avg training loss: 7.0360
batch: [7220/21305] batch time: 0.056 trainign loss: 4.9541 avg training loss: 7.0360
batch: [7230/21305] batch time: 0.062 trainign loss: 4.5411 avg training loss: 7.0359
batch: [7240/21305] batch time: 0.051 trainign loss: 7.0406 avg training loss: 7.0358
batch: [7250/21305] batch time: 0.062 trainign loss: 5.7676 avg training loss: 7.0358
batch: [7260/21305] batch time: 0.054 trainign loss: 5.1113 avg training loss: 7.0358
batch: [7270/21305] batch time: 0.061 trainign loss: 7.0293 avg training loss: 7.0358
batch: [7280/21305] batch time: 0.062 trainign loss: 5.9741 avg training loss: 7.0357
batch: [7290/21305] batch time: 0.056 trainign loss: 4.9244 avg training loss: 7.0357
batch: [7300/21305] batch time: 0.061 trainign loss: 6.8497 avg training loss: 7.0356
batch: [7310/21305] batch time: 0.369 trainign loss: 6.8495 avg training loss: 7.0356
batch: [7320/21305] batch time: 0.061 trainign loss: 6.9842 avg training loss: 7.0356
batch: [7330/21305] batch time: 0.056 trainign loss: 5.2917 avg training loss: 7.0356
batch: [7340/21305] batch time: 0.050 trainign loss: 6.9846 avg training loss: 7.0355
batch: [7350/21305] batch time: 0.368 trainign loss: 6.8322 avg training loss: 7.0355
batch: [7360/21305] batch time: 0.062 trainign loss: 6.8581 avg training loss: 7.0354
batch: [7370/21305] batch time: 0.488 trainign loss: 2.4903 avg training loss: 7.0353
batch: [7380/21305] batch time: 0.056 trainign loss: 6.1286 avg training loss: 7.0353
batch: [7390/21305] batch time: 0.060 trainign loss: 7.1012 avg training loss: 7.0352
batch: [7400/21305] batch time: 0.063 trainign loss: 4.8136 avg training loss: 7.0352
batch: [7410/21305] batch time: 0.959 trainign loss: 6.1701 avg training loss: 7.0350
batch: [7420/21305] batch time: 0.056 trainign loss: 6.8880 avg training loss: 7.0350
batch: [7430/21305] batch time: 1.042 trainign loss: 6.4362 avg training loss: 7.0350
batch: [7440/21305] batch time: 0.058 trainign loss: 5.9822 avg training loss: 7.0350
batch: [7450/21305] batch time: 0.877 trainign loss: 6.4902 avg training loss: 7.0349
batch: [7460/21305] batch time: 0.056 trainign loss: 3.0661 avg training loss: 7.0348
batch: [7470/21305] batch time: 0.836 trainign loss: 4.2426 avg training loss: 7.0348
batch: [7480/21305] batch time: 0.057 trainign loss: 6.5403 avg training loss: 7.0347
batch: [7490/21305] batch time: 1.624 trainign loss: 6.0460 avg training loss: 7.0347
batch: [7500/21305] batch time: 0.057 trainign loss: 7.1743 avg training loss: 7.0347
batch: [7510/21305] batch time: 1.010 trainign loss: 6.0292 avg training loss: 7.0346
batch: [7520/21305] batch time: 0.056 trainign loss: 6.9376 avg training loss: 7.0346
batch: [7530/21305] batch time: 1.091 trainign loss: 5.2841 avg training loss: 7.0346
batch: [7540/21305] batch time: 0.062 trainign loss: 7.0715 avg training loss: 7.0345
batch: [7550/21305] batch time: 1.519 trainign loss: 7.2911 avg training loss: 7.0345
batch: [7560/21305] batch time: 0.062 trainign loss: 6.5936 avg training loss: 7.0345
batch: [7570/21305] batch time: 1.896 trainign loss: 6.7921 avg training loss: 7.0344
batch: [7580/21305] batch time: 0.062 trainign loss: 6.0149 avg training loss: 7.0344
batch: [7590/21305] batch time: 2.320 trainign loss: 7.2477 avg training loss: 7.0344
batch: [7600/21305] batch time: 0.056 trainign loss: 5.8528 avg training loss: 7.0343
batch: [7610/21305] batch time: 2.368 trainign loss: 5.9748 avg training loss: 7.0343
batch: [7620/21305] batch time: 0.052 trainign loss: 5.0132 avg training loss: 7.0342
batch: [7630/21305] batch time: 2.382 trainign loss: 7.4026 avg training loss: 7.0341
batch: [7640/21305] batch time: 0.057 trainign loss: 6.5813 avg training loss: 7.0341
batch: [7650/21305] batch time: 2.932 trainign loss: 5.3048 avg training loss: 7.0341
batch: [7660/21305] batch time: 0.056 trainign loss: 6.7174 avg training loss: 7.0340
batch: [7670/21305] batch time: 2.378 trainign loss: 7.1198 avg training loss: 7.0340
batch: [7680/21305] batch time: 0.056 trainign loss: 5.3757 avg training loss: 7.0339
batch: [7690/21305] batch time: 2.038 trainign loss: 4.9144 avg training loss: 7.0339
batch: [7700/21305] batch time: 0.406 trainign loss: 6.9989 avg training loss: 7.0338
batch: [7710/21305] batch time: 1.527 trainign loss: 5.3503 avg training loss: 7.0337
batch: [7720/21305] batch time: 1.766 trainign loss: 5.8427 avg training loss: 7.0337
batch: [7730/21305] batch time: 0.244 trainign loss: 4.3439 avg training loss: 7.0335
batch: [7740/21305] batch time: 2.311 trainign loss: 7.2107 avg training loss: 7.0334
batch: [7750/21305] batch time: 0.057 trainign loss: 7.7464 avg training loss: 7.0335
batch: [7760/21305] batch time: 2.503 trainign loss: 6.7382 avg training loss: 7.0335
batch: [7770/21305] batch time: 0.055 trainign loss: 4.3527 avg training loss: 7.0334
batch: [7780/21305] batch time: 1.793 trainign loss: 6.7929 avg training loss: 7.0334
batch: [7790/21305] batch time: 0.063 trainign loss: 6.7388 avg training loss: 7.0333
batch: [7800/21305] batch time: 2.290 trainign loss: 6.9698 avg training loss: 7.0333
batch: [7810/21305] batch time: 0.062 trainign loss: 6.2242 avg training loss: 7.0332
batch: [7820/21305] batch time: 2.651 trainign loss: 6.7373 avg training loss: 7.0332
batch: [7830/21305] batch time: 0.056 trainign loss: 5.7822 avg training loss: 7.0332
batch: [7840/21305] batch time: 2.273 trainign loss: 6.9330 avg training loss: 7.0332
batch: [7850/21305] batch time: 0.056 trainign loss: 5.0220 avg training loss: 7.0331
batch: [7860/21305] batch time: 2.277 trainign loss: 5.3861 avg training loss: 7.0331
batch: [7870/21305] batch time: 0.163 trainign loss: 6.3743 avg training loss: 7.0331
batch: [7880/21305] batch time: 1.577 trainign loss: 4.8467 avg training loss: 7.0330
batch: [7890/21305] batch time: 1.253 trainign loss: 5.6508 avg training loss: 7.0330
batch: [7900/21305] batch time: 1.008 trainign loss: 5.3157 avg training loss: 7.0328
batch: [7910/21305] batch time: 1.918 trainign loss: 7.2300 avg training loss: 7.0328
batch: [7920/21305] batch time: 0.285 trainign loss: 5.7339 avg training loss: 7.0328
batch: [7930/21305] batch time: 2.264 trainign loss: 5.8977 avg training loss: 7.0328
batch: [7940/21305] batch time: 0.061 trainign loss: 6.8740 avg training loss: 7.0328
batch: [7950/21305] batch time: 1.529 trainign loss: 6.9108 avg training loss: 7.0327
batch: [7960/21305] batch time: 0.092 trainign loss: 5.8212 avg training loss: 7.0327
batch: [7970/21305] batch time: 2.360 trainign loss: 6.1207 avg training loss: 7.0327
batch: [7980/21305] batch time: 0.057 trainign loss: 7.1341 avg training loss: 7.0326
batch: [7990/21305] batch time: 2.118 trainign loss: 7.2145 avg training loss: 7.0326
batch: [8000/21305] batch time: 0.056 trainign loss: 6.6015 avg training loss: 7.0326
batch: [8010/21305] batch time: 2.280 trainign loss: 5.2277 avg training loss: 7.0325
batch: [8020/21305] batch time: 0.058 trainign loss: 2.3713 avg training loss: 7.0324
batch: [8030/21305] batch time: 2.201 trainign loss: 5.1417 avg training loss: 7.0322
batch: [8040/21305] batch time: 0.056 trainign loss: 7.3148 avg training loss: 7.0322
batch: [8050/21305] batch time: 2.264 trainign loss: 8.1406 avg training loss: 7.0322
batch: [8060/21305] batch time: 0.056 trainign loss: 7.2931 avg training loss: 7.0323
batch: [8070/21305] batch time: 2.190 trainign loss: 6.0736 avg training loss: 7.0323
batch: [8080/21305] batch time: 0.056 trainign loss: 6.8519 avg training loss: 7.0322
batch: [8090/21305] batch time: 2.348 trainign loss: 6.9960 avg training loss: 7.0322
batch: [8100/21305] batch time: 0.056 trainign loss: 6.2641 avg training loss: 7.0322
batch: [8110/21305] batch time: 2.366 trainign loss: 7.2023 avg training loss: 7.0321
batch: [8120/21305] batch time: 0.060 trainign loss: 6.1646 avg training loss: 7.0321
batch: [8130/21305] batch time: 2.027 trainign loss: 5.9590 avg training loss: 7.0320
batch: [8140/21305] batch time: 0.052 trainign loss: 6.1178 avg training loss: 7.0320
batch: [8150/21305] batch time: 1.876 trainign loss: 6.0250 avg training loss: 7.0320
batch: [8160/21305] batch time: 0.054 trainign loss: 6.4853 avg training loss: 7.0319
batch: [8170/21305] batch time: 1.860 trainign loss: 6.4669 avg training loss: 7.0319
batch: [8180/21305] batch time: 0.062 trainign loss: 5.7979 avg training loss: 7.0318
batch: [8190/21305] batch time: 2.420 trainign loss: 6.8020 avg training loss: 7.0318
batch: [8200/21305] batch time: 0.063 trainign loss: 6.7596 avg training loss: 7.0318
batch: [8210/21305] batch time: 2.163 trainign loss: 6.0631 avg training loss: 7.0318
batch: [8220/21305] batch time: 0.418 trainign loss: 6.3010 avg training loss: 7.0318
batch: [8230/21305] batch time: 1.788 trainign loss: 6.8315 avg training loss: 7.0317
batch: [8240/21305] batch time: 0.637 trainign loss: 7.4324 avg training loss: 7.0317
batch: [8250/21305] batch time: 1.595 trainign loss: 6.3419 avg training loss: 7.0317
batch: [8260/21305] batch time: 0.776 trainign loss: 6.5304 avg training loss: 7.0316
batch: [8270/21305] batch time: 1.593 trainign loss: 7.5742 avg training loss: 7.0316
batch: [8280/21305] batch time: 1.701 trainign loss: 6.6969 avg training loss: 7.0316
batch: [8290/21305] batch time: 0.562 trainign loss: 5.9068 avg training loss: 7.0316
batch: [8300/21305] batch time: 1.834 trainign loss: 5.7147 avg training loss: 7.0315
batch: [8310/21305] batch time: 0.290 trainign loss: 7.5515 avg training loss: 7.0315
batch: [8320/21305] batch time: 2.073 trainign loss: 6.6493 avg training loss: 7.0315
batch: [8330/21305] batch time: 0.063 trainign loss: 7.0784 avg training loss: 7.0315
batch: [8340/21305] batch time: 2.137 trainign loss: 6.0832 avg training loss: 7.0314
batch: [8350/21305] batch time: 0.142 trainign loss: 4.6177 avg training loss: 7.0314
batch: [8360/21305] batch time: 2.424 trainign loss: 5.6638 avg training loss: 7.0313
batch: [8370/21305] batch time: 1.148 trainign loss: 7.3008 avg training loss: 7.0313
batch: [8380/21305] batch time: 1.234 trainign loss: 5.1131 avg training loss: 7.0312
batch: [8390/21305] batch time: 1.016 trainign loss: 4.4595 avg training loss: 7.0311
batch: [8400/21305] batch time: 1.621 trainign loss: 8.4869 avg training loss: 7.0310
batch: [8410/21305] batch time: 1.564 trainign loss: 6.2180 avg training loss: 7.0309
batch: [8420/21305] batch time: 0.839 trainign loss: 6.5785 avg training loss: 7.0308
batch: [8430/21305] batch time: 2.006 trainign loss: 8.0150 avg training loss: 7.0308
batch: [8440/21305] batch time: 0.056 trainign loss: 7.8055 avg training loss: 7.0308
batch: [8450/21305] batch time: 2.447 trainign loss: 7.3094 avg training loss: 7.0308
batch: [8460/21305] batch time: 0.056 trainign loss: 7.3204 avg training loss: 7.0308
batch: [8470/21305] batch time: 2.401 trainign loss: 7.5740 avg training loss: 7.0308
batch: [8480/21305] batch time: 0.056 trainign loss: 6.9364 avg training loss: 7.0308
batch: [8490/21305] batch time: 1.501 trainign loss: 6.6237 avg training loss: 7.0307
batch: [8500/21305] batch time: 0.463 trainign loss: 5.4346 avg training loss: 7.0307
batch: [8510/21305] batch time: 1.667 trainign loss: 5.5878 avg training loss: 7.0306
batch: [8520/21305] batch time: 0.846 trainign loss: 6.6329 avg training loss: 7.0305
batch: [8530/21305] batch time: 1.646 trainign loss: 6.5478 avg training loss: 7.0305
batch: [8540/21305] batch time: 1.927 trainign loss: 6.6421 avg training loss: 7.0305
batch: [8550/21305] batch time: 0.062 trainign loss: 7.2670 avg training loss: 7.0304
batch: [8560/21305] batch time: 2.285 trainign loss: 6.4949 avg training loss: 7.0304
batch: [8570/21305] batch time: 0.059 trainign loss: 5.7948 avg training loss: 7.0303
batch: [8580/21305] batch time: 2.083 trainign loss: 6.2078 avg training loss: 7.0303
batch: [8590/21305] batch time: 0.057 trainign loss: 6.9290 avg training loss: 7.0303
batch: [8600/21305] batch time: 2.375 trainign loss: 6.0939 avg training loss: 7.0302
batch: [8610/21305] batch time: 0.056 trainign loss: 5.5033 avg training loss: 7.0301
batch: [8620/21305] batch time: 2.419 trainign loss: 6.7655 avg training loss: 7.0301
batch: [8630/21305] batch time: 0.056 trainign loss: 5.0964 avg training loss: 7.0300
batch: [8640/21305] batch time: 2.326 trainign loss: 6.4554 avg training loss: 7.0300
batch: [8650/21305] batch time: 0.056 trainign loss: 4.4424 avg training loss: 7.0299
batch: [8660/21305] batch time: 2.020 trainign loss: 3.7826 avg training loss: 7.0299
batch: [8670/21305] batch time: 0.056 trainign loss: 5.0469 avg training loss: 7.0298
batch: [8680/21305] batch time: 2.357 trainign loss: 6.7060 avg training loss: 7.0298
batch: [8690/21305] batch time: 0.051 trainign loss: 5.4139 avg training loss: 7.0297
batch: [8700/21305] batch time: 2.330 trainign loss: 4.9260 avg training loss: 7.0296
batch: [8710/21305] batch time: 0.056 trainign loss: 1.2451 avg training loss: 7.0295
batch: [8720/21305] batch time: 2.434 trainign loss: 0.0021 avg training loss: 7.0292
batch: [8730/21305] batch time: 0.056 trainign loss: 0.0001 avg training loss: 7.0288
batch: [8740/21305] batch time: 2.410 trainign loss: 7.7116 avg training loss: 7.0288
batch: [8750/21305] batch time: 0.055 trainign loss: 7.7774 avg training loss: 7.0289
batch: [8760/21305] batch time: 1.871 trainign loss: 7.1044 avg training loss: 7.0288
batch: [8770/21305] batch time: 0.052 trainign loss: 6.5280 avg training loss: 7.0288
batch: [8780/21305] batch time: 2.605 trainign loss: 5.5543 avg training loss: 7.0287
batch: [8790/21305] batch time: 0.055 trainign loss: 6.0650 avg training loss: 7.0287
batch: [8800/21305] batch time: 2.315 trainign loss: 5.6347 avg training loss: 7.0287
batch: [8810/21305] batch time: 0.060 trainign loss: 7.2325 avg training loss: 7.0286
batch: [8820/21305] batch time: 2.243 trainign loss: 6.3016 avg training loss: 7.0286
batch: [8830/21305] batch time: 0.058 trainign loss: 6.8068 avg training loss: 7.0286
batch: [8840/21305] batch time: 2.108 trainign loss: 6.1623 avg training loss: 7.0285
batch: [8850/21305] batch time: 0.566 trainign loss: 6.0794 avg training loss: 7.0285
batch: [8860/21305] batch time: 1.658 trainign loss: 6.8402 avg training loss: 7.0285
batch: [8870/21305] batch time: 0.288 trainign loss: 6.8548 avg training loss: 7.0285
batch: [8880/21305] batch time: 1.803 trainign loss: 5.5399 avg training loss: 7.0284
batch: [8890/21305] batch time: 0.316 trainign loss: 7.1156 avg training loss: 7.0284
batch: [8900/21305] batch time: 1.796 trainign loss: 5.9363 avg training loss: 7.0284
batch: [8910/21305] batch time: 1.935 trainign loss: 5.0512 avg training loss: 7.0283
batch: [8920/21305] batch time: 0.692 trainign loss: 1.8962 avg training loss: 7.0282
batch: [8930/21305] batch time: 1.298 trainign loss: 7.6208 avg training loss: 7.0282
batch: [8940/21305] batch time: 1.505 trainign loss: 6.4945 avg training loss: 7.0281
batch: [8950/21305] batch time: 0.162 trainign loss: 6.2450 avg training loss: 7.0281
batch: [8960/21305] batch time: 1.953 trainign loss: 6.0438 avg training loss: 7.0281
batch: [8970/21305] batch time: 1.416 trainign loss: 6.3057 avg training loss: 7.0281
batch: [8980/21305] batch time: 0.187 trainign loss: 5.3936 avg training loss: 7.0280
batch: [8990/21305] batch time: 2.500 trainign loss: 6.4240 avg training loss: 7.0279
batch: [9000/21305] batch time: 0.052 trainign loss: 0.8415 avg training loss: 7.0278
batch: [9010/21305] batch time: 2.273 trainign loss: 7.3980 avg training loss: 7.0278
batch: [9020/21305] batch time: 0.055 trainign loss: 5.9866 avg training loss: 7.0277
batch: [9030/21305] batch time: 1.992 trainign loss: 5.4879 avg training loss: 7.0277
batch: [9040/21305] batch time: 0.056 trainign loss: 6.7183 avg training loss: 7.0277
batch: [9050/21305] batch time: 1.993 trainign loss: 6.5501 avg training loss: 7.0276
batch: [9060/21305] batch time: 0.056 trainign loss: 5.0829 avg training loss: 7.0276
batch: [9070/21305] batch time: 2.372 trainign loss: 3.3542 avg training loss: 7.0275
batch: [9080/21305] batch time: 0.057 trainign loss: 8.0652 avg training loss: 7.0275
batch: [9090/21305] batch time: 2.253 trainign loss: 6.5725 avg training loss: 7.0275
batch: [9100/21305] batch time: 0.057 trainign loss: 7.0285 avg training loss: 7.0275
batch: [9110/21305] batch time: 1.896 trainign loss: 5.8078 avg training loss: 7.0274
batch: [9120/21305] batch time: 0.060 trainign loss: 6.8300 avg training loss: 7.0274
batch: [9130/21305] batch time: 2.412 trainign loss: 4.8948 avg training loss: 7.0273
batch: [9140/21305] batch time: 0.056 trainign loss: 6.4281 avg training loss: 7.0272
batch: [9150/21305] batch time: 2.324 trainign loss: 6.2567 avg training loss: 7.0272
batch: [9160/21305] batch time: 0.062 trainign loss: 6.8269 avg training loss: 7.0271
batch: [9170/21305] batch time: 2.322 trainign loss: 5.4887 avg training loss: 7.0270
batch: [9180/21305] batch time: 0.056 trainign loss: 6.9023 avg training loss: 7.0270
batch: [9190/21305] batch time: 2.561 trainign loss: 6.6002 avg training loss: 7.0270
batch: [9200/21305] batch time: 0.056 trainign loss: 6.8690 avg training loss: 7.0270
batch: [9210/21305] batch time: 1.867 trainign loss: 5.9836 avg training loss: 7.0269
batch: [9220/21305] batch time: 0.062 trainign loss: 4.8854 avg training loss: 7.0269
batch: [9230/21305] batch time: 2.162 trainign loss: 6.4473 avg training loss: 7.0268
batch: [9240/21305] batch time: 0.056 trainign loss: 4.6783 avg training loss: 7.0268
batch: [9250/21305] batch time: 2.201 trainign loss: 6.6591 avg training loss: 7.0267
batch: [9260/21305] batch time: 0.052 trainign loss: 6.6414 avg training loss: 7.0267
batch: [9270/21305] batch time: 2.505 trainign loss: 6.5926 avg training loss: 7.0267
batch: [9280/21305] batch time: 0.060 trainign loss: 6.2285 avg training loss: 7.0266
batch: [9290/21305] batch time: 2.483 trainign loss: 4.4533 avg training loss: 7.0265
batch: [9300/21305] batch time: 0.058 trainign loss: 7.7423 avg training loss: 7.0264
batch: [9310/21305] batch time: 2.361 trainign loss: 7.5108 avg training loss: 7.0264
batch: [9320/21305] batch time: 0.055 trainign loss: 4.7904 avg training loss: 7.0264
batch: [9330/21305] batch time: 2.566 trainign loss: 4.6576 avg training loss: 7.0263
batch: [9340/21305] batch time: 0.056 trainign loss: 7.1263 avg training loss: 7.0263
batch: [9350/21305] batch time: 2.325 trainign loss: 6.9871 avg training loss: 7.0263
batch: [9360/21305] batch time: 0.056 trainign loss: 5.0013 avg training loss: 7.0263
batch: [9370/21305] batch time: 2.571 trainign loss: 5.2656 avg training loss: 7.0262
batch: [9380/21305] batch time: 0.055 trainign loss: 5.9338 avg training loss: 7.0262
batch: [9390/21305] batch time: 2.121 trainign loss: 6.2463 avg training loss: 7.0261
batch: [9400/21305] batch time: 0.063 trainign loss: 6.7417 avg training loss: 7.0261
batch: [9410/21305] batch time: 1.358 trainign loss: 5.8908 avg training loss: 7.0260
batch: [9420/21305] batch time: 0.056 trainign loss: 3.5545 avg training loss: 7.0260
batch: [9430/21305] batch time: 0.112 trainign loss: 6.8207 avg training loss: 7.0259
batch: [9440/21305] batch time: 0.053 trainign loss: 7.1469 avg training loss: 7.0259
batch: [9450/21305] batch time: 0.063 trainign loss: 5.6528 avg training loss: 7.0258
batch: [9460/21305] batch time: 0.056 trainign loss: 7.0528 avg training loss: 7.0258
batch: [9470/21305] batch time: 0.057 trainign loss: 7.3785 avg training loss: 7.0257
batch: [9480/21305] batch time: 0.056 trainign loss: 5.5387 avg training loss: 7.0257
batch: [9490/21305] batch time: 0.119 trainign loss: 7.3992 avg training loss: 7.0256
batch: [9500/21305] batch time: 0.056 trainign loss: 7.4185 avg training loss: 7.0256
batch: [9510/21305] batch time: 0.285 trainign loss: 5.4976 avg training loss: 7.0255
batch: [9520/21305] batch time: 0.062 trainign loss: 7.2746 avg training loss: 7.0255
batch: [9530/21305] batch time: 0.062 trainign loss: 6.8896 avg training loss: 7.0255
batch: [9540/21305] batch time: 0.054 trainign loss: 6.0986 avg training loss: 7.0255
batch: [9550/21305] batch time: 0.470 trainign loss: 5.2831 avg training loss: 7.0254
batch: [9560/21305] batch time: 0.057 trainign loss: 5.1767 avg training loss: 7.0254
batch: [9570/21305] batch time: 0.057 trainign loss: 6.9662 avg training loss: 7.0253
batch: [9580/21305] batch time: 0.052 trainign loss: 7.9983 avg training loss: 7.0252
batch: [9590/21305] batch time: 0.647 trainign loss: 7.1173 avg training loss: 7.0253
batch: [9600/21305] batch time: 0.056 trainign loss: 6.9501 avg training loss: 7.0253
batch: [9610/21305] batch time: 0.063 trainign loss: 5.9900 avg training loss: 7.0252
batch: [9620/21305] batch time: 0.063 trainign loss: 6.4584 avg training loss: 7.0251
batch: [9630/21305] batch time: 0.056 trainign loss: 7.2981 avg training loss: 7.0251
batch: [9640/21305] batch time: 0.057 trainign loss: 7.0926 avg training loss: 7.0251
batch: [9650/21305] batch time: 0.062 trainign loss: 6.2522 avg training loss: 7.0250
batch: [9660/21305] batch time: 0.056 trainign loss: 3.6343 avg training loss: 7.0250
batch: [9670/21305] batch time: 0.057 trainign loss: 7.0021 avg training loss: 7.0250
batch: [9680/21305] batch time: 0.053 trainign loss: 6.8129 avg training loss: 7.0249
batch: [9690/21305] batch time: 0.057 trainign loss: 6.6215 avg training loss: 7.0249
batch: [9700/21305] batch time: 0.058 trainign loss: 6.3635 avg training loss: 7.0249
batch: [9710/21305] batch time: 0.056 trainign loss: 6.2116 avg training loss: 7.0249
batch: [9720/21305] batch time: 0.053 trainign loss: 6.7842 avg training loss: 7.0248
batch: [9730/21305] batch time: 0.062 trainign loss: 6.6398 avg training loss: 7.0248
batch: [9740/21305] batch time: 0.062 trainign loss: 6.3457 avg training loss: 7.0248
batch: [9750/21305] batch time: 0.056 trainign loss: 6.8775 avg training loss: 7.0247
batch: [9760/21305] batch time: 0.062 trainign loss: 6.4264 avg training loss: 7.0247
batch: [9770/21305] batch time: 0.057 trainign loss: 6.8134 avg training loss: 7.0247
batch: [9780/21305] batch time: 0.058 trainign loss: 7.6092 avg training loss: 7.0247
batch: [9790/21305] batch time: 0.136 trainign loss: 6.4048 avg training loss: 7.0246
batch: [9800/21305] batch time: 0.056 trainign loss: 6.1665 avg training loss: 7.0246
batch: [9810/21305] batch time: 0.051 trainign loss: 7.5822 avg training loss: 7.0245
batch: [9820/21305] batch time: 0.056 trainign loss: 5.5194 avg training loss: 7.0245
batch: [9830/21305] batch time: 0.880 trainign loss: 5.6995 avg training loss: 7.0244
batch: [9840/21305] batch time: 0.056 trainign loss: 4.6174 avg training loss: 7.0243
batch: [9850/21305] batch time: 1.265 trainign loss: 6.7105 avg training loss: 7.0243
batch: [9860/21305] batch time: 0.056 trainign loss: 2.5246 avg training loss: 7.0242
batch: [9870/21305] batch time: 0.606 trainign loss: 5.5280 avg training loss: 7.0242
batch: [9880/21305] batch time: 0.056 trainign loss: 7.1123 avg training loss: 7.0241
batch: [9890/21305] batch time: 0.475 trainign loss: 6.6054 avg training loss: 7.0241
batch: [9900/21305] batch time: 0.058 trainign loss: 5.2815 avg training loss: 7.0241
batch: [9910/21305] batch time: 0.062 trainign loss: 6.3588 avg training loss: 7.0240
batch: [9920/21305] batch time: 0.055 trainign loss: 6.1412 avg training loss: 7.0240
batch: [9930/21305] batch time: 0.055 trainign loss: 5.4598 avg training loss: 7.0239
batch: [9940/21305] batch time: 0.057 trainign loss: 7.5287 avg training loss: 7.0239
batch: [9950/21305] batch time: 0.056 trainign loss: 6.3907 avg training loss: 7.0238
batch: [9960/21305] batch time: 0.062 trainign loss: 6.9830 avg training loss: 7.0238
batch: [9970/21305] batch time: 0.056 trainign loss: 6.8537 avg training loss: 7.0238
batch: [9980/21305] batch time: 0.058 trainign loss: 5.8876 avg training loss: 7.0238
batch: [9990/21305] batch time: 0.151 trainign loss: 6.6007 avg training loss: 7.0237
batch: [10000/21305] batch time: 0.053 trainign loss: 5.3095 avg training loss: 7.0237
batch: [10010/21305] batch time: 0.053 trainign loss: 6.0364 avg training loss: 7.0236
batch: [10020/21305] batch time: 0.062 trainign loss: 6.8555 avg training loss: 7.0236
batch: [10030/21305] batch time: 0.056 trainign loss: 6.0011 avg training loss: 7.0236
batch: [10040/21305] batch time: 0.056 trainign loss: 6.0252 avg training loss: 7.0236
batch: [10050/21305] batch time: 0.056 trainign loss: 5.8979 avg training loss: 7.0235
batch: [10060/21305] batch time: 0.056 trainign loss: 4.6253 avg training loss: 7.0234
batch: [10070/21305] batch time: 0.050 trainign loss: 5.9699 avg training loss: 7.0233
batch: [10080/21305] batch time: 0.056 trainign loss: 5.0519 avg training loss: 7.0233
batch: [10090/21305] batch time: 0.056 trainign loss: 8.1691 avg training loss: 7.0232
batch: [10100/21305] batch time: 0.056 trainign loss: 8.0384 avg training loss: 7.0232
batch: [10110/21305] batch time: 0.051 trainign loss: 6.4499 avg training loss: 7.0232
batch: [10120/21305] batch time: 0.056 trainign loss: 7.0314 avg training loss: 7.0232
batch: [10130/21305] batch time: 0.051 trainign loss: 5.9248 avg training loss: 7.0232
batch: [10140/21305] batch time: 0.056 trainign loss: 6.9876 avg training loss: 7.0231
batch: [10150/21305] batch time: 0.051 trainign loss: 6.4997 avg training loss: 7.0231
batch: [10160/21305] batch time: 0.062 trainign loss: 5.7125 avg training loss: 7.0231
batch: [10170/21305] batch time: 0.061 trainign loss: 7.0309 avg training loss: 7.0230
batch: [10180/21305] batch time: 0.062 trainign loss: 6.9293 avg training loss: 7.0230
batch: [10190/21305] batch time: 0.057 trainign loss: 6.8712 avg training loss: 7.0230
batch: [10200/21305] batch time: 0.062 trainign loss: 5.5088 avg training loss: 7.0229
batch: [10210/21305] batch time: 0.647 trainign loss: 6.4846 avg training loss: 7.0229
batch: [10220/21305] batch time: 0.062 trainign loss: 5.7463 avg training loss: 7.0228
batch: [10230/21305] batch time: 0.250 trainign loss: 5.4073 avg training loss: 7.0228
batch: [10240/21305] batch time: 0.062 trainign loss: 6.8729 avg training loss: 7.0227
batch: [10250/21305] batch time: 0.056 trainign loss: 7.2521 avg training loss: 7.0227
batch: [10260/21305] batch time: 0.731 trainign loss: 7.4686 avg training loss: 7.0227
batch: [10270/21305] batch time: 0.056 trainign loss: 6.7674 avg training loss: 7.0226
batch: [10280/21305] batch time: 0.922 trainign loss: 6.8371 avg training loss: 7.0226
batch: [10290/21305] batch time: 0.057 trainign loss: 6.4955 avg training loss: 7.0226
batch: [10300/21305] batch time: 0.106 trainign loss: 5.9246 avg training loss: 7.0225
batch: [10310/21305] batch time: 0.055 trainign loss: 5.8848 avg training loss: 7.0224
batch: [10320/21305] batch time: 0.057 trainign loss: 2.3011 avg training loss: 7.0224
batch: [10330/21305] batch time: 0.056 trainign loss: 7.2749 avg training loss: 7.0223
batch: [10340/21305] batch time: 0.057 trainign loss: 7.1327 avg training loss: 7.0223
batch: [10350/21305] batch time: 0.055 trainign loss: 7.2383 avg training loss: 7.0223
batch: [10360/21305] batch time: 0.062 trainign loss: 6.3405 avg training loss: 7.0223
batch: [10370/21305] batch time: 0.389 trainign loss: 6.1685 avg training loss: 7.0223
batch: [10380/21305] batch time: 0.057 trainign loss: 6.7211 avg training loss: 7.0222
batch: [10390/21305] batch time: 0.395 trainign loss: 6.3308 avg training loss: 7.0222
batch: [10400/21305] batch time: 0.056 trainign loss: 6.3025 avg training loss: 7.0221
batch: [10410/21305] batch time: 0.053 trainign loss: 6.5820 avg training loss: 7.0221
batch: [10420/21305] batch time: 0.057 trainign loss: 7.5634 avg training loss: 7.0221
batch: [10430/21305] batch time: 0.056 trainign loss: 5.2511 avg training loss: 7.0220
batch: [10440/21305] batch time: 0.059 trainign loss: 6.2038 avg training loss: 7.0220
batch: [10450/21305] batch time: 0.054 trainign loss: 6.4069 avg training loss: 7.0219
batch: [10460/21305] batch time: 0.056 trainign loss: 7.0639 avg training loss: 7.0219
batch: [10470/21305] batch time: 0.051 trainign loss: 5.6274 avg training loss: 7.0219
batch: [10480/21305] batch time: 0.056 trainign loss: 7.3209 avg training loss: 7.0218
batch: [10490/21305] batch time: 0.053 trainign loss: 7.1279 avg training loss: 7.0218
batch: [10500/21305] batch time: 0.055 trainign loss: 5.3547 avg training loss: 7.0218
batch: [10510/21305] batch time: 0.063 trainign loss: 4.5895 avg training loss: 7.0217
batch: [10520/21305] batch time: 0.056 trainign loss: 6.8025 avg training loss: 7.0217
batch: [10530/21305] batch time: 0.056 trainign loss: 6.8599 avg training loss: 7.0217
batch: [10540/21305] batch time: 0.056 trainign loss: 5.7867 avg training loss: 7.0216
batch: [10550/21305] batch time: 0.053 trainign loss: 4.3568 avg training loss: 7.0216
batch: [10560/21305] batch time: 0.056 trainign loss: 9.0322 avg training loss: 7.0214
batch: [10570/21305] batch time: 0.056 trainign loss: 6.8397 avg training loss: 7.0214
batch: [10580/21305] batch time: 0.061 trainign loss: 7.1392 avg training loss: 7.0214
batch: [10590/21305] batch time: 0.057 trainign loss: 6.6769 avg training loss: 7.0214
batch: [10600/21305] batch time: 0.056 trainign loss: 6.8775 avg training loss: 7.0213
batch: [10610/21305] batch time: 0.056 trainign loss: 4.4483 avg training loss: 7.0213
batch: [10620/21305] batch time: 0.056 trainign loss: 6.7239 avg training loss: 7.0213
batch: [10630/21305] batch time: 0.051 trainign loss: 6.1568 avg training loss: 7.0212
batch: [10640/21305] batch time: 0.056 trainign loss: 5.8005 avg training loss: 7.0211
batch: [10650/21305] batch time: 0.061 trainign loss: 7.5195 avg training loss: 7.0211
batch: [10660/21305] batch time: 0.057 trainign loss: 6.8912 avg training loss: 7.0211
batch: [10670/21305] batch time: 0.063 trainign loss: 4.2731 avg training loss: 7.0210
batch: [10680/21305] batch time: 0.053 trainign loss: 6.8492 avg training loss: 7.0210
batch: [10690/21305] batch time: 0.056 trainign loss: 6.2444 avg training loss: 7.0209
batch: [10700/21305] batch time: 0.116 trainign loss: 7.0621 avg training loss: 7.0209
batch: [10710/21305] batch time: 0.058 trainign loss: 6.3021 avg training loss: 7.0208
batch: [10720/21305] batch time: 0.058 trainign loss: 6.5007 avg training loss: 7.0207
batch: [10730/21305] batch time: 0.060 trainign loss: 7.2110 avg training loss: 7.0206
batch: [10740/21305] batch time: 1.266 trainign loss: 6.8246 avg training loss: 7.0206
batch: [10750/21305] batch time: 0.056 trainign loss: 6.5566 avg training loss: 7.0206
batch: [10760/21305] batch time: 0.441 trainign loss: 4.7274 avg training loss: 7.0205
batch: [10770/21305] batch time: 0.056 trainign loss: 6.1045 avg training loss: 7.0204
batch: [10780/21305] batch time: 0.412 trainign loss: 7.6489 avg training loss: 7.0205
batch: [10790/21305] batch time: 0.056 trainign loss: 6.7270 avg training loss: 7.0204
batch: [10800/21305] batch time: 0.714 trainign loss: 5.8283 avg training loss: 7.0204
batch: [10810/21305] batch time: 0.054 trainign loss: 6.5332 avg training loss: 7.0204
batch: [10820/21305] batch time: 0.056 trainign loss: 3.7862 avg training loss: 7.0203
batch: [10830/21305] batch time: 0.056 trainign loss: 1.6568 avg training loss: 7.0201
batch: [10840/21305] batch time: 0.062 trainign loss: 6.9066 avg training loss: 7.0202
batch: [10850/21305] batch time: 0.056 trainign loss: 7.2591 avg training loss: 7.0202
batch: [10860/21305] batch time: 0.634 trainign loss: 6.4525 avg training loss: 7.0202
batch: [10870/21305] batch time: 0.057 trainign loss: 5.7462 avg training loss: 7.0201
batch: [10880/21305] batch time: 0.062 trainign loss: 5.3717 avg training loss: 7.0201
batch: [10890/21305] batch time: 0.062 trainign loss: 6.8791 avg training loss: 7.0200
batch: [10900/21305] batch time: 0.679 trainign loss: 7.5175 avg training loss: 7.0200
batch: [10910/21305] batch time: 0.057 trainign loss: 7.1465 avg training loss: 7.0200
batch: [10920/21305] batch time: 0.259 trainign loss: 6.8628 avg training loss: 7.0200
batch: [10930/21305] batch time: 0.061 trainign loss: 5.0655 avg training loss: 7.0199
batch: [10940/21305] batch time: 0.979 trainign loss: 6.6770 avg training loss: 7.0198
batch: [10950/21305] batch time: 0.052 trainign loss: 5.6789 avg training loss: 7.0198
batch: [10960/21305] batch time: 0.678 trainign loss: 6.3386 avg training loss: 7.0198
batch: [10970/21305] batch time: 0.063 trainign loss: 6.8225 avg training loss: 7.0198
batch: [10980/21305] batch time: 0.062 trainign loss: 5.2270 avg training loss: 7.0197
batch: [10990/21305] batch time: 0.053 trainign loss: 6.6977 avg training loss: 7.0196
batch: [11000/21305] batch time: 0.062 trainign loss: 6.3439 avg training loss: 7.0196
batch: [11010/21305] batch time: 0.050 trainign loss: 5.6541 avg training loss: 7.0195
batch: [11020/21305] batch time: 0.062 trainign loss: 7.5510 avg training loss: 7.0195
batch: [11030/21305] batch time: 0.052 trainign loss: 5.4113 avg training loss: 7.0195
batch: [11040/21305] batch time: 0.058 trainign loss: 7.2420 avg training loss: 7.0194
batch: [11050/21305] batch time: 0.056 trainign loss: 6.3775 avg training loss: 7.0194
batch: [11060/21305] batch time: 0.059 trainign loss: 7.0779 avg training loss: 7.0194
batch: [11070/21305] batch time: 0.057 trainign loss: 5.3223 avg training loss: 7.0193
batch: [11080/21305] batch time: 0.056 trainign loss: 7.2421 avg training loss: 7.0193
batch: [11090/21305] batch time: 0.054 trainign loss: 7.9313 avg training loss: 7.0193
batch: [11100/21305] batch time: 0.057 trainign loss: 6.7039 avg training loss: 7.0193
batch: [11110/21305] batch time: 0.056 trainign loss: 6.5816 avg training loss: 7.0193
batch: [11120/21305] batch time: 0.060 trainign loss: 6.7475 avg training loss: 7.0193
batch: [11130/21305] batch time: 0.051 trainign loss: 4.7342 avg training loss: 7.0192
batch: [11140/21305] batch time: 0.061 trainign loss: 3.7993 avg training loss: 7.0191
batch: [11150/21305] batch time: 0.053 trainign loss: 6.3249 avg training loss: 7.0191
batch: [11160/21305] batch time: 0.056 trainign loss: 6.8340 avg training loss: 7.0191
batch: [11170/21305] batch time: 0.050 trainign loss: 4.2503 avg training loss: 7.0190
batch: [11180/21305] batch time: 0.056 trainign loss: 8.3450 avg training loss: 7.0189
batch: [11190/21305] batch time: 0.056 trainign loss: 7.2034 avg training loss: 7.0188
batch: [11200/21305] batch time: 0.058 trainign loss: 7.7335 avg training loss: 7.0189
batch: [11210/21305] batch time: 0.053 trainign loss: 5.3637 avg training loss: 7.0188
batch: [11220/21305] batch time: 0.057 trainign loss: 6.4439 avg training loss: 7.0188
batch: [11230/21305] batch time: 0.052 trainign loss: 5.6588 avg training loss: 7.0188
batch: [11240/21305] batch time: 0.056 trainign loss: 6.0845 avg training loss: 7.0187
batch: [11250/21305] batch time: 0.051 trainign loss: 0.4792 avg training loss: 7.0186
batch: [11260/21305] batch time: 0.057 trainign loss: 8.0726 avg training loss: 7.0186
batch: [11270/21305] batch time: 0.053 trainign loss: 6.5939 avg training loss: 7.0186
batch: [11280/21305] batch time: 0.062 trainign loss: 5.6369 avg training loss: 7.0186
batch: [11290/21305] batch time: 0.055 trainign loss: 6.6007 avg training loss: 7.0185
batch: [11300/21305] batch time: 0.503 trainign loss: 2.2217 avg training loss: 7.0184
batch: [11310/21305] batch time: 0.056 trainign loss: 6.3160 avg training loss: 7.0184
batch: [11320/21305] batch time: 0.700 trainign loss: 7.0425 avg training loss: 7.0184
batch: [11330/21305] batch time: 0.053 trainign loss: 7.6317 avg training loss: 7.0183
batch: [11340/21305] batch time: 0.268 trainign loss: 7.4588 avg training loss: 7.0183
batch: [11350/21305] batch time: 0.055 trainign loss: 7.0740 avg training loss: 7.0183
batch: [11360/21305] batch time: 0.325 trainign loss: 6.1238 avg training loss: 7.0183
batch: [11370/21305] batch time: 0.500 trainign loss: 5.6988 avg training loss: 7.0182
batch: [11380/21305] batch time: 0.063 trainign loss: 3.8787 avg training loss: 7.0181
batch: [11390/21305] batch time: 0.484 trainign loss: 6.9012 avg training loss: 7.0179
batch: [11400/21305] batch time: 0.056 trainign loss: 8.0638 avg training loss: 7.0179
batch: [11410/21305] batch time: 0.961 trainign loss: 6.4387 avg training loss: 7.0179
batch: [11420/21305] batch time: 0.403 trainign loss: 6.0491 avg training loss: 7.0179
batch: [11430/21305] batch time: 1.133 trainign loss: 6.4742 avg training loss: 7.0179
batch: [11440/21305] batch time: 0.552 trainign loss: 6.2092 avg training loss: 7.0179
batch: [11450/21305] batch time: 0.850 trainign loss: 6.2302 avg training loss: 7.0178
batch: [11460/21305] batch time: 0.469 trainign loss: 6.1101 avg training loss: 7.0178
batch: [11470/21305] batch time: 0.851 trainign loss: 6.9028 avg training loss: 7.0177
batch: [11480/21305] batch time: 1.178 trainign loss: 6.2480 avg training loss: 7.0177
batch: [11490/21305] batch time: 0.056 trainign loss: 6.7612 avg training loss: 7.0176
batch: [11500/21305] batch time: 0.244 trainign loss: 6.7560 avg training loss: 7.0176
batch: [11510/21305] batch time: 0.057 trainign loss: 5.7120 avg training loss: 7.0175
batch: [11520/21305] batch time: 0.374 trainign loss: 6.6929 avg training loss: 7.0174
batch: [11530/21305] batch time: 0.056 trainign loss: 5.7536 avg training loss: 7.0174
batch: [11540/21305] batch time: 0.057 trainign loss: 7.8639 avg training loss: 7.0174
batch: [11550/21305] batch time: 0.056 trainign loss: 5.2502 avg training loss: 7.0173
batch: [11560/21305] batch time: 0.466 trainign loss: 7.0306 avg training loss: 7.0173
batch: [11570/21305] batch time: 0.056 trainign loss: 4.9413 avg training loss: 7.0173
batch: [11580/21305] batch time: 0.928 trainign loss: 7.7865 avg training loss: 7.0172
batch: [11590/21305] batch time: 0.063 trainign loss: 6.7516 avg training loss: 7.0172
batch: [11600/21305] batch time: 0.454 trainign loss: 4.5386 avg training loss: 7.0172
batch: [11610/21305] batch time: 0.268 trainign loss: 5.6800 avg training loss: 7.0171
batch: [11620/21305] batch time: 1.784 trainign loss: 6.9206 avg training loss: 7.0171
batch: [11630/21305] batch time: 0.056 trainign loss: 7.3356 avg training loss: 7.0170
batch: [11640/21305] batch time: 2.177 trainign loss: 6.2067 avg training loss: 7.0170
batch: [11650/21305] batch time: 0.061 trainign loss: 7.2407 avg training loss: 7.0170
batch: [11660/21305] batch time: 2.117 trainign loss: 5.1949 avg training loss: 7.0169
batch: [11670/21305] batch time: 0.059 trainign loss: 6.8166 avg training loss: 7.0169
batch: [11680/21305] batch time: 2.226 trainign loss: 5.4370 avg training loss: 7.0169
batch: [11690/21305] batch time: 0.056 trainign loss: 6.1543 avg training loss: 7.0168
batch: [11700/21305] batch time: 1.311 trainign loss: 6.0900 avg training loss: 7.0167
batch: [11710/21305] batch time: 0.051 trainign loss: 4.2800 avg training loss: 7.0166
batch: [11720/21305] batch time: 2.158 trainign loss: 7.3457 avg training loss: 7.0166
batch: [11730/21305] batch time: 0.053 trainign loss: 5.6720 avg training loss: 7.0166
batch: [11740/21305] batch time: 2.084 trainign loss: 5.8475 avg training loss: 7.0165
batch: [11750/21305] batch time: 0.056 trainign loss: 6.6712 avg training loss: 7.0165
batch: [11760/21305] batch time: 1.136 trainign loss: 6.7231 avg training loss: 7.0165
batch: [11770/21305] batch time: 0.055 trainign loss: 6.6038 avg training loss: 7.0165
batch: [11780/21305] batch time: 1.292 trainign loss: 4.5688 avg training loss: 7.0164
batch: [11790/21305] batch time: 0.062 trainign loss: 0.9448 avg training loss: 7.0163
batch: [11800/21305] batch time: 2.066 trainign loss: 0.0007 avg training loss: 7.0159
batch: [11810/21305] batch time: 0.056 trainign loss: 6.2557 avg training loss: 7.0159
batch: [11820/21305] batch time: 2.221 trainign loss: 6.9124 avg training loss: 7.0159
batch: [11830/21305] batch time: 0.056 trainign loss: 7.8000 avg training loss: 7.0158
batch: [11840/21305] batch time: 1.689 trainign loss: 7.2322 avg training loss: 7.0158
batch: [11850/21305] batch time: 0.056 trainign loss: 5.7661 avg training loss: 7.0158
batch: [11860/21305] batch time: 2.031 trainign loss: 4.0977 avg training loss: 7.0157
batch: [11870/21305] batch time: 0.059 trainign loss: 6.4099 avg training loss: 7.0156
batch: [11880/21305] batch time: 2.140 trainign loss: 6.9941 avg training loss: 7.0155
batch: [11890/21305] batch time: 0.056 trainign loss: 7.3082 avg training loss: 7.0155
batch: [11900/21305] batch time: 2.063 trainign loss: 5.2477 avg training loss: 7.0155
batch: [11910/21305] batch time: 0.056 trainign loss: 7.0386 avg training loss: 7.0154
batch: [11920/21305] batch time: 2.282 trainign loss: 6.5702 avg training loss: 7.0154
batch: [11930/21305] batch time: 0.054 trainign loss: 5.2958 avg training loss: 7.0153
batch: [11940/21305] batch time: 1.975 trainign loss: 7.4686 avg training loss: 7.0153
batch: [11950/21305] batch time: 0.055 trainign loss: 6.5568 avg training loss: 7.0153
batch: [11960/21305] batch time: 1.019 trainign loss: 6.8004 avg training loss: 7.0153
batch: [11970/21305] batch time: 0.062 trainign loss: 6.1815 avg training loss: 7.0152
batch: [11980/21305] batch time: 1.179 trainign loss: 6.4915 avg training loss: 7.0152
batch: [11990/21305] batch time: 0.054 trainign loss: 5.4725 avg training loss: 7.0152
batch: [12000/21305] batch time: 0.800 trainign loss: 6.6537 avg training loss: 7.0151
batch: [12010/21305] batch time: 0.058 trainign loss: 7.2959 avg training loss: 7.0151
batch: [12020/21305] batch time: 1.113 trainign loss: 6.7693 avg training loss: 7.0151
batch: [12030/21305] batch time: 0.058 trainign loss: 6.1123 avg training loss: 7.0151
batch: [12040/21305] batch time: 1.165 trainign loss: 5.5764 avg training loss: 7.0150
batch: [12050/21305] batch time: 0.058 trainign loss: 6.0983 avg training loss: 7.0150
batch: [12060/21305] batch time: 0.056 trainign loss: 6.7119 avg training loss: 7.0149
batch: [12070/21305] batch time: 0.062 trainign loss: 3.3305 avg training loss: 7.0149
batch: [12080/21305] batch time: 0.055 trainign loss: 6.1339 avg training loss: 7.0147
batch: [12090/21305] batch time: 0.056 trainign loss: 5.8913 avg training loss: 7.0147
batch: [12100/21305] batch time: 0.056 trainign loss: 5.9876 avg training loss: 7.0146
batch: [12110/21305] batch time: 0.056 trainign loss: 6.9677 avg training loss: 7.0146
batch: [12120/21305] batch time: 0.056 trainign loss: 4.9523 avg training loss: 7.0145
batch: [12130/21305] batch time: 0.057 trainign loss: 7.0740 avg training loss: 7.0145
batch: [12140/21305] batch time: 0.062 trainign loss: 5.1427 avg training loss: 7.0144
batch: [12150/21305] batch time: 0.055 trainign loss: 7.2507 avg training loss: 7.0144
batch: [12160/21305] batch time: 0.058 trainign loss: 7.2094 avg training loss: 7.0144
batch: [12170/21305] batch time: 0.052 trainign loss: 6.6497 avg training loss: 7.0144
batch: [12180/21305] batch time: 0.062 trainign loss: 6.6640 avg training loss: 7.0144
batch: [12190/21305] batch time: 0.052 trainign loss: 6.8884 avg training loss: 7.0144
batch: [12200/21305] batch time: 0.059 trainign loss: 5.8255 avg training loss: 7.0143
batch: [12210/21305] batch time: 0.052 trainign loss: 7.0329 avg training loss: 7.0143
batch: [12220/21305] batch time: 0.056 trainign loss: 6.6363 avg training loss: 7.0143
batch: [12230/21305] batch time: 0.057 trainign loss: 6.9825 avg training loss: 7.0143
batch: [12240/21305] batch time: 0.053 trainign loss: 6.1813 avg training loss: 7.0142
batch: [12250/21305] batch time: 0.056 trainign loss: 5.6392 avg training loss: 7.0142
batch: [12260/21305] batch time: 0.056 trainign loss: 6.8955 avg training loss: 7.0142
batch: [12270/21305] batch time: 0.058 trainign loss: 5.4148 avg training loss: 7.0141
batch: [12280/21305] batch time: 0.054 trainign loss: 4.2655 avg training loss: 7.0140
batch: [12290/21305] batch time: 0.058 trainign loss: 7.0984 avg training loss: 7.0140
batch: [12300/21305] batch time: 0.053 trainign loss: 7.1465 avg training loss: 7.0140
batch: [12310/21305] batch time: 0.056 trainign loss: 6.7578 avg training loss: 7.0140
batch: [12320/21305] batch time: 0.057 trainign loss: 6.2818 avg training loss: 7.0140
batch: [12330/21305] batch time: 0.061 trainign loss: 6.7872 avg training loss: 7.0139
batch: [12340/21305] batch time: 0.056 trainign loss: 6.5947 avg training loss: 7.0139
batch: [12350/21305] batch time: 0.056 trainign loss: 7.0158 avg training loss: 7.0139
batch: [12360/21305] batch time: 0.056 trainign loss: 4.8030 avg training loss: 7.0139
batch: [12370/21305] batch time: 0.056 trainign loss: 5.6516 avg training loss: 7.0138
batch: [12380/21305] batch time: 0.057 trainign loss: 6.7322 avg training loss: 7.0138
batch: [12390/21305] batch time: 0.056 trainign loss: 7.8465 avg training loss: 7.0138
batch: [12400/21305] batch time: 0.055 trainign loss: 6.2674 avg training loss: 7.0138
batch: [12410/21305] batch time: 0.056 trainign loss: 7.0212 avg training loss: 7.0137
batch: [12420/21305] batch time: 0.053 trainign loss: 5.4764 avg training loss: 7.0137
batch: [12430/21305] batch time: 0.056 trainign loss: 5.5933 avg training loss: 7.0137
batch: [12440/21305] batch time: 0.056 trainign loss: 6.8449 avg training loss: 7.0136
batch: [12450/21305] batch time: 0.056 trainign loss: 7.3692 avg training loss: 7.0136
batch: [12460/21305] batch time: 0.055 trainign loss: 7.4162 avg training loss: 7.0136
batch: [12470/21305] batch time: 0.056 trainign loss: 5.6490 avg training loss: 7.0135
batch: [12480/21305] batch time: 0.056 trainign loss: 5.8266 avg training loss: 7.0135
batch: [12490/21305] batch time: 0.056 trainign loss: 6.0795 avg training loss: 7.0135
batch: [12500/21305] batch time: 0.054 trainign loss: 6.8435 avg training loss: 7.0134
batch: [12510/21305] batch time: 0.063 trainign loss: 7.1533 avg training loss: 7.0134
batch: [12520/21305] batch time: 0.057 trainign loss: 7.1716 avg training loss: 7.0134
batch: [12530/21305] batch time: 0.058 trainign loss: 6.6738 avg training loss: 7.0133
batch: [12540/21305] batch time: 0.056 trainign loss: 5.0844 avg training loss: 7.0132
batch: [12550/21305] batch time: 0.057 trainign loss: 6.3215 avg training loss: 7.0132
batch: [12560/21305] batch time: 0.051 trainign loss: 4.2195 avg training loss: 7.0131
batch: [12570/21305] batch time: 0.056 trainign loss: 0.0069 avg training loss: 7.0128
batch: [12580/21305] batch time: 0.054 trainign loss: 7.1842 avg training loss: 7.0129
batch: [12590/21305] batch time: 0.056 trainign loss: 3.8668 avg training loss: 7.0128
batch: [12600/21305] batch time: 0.053 trainign loss: 7.6219 avg training loss: 7.0128
batch: [12610/21305] batch time: 0.056 trainign loss: 4.8068 avg training loss: 7.0128
batch: [12620/21305] batch time: 0.057 trainign loss: 6.5444 avg training loss: 7.0128
batch: [12630/21305] batch time: 0.056 trainign loss: 6.1205 avg training loss: 7.0128
batch: [12640/21305] batch time: 0.058 trainign loss: 7.1075 avg training loss: 7.0128
batch: [12650/21305] batch time: 0.063 trainign loss: 5.5809 avg training loss: 7.0127
batch: [12660/21305] batch time: 0.052 trainign loss: 6.2117 avg training loss: 7.0127
batch: [12670/21305] batch time: 0.056 trainign loss: 7.2963 avg training loss: 7.0127
batch: [12680/21305] batch time: 0.054 trainign loss: 4.3895 avg training loss: 7.0126
batch: [12690/21305] batch time: 0.062 trainign loss: 6.8636 avg training loss: 7.0125
batch: [12700/21305] batch time: 0.060 trainign loss: 6.7675 avg training loss: 7.0125
batch: [12710/21305] batch time: 0.056 trainign loss: 5.4897 avg training loss: 7.0124
batch: [12720/21305] batch time: 0.051 trainign loss: 5.1019 avg training loss: 7.0124
batch: [12730/21305] batch time: 0.063 trainign loss: 5.5649 avg training loss: 7.0123
batch: [12740/21305] batch time: 0.051 trainign loss: 4.8542 avg training loss: 7.0123
batch: [12750/21305] batch time: 0.056 trainign loss: 3.6329 avg training loss: 7.0121
batch: [12760/21305] batch time: 0.054 trainign loss: 6.3214 avg training loss: 7.0121
batch: [12770/21305] batch time: 0.056 trainign loss: 6.1705 avg training loss: 7.0121
batch: [12780/21305] batch time: 0.056 trainign loss: 7.2638 avg training loss: 7.0121
batch: [12790/21305] batch time: 0.056 trainign loss: 5.1886 avg training loss: 7.0121
batch: [12800/21305] batch time: 0.053 trainign loss: 6.8205 avg training loss: 7.0120
batch: [12810/21305] batch time: 0.055 trainign loss: 6.8379 avg training loss: 7.0120
batch: [12820/21305] batch time: 0.051 trainign loss: 6.4320 avg training loss: 7.0120
batch: [12830/21305] batch time: 0.057 trainign loss: 4.7746 avg training loss: 7.0119
batch: [12840/21305] batch time: 0.051 trainign loss: 6.3193 avg training loss: 7.0119
batch: [12850/21305] batch time: 0.058 trainign loss: 6.9156 avg training loss: 7.0119
batch: [12860/21305] batch time: 0.062 trainign loss: 6.9383 avg training loss: 7.0118
batch: [12870/21305] batch time: 0.157 trainign loss: 6.3932 avg training loss: 7.0118
batch: [12880/21305] batch time: 0.063 trainign loss: 5.5705 avg training loss: 7.0117
batch: [12890/21305] batch time: 0.054 trainign loss: 6.2911 avg training loss: 7.0117
batch: [12900/21305] batch time: 0.057 trainign loss: 7.0431 avg training loss: 7.0117
batch: [12910/21305] batch time: 0.053 trainign loss: 6.2381 avg training loss: 7.0117
batch: [12920/21305] batch time: 0.060 trainign loss: 6.1688 avg training loss: 7.0116
batch: [12930/21305] batch time: 0.052 trainign loss: 5.8130 avg training loss: 7.0116
batch: [12940/21305] batch time: 0.056 trainign loss: 6.3340 avg training loss: 7.0116
batch: [12950/21305] batch time: 0.056 trainign loss: 4.7895 avg training loss: 7.0115
batch: [12960/21305] batch time: 0.061 trainign loss: 6.8493 avg training loss: 7.0114
batch: [12970/21305] batch time: 0.057 trainign loss: 5.5041 avg training loss: 7.0114
batch: [12980/21305] batch time: 0.060 trainign loss: 6.6340 avg training loss: 7.0113
batch: [12990/21305] batch time: 0.056 trainign loss: 6.4081 avg training loss: 7.0113
batch: [13000/21305] batch time: 0.058 trainign loss: 6.9804 avg training loss: 7.0113
batch: [13010/21305] batch time: 0.056 trainign loss: 6.2941 avg training loss: 7.0112
batch: [13020/21305] batch time: 0.056 trainign loss: 7.7459 avg training loss: 7.0112
batch: [13030/21305] batch time: 0.063 trainign loss: 6.9455 avg training loss: 7.0112
batch: [13040/21305] batch time: 0.063 trainign loss: 7.0408 avg training loss: 7.0112
batch: [13050/21305] batch time: 0.053 trainign loss: 2.4199 avg training loss: 7.0111
batch: [13060/21305] batch time: 0.056 trainign loss: 6.5986 avg training loss: 7.0110
batch: [13070/21305] batch time: 0.052 trainign loss: 6.6407 avg training loss: 7.0110
batch: [13080/21305] batch time: 0.057 trainign loss: 6.3149 avg training loss: 7.0110
batch: [13090/21305] batch time: 0.052 trainign loss: 4.5363 avg training loss: 7.0109
batch: [13100/21305] batch time: 0.063 trainign loss: 7.5698 avg training loss: 7.0108
batch: [13110/21305] batch time: 0.056 trainign loss: 6.9630 avg training loss: 7.0107
batch: [13120/21305] batch time: 0.050 trainign loss: 6.7313 avg training loss: 7.0108
batch: [13130/21305] batch time: 0.054 trainign loss: 5.4193 avg training loss: 7.0107
batch: [13140/21305] batch time: 0.062 trainign loss: 2.3697 avg training loss: 7.0106
batch: [13150/21305] batch time: 0.058 trainign loss: 6.8442 avg training loss: 7.0106
batch: [13160/21305] batch time: 0.052 trainign loss: 5.8837 avg training loss: 7.0106
batch: [13170/21305] batch time: 0.056 trainign loss: 7.0501 avg training loss: 7.0105
batch: [13180/21305] batch time: 0.061 trainign loss: 5.1667 avg training loss: 7.0105
batch: [13190/21305] batch time: 0.053 trainign loss: 6.3653 avg training loss: 7.0104
batch: [13200/21305] batch time: 0.063 trainign loss: 3.2698 avg training loss: 7.0103
batch: [13210/21305] batch time: 0.063 trainign loss: 0.0020 avg training loss: 7.0100
batch: [13220/21305] batch time: 0.063 trainign loss: 0.0001 avg training loss: 7.0097
batch: [13230/21305] batch time: 0.062 trainign loss: 0.0001 avg training loss: 7.0093
batch: [13240/21305] batch time: 0.050 trainign loss: 7.4492 avg training loss: 7.0093
batch: [13250/21305] batch time: 0.056 trainign loss: 7.3137 avg training loss: 7.0094
batch: [13260/21305] batch time: 0.062 trainign loss: 7.2531 avg training loss: 7.0094
batch: [13270/21305] batch time: 0.062 trainign loss: 6.4245 avg training loss: 7.0093
batch: [13280/21305] batch time: 0.057 trainign loss: 7.2418 avg training loss: 7.0093
batch: [13290/21305] batch time: 0.056 trainign loss: 6.6508 avg training loss: 7.0093
batch: [13300/21305] batch time: 0.051 trainign loss: 7.0206 avg training loss: 7.0092
batch: [13310/21305] batch time: 0.058 trainign loss: 5.0810 avg training loss: 7.0092
batch: [13320/21305] batch time: 0.051 trainign loss: 5.4592 avg training loss: 7.0090
batch: [13330/21305] batch time: 0.056 trainign loss: 6.7116 avg training loss: 7.0090
batch: [13340/21305] batch time: 0.056 trainign loss: 7.4336 avg training loss: 7.0090
batch: [13350/21305] batch time: 0.056 trainign loss: 6.2048 avg training loss: 7.0090
batch: [13360/21305] batch time: 0.057 trainign loss: 7.9357 avg training loss: 7.0090
batch: [13370/21305] batch time: 0.058 trainign loss: 5.9432 avg training loss: 7.0090
batch: [13380/21305] batch time: 0.056 trainign loss: 4.2513 avg training loss: 7.0089
batch: [13390/21305] batch time: 0.056 trainign loss: 5.7551 avg training loss: 7.0088
batch: [13400/21305] batch time: 0.051 trainign loss: 4.6603 avg training loss: 7.0088
batch: [13410/21305] batch time: 0.058 trainign loss: 7.6216 avg training loss: 7.0088
batch: [13420/21305] batch time: 0.053 trainign loss: 6.5402 avg training loss: 7.0088
batch: [13430/21305] batch time: 0.056 trainign loss: 6.4731 avg training loss: 7.0088
batch: [13440/21305] batch time: 0.062 trainign loss: 5.5193 avg training loss: 7.0087
batch: [13450/21305] batch time: 0.056 trainign loss: 6.7396 avg training loss: 7.0087
batch: [13460/21305] batch time: 0.054 trainign loss: 6.4093 avg training loss: 7.0087
batch: [13470/21305] batch time: 0.056 trainign loss: 3.6758 avg training loss: 7.0086
batch: [13480/21305] batch time: 0.050 trainign loss: 3.9336 avg training loss: 7.0085
batch: [13490/21305] batch time: 0.056 trainign loss: 6.9652 avg training loss: 7.0085
batch: [13500/21305] batch time: 0.056 trainign loss: 6.6227 avg training loss: 7.0085
batch: [13510/21305] batch time: 0.056 trainign loss: 4.8794 avg training loss: 7.0084
batch: [13520/21305] batch time: 0.051 trainign loss: 6.5837 avg training loss: 7.0084
batch: [13530/21305] batch time: 0.056 trainign loss: 6.0690 avg training loss: 7.0083
batch: [13540/21305] batch time: 0.056 trainign loss: 2.8721 avg training loss: 7.0083
batch: [13550/21305] batch time: 0.057 trainign loss: 7.1763 avg training loss: 7.0082
batch: [13560/21305] batch time: 0.061 trainign loss: 6.1327 avg training loss: 7.0082
batch: [13570/21305] batch time: 0.055 trainign loss: 6.6127 avg training loss: 7.0082
batch: [13580/21305] batch time: 0.051 trainign loss: 7.0242 avg training loss: 7.0081
batch: [13590/21305] batch time: 0.056 trainign loss: 4.8771 avg training loss: 7.0081
batch: [13600/21305] batch time: 0.052 trainign loss: 6.3667 avg training loss: 7.0080
batch: [13610/21305] batch time: 0.062 trainign loss: 7.5367 avg training loss: 7.0080
batch: [13620/21305] batch time: 0.051 trainign loss: 6.7397 avg training loss: 7.0080
batch: [13630/21305] batch time: 0.056 trainign loss: 6.3285 avg training loss: 7.0080
batch: [13640/21305] batch time: 0.056 trainign loss: 6.7494 avg training loss: 7.0079
batch: [13650/21305] batch time: 0.056 trainign loss: 6.1057 avg training loss: 7.0079
batch: [13660/21305] batch time: 0.054 trainign loss: 5.7012 avg training loss: 7.0079
batch: [13670/21305] batch time: 0.056 trainign loss: 6.2988 avg training loss: 7.0078
batch: [13680/21305] batch time: 0.052 trainign loss: 5.2192 avg training loss: 7.0078
batch: [13690/21305] batch time: 0.064 trainign loss: 3.7084 avg training loss: 7.0077
batch: [13700/21305] batch time: 0.689 trainign loss: 7.8304 avg training loss: 7.0077
batch: [13710/21305] batch time: 0.770 trainign loss: 7.0619 avg training loss: 7.0077
batch: [13720/21305] batch time: 0.056 trainign loss: 6.5514 avg training loss: 7.0077
batch: [13730/21305] batch time: 0.529 trainign loss: 7.3171 avg training loss: 7.0077
batch: [13740/21305] batch time: 0.062 trainign loss: 5.5768 avg training loss: 7.0076
batch: [13750/21305] batch time: 0.058 trainign loss: 8.1138 avg training loss: 7.0076
batch: [13760/21305] batch time: 0.061 trainign loss: 7.3806 avg training loss: 7.0076
batch: [13770/21305] batch time: 0.113 trainign loss: 6.8341 avg training loss: 7.0076
batch: [13780/21305] batch time: 0.063 trainign loss: 5.5767 avg training loss: 7.0076
batch: [13790/21305] batch time: 0.056 trainign loss: 6.9327 avg training loss: 7.0076
batch: [13800/21305] batch time: 0.052 trainign loss: 5.8266 avg training loss: 7.0075
batch: [13810/21305] batch time: 0.056 trainign loss: 6.5220 avg training loss: 7.0075
batch: [13820/21305] batch time: 0.056 trainign loss: 7.1698 avg training loss: 7.0075
batch: [13830/21305] batch time: 0.063 trainign loss: 5.7327 avg training loss: 7.0074
batch: [13840/21305] batch time: 0.059 trainign loss: 5.9497 avg training loss: 7.0074
batch: [13850/21305] batch time: 0.056 trainign loss: 0.9520 avg training loss: 7.0072
batch: [13860/21305] batch time: 0.054 trainign loss: 6.0041 avg training loss: 7.0071
batch: [13870/21305] batch time: 0.062 trainign loss: 7.5169 avg training loss: 7.0071
batch: [13880/21305] batch time: 0.056 trainign loss: 5.7750 avg training loss: 7.0071
batch: [13890/21305] batch time: 0.058 trainign loss: 6.5804 avg training loss: 7.0070
batch: [13900/21305] batch time: 0.050 trainign loss: 4.8918 avg training loss: 7.0070
batch: [13910/21305] batch time: 0.059 trainign loss: 7.6676 avg training loss: 7.0069
batch: [13920/21305] batch time: 0.056 trainign loss: 4.6600 avg training loss: 7.0069
batch: [13930/21305] batch time: 0.056 trainign loss: 6.3044 avg training loss: 7.0068
batch: [13940/21305] batch time: 0.052 trainign loss: 7.3513 avg training loss: 7.0068
batch: [13950/21305] batch time: 0.057 trainign loss: 4.6147 avg training loss: 7.0067
batch: [13960/21305] batch time: 0.055 trainign loss: 5.6761 avg training loss: 7.0067
batch: [13970/21305] batch time: 0.057 trainign loss: 7.1899 avg training loss: 7.0067
batch: [13980/21305] batch time: 0.062 trainign loss: 6.8996 avg training loss: 7.0067
batch: [13990/21305] batch time: 0.060 trainign loss: 6.7572 avg training loss: 7.0066
batch: [14000/21305] batch time: 0.051 trainign loss: 6.0416 avg training loss: 7.0066
batch: [14010/21305] batch time: 0.061 trainign loss: 5.2080 avg training loss: 7.0065
batch: [14020/21305] batch time: 0.052 trainign loss: 7.9400 avg training loss: 7.0064
batch: [14030/21305] batch time: 0.203 trainign loss: 7.4046 avg training loss: 7.0064
batch: [14040/21305] batch time: 0.062 trainign loss: 7.5619 avg training loss: 7.0064
batch: [14050/21305] batch time: 0.417 trainign loss: 7.0577 avg training loss: 7.0064
batch: [14060/21305] batch time: 0.056 trainign loss: 6.4742 avg training loss: 7.0064
batch: [14070/21305] batch time: 0.240 trainign loss: 6.9491 avg training loss: 7.0064
batch: [14080/21305] batch time: 0.059 trainign loss: 6.5269 avg training loss: 7.0064
batch: [14090/21305] batch time: 0.063 trainign loss: 7.1090 avg training loss: 7.0064
batch: [14100/21305] batch time: 0.057 trainign loss: 6.3728 avg training loss: 7.0063
batch: [14110/21305] batch time: 1.501 trainign loss: 6.0313 avg training loss: 7.0063
batch: [14120/21305] batch time: 0.056 trainign loss: 7.4449 avg training loss: 7.0062
batch: [14130/21305] batch time: 0.811 trainign loss: 5.8222 avg training loss: 7.0061
batch: [14140/21305] batch time: 0.055 trainign loss: 6.5134 avg training loss: 7.0061
batch: [14150/21305] batch time: 0.995 trainign loss: 6.9114 avg training loss: 7.0061
batch: [14160/21305] batch time: 0.063 trainign loss: 6.5973 avg training loss: 7.0060
batch: [14170/21305] batch time: 0.361 trainign loss: 6.2317 avg training loss: 7.0060
batch: [14180/21305] batch time: 0.058 trainign loss: 6.1027 avg training loss: 7.0059
batch: [14190/21305] batch time: 0.330 trainign loss: 1.8846 avg training loss: 7.0058
batch: [14200/21305] batch time: 0.055 trainign loss: 7.1582 avg training loss: 7.0058
batch: [14210/21305] batch time: 0.062 trainign loss: 5.9488 avg training loss: 7.0058
batch: [14220/21305] batch time: 0.054 trainign loss: 7.8393 avg training loss: 7.0057
batch: [14230/21305] batch time: 0.057 trainign loss: 6.7288 avg training loss: 7.0057
batch: [14240/21305] batch time: 0.052 trainign loss: 7.3072 avg training loss: 7.0057
batch: [14250/21305] batch time: 0.057 trainign loss: 7.0174 avg training loss: 7.0057
batch: [14260/21305] batch time: 0.053 trainign loss: 5.0995 avg training loss: 7.0056
batch: [14270/21305] batch time: 0.056 trainign loss: 4.8415 avg training loss: 7.0056
batch: [14280/21305] batch time: 0.056 trainign loss: 5.0931 avg training loss: 7.0056
batch: [14290/21305] batch time: 0.056 trainign loss: 6.6420 avg training loss: 7.0055
batch: [14300/21305] batch time: 0.055 trainign loss: 6.9269 avg training loss: 7.0055
batch: [14310/21305] batch time: 0.056 trainign loss: 6.6242 avg training loss: 7.0055
batch: [14320/21305] batch time: 0.050 trainign loss: 2.3847 avg training loss: 7.0054
batch: [14330/21305] batch time: 0.061 trainign loss: 7.1641 avg training loss: 7.0053
batch: [14340/21305] batch time: 0.056 trainign loss: 6.2390 avg training loss: 7.0053
batch: [14350/21305] batch time: 0.056 trainign loss: 5.5497 avg training loss: 7.0053
batch: [14360/21305] batch time: 0.115 trainign loss: 5.8525 avg training loss: 7.0053
batch: [14370/21305] batch time: 0.056 trainign loss: 6.1715 avg training loss: 7.0053
batch: [14380/21305] batch time: 0.301 trainign loss: 6.1382 avg training loss: 7.0053
batch: [14390/21305] batch time: 0.056 trainign loss: 6.7532 avg training loss: 7.0052
batch: [14400/21305] batch time: 0.056 trainign loss: 6.1499 avg training loss: 7.0052
batch: [14410/21305] batch time: 0.057 trainign loss: 6.3877 avg training loss: 7.0051
batch: [14420/21305] batch time: 0.059 trainign loss: 6.4656 avg training loss: 7.0051
batch: [14430/21305] batch time: 0.062 trainign loss: 6.6266 avg training loss: 7.0051
batch: [14440/21305] batch time: 0.056 trainign loss: 6.9032 avg training loss: 7.0051
batch: [14450/21305] batch time: 0.970 trainign loss: 5.3163 avg training loss: 7.0050
batch: [14460/21305] batch time: 0.058 trainign loss: 6.6540 avg training loss: 7.0050
batch: [14470/21305] batch time: 1.595 trainign loss: 5.2848 avg training loss: 7.0049
batch: [14480/21305] batch time: 0.062 trainign loss: 3.9834 avg training loss: 7.0049
batch: [14490/21305] batch time: 0.954 trainign loss: 5.6932 avg training loss: 7.0048
batch: [14500/21305] batch time: 0.055 trainign loss: 1.6483 avg training loss: 7.0047
batch: [14510/21305] batch time: 0.457 trainign loss: 0.0020 avg training loss: 7.0044
batch: [14520/21305] batch time: 0.062 trainign loss: 0.0002 avg training loss: 7.0040
batch: [14530/21305] batch time: 0.246 trainign loss: 0.0001 avg training loss: 7.0037
batch: [14540/21305] batch time: 0.236 trainign loss: 0.0001 avg training loss: 7.0033
batch: [14550/21305] batch time: 0.330 trainign loss: 0.0001 avg training loss: 7.0030
batch: [14560/21305] batch time: 0.091 trainign loss: 0.0001 avg training loss: 7.0027
batch: [14570/21305] batch time: 0.622 trainign loss: 0.0001 avg training loss: 7.0023
batch: [14580/21305] batch time: 0.283 trainign loss: 0.0001 avg training loss: 7.0020
batch: [14590/21305] batch time: 0.821 trainign loss: 0.0001 avg training loss: 7.0016
batch: [14600/21305] batch time: 0.503 trainign loss: 6.4084 avg training loss: 7.0016
batch: [14610/21305] batch time: 1.386 trainign loss: 7.6757 avg training loss: 7.0016
batch: [14620/21305] batch time: 0.395 trainign loss: 6.5303 avg training loss: 7.0016
batch: [14630/21305] batch time: 1.171 trainign loss: 5.8320 avg training loss: 7.0015
batch: [14640/21305] batch time: 0.062 trainign loss: 5.9166 avg training loss: 7.0015
batch: [14650/21305] batch time: 0.494 trainign loss: 6.9276 avg training loss: 7.0015
batch: [14660/21305] batch time: 0.086 trainign loss: 6.0370 avg training loss: 7.0015
batch: [14670/21305] batch time: 0.056 trainign loss: 5.0820 avg training loss: 7.0014
batch: [14680/21305] batch time: 0.467 trainign loss: 2.2202 avg training loss: 7.0013
batch: [14690/21305] batch time: 0.063 trainign loss: 6.8198 avg training loss: 7.0013
batch: [14700/21305] batch time: 0.056 trainign loss: 4.3023 avg training loss: 7.0012
batch: [14710/21305] batch time: 0.056 trainign loss: 0.0066 avg training loss: 7.0009
batch: [14720/21305] batch time: 0.277 trainign loss: 7.7943 avg training loss: 7.0009
batch: [14730/21305] batch time: 0.056 trainign loss: 5.2806 avg training loss: 7.0009
batch: [14740/21305] batch time: 0.456 trainign loss: 5.3227 avg training loss: 7.0008
batch: [14750/21305] batch time: 0.056 trainign loss: 7.4477 avg training loss: 7.0008
batch: [14760/21305] batch time: 0.797 trainign loss: 7.1376 avg training loss: 7.0008
batch: [14770/21305] batch time: 0.053 trainign loss: 6.7423 avg training loss: 7.0008
batch: [14780/21305] batch time: 1.283 trainign loss: 6.3561 avg training loss: 7.0008
batch: [14790/21305] batch time: 0.056 trainign loss: 7.2012 avg training loss: 7.0008
batch: [14800/21305] batch time: 1.573 trainign loss: 6.4085 avg training loss: 7.0008
batch: [14810/21305] batch time: 0.063 trainign loss: 5.7207 avg training loss: 7.0007
batch: [14820/21305] batch time: 2.244 trainign loss: 6.3139 avg training loss: 7.0007
batch: [14830/21305] batch time: 0.053 trainign loss: 6.2887 avg training loss: 7.0007
batch: [14840/21305] batch time: 2.097 trainign loss: 5.4642 avg training loss: 7.0006
batch: [14850/21305] batch time: 0.056 trainign loss: 6.7645 avg training loss: 7.0006
batch: [14860/21305] batch time: 2.305 trainign loss: 7.1488 avg training loss: 7.0006
batch: [14870/21305] batch time: 0.063 trainign loss: 6.4333 avg training loss: 7.0006
batch: [14880/21305] batch time: 2.474 trainign loss: 6.5767 avg training loss: 7.0005
batch: [14890/21305] batch time: 0.057 trainign loss: 6.1802 avg training loss: 7.0005
batch: [14900/21305] batch time: 2.337 trainign loss: 6.2961 avg training loss: 7.0005
batch: [14910/21305] batch time: 0.057 trainign loss: 2.9846 avg training loss: 7.0004
batch: [14920/21305] batch time: 1.067 trainign loss: 5.3613 avg training loss: 7.0004
batch: [14930/21305] batch time: 0.056 trainign loss: 5.9161 avg training loss: 7.0003
batch: [14940/21305] batch time: 1.180 trainign loss: 6.5219 avg training loss: 7.0002
batch: [14950/21305] batch time: 0.056 trainign loss: 6.7521 avg training loss: 7.0002
batch: [14960/21305] batch time: 1.209 trainign loss: 6.8036 avg training loss: 7.0001
batch: [14970/21305] batch time: 0.056 trainign loss: 7.3350 avg training loss: 7.0000
batch: [14980/21305] batch time: 0.906 trainign loss: 7.2458 avg training loss: 7.0000
batch: [14990/21305] batch time: 0.056 trainign loss: 5.4484 avg training loss: 7.0000
batch: [15000/21305] batch time: 1.170 trainign loss: 5.5202 avg training loss: 6.9999
batch: [15010/21305] batch time: 0.063 trainign loss: 7.2426 avg training loss: 6.9999
batch: [15020/21305] batch time: 0.636 trainign loss: 5.6904 avg training loss: 6.9998
batch: [15030/21305] batch time: 0.059 trainign loss: 6.8703 avg training loss: 6.9998
batch: [15040/21305] batch time: 0.745 trainign loss: 2.0355 avg training loss: 6.9997
batch: [15050/21305] batch time: 0.062 trainign loss: 6.3970 avg training loss: 6.9997
batch: [15060/21305] batch time: 0.400 trainign loss: 6.2638 avg training loss: 6.9997
batch: [15070/21305] batch time: 0.056 trainign loss: 5.6979 avg training loss: 6.9996
batch: [15080/21305] batch time: 0.728 trainign loss: 3.5389 avg training loss: 6.9994
batch: [15090/21305] batch time: 0.056 trainign loss: 6.3160 avg training loss: 6.9994
batch: [15100/21305] batch time: 1.180 trainign loss: 5.6941 avg training loss: 6.9994
batch: [15110/21305] batch time: 0.056 trainign loss: 3.8603 avg training loss: 6.9994
batch: [15120/21305] batch time: 1.031 trainign loss: 6.8191 avg training loss: 6.9993
batch: [15130/21305] batch time: 0.063 trainign loss: 6.1927 avg training loss: 6.9993
batch: [15140/21305] batch time: 1.551 trainign loss: 7.5767 avg training loss: 6.9993
batch: [15150/21305] batch time: 0.057 trainign loss: 6.7254 avg training loss: 6.9993
batch: [15160/21305] batch time: 1.003 trainign loss: 6.5759 avg training loss: 6.9992
batch: [15170/21305] batch time: 0.056 trainign loss: 6.1221 avg training loss: 6.9992
batch: [15180/21305] batch time: 0.657 trainign loss: 5.4051 avg training loss: 6.9991
batch: [15190/21305] batch time: 0.056 trainign loss: 6.4599 avg training loss: 6.9991
batch: [15200/21305] batch time: 0.849 trainign loss: 7.1573 avg training loss: 6.9991
batch: [15210/21305] batch time: 0.056 trainign loss: 5.6706 avg training loss: 6.9990
batch: [15220/21305] batch time: 0.690 trainign loss: 6.3501 avg training loss: 6.9990
batch: [15230/21305] batch time: 0.056 trainign loss: 6.9578 avg training loss: 6.9990
batch: [15240/21305] batch time: 1.166 trainign loss: 6.7957 avg training loss: 6.9990
batch: [15250/21305] batch time: 0.063 trainign loss: 7.0616 avg training loss: 6.9989
batch: [15260/21305] batch time: 2.397 trainign loss: 5.7778 avg training loss: 6.9989
batch: [15270/21305] batch time: 0.056 trainign loss: 7.3234 avg training loss: 6.9988
batch: [15280/21305] batch time: 1.764 trainign loss: 6.3636 avg training loss: 6.9988
batch: [15290/21305] batch time: 0.223 trainign loss: 7.1948 avg training loss: 6.9988
batch: [15300/21305] batch time: 1.684 trainign loss: 6.4965 avg training loss: 6.9988
batch: [15310/21305] batch time: 0.293 trainign loss: 5.7228 avg training loss: 6.9987
batch: [15320/21305] batch time: 2.250 trainign loss: 7.5671 avg training loss: 6.9986
batch: [15330/21305] batch time: 0.661 trainign loss: 6.3407 avg training loss: 6.9987
batch: [15340/21305] batch time: 2.006 trainign loss: 4.9910 avg training loss: 6.9986
batch: [15350/21305] batch time: 0.554 trainign loss: 6.1214 avg training loss: 6.9986
batch: [15360/21305] batch time: 1.199 trainign loss: 7.4807 avg training loss: 6.9985
batch: [15370/21305] batch time: 0.344 trainign loss: 5.0001 avg training loss: 6.9985
batch: [15380/21305] batch time: 2.205 trainign loss: 5.7040 avg training loss: 6.9985
batch: [15390/21305] batch time: 0.298 trainign loss: 6.7311 avg training loss: 6.9984
batch: [15400/21305] batch time: 2.581 trainign loss: 6.9194 avg training loss: 6.9984
batch: [15410/21305] batch time: 0.056 trainign loss: 6.6551 avg training loss: 6.9984
batch: [15420/21305] batch time: 2.148 trainign loss: 6.4590 avg training loss: 6.9984
batch: [15430/21305] batch time: 0.061 trainign loss: 5.9251 avg training loss: 6.9983
batch: [15440/21305] batch time: 2.267 trainign loss: 7.0993 avg training loss: 6.9983
batch: [15450/21305] batch time: 0.060 trainign loss: 6.7025 avg training loss: 6.9983
batch: [15460/21305] batch time: 2.142 trainign loss: 6.6641 avg training loss: 6.9983
batch: [15470/21305] batch time: 0.159 trainign loss: 6.2307 avg training loss: 6.9982
batch: [15480/21305] batch time: 2.126 trainign loss: 6.8049 avg training loss: 6.9982
batch: [15490/21305] batch time: 0.363 trainign loss: 5.8809 avg training loss: 6.9982
batch: [15500/21305] batch time: 2.347 trainign loss: 4.3731 avg training loss: 6.9981
batch: [15510/21305] batch time: 0.061 trainign loss: 4.1735 avg training loss: 6.9980
batch: [15520/21305] batch time: 2.065 trainign loss: 7.7788 avg training loss: 6.9979
batch: [15530/21305] batch time: 0.055 trainign loss: 7.5875 avg training loss: 6.9979
batch: [15540/21305] batch time: 2.134 trainign loss: 7.2721 avg training loss: 6.9979
batch: [15550/21305] batch time: 0.056 trainign loss: 5.4461 avg training loss: 6.9979
batch: [15560/21305] batch time: 2.309 trainign loss: 8.0338 avg training loss: 6.9979
batch: [15570/21305] batch time: 0.058 trainign loss: 6.7153 avg training loss: 6.9979
batch: [15580/21305] batch time: 2.258 trainign loss: 6.4913 avg training loss: 6.9979
batch: [15590/21305] batch time: 0.061 trainign loss: 5.6967 avg training loss: 6.9979
batch: [15600/21305] batch time: 1.436 trainign loss: 6.1507 avg training loss: 6.9978
batch: [15610/21305] batch time: 0.056 trainign loss: 6.1596 avg training loss: 6.9977
batch: [15620/21305] batch time: 0.183 trainign loss: 6.2761 avg training loss: 6.9977
batch: [15630/21305] batch time: 0.057 trainign loss: 6.0430 avg training loss: 6.9977
batch: [15640/21305] batch time: 0.848 trainign loss: 7.3269 avg training loss: 6.9977
batch: [15650/21305] batch time: 0.057 trainign loss: 5.1001 avg training loss: 6.9977
batch: [15660/21305] batch time: 1.294 trainign loss: 4.8888 avg training loss: 6.9976
batch: [15670/21305] batch time: 0.062 trainign loss: 6.4531 avg training loss: 6.9976
batch: [15680/21305] batch time: 1.610 trainign loss: 7.1548 avg training loss: 6.9976
batch: [15690/21305] batch time: 1.409 trainign loss: 6.9171 avg training loss: 6.9976
batch: [15700/21305] batch time: 0.445 trainign loss: 6.6541 avg training loss: 6.9975
batch: [15710/21305] batch time: 1.419 trainign loss: 6.4224 avg training loss: 6.9975
batch: [15720/21305] batch time: 0.061 trainign loss: 4.7108 avg training loss: 6.9974
batch: [15730/21305] batch time: 2.000 trainign loss: 0.2167 avg training loss: 6.9972
batch: [15740/21305] batch time: 0.057 trainign loss: 11.9977 avg training loss: 6.9970
batch: [15750/21305] batch time: 2.372 trainign loss: 7.1213 avg training loss: 6.9971
batch: [15760/21305] batch time: 0.052 trainign loss: 6.5323 avg training loss: 6.9971
batch: [15770/21305] batch time: 2.238 trainign loss: 6.1232 avg training loss: 6.9971
batch: [15780/21305] batch time: 0.054 trainign loss: 6.2648 avg training loss: 6.9971
batch: [15790/21305] batch time: 1.971 trainign loss: 6.1692 avg training loss: 6.9971
batch: [15800/21305] batch time: 0.057 trainign loss: 4.6611 avg training loss: 6.9970
batch: [15810/21305] batch time: 2.358 trainign loss: 7.1845 avg training loss: 6.9970
batch: [15820/21305] batch time: 0.056 trainign loss: 5.9263 avg training loss: 6.9969
batch: [15830/21305] batch time: 1.955 trainign loss: 6.1565 avg training loss: 6.9969
batch: [15840/21305] batch time: 0.057 trainign loss: 5.9190 avg training loss: 6.9969
batch: [15850/21305] batch time: 2.315 trainign loss: 4.8662 avg training loss: 6.9969
batch: [15860/21305] batch time: 0.056 trainign loss: 7.4413 avg training loss: 6.9968
batch: [15870/21305] batch time: 2.379 trainign loss: 5.8439 avg training loss: 6.9968
batch: [15880/21305] batch time: 0.062 trainign loss: 6.9311 avg training loss: 6.9968
batch: [15890/21305] batch time: 2.520 trainign loss: 6.9051 avg training loss: 6.9968
batch: [15900/21305] batch time: 0.059 trainign loss: 5.5802 avg training loss: 6.9968
batch: [15910/21305] batch time: 2.088 trainign loss: 6.3994 avg training loss: 6.9967
batch: [15920/21305] batch time: 0.057 trainign loss: 6.3829 avg training loss: 6.9967
batch: [15930/21305] batch time: 2.104 trainign loss: 6.8456 avg training loss: 6.9966
batch: [15940/21305] batch time: 0.062 trainign loss: 6.3043 avg training loss: 6.9965
batch: [15950/21305] batch time: 2.170 trainign loss: 7.0449 avg training loss: 6.9965
batch: [15960/21305] batch time: 0.276 trainign loss: 4.8038 avg training loss: 6.9965
batch: [15970/21305] batch time: 1.719 trainign loss: 5.0979 avg training loss: 6.9965
batch: [15980/21305] batch time: 0.056 trainign loss: 5.8260 avg training loss: 6.9964
batch: [15990/21305] batch time: 1.787 trainign loss: 6.8210 avg training loss: 6.9964
batch: [16000/21305] batch time: 0.056 trainign loss: 6.5007 avg training loss: 6.9964
batch: [16010/21305] batch time: 0.625 trainign loss: 0.7734 avg training loss: 6.9962
batch: [16020/21305] batch time: 0.511 trainign loss: 6.0772 avg training loss: 6.9962
batch: [16030/21305] batch time: 1.815 trainign loss: 7.6840 avg training loss: 6.9962
batch: [16040/21305] batch time: 1.089 trainign loss: 4.6550 avg training loss: 6.9962
batch: [16050/21305] batch time: 1.956 trainign loss: 6.5716 avg training loss: 6.9961
batch: [16060/21305] batch time: 0.678 trainign loss: 5.6705 avg training loss: 6.9961
batch: [16070/21305] batch time: 1.328 trainign loss: 7.2570 avg training loss: 6.9961
batch: [16080/21305] batch time: 0.534 trainign loss: 6.8888 avg training loss: 6.9960
batch: [16090/21305] batch time: 2.444 trainign loss: 6.3001 avg training loss: 6.9960
batch: [16100/21305] batch time: 0.150 trainign loss: 5.7750 avg training loss: 6.9960
batch: [16110/21305] batch time: 2.716 trainign loss: 5.2714 avg training loss: 6.9960
batch: [16120/21305] batch time: 0.052 trainign loss: 4.2897 avg training loss: 6.9959
batch: [16130/21305] batch time: 2.398 trainign loss: 5.8202 avg training loss: 6.9958
batch: [16140/21305] batch time: 0.056 trainign loss: 6.5148 avg training loss: 6.9958
batch: [16150/21305] batch time: 2.563 trainign loss: 6.4580 avg training loss: 6.9958
batch: [16160/21305] batch time: 0.062 trainign loss: 4.0793 avg training loss: 6.9957
batch: [16170/21305] batch time: 2.671 trainign loss: 7.3809 avg training loss: 6.9957
batch: [16180/21305] batch time: 0.056 trainign loss: 7.0508 avg training loss: 6.9957
batch: [16190/21305] batch time: 2.272 trainign loss: 6.8434 avg training loss: 6.9957
batch: [16200/21305] batch time: 0.056 trainign loss: 5.1435 avg training loss: 6.9956
batch: [16210/21305] batch time: 0.880 trainign loss: 6.6322 avg training loss: 6.9956
batch: [16220/21305] batch time: 0.052 trainign loss: 6.4475 avg training loss: 6.9956
batch: [16230/21305] batch time: 1.561 trainign loss: 6.6484 avg training loss: 6.9956
batch: [16240/21305] batch time: 0.353 trainign loss: 7.0612 avg training loss: 6.9955
batch: [16250/21305] batch time: 2.541 trainign loss: 6.4160 avg training loss: 6.9955
batch: [16260/21305] batch time: 0.897 trainign loss: 6.6182 avg training loss: 6.9954
batch: [16270/21305] batch time: 1.832 trainign loss: 7.2607 avg training loss: 6.9954
batch: [16280/21305] batch time: 0.452 trainign loss: 7.2168 avg training loss: 6.9954
batch: [16290/21305] batch time: 2.232 trainign loss: 6.4885 avg training loss: 6.9954
batch: [16300/21305] batch time: 0.166 trainign loss: 6.4835 avg training loss: 6.9953
batch: [16310/21305] batch time: 1.819 trainign loss: 6.3289 avg training loss: 6.9953
batch: [16320/21305] batch time: 1.161 trainign loss: 6.8883 avg training loss: 6.9952
batch: [16330/21305] batch time: 0.612 trainign loss: 5.8513 avg training loss: 6.9952
batch: [16340/21305] batch time: 1.598 trainign loss: 4.5958 avg training loss: 6.9951
batch: [16350/21305] batch time: 0.667 trainign loss: 7.4109 avg training loss: 6.9951
batch: [16360/21305] batch time: 1.124 trainign loss: 6.2131 avg training loss: 6.9950
batch: [16370/21305] batch time: 1.272 trainign loss: 5.8538 avg training loss: 6.9950
batch: [16380/21305] batch time: 1.382 trainign loss: 6.2878 avg training loss: 6.9949
batch: [16390/21305] batch time: 0.984 trainign loss: 5.9381 avg training loss: 6.9949
batch: [16400/21305] batch time: 0.717 trainign loss: 7.0597 avg training loss: 6.9948
batch: [16410/21305] batch time: 0.488 trainign loss: 7.1072 avg training loss: 6.9948
batch: [16420/21305] batch time: 1.444 trainign loss: 6.3899 avg training loss: 6.9948
batch: [16430/21305] batch time: 1.064 trainign loss: 5.6775 avg training loss: 6.9947
batch: [16440/21305] batch time: 1.211 trainign loss: 7.0803 avg training loss: 6.9947
batch: [16450/21305] batch time: 0.430 trainign loss: 7.5339 avg training loss: 6.9947
batch: [16460/21305] batch time: 1.090 trainign loss: 6.9188 avg training loss: 6.9947
batch: [16470/21305] batch time: 0.439 trainign loss: 6.7056 avg training loss: 6.9947
batch: [16480/21305] batch time: 0.440 trainign loss: 5.5835 avg training loss: 6.9946
batch: [16490/21305] batch time: 0.059 trainign loss: 5.3098 avg training loss: 6.9946
batch: [16500/21305] batch time: 0.829 trainign loss: 5.7936 avg training loss: 6.9945
batch: [16510/21305] batch time: 0.058 trainign loss: 6.5462 avg training loss: 6.9945
batch: [16520/21305] batch time: 0.732 trainign loss: 6.2790 avg training loss: 6.9945
batch: [16530/21305] batch time: 0.056 trainign loss: 6.5333 avg training loss: 6.9944
batch: [16540/21305] batch time: 1.042 trainign loss: 5.7486 avg training loss: 6.9943
batch: [16550/21305] batch time: 0.060 trainign loss: 6.9419 avg training loss: 6.9943
batch: [16560/21305] batch time: 0.057 trainign loss: 7.1205 avg training loss: 6.9943
batch: [16570/21305] batch time: 0.056 trainign loss: 5.6955 avg training loss: 6.9943
batch: [16580/21305] batch time: 0.056 trainign loss: 6.9516 avg training loss: 6.9942
batch: [16590/21305] batch time: 0.054 trainign loss: 7.1076 avg training loss: 6.9942
batch: [16600/21305] batch time: 0.144 trainign loss: 6.5752 avg training loss: 6.9941
batch: [16610/21305] batch time: 0.061 trainign loss: 5.0309 avg training loss: 6.9940
batch: [16620/21305] batch time: 1.135 trainign loss: 6.0310 avg training loss: 6.9940
batch: [16630/21305] batch time: 0.062 trainign loss: 5.4324 avg training loss: 6.9939
batch: [16640/21305] batch time: 0.494 trainign loss: 7.5010 avg training loss: 6.9939
batch: [16650/21305] batch time: 0.057 trainign loss: 6.8800 avg training loss: 6.9939
batch: [16660/21305] batch time: 0.606 trainign loss: 6.8530 avg training loss: 6.9939
batch: [16670/21305] batch time: 0.063 trainign loss: 6.2945 avg training loss: 6.9938
batch: [16680/21305] batch time: 0.997 trainign loss: 7.0942 avg training loss: 6.9938
batch: [16690/21305] batch time: 0.060 trainign loss: 6.9285 avg training loss: 6.9938
batch: [16700/21305] batch time: 1.266 trainign loss: 6.9186 avg training loss: 6.9937
batch: [16710/21305] batch time: 0.054 trainign loss: 6.4085 avg training loss: 6.9937
batch: [16720/21305] batch time: 1.580 trainign loss: 6.6725 avg training loss: 6.9937
batch: [16730/21305] batch time: 0.056 trainign loss: 5.0308 avg training loss: 6.9936
batch: [16740/21305] batch time: 1.959 trainign loss: 6.4217 avg training loss: 6.9936
batch: [16750/21305] batch time: 0.057 trainign loss: 6.4642 avg training loss: 6.9936
batch: [16760/21305] batch time: 1.873 trainign loss: 5.8561 avg training loss: 6.9936
batch: [16770/21305] batch time: 0.056 trainign loss: 7.1027 avg training loss: 6.9935
batch: [16780/21305] batch time: 1.158 trainign loss: 6.2866 avg training loss: 6.9935
batch: [16790/21305] batch time: 0.052 trainign loss: 6.2220 avg training loss: 6.9935
batch: [16800/21305] batch time: 0.108 trainign loss: 2.0782 avg training loss: 6.9933
batch: [16810/21305] batch time: 0.062 trainign loss: 7.7868 avg training loss: 6.9933
batch: [16820/21305] batch time: 0.058 trainign loss: 6.0858 avg training loss: 6.9933
batch: [16830/21305] batch time: 0.055 trainign loss: 6.4414 avg training loss: 6.9933
batch: [16840/21305] batch time: 0.717 trainign loss: 6.8404 avg training loss: 6.9932
batch: [16850/21305] batch time: 0.056 trainign loss: 6.3004 avg training loss: 6.9932
batch: [16860/21305] batch time: 0.716 trainign loss: 6.8215 avg training loss: 6.9932
batch: [16870/21305] batch time: 0.056 trainign loss: 6.2390 avg training loss: 6.9931
batch: [16880/21305] batch time: 0.824 trainign loss: 6.5218 avg training loss: 6.9931
batch: [16890/21305] batch time: 0.056 trainign loss: 6.5507 avg training loss: 6.9930
batch: [16900/21305] batch time: 0.427 trainign loss: 4.2419 avg training loss: 6.9929
batch: [16910/21305] batch time: 0.056 trainign loss: 0.0082 avg training loss: 6.9927
batch: [16920/21305] batch time: 0.596 trainign loss: 8.6020 avg training loss: 6.9927
batch: [16930/21305] batch time: 0.056 trainign loss: 4.6125 avg training loss: 6.9927
batch: [16940/21305] batch time: 1.977 trainign loss: 6.5422 avg training loss: 6.9926
batch: [16950/21305] batch time: 0.052 trainign loss: 7.1040 avg training loss: 6.9926
batch: [16960/21305] batch time: 1.756 trainign loss: 4.4832 avg training loss: 6.9926
batch: [16970/21305] batch time: 0.056 trainign loss: 5.4184 avg training loss: 6.9925
batch: [16980/21305] batch time: 2.351 trainign loss: 3.0538 avg training loss: 6.9924
batch: [16990/21305] batch time: 0.056 trainign loss: 3.5533 avg training loss: 6.9923
batch: [17000/21305] batch time: 2.026 trainign loss: 1.6974 avg training loss: 6.9922
batch: [17010/21305] batch time: 0.062 trainign loss: 7.1133 avg training loss: 6.9922
batch: [17020/21305] batch time: 2.162 trainign loss: 0.4204 avg training loss: 6.9921
batch: [17030/21305] batch time: 0.056 trainign loss: 7.9435 avg training loss: 6.9920
batch: [17040/21305] batch time: 2.415 trainign loss: 7.3029 avg training loss: 6.9920
batch: [17050/21305] batch time: 0.056 trainign loss: 6.5554 avg training loss: 6.9920
batch: [17060/21305] batch time: 2.099 trainign loss: 6.6930 avg training loss: 6.9919
batch: [17070/21305] batch time: 0.052 trainign loss: 6.8554 avg training loss: 6.9918
batch: [17080/21305] batch time: 2.192 trainign loss: 7.2819 avg training loss: 6.9918
batch: [17090/21305] batch time: 0.056 trainign loss: 6.3168 avg training loss: 6.9918
batch: [17100/21305] batch time: 2.389 trainign loss: 1.9503 avg training loss: 6.9917
batch: [17110/21305] batch time: 0.056 trainign loss: 7.3438 avg training loss: 6.9917
batch: [17120/21305] batch time: 2.264 trainign loss: 6.8926 avg training loss: 6.9917
batch: [17130/21305] batch time: 0.056 trainign loss: 5.4478 avg training loss: 6.9916
batch: [17140/21305] batch time: 2.484 trainign loss: 5.5591 avg training loss: 6.9916
batch: [17150/21305] batch time: 0.052 trainign loss: 6.4997 avg training loss: 6.9916
batch: [17160/21305] batch time: 2.310 trainign loss: 6.5497 avg training loss: 6.9915
batch: [17170/21305] batch time: 0.055 trainign loss: 4.9274 avg training loss: 6.9914
batch: [17180/21305] batch time: 2.250 trainign loss: 7.1232 avg training loss: 6.9914
batch: [17190/21305] batch time: 0.056 trainign loss: 4.9767 avg training loss: 6.9913
batch: [17200/21305] batch time: 2.703 trainign loss: 6.6864 avg training loss: 6.9913
batch: [17210/21305] batch time: 0.062 trainign loss: 6.4174 avg training loss: 6.9912
batch: [17220/21305] batch time: 2.198 trainign loss: 6.7168 avg training loss: 6.9912
batch: [17230/21305] batch time: 0.054 trainign loss: 4.9560 avg training loss: 6.9912
batch: [17240/21305] batch time: 2.346 trainign loss: 6.2538 avg training loss: 6.9911
batch: [17250/21305] batch time: 0.057 trainign loss: 7.6725 avg training loss: 6.9910
batch: [17260/21305] batch time: 2.491 trainign loss: 7.2702 avg training loss: 6.9910
batch: [17270/21305] batch time: 0.054 trainign loss: 6.0535 avg training loss: 6.9910
batch: [17280/21305] batch time: 2.265 trainign loss: 6.7061 avg training loss: 6.9910
batch: [17290/21305] batch time: 0.056 trainign loss: 4.6820 avg training loss: 6.9909
batch: [17300/21305] batch time: 2.569 trainign loss: 6.2067 avg training loss: 6.9909
batch: [17310/21305] batch time: 0.056 trainign loss: 3.3877 avg training loss: 6.9908
batch: [17320/21305] batch time: 2.136 trainign loss: 11.4347 avg training loss: 6.9905
batch: [17330/21305] batch time: 0.052 trainign loss: 8.0245 avg training loss: 6.9906
batch: [17340/21305] batch time: 2.518 trainign loss: 7.1816 avg training loss: 6.9906
batch: [17350/21305] batch time: 0.056 trainign loss: 7.5029 avg training loss: 6.9905
batch: [17360/21305] batch time: 2.353 trainign loss: 6.2175 avg training loss: 6.9905
batch: [17370/21305] batch time: 0.061 trainign loss: 6.0853 avg training loss: 6.9904
batch: [17380/21305] batch time: 2.109 trainign loss: 6.7015 avg training loss: 6.9904
batch: [17390/21305] batch time: 0.062 trainign loss: 7.2998 avg training loss: 6.9905
batch: [17400/21305] batch time: 2.397 trainign loss: 6.7432 avg training loss: 6.9904
batch: [17410/21305] batch time: 0.062 trainign loss: 6.7728 avg training loss: 6.9904
batch: [17420/21305] batch time: 2.252 trainign loss: 5.9581 avg training loss: 6.9904
batch: [17430/21305] batch time: 0.061 trainign loss: 5.3030 avg training loss: 6.9904
batch: [17440/21305] batch time: 2.226 trainign loss: 6.2610 avg training loss: 6.9903
batch: [17450/21305] batch time: 0.058 trainign loss: 6.2336 avg training loss: 6.9903
batch: [17460/21305] batch time: 2.130 trainign loss: 5.0361 avg training loss: 6.9902
batch: [17470/21305] batch time: 0.074 trainign loss: 6.7885 avg training loss: 6.9902
batch: [17480/21305] batch time: 2.445 trainign loss: 5.8169 avg training loss: 6.9901
batch: [17490/21305] batch time: 0.545 trainign loss: 6.7228 avg training loss: 6.9901
batch: [17500/21305] batch time: 1.690 trainign loss: 5.9916 avg training loss: 6.9901
batch: [17510/21305] batch time: 0.060 trainign loss: 6.0735 avg training loss: 6.9901
batch: [17520/21305] batch time: 1.235 trainign loss: 5.3336 avg training loss: 6.9900
batch: [17530/21305] batch time: 0.058 trainign loss: 4.8822 avg training loss: 6.9899
batch: [17540/21305] batch time: 0.128 trainign loss: 6.1095 avg training loss: 6.9899
batch: [17550/21305] batch time: 0.961 trainign loss: 6.4438 avg training loss: 6.9899
batch: [17560/21305] batch time: 0.390 trainign loss: 5.8069 avg training loss: 6.9898
batch: [17570/21305] batch time: 1.174 trainign loss: 6.9244 avg training loss: 6.9898
batch: [17580/21305] batch time: 0.062 trainign loss: 1.4056 avg training loss: 6.9896
batch: [17590/21305] batch time: 2.359 trainign loss: 10.6114 avg training loss: 6.9894
batch: [17600/21305] batch time: 0.321 trainign loss: 7.9682 avg training loss: 6.9893
batch: [17610/21305] batch time: 1.988 trainign loss: 8.2956 avg training loss: 6.9893
batch: [17620/21305] batch time: 0.568 trainign loss: 5.8225 avg training loss: 6.9893
batch: [17630/21305] batch time: 1.881 trainign loss: 6.7254 avg training loss: 6.9893
batch: [17640/21305] batch time: 0.055 trainign loss: 5.0117 avg training loss: 6.9893
batch: [17650/21305] batch time: 1.687 trainign loss: 5.5023 avg training loss: 6.9893
batch: [17660/21305] batch time: 0.380 trainign loss: 6.1324 avg training loss: 6.9893
batch: [17670/21305] batch time: 2.867 trainign loss: 5.7971 avg training loss: 6.9892
batch: [17680/21305] batch time: 0.056 trainign loss: 6.9603 avg training loss: 6.9892
batch: [17690/21305] batch time: 2.692 trainign loss: 5.3461 avg training loss: 6.9892
batch: [17700/21305] batch time: 0.052 trainign loss: 5.8905 avg training loss: 6.9891
batch: [17710/21305] batch time: 2.433 trainign loss: 6.4524 avg training loss: 6.9891
batch: [17720/21305] batch time: 0.062 trainign loss: 6.5815 avg training loss: 6.9890
batch: [17730/21305] batch time: 2.590 trainign loss: 6.7448 avg training loss: 6.9890
batch: [17740/21305] batch time: 0.060 trainign loss: 5.6661 avg training loss: 6.9890
batch: [17750/21305] batch time: 2.159 trainign loss: 6.3088 avg training loss: 6.9890
batch: [17760/21305] batch time: 0.056 trainign loss: 6.0197 avg training loss: 6.9890
batch: [17770/21305] batch time: 2.463 trainign loss: 6.6448 avg training loss: 6.9889
batch: [17780/21305] batch time: 0.052 trainign loss: 5.9268 avg training loss: 6.9889
batch: [17790/21305] batch time: 2.390 trainign loss: 6.9587 avg training loss: 6.9888
batch: [17800/21305] batch time: 0.056 trainign loss: 5.8647 avg training loss: 6.9888
batch: [17810/21305] batch time: 2.203 trainign loss: 6.9555 avg training loss: 6.9888
batch: [17820/21305] batch time: 0.055 trainign loss: 6.3228 avg training loss: 6.9888
batch: [17830/21305] batch time: 2.423 trainign loss: 5.3768 avg training loss: 6.9887
batch: [17840/21305] batch time: 0.158 trainign loss: 7.2134 avg training loss: 6.9887
batch: [17850/21305] batch time: 2.128 trainign loss: 6.7322 avg training loss: 6.9887
batch: [17860/21305] batch time: 0.278 trainign loss: 6.5734 avg training loss: 6.9886
batch: [17870/21305] batch time: 1.297 trainign loss: 6.1267 avg training loss: 6.9886
batch: [17880/21305] batch time: 0.704 trainign loss: 6.7050 avg training loss: 6.9886
batch: [17890/21305] batch time: 0.839 trainign loss: 6.7458 avg training loss: 6.9886
batch: [17900/21305] batch time: 0.760 trainign loss: 6.0785 avg training loss: 6.9885
batch: [17910/21305] batch time: 1.145 trainign loss: 3.0419 avg training loss: 6.9884
batch: [17920/21305] batch time: 1.406 trainign loss: 7.2764 avg training loss: 6.9884
batch: [17930/21305] batch time: 1.715 trainign loss: 5.8963 avg training loss: 6.9884
batch: [17940/21305] batch time: 0.497 trainign loss: 6.1142 avg training loss: 6.9884
batch: [17950/21305] batch time: 1.917 trainign loss: 6.1799 avg training loss: 6.9883
batch: [17960/21305] batch time: 1.207 trainign loss: 6.4229 avg training loss: 6.9882
batch: [17970/21305] batch time: 0.538 trainign loss: 6.2309 avg training loss: 6.9882
batch: [17980/21305] batch time: 1.378 trainign loss: 1.7117 avg training loss: 6.9881
batch: [17990/21305] batch time: 0.282 trainign loss: 5.1194 avg training loss: 6.9881
batch: [18000/21305] batch time: 1.278 trainign loss: 6.9465 avg training loss: 6.9880
batch: [18010/21305] batch time: 0.569 trainign loss: 8.0209 avg training loss: 6.9880
batch: [18020/21305] batch time: 1.129 trainign loss: 4.1668 avg training loss: 6.9880
batch: [18030/21305] batch time: 0.056 trainign loss: 6.8311 avg training loss: 6.9879
batch: [18040/21305] batch time: 2.616 trainign loss: 6.3141 avg training loss: 6.9879
batch: [18050/21305] batch time: 0.056 trainign loss: 6.2991 avg training loss: 6.9879
batch: [18060/21305] batch time: 2.683 trainign loss: 6.7026 avg training loss: 6.9879
batch: [18070/21305] batch time: 0.054 trainign loss: 7.3716 avg training loss: 6.9878
batch: [18080/21305] batch time: 2.243 trainign loss: 6.2134 avg training loss: 6.9878
batch: [18090/21305] batch time: 0.053 trainign loss: 5.0452 avg training loss: 6.9878
batch: [18100/21305] batch time: 1.907 trainign loss: 5.7014 avg training loss: 6.9877
batch: [18110/21305] batch time: 0.053 trainign loss: 5.4317 avg training loss: 6.9877
batch: [18120/21305] batch time: 2.370 trainign loss: 6.7328 avg training loss: 6.9876
batch: [18130/21305] batch time: 0.054 trainign loss: 7.3776 avg training loss: 6.9876
batch: [18140/21305] batch time: 2.405 trainign loss: 7.3357 avg training loss: 6.9876
batch: [18150/21305] batch time: 0.058 trainign loss: 7.1038 avg training loss: 6.9876
batch: [18160/21305] batch time: 1.541 trainign loss: 5.0685 avg training loss: 6.9875
batch: [18170/21305] batch time: 0.859 trainign loss: 5.6133 avg training loss: 6.9875
batch: [18180/21305] batch time: 1.121 trainign loss: 0.1072 avg training loss: 6.9873
batch: [18190/21305] batch time: 1.330 trainign loss: 7.2075 avg training loss: 6.9872
batch: [18200/21305] batch time: 0.587 trainign loss: 6.6707 avg training loss: 6.9873
batch: [18210/21305] batch time: 1.709 trainign loss: 6.4336 avg training loss: 6.9872
batch: [18220/21305] batch time: 0.270 trainign loss: 4.3073 avg training loss: 6.9871
batch: [18230/21305] batch time: 2.071 trainign loss: 8.3440 avg training loss: 6.9871
batch: [18240/21305] batch time: 0.057 trainign loss: 6.4336 avg training loss: 6.9872
batch: [18250/21305] batch time: 2.146 trainign loss: 5.5666 avg training loss: 6.9871
batch: [18260/21305] batch time: 0.056 trainign loss: 1.5869 avg training loss: 6.9870
batch: [18270/21305] batch time: 1.304 trainign loss: 6.2413 avg training loss: 6.9870
batch: [18280/21305] batch time: 0.056 trainign loss: 6.5980 avg training loss: 6.9869
batch: [18290/21305] batch time: 2.297 trainign loss: 7.3451 avg training loss: 6.9869
batch: [18300/21305] batch time: 0.057 trainign loss: 6.1935 avg training loss: 6.9870
batch: [18310/21305] batch time: 2.243 trainign loss: 6.6882 avg training loss: 6.9869
batch: [18320/21305] batch time: 0.059 trainign loss: 5.4917 avg training loss: 6.9869
batch: [18330/21305] batch time: 2.091 trainign loss: 5.8323 avg training loss: 6.9868
batch: [18340/21305] batch time: 0.056 trainign loss: 7.6160 avg training loss: 6.9868
batch: [18350/21305] batch time: 2.196 trainign loss: 7.3428 avg training loss: 6.9868
batch: [18360/21305] batch time: 0.058 trainign loss: 5.5728 avg training loss: 6.9868
batch: [18370/21305] batch time: 2.313 trainign loss: 7.1046 avg training loss: 6.9868
batch: [18380/21305] batch time: 0.063 trainign loss: 5.9500 avg training loss: 6.9867
batch: [18390/21305] batch time: 2.109 trainign loss: 5.5362 avg training loss: 6.9866
batch: [18400/21305] batch time: 0.056 trainign loss: 7.4328 avg training loss: 6.9867
batch: [18410/21305] batch time: 2.059 trainign loss: 5.1348 avg training loss: 6.9866
batch: [18420/21305] batch time: 0.057 trainign loss: 6.2198 avg training loss: 6.9866
batch: [18430/21305] batch time: 2.560 trainign loss: 6.1732 avg training loss: 6.9866
batch: [18440/21305] batch time: 0.062 trainign loss: 0.0895 avg training loss: 6.9864
batch: [18450/21305] batch time: 2.156 trainign loss: 8.1251 avg training loss: 6.9863
batch: [18460/21305] batch time: 0.056 trainign loss: 6.5252 avg training loss: 6.9863
batch: [18470/21305] batch time: 2.171 trainign loss: 6.8586 avg training loss: 6.9863
batch: [18480/21305] batch time: 0.056 trainign loss: 5.8312 avg training loss: 6.9862
batch: [18490/21305] batch time: 1.827 trainign loss: 4.0435 avg training loss: 6.9861
batch: [18500/21305] batch time: 0.063 trainign loss: 0.0084 avg training loss: 6.9858
batch: [18510/21305] batch time: 2.015 trainign loss: 8.0615 avg training loss: 6.9858
batch: [18520/21305] batch time: 0.058 trainign loss: 7.6187 avg training loss: 6.9859
batch: [18530/21305] batch time: 2.035 trainign loss: 6.0247 avg training loss: 6.9858
batch: [18540/21305] batch time: 0.061 trainign loss: 5.9258 avg training loss: 6.9858
batch: [18550/21305] batch time: 2.440 trainign loss: 4.2177 avg training loss: 6.9857
batch: [18560/21305] batch time: 0.057 trainign loss: 6.9147 avg training loss: 6.9857
batch: [18570/21305] batch time: 2.184 trainign loss: 7.1663 avg training loss: 6.9857
batch: [18580/21305] batch time: 0.062 trainign loss: 6.6345 avg training loss: 6.9856
batch: [18590/21305] batch time: 1.956 trainign loss: 7.0762 avg training loss: 6.9856
batch: [18600/21305] batch time: 0.584 trainign loss: 5.8102 avg training loss: 6.9855
batch: [18610/21305] batch time: 2.177 trainign loss: 5.4941 avg training loss: 6.9854
batch: [18620/21305] batch time: 0.062 trainign loss: 7.6593 avg training loss: 6.9854
batch: [18630/21305] batch time: 2.331 trainign loss: 6.0701 avg training loss: 6.9854
batch: [18640/21305] batch time: 0.056 trainign loss: 4.9314 avg training loss: 6.9854
batch: [18650/21305] batch time: 1.842 trainign loss: 3.4637 avg training loss: 6.9853
batch: [18660/21305] batch time: 0.055 trainign loss: 0.0042 avg training loss: 6.9850
batch: [18670/21305] batch time: 2.309 trainign loss: 6.2515 avg training loss: 6.9849
batch: [18680/21305] batch time: 0.057 trainign loss: 6.7072 avg training loss: 6.9849
batch: [18690/21305] batch time: 2.302 trainign loss: 6.7563 avg training loss: 6.9849
batch: [18700/21305] batch time: 0.056 trainign loss: 6.6399 avg training loss: 6.9849
batch: [18710/21305] batch time: 2.444 trainign loss: 6.6594 avg training loss: 6.9848
batch: [18720/21305] batch time: 0.056 trainign loss: 5.9184 avg training loss: 6.9848
batch: [18730/21305] batch time: 2.806 trainign loss: 5.2890 avg training loss: 6.9848
batch: [18740/21305] batch time: 0.056 trainign loss: 7.3157 avg training loss: 6.9847
batch: [18750/21305] batch time: 2.191 trainign loss: 5.3137 avg training loss: 6.9847
batch: [18760/21305] batch time: 0.057 trainign loss: 5.3235 avg training loss: 6.9846
batch: [18770/21305] batch time: 2.215 trainign loss: 6.7708 avg training loss: 6.9846
batch: [18780/21305] batch time: 0.056 trainign loss: 6.9382 avg training loss: 6.9845
batch: [18790/21305] batch time: 2.065 trainign loss: 6.1641 avg training loss: 6.9845
batch: [18800/21305] batch time: 0.056 trainign loss: 6.9079 avg training loss: 6.9845
batch: [18810/21305] batch time: 2.787 trainign loss: 6.8135 avg training loss: 6.9845
batch: [18820/21305] batch time: 0.062 trainign loss: 4.4352 avg training loss: 6.9844
batch: [18830/21305] batch time: 2.427 trainign loss: 7.2749 avg training loss: 6.9844
batch: [18840/21305] batch time: 0.056 trainign loss: 6.6219 avg training loss: 6.9844
batch: [18850/21305] batch time: 2.312 trainign loss: 7.1947 avg training loss: 6.9843
batch: [18860/21305] batch time: 0.056 trainign loss: 6.5926 avg training loss: 6.9843
batch: [18870/21305] batch time: 2.417 trainign loss: 6.4173 avg training loss: 6.9842
batch: [18880/21305] batch time: 0.057 trainign loss: 4.2198 avg training loss: 6.9842
batch: [18890/21305] batch time: 2.353 trainign loss: 6.5638 avg training loss: 6.9841
batch: [18900/21305] batch time: 0.056 trainign loss: 6.0491 avg training loss: 6.9841
batch: [18910/21305] batch time: 2.437 trainign loss: 6.3953 avg training loss: 6.9841
batch: [18920/21305] batch time: 0.057 trainign loss: 6.8539 avg training loss: 6.9841
batch: [18930/21305] batch time: 1.910 trainign loss: 7.2083 avg training loss: 6.9840
batch: [18940/21305] batch time: 0.052 trainign loss: 6.4145 avg training loss: 6.9840
batch: [18950/21305] batch time: 2.234 trainign loss: 6.8731 avg training loss: 6.9840
batch: [18960/21305] batch time: 0.052 trainign loss: 6.7330 avg training loss: 6.9839
batch: [18970/21305] batch time: 2.126 trainign loss: 6.4714 avg training loss: 6.9839
batch: [18980/21305] batch time: 0.062 trainign loss: 8.0506 avg training loss: 6.9838
batch: [18990/21305] batch time: 2.037 trainign loss: 7.4436 avg training loss: 6.9838
batch: [19000/21305] batch time: 0.057 trainign loss: 5.7336 avg training loss: 6.9838
batch: [19010/21305] batch time: 2.237 trainign loss: 7.4758 avg training loss: 6.9837
batch: [19020/21305] batch time: 0.056 trainign loss: 6.7630 avg training loss: 6.9837
batch: [19030/21305] batch time: 2.427 trainign loss: 7.3505 avg training loss: 6.9837
batch: [19040/21305] batch time: 0.057 trainign loss: 6.9264 avg training loss: 6.9837
batch: [19050/21305] batch time: 2.289 trainign loss: 7.0779 avg training loss: 6.9836
batch: [19060/21305] batch time: 0.144 trainign loss: 5.1660 avg training loss: 6.9835
batch: [19070/21305] batch time: 1.695 trainign loss: 6.3140 avg training loss: 6.9835
batch: [19080/21305] batch time: 0.062 trainign loss: 6.7982 avg training loss: 6.9835
batch: [19090/21305] batch time: 1.023 trainign loss: 8.0419 avg training loss: 6.9835
batch: [19100/21305] batch time: 0.058 trainign loss: 7.5026 avg training loss: 6.9835
batch: [19110/21305] batch time: 1.484 trainign loss: 3.5982 avg training loss: 6.9834
batch: [19120/21305] batch time: 0.058 trainign loss: 6.5596 avg training loss: 6.9834
batch: [19130/21305] batch time: 2.194 trainign loss: 7.4374 avg training loss: 6.9834
batch: [19140/21305] batch time: 0.059 trainign loss: 6.3822 avg training loss: 6.9834
batch: [19150/21305] batch time: 1.628 trainign loss: 6.2890 avg training loss: 6.9834
batch: [19160/21305] batch time: 0.062 trainign loss: 6.9445 avg training loss: 6.9834
batch: [19170/21305] batch time: 2.396 trainign loss: 6.7587 avg training loss: 6.9833
batch: [19180/21305] batch time: 0.054 trainign loss: 4.9334 avg training loss: 6.9833
batch: [19190/21305] batch time: 2.322 trainign loss: 6.6008 avg training loss: 6.9833
batch: [19200/21305] batch time: 0.052 trainign loss: 4.2736 avg training loss: 6.9832
batch: [19210/21305] batch time: 2.485 trainign loss: 6.7460 avg training loss: 6.9832
batch: [19220/21305] batch time: 0.058 trainign loss: 6.9359 avg training loss: 6.9831
batch: [19230/21305] batch time: 2.140 trainign loss: 6.4121 avg training loss: 6.9831
batch: [19240/21305] batch time: 0.052 trainign loss: 5.6044 avg training loss: 6.9831
batch: [19250/21305] batch time: 2.547 trainign loss: 2.8694 avg training loss: 6.9830
batch: [19260/21305] batch time: 0.063 trainign loss: 5.4272 avg training loss: 6.9829
batch: [19270/21305] batch time: 1.500 trainign loss: 7.4931 avg training loss: 6.9829
batch: [19280/21305] batch time: 0.063 trainign loss: 8.0746 avg training loss: 6.9829
batch: [19290/21305] batch time: 1.929 trainign loss: 6.7703 avg training loss: 6.9829
batch: [19300/21305] batch time: 0.057 trainign loss: 7.0397 avg training loss: 6.9829
batch: [19310/21305] batch time: 1.509 trainign loss: 7.1702 avg training loss: 6.9829
batch: [19320/21305] batch time: 0.056 trainign loss: 7.3124 avg training loss: 6.9828
batch: [19330/21305] batch time: 0.055 trainign loss: 6.7601 avg training loss: 6.9828
batch: [19340/21305] batch time: 0.063 trainign loss: 5.2295 avg training loss: 6.9828
batch: [19350/21305] batch time: 0.057 trainign loss: 6.7648 avg training loss: 6.9828
batch: [19360/21305] batch time: 0.056 trainign loss: 5.8303 avg training loss: 6.9828
batch: [19370/21305] batch time: 0.056 trainign loss: 6.5661 avg training loss: 6.9827
batch: [19380/21305] batch time: 0.056 trainign loss: 5.9277 avg training loss: 6.9827
batch: [19390/21305] batch time: 0.056 trainign loss: 7.7696 avg training loss: 6.9826
batch: [19400/21305] batch time: 0.063 trainign loss: 6.8583 avg training loss: 6.9826
batch: [19410/21305] batch time: 0.536 trainign loss: 5.1860 avg training loss: 6.9826
batch: [19420/21305] batch time: 0.060 trainign loss: 6.4001 avg training loss: 6.9825
batch: [19430/21305] batch time: 0.586 trainign loss: 6.5381 avg training loss: 6.9825
batch: [19440/21305] batch time: 0.056 trainign loss: 7.1824 avg training loss: 6.9825
batch: [19450/21305] batch time: 0.445 trainign loss: 6.8754 avg training loss: 6.9825
batch: [19460/21305] batch time: 0.056 trainign loss: 6.2517 avg training loss: 6.9825
batch: [19470/21305] batch time: 0.056 trainign loss: 7.1894 avg training loss: 6.9825
batch: [19480/21305] batch time: 0.056 trainign loss: 5.9504 avg training loss: 6.9825
batch: [19490/21305] batch time: 0.209 trainign loss: 4.8903 avg training loss: 6.9824
batch: [19500/21305] batch time: 0.056 trainign loss: 6.5652 avg training loss: 6.9824
batch: [19510/21305] batch time: 0.301 trainign loss: 6.2122 avg training loss: 6.9824
batch: [19520/21305] batch time: 0.059 trainign loss: 6.7556 avg training loss: 6.9824
batch: [19530/21305] batch time: 0.831 trainign loss: 6.9249 avg training loss: 6.9823
batch: [19540/21305] batch time: 0.056 trainign loss: 6.2450 avg training loss: 6.9823
batch: [19550/21305] batch time: 1.264 trainign loss: 6.1051 avg training loss: 6.9822
batch: [19560/21305] batch time: 0.056 trainign loss: 6.9362 avg training loss: 6.9822
batch: [19570/21305] batch time: 0.954 trainign loss: 6.6579 avg training loss: 6.9822
batch: [19580/21305] batch time: 0.056 trainign loss: 5.3057 avg training loss: 6.9821
batch: [19590/21305] batch time: 0.811 trainign loss: 5.4022 avg training loss: 6.9820
batch: [19600/21305] batch time: 0.057 trainign loss: 6.2092 avg training loss: 6.9820
batch: [19610/21305] batch time: 0.095 trainign loss: 6.5622 avg training loss: 6.9820
batch: [19620/21305] batch time: 0.056 trainign loss: 6.6338 avg training loss: 6.9820
batch: [19630/21305] batch time: 0.056 trainign loss: 6.9048 avg training loss: 6.9819
batch: [19640/21305] batch time: 0.056 trainign loss: 7.4464 avg training loss: 6.9819
batch: [19650/21305] batch time: 0.051 trainign loss: 6.4400 avg training loss: 6.9819
batch: [19660/21305] batch time: 0.053 trainign loss: 6.4083 avg training loss: 6.9819
batch: [19670/21305] batch time: 0.058 trainign loss: 6.2658 avg training loss: 6.9818
batch: [19680/21305] batch time: 0.055 trainign loss: 0.0233 avg training loss: 6.9816
batch: [19690/21305] batch time: 0.055 trainign loss: 7.2115 avg training loss: 6.9816
batch: [19700/21305] batch time: 0.059 trainign loss: 7.4462 avg training loss: 6.9816
batch: [19710/21305] batch time: 0.058 trainign loss: 6.2705 avg training loss: 6.9816
batch: [19720/21305] batch time: 0.057 trainign loss: 5.8498 avg training loss: 6.9815
batch: [19730/21305] batch time: 0.058 trainign loss: 5.7955 avg training loss: 6.9815
batch: [19740/21305] batch time: 0.127 trainign loss: 5.3118 avg training loss: 6.9814
batch: [19750/21305] batch time: 0.056 trainign loss: 7.0000 avg training loss: 6.9814
batch: [19760/21305] batch time: 0.057 trainign loss: 6.1364 avg training loss: 6.9814
batch: [19770/21305] batch time: 0.051 trainign loss: 5.8680 avg training loss: 6.9813
batch: [19780/21305] batch time: 0.055 trainign loss: 4.9560 avg training loss: 6.9813
batch: [19790/21305] batch time: 0.062 trainign loss: 5.9967 avg training loss: 6.9812
batch: [19800/21305] batch time: 0.601 trainign loss: 6.0823 avg training loss: 6.9812
batch: [19810/21305] batch time: 0.054 trainign loss: 5.4338 avg training loss: 6.9811
batch: [19820/21305] batch time: 0.062 trainign loss: 6.6456 avg training loss: 6.9811
batch: [19830/21305] batch time: 0.085 trainign loss: 7.2385 avg training loss: 6.9811
batch: [19840/21305] batch time: 0.355 trainign loss: 6.2979 avg training loss: 6.9811
batch: [19850/21305] batch time: 0.192 trainign loss: 6.3769 avg training loss: 6.9810
batch: [19860/21305] batch time: 0.773 trainign loss: 7.2880 avg training loss: 6.9810
batch: [19870/21305] batch time: 0.056 trainign loss: 7.0397 avg training loss: 6.9810
batch: [19880/21305] batch time: 1.038 trainign loss: 5.0013 avg training loss: 6.9809
batch: [19890/21305] batch time: 0.061 trainign loss: 4.6008 avg training loss: 6.9809
batch: [19900/21305] batch time: 1.412 trainign loss: 8.9838 avg training loss: 6.9807
batch: [19910/21305] batch time: 0.056 trainign loss: 1.5333 avg training loss: 6.9805
batch: [19920/21305] batch time: 1.236 trainign loss: 6.8494 avg training loss: 6.9806
batch: [19930/21305] batch time: 0.056 trainign loss: 7.6720 avg training loss: 6.9806
batch: [19940/21305] batch time: 0.318 trainign loss: 6.0890 avg training loss: 6.9806
batch: [19950/21305] batch time: 0.053 trainign loss: 6.8162 avg training loss: 6.9805
batch: [19960/21305] batch time: 0.157 trainign loss: 6.6986 avg training loss: 6.9804
batch: [19970/21305] batch time: 0.051 trainign loss: 6.9922 avg training loss: 6.9804
batch: [19980/21305] batch time: 0.383 trainign loss: 7.3458 avg training loss: 6.9804
batch: [19990/21305] batch time: 0.057 trainign loss: 5.8417 avg training loss: 6.9804
batch: [20000/21305] batch time: 1.058 trainign loss: 6.5092 avg training loss: 6.9803
batch: [20010/21305] batch time: 0.051 trainign loss: 5.8447 avg training loss: 6.9803
batch: [20020/21305] batch time: 0.681 trainign loss: 0.4246 avg training loss: 6.9801
batch: [20030/21305] batch time: 0.056 trainign loss: 6.8064 avg training loss: 6.9801
batch: [20040/21305] batch time: 0.737 trainign loss: 4.8126 avg training loss: 6.9800
batch: [20050/21305] batch time: 0.056 trainign loss: 2.8341 avg training loss: 6.9799
batch: [20060/21305] batch time: 0.058 trainign loss: 4.3513 avg training loss: 6.9798
batch: [20070/21305] batch time: 0.055 trainign loss: 7.0495 avg training loss: 6.9798
batch: [20080/21305] batch time: 0.344 trainign loss: 6.6484 avg training loss: 6.9798
batch: [20090/21305] batch time: 0.063 trainign loss: 5.0594 avg training loss: 6.9797
batch: [20100/21305] batch time: 0.706 trainign loss: 9.3546 avg training loss: 6.9796
batch: [20110/21305] batch time: 0.050 trainign loss: 6.7520 avg training loss: 6.9795
batch: [20120/21305] batch time: 0.634 trainign loss: 7.5126 avg training loss: 6.9796
batch: [20130/21305] batch time: 0.062 trainign loss: 6.7062 avg training loss: 6.9796
batch: [20140/21305] batch time: 1.040 trainign loss: 6.7538 avg training loss: 6.9795
batch: [20150/21305] batch time: 0.063 trainign loss: 5.8112 avg training loss: 6.9795
batch: [20160/21305] batch time: 1.149 trainign loss: 6.7665 avg training loss: 6.9795
batch: [20170/21305] batch time: 0.056 trainign loss: 6.9400 avg training loss: 6.9794
batch: [20180/21305] batch time: 1.618 trainign loss: 6.1488 avg training loss: 6.9794
batch: [20190/21305] batch time: 0.062 trainign loss: 3.8534 avg training loss: 6.9794
batch: [20200/21305] batch time: 1.701 trainign loss: 4.9415 avg training loss: 6.9793
batch: [20210/21305] batch time: 0.372 trainign loss: 7.2314 avg training loss: 6.9793
batch: [20220/21305] batch time: 1.042 trainign loss: 6.8413 avg training loss: 6.9792
batch: [20230/21305] batch time: 0.090 trainign loss: 6.3021 avg training loss: 6.9792
batch: [20240/21305] batch time: 0.285 trainign loss: 4.8324 avg training loss: 6.9791
batch: [20250/21305] batch time: 0.056 trainign loss: 6.7366 avg training loss: 6.9791
batch: [20260/21305] batch time: 1.826 trainign loss: 6.6059 avg training loss: 6.9790
batch: [20270/21305] batch time: 0.056 trainign loss: 1.0816 avg training loss: 6.9789
batch: [20280/21305] batch time: 2.142 trainign loss: 7.2353 avg training loss: 6.9789
batch: [20290/21305] batch time: 0.054 trainign loss: 7.0934 avg training loss: 6.9789
batch: [20300/21305] batch time: 1.964 trainign loss: 6.6824 avg training loss: 6.9789
batch: [20310/21305] batch time: 0.056 trainign loss: 6.3301 avg training loss: 6.9788
batch: [20320/21305] batch time: 2.138 trainign loss: 6.3891 avg training loss: 6.9788
batch: [20330/21305] batch time: 0.059 trainign loss: 4.4623 avg training loss: 6.9787
batch: [20340/21305] batch time: 0.713 trainign loss: 6.0489 avg training loss: 6.9786
batch: [20350/21305] batch time: 0.056 trainign loss: 6.8977 avg training loss: 6.9786
batch: [20360/21305] batch time: 0.057 trainign loss: 6.8570 avg training loss: 6.9786
batch: [20370/21305] batch time: 0.063 trainign loss: 6.4564 avg training loss: 6.9785
batch: [20380/21305] batch time: 0.255 trainign loss: 6.0740 avg training loss: 6.9785
batch: [20390/21305] batch time: 0.058 trainign loss: 6.8837 avg training loss: 6.9785
batch: [20400/21305] batch time: 0.056 trainign loss: 6.6119 avg training loss: 6.9784
batch: [20410/21305] batch time: 0.053 trainign loss: 6.4795 avg training loss: 6.9784
batch: [20420/21305] batch time: 0.051 trainign loss: 5.8542 avg training loss: 6.9784
batch: [20430/21305] batch time: 0.052 trainign loss: 6.6020 avg training loss: 6.9783
batch: [20440/21305] batch time: 0.052 trainign loss: 6.4679 avg training loss: 6.9782
batch: [20450/21305] batch time: 0.063 trainign loss: 4.8257 avg training loss: 6.9782
batch: [20460/21305] batch time: 0.056 trainign loss: 6.9242 avg training loss: 6.9782
batch: [20470/21305] batch time: 0.050 trainign loss: 4.7165 avg training loss: 6.9781
batch: [20480/21305] batch time: 0.056 trainign loss: 0.0465 avg training loss: 6.9779
batch: [20490/21305] batch time: 0.062 trainign loss: 13.6015 avg training loss: 6.9777
batch: [20500/21305] batch time: 0.059 trainign loss: 7.4375 avg training loss: 6.9777
batch: [20510/21305] batch time: 0.052 trainign loss: 6.9441 avg training loss: 6.9778
batch: [20520/21305] batch time: 0.056 trainign loss: 6.3740 avg training loss: 6.9777
batch: [20530/21305] batch time: 0.061 trainign loss: 6.0003 avg training loss: 6.9776
batch: [20540/21305] batch time: 0.054 trainign loss: 7.5487 avg training loss: 6.9775
batch: [20550/21305] batch time: 0.276 trainign loss: 7.0823 avg training loss: 6.9775
batch: [20560/21305] batch time: 0.053 trainign loss: 6.3246 avg training loss: 6.9775
batch: [20570/21305] batch time: 0.058 trainign loss: 7.4202 avg training loss: 6.9775
batch: [20580/21305] batch time: 0.051 trainign loss: 6.5785 avg training loss: 6.9775
batch: [20590/21305] batch time: 0.052 trainign loss: 5.9314 avg training loss: 6.9774
batch: [20600/21305] batch time: 0.058 trainign loss: 6.4077 avg training loss: 6.9774
batch: [20610/21305] batch time: 0.063 trainign loss: 7.0256 avg training loss: 6.9774
batch: [20620/21305] batch time: 0.054 trainign loss: 6.8657 avg training loss: 6.9774
batch: [20630/21305] batch time: 0.056 trainign loss: 6.3713 avg training loss: 6.9774
batch: [20640/21305] batch time: 0.063 trainign loss: 6.5196 avg training loss: 6.9773
batch: [20650/21305] batch time: 0.058 trainign loss: 6.6799 avg training loss: 6.9773
batch: [20660/21305] batch time: 0.053 trainign loss: 5.3856 avg training loss: 6.9773
batch: [20670/21305] batch time: 0.063 trainign loss: 7.5421 avg training loss: 6.9772
batch: [20680/21305] batch time: 0.051 trainign loss: 5.1980 avg training loss: 6.9772
batch: [20690/21305] batch time: 0.056 trainign loss: 6.7363 avg training loss: 6.9772
batch: [20700/21305] batch time: 0.051 trainign loss: 6.9206 avg training loss: 6.9772
batch: [20710/21305] batch time: 0.062 trainign loss: 6.9505 avg training loss: 6.9771
batch: [20720/21305] batch time: 0.062 trainign loss: 7.1348 avg training loss: 6.9771
batch: [20730/21305] batch time: 0.056 trainign loss: 6.4239 avg training loss: 6.9771
batch: [20740/21305] batch time: 0.056 trainign loss: 6.4747 avg training loss: 6.9771
batch: [20750/21305] batch time: 0.056 trainign loss: 5.7450 avg training loss: 6.9771
batch: [20760/21305] batch time: 0.062 trainign loss: 7.0343 avg training loss: 6.9771
batch: [20770/21305] batch time: 0.056 trainign loss: 6.4173 avg training loss: 6.9771
batch: [20780/21305] batch time: 0.063 trainign loss: 6.6486 avg training loss: 6.9771
batch: [20790/21305] batch time: 0.058 trainign loss: 5.7964 avg training loss: 6.9770
batch: [20800/21305] batch time: 0.053 trainign loss: 6.0316 avg training loss: 6.9770
batch: [20810/21305] batch time: 0.056 trainign loss: 7.0777 avg training loss: 6.9770
batch: [20820/21305] batch time: 0.451 trainign loss: 6.3109 avg training loss: 6.9770
batch: [20830/21305] batch time: 0.056 trainign loss: 6.5771 avg training loss: 6.9770
batch: [20840/21305] batch time: 0.188 trainign loss: 5.5836 avg training loss: 6.9769
batch: [20850/21305] batch time: 0.063 trainign loss: 5.6771 avg training loss: 6.9768
batch: [20860/21305] batch time: 0.673 trainign loss: 6.2452 avg training loss: 6.9768
batch: [20870/21305] batch time: 0.057 trainign loss: 6.6450 avg training loss: 6.9768
batch: [20880/21305] batch time: 1.299 trainign loss: 5.5616 avg training loss: 6.9767
batch: [20890/21305] batch time: 0.057 trainign loss: 7.3085 avg training loss: 6.9767
batch: [20900/21305] batch time: 0.642 trainign loss: 4.2101 avg training loss: 6.9767
batch: [20910/21305] batch time: 0.056 trainign loss: 6.2949 avg training loss: 6.9766
batch: [20920/21305] batch time: 0.062 trainign loss: 4.1317 avg training loss: 6.9766
batch: [20930/21305] batch time: 0.058 trainign loss: 8.0289 avg training loss: 6.9765
batch: [20940/21305] batch time: 0.056 trainign loss: 2.3171 avg training loss: 6.9763
batch: [20950/21305] batch time: 0.058 trainign loss: 7.0449 avg training loss: 6.9763
batch: [20960/21305] batch time: 0.054 trainign loss: 6.8818 avg training loss: 6.9764
batch: [20970/21305] batch time: 0.062 trainign loss: 5.2066 avg training loss: 6.9763
batch: [20980/21305] batch time: 0.055 trainign loss: 7.1585 avg training loss: 6.9763
batch: [20990/21305] batch time: 0.056 trainign loss: 7.3508 avg training loss: 6.9763
batch: [21000/21305] batch time: 0.052 trainign loss: 6.1174 avg training loss: 6.9763
batch: [21010/21305] batch time: 0.059 trainign loss: 6.4759 avg training loss: 6.9763
batch: [21020/21305] batch time: 0.055 trainign loss: 5.9554 avg training loss: 6.9762
batch: [21030/21305] batch time: 0.062 trainign loss: 6.9401 avg training loss: 6.9762
batch: [21040/21305] batch time: 0.058 trainign loss: 7.0518 avg training loss: 6.9762
batch: [21050/21305] batch time: 0.056 trainign loss: 5.9710 avg training loss: 6.9762
batch: [21060/21305] batch time: 0.051 trainign loss: 5.9383 avg training loss: 6.9761
batch: [21070/21305] batch time: 0.056 trainign loss: 5.6609 avg training loss: 6.9761
batch: [21080/21305] batch time: 0.053 trainign loss: 5.9417 avg training loss: 6.9760
batch: [21090/21305] batch time: 0.057 trainign loss: 7.0882 avg training loss: 6.9759
batch: [21100/21305] batch time: 0.056 trainign loss: 5.0219 avg training loss: 6.9758
batch: [21110/21305] batch time: 0.057 trainign loss: 7.1940 avg training loss: 6.9758
batch: [21120/21305] batch time: 0.056 trainign loss: 7.2269 avg training loss: 6.9758
batch: [21130/21305] batch time: 0.060 trainign loss: 7.1043 avg training loss: 6.9757
batch: [21140/21305] batch time: 0.063 trainign loss: 6.8453 avg training loss: 6.9757
batch: [21150/21305] batch time: 0.063 trainign loss: 6.9042 avg training loss: 6.9757
batch: [21160/21305] batch time: 0.059 trainign loss: 6.3496 avg training loss: 6.9756
batch: [21170/21305] batch time: 0.612 trainign loss: 4.8810 avg training loss: 6.9755
batch: [21180/21305] batch time: 0.058 trainign loss: 7.0553 avg training loss: 6.9755
batch: [21190/21305] batch time: 0.503 trainign loss: 6.7199 avg training loss: 6.9755
batch: [21200/21305] batch time: 0.057 trainign loss: 6.2101 avg training loss: 6.9755
batch: [21210/21305] batch time: 1.840 trainign loss: 6.2787 avg training loss: 6.9755
batch: [21220/21305] batch time: 0.062 trainign loss: 6.2059 avg training loss: 6.9754
batch: [21230/21305] batch time: 0.534 trainign loss: 6.6239 avg training loss: 6.9754
batch: [21240/21305] batch time: 0.063 trainign loss: 5.3929 avg training loss: 6.9754
batch: [21250/21305] batch time: 0.498 trainign loss: 6.5667 avg training loss: 6.9753
batch: [21260/21305] batch time: 0.060 trainign loss: 6.9351 avg training loss: 6.9753
batch: [21270/21305] batch time: 1.552 trainign loss: 3.1463 avg training loss: 6.9752
batch: [21280/21305] batch time: 0.057 trainign loss: 5.7890 avg training loss: 6.9752
batch: [21290/21305] batch time: 1.765 trainign loss: 4.4386 avg training loss: 6.9751
batch: [21300/21305] batch time: 0.053 trainign loss: 7.4742 avg training loss: 6.9751
