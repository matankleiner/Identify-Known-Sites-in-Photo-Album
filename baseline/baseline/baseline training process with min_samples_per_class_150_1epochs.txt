total batches: 3682
Epoch: 1
----------------------------------------------------------------------
batch: [0/3682] batch time: 0.567 trainign loss: 6.7525 avg training loss: 6.7525
batch: [10/3682] batch time: 0.059 trainign loss: 6.8711 avg training loss: 6.7761
batch: [20/3682] batch time: 0.218 trainign loss: 6.7861 avg training loss: 6.7828
batch: [30/3682] batch time: 0.057 trainign loss: 6.8529 avg training loss: 6.8045
batch: [40/3682] batch time: 0.241 trainign loss: 6.7966 avg training loss: 6.7964
batch: [50/3682] batch time: 0.058 trainign loss: 6.8816 avg training loss: 6.8008
batch: [60/3682] batch time: 0.241 trainign loss: 6.8132 avg training loss: 6.8109
batch: [70/3682] batch time: 0.058 trainign loss: 6.8119 avg training loss: 6.8088
batch: [80/3682] batch time: 0.289 trainign loss: 6.8267 avg training loss: 6.8114
batch: [90/3682] batch time: 0.059 trainign loss: 6.7995 avg training loss: 6.8113
batch: [100/3682] batch time: 0.251 trainign loss: 6.7064 avg training loss: 6.8102
batch: [110/3682] batch time: 0.058 trainign loss: 6.8323 avg training loss: 6.8089
batch: [120/3682] batch time: 0.268 trainign loss: 6.7912 avg training loss: 6.8089
batch: [130/3682] batch time: 0.057 trainign loss: 6.8473 avg training loss: 6.8085
batch: [140/3682] batch time: 0.223 trainign loss: 6.8092 avg training loss: 6.8043
batch: [150/3682] batch time: 0.057 trainign loss: 6.7010 avg training loss: 6.8046
batch: [160/3682] batch time: 0.244 trainign loss: 6.8629 avg training loss: 6.7800
batch: [170/3682] batch time: 0.058 trainign loss: 6.8829 avg training loss: 6.7835
batch: [180/3682] batch time: 0.251 trainign loss: 6.0815 avg training loss: 6.7674
batch: [190/3682] batch time: 0.058 trainign loss: 6.8126 avg training loss: 6.7658
batch: [200/3682] batch time: 0.262 trainign loss: 6.7983 avg training loss: 6.7691
batch: [210/3682] batch time: 0.058 trainign loss: 6.8386 avg training loss: 6.7710
batch: [220/3682] batch time: 0.213 trainign loss: 6.8182 avg training loss: 6.7719
batch: [230/3682] batch time: 0.058 trainign loss: 6.7098 avg training loss: 6.7742
batch: [240/3682] batch time: 0.243 trainign loss: 6.8412 avg training loss: 6.7583
batch: [250/3682] batch time: 0.058 trainign loss: 6.6252 avg training loss: 6.7604
batch: [260/3682] batch time: 0.252 trainign loss: 6.8511 avg training loss: 6.7634
batch: [270/3682] batch time: 0.058 trainign loss: 6.8345 avg training loss: 6.7661
batch: [280/3682] batch time: 0.227 trainign loss: 6.8500 avg training loss: 6.7665
batch: [290/3682] batch time: 0.058 trainign loss: 6.9104 avg training loss: 6.7695
batch: [300/3682] batch time: 0.240 trainign loss: 6.4440 avg training loss: 6.7696
batch: [310/3682] batch time: 0.056 trainign loss: 6.8742 avg training loss: 6.7656
batch: [320/3682] batch time: 0.259 trainign loss: 6.3121 avg training loss: 6.7624
batch: [330/3682] batch time: 0.057 trainign loss: 7.0277 avg training loss: 6.7410
batch: [340/3682] batch time: 0.250 trainign loss: 5.5446 avg training loss: 6.7303
batch: [350/3682] batch time: 0.057 trainign loss: 2.9672 avg training loss: 6.6578
batch: [360/3682] batch time: 0.284 trainign loss: 8.5058 avg training loss: 6.5776
batch: [370/3682] batch time: 0.058 trainign loss: 7.3865 avg training loss: 6.6276
batch: [380/3682] batch time: 0.260 trainign loss: 6.9235 avg training loss: 6.6420
batch: [390/3682] batch time: 0.058 trainign loss: 6.6890 avg training loss: 6.6462
batch: [400/3682] batch time: 0.287 trainign loss: 6.8704 avg training loss: 6.6510
batch: [410/3682] batch time: 0.057 trainign loss: 6.8676 avg training loss: 6.6551
batch: [420/3682] batch time: 0.273 trainign loss: 6.8107 avg training loss: 6.6584
batch: [430/3682] batch time: 0.059 trainign loss: 6.2720 avg training loss: 6.6518
batch: [440/3682] batch time: 0.239 trainign loss: 6.6883 avg training loss: 6.6543
batch: [450/3682] batch time: 0.058 trainign loss: 6.7185 avg training loss: 6.6550
batch: [460/3682] batch time: 0.255 trainign loss: 6.7126 avg training loss: 6.6530
batch: [470/3682] batch time: 0.079 trainign loss: 6.6476 avg training loss: 6.6413
batch: [480/3682] batch time: 0.235 trainign loss: 6.7391 avg training loss: 6.6415
batch: [490/3682] batch time: 0.058 trainign loss: 5.3462 avg training loss: 6.6359
batch: [500/3682] batch time: 0.249 trainign loss: 6.6735 avg training loss: 6.6358
batch: [510/3682] batch time: 0.064 trainign loss: 6.9286 avg training loss: 6.6324
batch: [520/3682] batch time: 0.241 trainign loss: 6.8886 avg training loss: 6.6283
batch: [530/3682] batch time: 0.059 trainign loss: 6.5594 avg training loss: 6.6296
batch: [540/3682] batch time: 0.259 trainign loss: 6.0376 avg training loss: 6.6281
batch: [550/3682] batch time: 0.058 trainign loss: 6.5610 avg training loss: 6.6279
batch: [560/3682] batch time: 0.247 trainign loss: 7.0143 avg training loss: 6.6223
batch: [570/3682] batch time: 0.059 trainign loss: 6.6274 avg training loss: 6.6242
batch: [580/3682] batch time: 0.265 trainign loss: 6.5787 avg training loss: 6.6245
batch: [590/3682] batch time: 0.064 trainign loss: 6.8717 avg training loss: 6.6272
batch: [600/3682] batch time: 0.235 trainign loss: 6.6273 avg training loss: 6.6291
batch: [610/3682] batch time: 0.086 trainign loss: 6.7615 avg training loss: 6.6286
batch: [620/3682] batch time: 0.202 trainign loss: 6.8333 avg training loss: 6.6317
batch: [630/3682] batch time: 0.114 trainign loss: 6.7900 avg training loss: 6.6342
batch: [640/3682] batch time: 0.199 trainign loss: 6.5427 avg training loss: 6.6362
batch: [650/3682] batch time: 0.139 trainign loss: 4.4252 avg training loss: 6.6098
batch: [660/3682] batch time: 0.195 trainign loss: 7.3686 avg training loss: 6.6187
batch: [670/3682] batch time: 0.146 trainign loss: 5.5136 avg training loss: 6.6181
batch: [680/3682] batch time: 0.175 trainign loss: 6.1648 avg training loss: 6.5989
batch: [690/3682] batch time: 0.154 trainign loss: 6.7597 avg training loss: 6.6016
batch: [700/3682] batch time: 0.116 trainign loss: 6.9400 avg training loss: 6.6022
batch: [710/3682] batch time: 0.179 trainign loss: 6.8884 avg training loss: 6.5971
batch: [720/3682] batch time: 0.115 trainign loss: 4.9955 avg training loss: 6.5913
batch: [730/3682] batch time: 0.190 trainign loss: 7.0070 avg training loss: 6.5799
batch: [740/3682] batch time: 0.082 trainign loss: 6.4019 avg training loss: 6.5839
batch: [750/3682] batch time: 0.203 trainign loss: 6.6502 avg training loss: 6.5817
batch: [760/3682] batch time: 0.098 trainign loss: 3.0497 avg training loss: 6.5631
batch: [770/3682] batch time: 0.214 trainign loss: 5.7766 avg training loss: 6.5622
batch: [780/3682] batch time: 0.110 trainign loss: 6.0828 avg training loss: 6.5581
batch: [790/3682] batch time: 0.171 trainign loss: 6.6734 avg training loss: 6.5602
batch: [800/3682] batch time: 0.205 trainign loss: 6.5991 avg training loss: 6.5611
batch: [810/3682] batch time: 0.156 trainign loss: 6.8251 avg training loss: 6.5609
batch: [820/3682] batch time: 0.144 trainign loss: 6.5567 avg training loss: 6.5627
batch: [830/3682] batch time: 0.192 trainign loss: 6.2326 avg training loss: 6.5606
batch: [840/3682] batch time: 0.121 trainign loss: 7.3530 avg training loss: 6.5401
batch: [850/3682] batch time: 0.173 trainign loss: 6.7653 avg training loss: 6.5482
batch: [860/3682] batch time: 0.122 trainign loss: 6.1591 avg training loss: 6.5470
batch: [870/3682] batch time: 0.188 trainign loss: 1.3417 avg training loss: 6.5135
batch: [880/3682] batch time: 0.078 trainign loss: 6.5717 avg training loss: 6.5241
batch: [890/3682] batch time: 0.222 trainign loss: 6.7211 avg training loss: 6.5242
batch: [900/3682] batch time: 0.070 trainign loss: 2.4236 avg training loss: 6.5062
batch: [910/3682] batch time: 0.222 trainign loss: 6.1780 avg training loss: 6.5071
batch: [920/3682] batch time: 0.059 trainign loss: 7.0749 avg training loss: 6.5062
batch: [930/3682] batch time: 0.219 trainign loss: 6.7314 avg training loss: 6.5074
batch: [940/3682] batch time: 0.060 trainign loss: 5.8385 avg training loss: 6.5087
batch: [950/3682] batch time: 0.205 trainign loss: 4.3288 avg training loss: 6.5031
batch: [960/3682] batch time: 0.749 trainign loss: 6.1565 avg training loss: 6.5034
batch: [970/3682] batch time: 1.710 trainign loss: 7.7529 avg training loss: 6.4928
batch: [980/3682] batch time: 0.484 trainign loss: 6.4547 avg training loss: 6.4972
batch: [990/3682] batch time: 2.212 trainign loss: 5.9442 avg training loss: 6.4979
batch: [1000/3682] batch time: 0.653 trainign loss: 5.8818 avg training loss: 6.4961
batch: [1010/3682] batch time: 1.783 trainign loss: 6.2779 avg training loss: 6.4967
batch: [1020/3682] batch time: 0.208 trainign loss: 6.7963 avg training loss: 6.4965
batch: [1030/3682] batch time: 2.645 trainign loss: 5.5222 avg training loss: 6.4952
batch: [1040/3682] batch time: 0.046 trainign loss: 6.2851 avg training loss: 6.4936
batch: [1050/3682] batch time: 2.847 trainign loss: 6.5647 avg training loss: 6.4947
batch: [1060/3682] batch time: 0.053 trainign loss: 5.8414 avg training loss: 6.4937
batch: [1070/3682] batch time: 2.009 trainign loss: 6.8787 avg training loss: 6.4956
batch: [1080/3682] batch time: 0.046 trainign loss: 6.4311 avg training loss: 6.4965
batch: [1090/3682] batch time: 2.489 trainign loss: 6.3118 avg training loss: 6.4973
batch: [1100/3682] batch time: 0.056 trainign loss: 6.5764 avg training loss: 6.4983
batch: [1110/3682] batch time: 2.152 trainign loss: 6.9200 avg training loss: 6.4980
batch: [1120/3682] batch time: 0.047 trainign loss: 6.7895 avg training loss: 6.4985
batch: [1130/3682] batch time: 1.894 trainign loss: 5.8873 avg training loss: 6.4990
batch: [1140/3682] batch time: 0.055 trainign loss: 6.7656 avg training loss: 6.5006
batch: [1150/3682] batch time: 1.892 trainign loss: 6.4277 avg training loss: 6.5014
batch: [1160/3682] batch time: 0.056 trainign loss: 6.9479 avg training loss: 6.4927
batch: [1170/3682] batch time: 2.825 trainign loss: 6.1616 avg training loss: 6.4925
batch: [1180/3682] batch time: 0.049 trainign loss: 5.4177 avg training loss: 6.4895
batch: [1190/3682] batch time: 1.355 trainign loss: 5.9954 avg training loss: 6.4905
batch: [1200/3682] batch time: 0.282 trainign loss: 5.8601 avg training loss: 6.4904
batch: [1210/3682] batch time: 0.190 trainign loss: 6.9233 avg training loss: 6.4894
batch: [1220/3682] batch time: 1.523 trainign loss: 5.5309 avg training loss: 6.4892
batch: [1230/3682] batch time: 0.046 trainign loss: 6.3340 avg training loss: 6.4885
batch: [1240/3682] batch time: 2.091 trainign loss: 6.2324 avg training loss: 6.4898
batch: [1250/3682] batch time: 0.056 trainign loss: 6.4877 avg training loss: 6.4892
batch: [1260/3682] batch time: 1.451 trainign loss: 6.5183 avg training loss: 6.4859
batch: [1270/3682] batch time: 0.049 trainign loss: 6.8418 avg training loss: 6.4806
batch: [1280/3682] batch time: 1.215 trainign loss: 6.0853 avg training loss: 6.4812
batch: [1290/3682] batch time: 0.046 trainign loss: 6.2778 avg training loss: 6.4804
batch: [1300/3682] batch time: 0.057 trainign loss: 2.9033 avg training loss: 6.4697
batch: [1310/3682] batch time: 0.046 trainign loss: 6.6040 avg training loss: 6.4695
batch: [1320/3682] batch time: 0.051 trainign loss: 6.4086 avg training loss: 6.4700
batch: [1330/3682] batch time: 0.046 trainign loss: 6.5534 avg training loss: 6.4662
batch: [1340/3682] batch time: 0.045 trainign loss: 5.0164 avg training loss: 6.4656
batch: [1350/3682] batch time: 0.046 trainign loss: 6.9483 avg training loss: 6.4599
batch: [1360/3682] batch time: 0.046 trainign loss: 6.8012 avg training loss: 6.4617
batch: [1370/3682] batch time: 0.049 trainign loss: 6.5071 avg training loss: 6.4623
batch: [1380/3682] batch time: 0.048 trainign loss: 5.2107 avg training loss: 6.4598
batch: [1390/3682] batch time: 0.048 trainign loss: 6.4696 avg training loss: 6.4548
batch: [1400/3682] batch time: 0.045 trainign loss: 7.0925 avg training loss: 6.4519
batch: [1410/3682] batch time: 0.055 trainign loss: 6.6688 avg training loss: 6.4542
batch: [1420/3682] batch time: 0.046 trainign loss: 6.5136 avg training loss: 6.4556
batch: [1430/3682] batch time: 0.043 trainign loss: 6.8719 avg training loss: 6.4571
batch: [1440/3682] batch time: 0.050 trainign loss: 6.8606 avg training loss: 6.4586
batch: [1450/3682] batch time: 0.046 trainign loss: 4.3457 avg training loss: 6.4557
batch: [1460/3682] batch time: 0.056 trainign loss: 0.0328 avg training loss: 6.4193
batch: [1470/3682] batch time: 0.055 trainign loss: 0.0025 avg training loss: 6.3757
batch: [1480/3682] batch time: 0.049 trainign loss: 7.0277 avg training loss: 6.3940
batch: [1490/3682] batch time: 0.045 trainign loss: 6.7250 avg training loss: 6.3956
batch: [1500/3682] batch time: 0.046 trainign loss: 6.9980 avg training loss: 6.3957
batch: [1510/3682] batch time: 0.046 trainign loss: 5.5149 avg training loss: 6.3908
batch: [1520/3682] batch time: 0.055 trainign loss: 6.7126 avg training loss: 6.3937
batch: [1530/3682] batch time: 0.046 trainign loss: 6.2774 avg training loss: 6.3940
batch: [1540/3682] batch time: 0.912 trainign loss: 4.4554 avg training loss: 6.3917
batch: [1550/3682] batch time: 0.046 trainign loss: 5.7126 avg training loss: 6.3928
batch: [1560/3682] batch time: 2.580 trainign loss: 6.8115 avg training loss: 6.3936
batch: [1570/3682] batch time: 0.046 trainign loss: 6.2612 avg training loss: 6.3935
batch: [1580/3682] batch time: 2.240 trainign loss: 6.6492 avg training loss: 6.3954
batch: [1590/3682] batch time: 0.046 trainign loss: 6.5806 avg training loss: 6.3939
batch: [1600/3682] batch time: 2.273 trainign loss: 7.0281 avg training loss: 6.3944
batch: [1610/3682] batch time: 0.045 trainign loss: 6.4430 avg training loss: 6.3939
batch: [1620/3682] batch time: 2.129 trainign loss: 6.4351 avg training loss: 6.3932
batch: [1630/3682] batch time: 0.046 trainign loss: 6.7431 avg training loss: 6.3946
batch: [1640/3682] batch time: 1.479 trainign loss: 6.8858 avg training loss: 6.3958
batch: [1650/3682] batch time: 0.046 trainign loss: 6.6438 avg training loss: 6.3973
batch: [1660/3682] batch time: 0.456 trainign loss: 6.7315 avg training loss: 6.3983
batch: [1670/3682] batch time: 0.048 trainign loss: 6.6923 avg training loss: 6.4003
batch: [1680/3682] batch time: 0.046 trainign loss: 5.3566 avg training loss: 6.4008
batch: [1690/3682] batch time: 0.049 trainign loss: 6.5881 avg training loss: 6.4018
batch: [1700/3682] batch time: 0.046 trainign loss: 6.1827 avg training loss: 6.4032
batch: [1710/3682] batch time: 0.422 trainign loss: 4.3299 avg training loss: 6.4019
batch: [1720/3682] batch time: 0.046 trainign loss: 6.6962 avg training loss: 6.4039
batch: [1730/3682] batch time: 0.291 trainign loss: 6.3062 avg training loss: 6.4035
batch: [1740/3682] batch time: 0.047 trainign loss: 7.0785 avg training loss: 6.4036
batch: [1750/3682] batch time: 1.621 trainign loss: 7.2509 avg training loss: 6.3979
batch: [1760/3682] batch time: 0.052 trainign loss: 6.9916 avg training loss: 6.4011
batch: [1770/3682] batch time: 0.918 trainign loss: 6.5575 avg training loss: 6.4027
batch: [1780/3682] batch time: 0.048 trainign loss: 6.6969 avg training loss: 6.4029
batch: [1790/3682] batch time: 0.291 trainign loss: 7.0172 avg training loss: 6.4039
batch: [1800/3682] batch time: 0.055 trainign loss: 4.6570 avg training loss: 6.4029
batch: [1810/3682] batch time: 0.410 trainign loss: 6.9789 avg training loss: 6.4045
batch: [1820/3682] batch time: 1.699 trainign loss: 6.8138 avg training loss: 6.4053
batch: [1830/3682] batch time: 0.473 trainign loss: 7.0532 avg training loss: 6.4055
batch: [1840/3682] batch time: 1.269 trainign loss: 7.0530 avg training loss: 6.4059
batch: [1850/3682] batch time: 0.046 trainign loss: 6.4440 avg training loss: 6.4072
batch: [1860/3682] batch time: 0.600 trainign loss: 3.7450 avg training loss: 6.4044
batch: [1870/3682] batch time: 0.046 trainign loss: 6.1460 avg training loss: 6.4020
batch: [1880/3682] batch time: 0.047 trainign loss: 6.6608 avg training loss: 6.3989
batch: [1890/3682] batch time: 0.057 trainign loss: 6.9059 avg training loss: 6.3983
batch: [1900/3682] batch time: 0.046 trainign loss: 6.3387 avg training loss: 6.3996
batch: [1910/3682] batch time: 0.278 trainign loss: 7.3195 avg training loss: 6.3926
batch: [1920/3682] batch time: 0.046 trainign loss: 6.2576 avg training loss: 6.3935
batch: [1930/3682] batch time: 1.014 trainign loss: 5.9489 avg training loss: 6.3951
batch: [1940/3682] batch time: 0.047 trainign loss: 6.7402 avg training loss: 6.3966
batch: [1950/3682] batch time: 0.994 trainign loss: 6.5254 avg training loss: 6.3979
batch: [1960/3682] batch time: 0.048 trainign loss: 6.5268 avg training loss: 6.3981
batch: [1970/3682] batch time: 0.812 trainign loss: 0.1822 avg training loss: 6.3797
batch: [1980/3682] batch time: 0.046 trainign loss: 7.8687 avg training loss: 6.3740
batch: [1990/3682] batch time: 0.208 trainign loss: 5.8712 avg training loss: 6.3745
batch: [2000/3682] batch time: 0.046 trainign loss: 7.3905 avg training loss: 6.3747
batch: [2010/3682] batch time: 0.168 trainign loss: 7.3941 avg training loss: 6.3727
batch: [2020/3682] batch time: 0.048 trainign loss: 6.4817 avg training loss: 6.3743
batch: [2030/3682] batch time: 0.420 trainign loss: 6.8892 avg training loss: 6.3746
batch: [2040/3682] batch time: 0.046 trainign loss: 6.8714 avg training loss: 6.3719
batch: [2050/3682] batch time: 0.057 trainign loss: 6.9527 avg training loss: 6.3733
batch: [2060/3682] batch time: 0.047 trainign loss: 7.4094 avg training loss: 6.3717
batch: [2070/3682] batch time: 0.057 trainign loss: 6.7140 avg training loss: 6.3734
batch: [2080/3682] batch time: 0.057 trainign loss: 6.3025 avg training loss: 6.3740
batch: [2090/3682] batch time: 0.044 trainign loss: 5.3572 avg training loss: 6.3750
batch: [2100/3682] batch time: 0.047 trainign loss: 5.4656 avg training loss: 6.3758
batch: [2110/3682] batch time: 0.181 trainign loss: 11.1305 avg training loss: 6.3605
batch: [2120/3682] batch time: 0.213 trainign loss: 6.1830 avg training loss: 6.3674
batch: [2130/3682] batch time: 0.046 trainign loss: 6.3489 avg training loss: 6.3679
batch: [2140/3682] batch time: 0.046 trainign loss: 6.7864 avg training loss: 6.3698
batch: [2150/3682] batch time: 0.044 trainign loss: 5.3055 avg training loss: 6.3695
batch: [2160/3682] batch time: 0.047 trainign loss: 6.6923 avg training loss: 6.3716
batch: [2170/3682] batch time: 0.046 trainign loss: 6.3945 avg training loss: 6.3723
batch: [2180/3682] batch time: 0.048 trainign loss: 6.9938 avg training loss: 6.3716
batch: [2190/3682] batch time: 0.046 trainign loss: 6.2838 avg training loss: 6.3728
batch: [2200/3682] batch time: 0.046 trainign loss: 5.3380 avg training loss: 6.3713
batch: [2210/3682] batch time: 0.050 trainign loss: 6.9804 avg training loss: 6.3706
batch: [2220/3682] batch time: 0.050 trainign loss: 6.5591 avg training loss: 6.3702
batch: [2230/3682] batch time: 0.048 trainign loss: 3.6249 avg training loss: 6.3679
batch: [2240/3682] batch time: 0.057 trainign loss: 0.0288 avg training loss: 6.3427
batch: [2250/3682] batch time: 0.044 trainign loss: 0.0007 avg training loss: 6.3145
batch: [2260/3682] batch time: 0.045 trainign loss: 16.6480 avg training loss: 6.3042
batch: [2270/3682] batch time: 0.047 trainign loss: 7.1714 avg training loss: 6.3143
batch: [2280/3682] batch time: 0.046 trainign loss: 7.0304 avg training loss: 6.3158
batch: [2290/3682] batch time: 0.045 trainign loss: 6.3052 avg training loss: 6.3182
batch: [2300/3682] batch time: 0.048 trainign loss: 7.0229 avg training loss: 6.3194
batch: [2310/3682] batch time: 0.049 trainign loss: 6.6677 avg training loss: 6.3212
batch: [2320/3682] batch time: 0.057 trainign loss: 6.6092 avg training loss: 6.3228
batch: [2330/3682] batch time: 0.043 trainign loss: 6.9576 avg training loss: 6.3245
batch: [2340/3682] batch time: 0.608 trainign loss: 5.3402 avg training loss: 6.3247
batch: [2350/3682] batch time: 0.045 trainign loss: 7.2939 avg training loss: 6.3246
batch: [2360/3682] batch time: 0.880 trainign loss: 6.9232 avg training loss: 6.3251
batch: [2370/3682] batch time: 0.046 trainign loss: 7.2551 avg training loss: 6.3265
batch: [2380/3682] batch time: 1.535 trainign loss: 6.1504 avg training loss: 6.3284
batch: [2390/3682] batch time: 0.056 trainign loss: 6.2006 avg training loss: 6.3295
batch: [2400/3682] batch time: 0.826 trainign loss: 5.9919 avg training loss: 6.3304
batch: [2410/3682] batch time: 0.047 trainign loss: 5.7549 avg training loss: 6.3312
batch: [2420/3682] batch time: 1.756 trainign loss: 4.7481 avg training loss: 6.3315
batch: [2430/3682] batch time: 0.046 trainign loss: 7.3912 avg training loss: 6.3292
batch: [2440/3682] batch time: 1.466 trainign loss: 5.6104 avg training loss: 6.3304
batch: [2450/3682] batch time: 0.046 trainign loss: 7.1804 avg training loss: 6.3308
batch: [2460/3682] batch time: 1.956 trainign loss: 5.5621 avg training loss: 6.3314
batch: [2470/3682] batch time: 0.055 trainign loss: 0.0477 avg training loss: 6.3138
batch: [2480/3682] batch time: 1.924 trainign loss: 0.0002 avg training loss: 6.2883
batch: [2490/3682] batch time: 0.046 trainign loss: 0.0000 avg training loss: 6.2631
batch: [2500/3682] batch time: 1.753 trainign loss: 0.0000 avg training loss: 6.2381
batch: [2510/3682] batch time: 0.057 trainign loss: 0.0000 avg training loss: 6.2132
batch: [2520/3682] batch time: 1.939 trainign loss: 0.0000 avg training loss: 6.1886
batch: [2530/3682] batch time: 0.056 trainign loss: 0.0000 avg training loss: 6.1641
batch: [2540/3682] batch time: 2.042 trainign loss: 0.0000 avg training loss: 6.1399
batch: [2550/3682] batch time: 0.045 trainign loss: 0.0000 avg training loss: 6.1158
batch: [2560/3682] batch time: 2.098 trainign loss: 7.1331 avg training loss: 6.1268
batch: [2570/3682] batch time: 0.045 trainign loss: 4.5067 avg training loss: 6.1264
batch: [2580/3682] batch time: 2.470 trainign loss: 8.5468 avg training loss: 6.1101
batch: [2590/3682] batch time: 0.047 trainign loss: 6.5579 avg training loss: 6.1199
batch: [2600/3682] batch time: 2.277 trainign loss: 6.8477 avg training loss: 6.1221
batch: [2610/3682] batch time: 0.046 trainign loss: 5.3599 avg training loss: 6.1231
batch: [2620/3682] batch time: 2.472 trainign loss: 6.8716 avg training loss: 6.1257
batch: [2630/3682] batch time: 0.046 trainign loss: 6.9577 avg training loss: 6.1268
batch: [2640/3682] batch time: 2.131 trainign loss: 8.1107 avg training loss: 6.1249
batch: [2650/3682] batch time: 0.055 trainign loss: 6.3546 avg training loss: 6.1249
batch: [2660/3682] batch time: 1.881 trainign loss: 6.8357 avg training loss: 6.1279
batch: [2670/3682] batch time: 0.049 trainign loss: 6.4134 avg training loss: 6.1290
batch: [2680/3682] batch time: 1.193 trainign loss: 7.1099 avg training loss: 6.1315
batch: [2690/3682] batch time: 0.046 trainign loss: 6.9508 avg training loss: 6.1337
batch: [2700/3682] batch time: 1.804 trainign loss: 6.1850 avg training loss: 6.1344
batch: [2710/3682] batch time: 0.092 trainign loss: 3.9337 avg training loss: 6.1339
batch: [2720/3682] batch time: 1.946 trainign loss: 7.0076 avg training loss: 6.1356
batch: [2730/3682] batch time: 0.057 trainign loss: 4.5582 avg training loss: 6.1362
batch: [2740/3682] batch time: 2.574 trainign loss: 9.9011 avg training loss: 6.1212
batch: [2750/3682] batch time: 0.046 trainign loss: 6.5187 avg training loss: 6.1282
batch: [2760/3682] batch time: 2.766 trainign loss: 7.6086 avg training loss: 6.1292
batch: [2770/3682] batch time: 0.057 trainign loss: 7.7840 avg training loss: 6.1278
batch: [2780/3682] batch time: 2.209 trainign loss: 6.5752 avg training loss: 6.1314
batch: [2790/3682] batch time: 0.056 trainign loss: 6.7516 avg training loss: 6.1334
batch: [2800/3682] batch time: 0.244 trainign loss: 6.2653 avg training loss: 6.1356
batch: [2810/3682] batch time: 0.056 trainign loss: 6.6805 avg training loss: 6.1372
batch: [2820/3682] batch time: 0.279 trainign loss: 6.4335 avg training loss: 6.1391
batch: [2830/3682] batch time: 0.056 trainign loss: 5.5525 avg training loss: 6.1392
batch: [2840/3682] batch time: 0.231 trainign loss: 7.2018 avg training loss: 6.1408
batch: [2850/3682] batch time: 0.056 trainign loss: 6.6492 avg training loss: 6.1429
batch: [2860/3682] batch time: 0.237 trainign loss: 7.2477 avg training loss: 6.1441
batch: [2870/3682] batch time: 0.056 trainign loss: 6.7308 avg training loss: 6.1465
batch: [2880/3682] batch time: 0.249 trainign loss: 0.5191 avg training loss: 6.1371
batch: [2890/3682] batch time: 0.056 trainign loss: 7.2721 avg training loss: 6.1424
batch: [2900/3682] batch time: 0.254 trainign loss: 5.3166 avg training loss: 6.1436
batch: [2910/3682] batch time: 0.056 trainign loss: 7.2094 avg training loss: 6.1451
batch: [2920/3682] batch time: 0.259 trainign loss: 5.1034 avg training loss: 6.1378
batch: [2930/3682] batch time: 0.056 trainign loss: 6.7448 avg training loss: 6.1448
batch: [2940/3682] batch time: 0.289 trainign loss: 7.1685 avg training loss: 6.1449
batch: [2950/3682] batch time: 0.056 trainign loss: 6.7210 avg training loss: 6.1454
batch: [2960/3682] batch time: 0.238 trainign loss: 6.9282 avg training loss: 6.1475
batch: [2970/3682] batch time: 0.057 trainign loss: 6.5374 avg training loss: 6.1494
batch: [2980/3682] batch time: 0.253 trainign loss: 6.8647 avg training loss: 6.1514
batch: [2990/3682] batch time: 0.057 trainign loss: 1.9290 avg training loss: 6.1479
batch: [3000/3682] batch time: 0.221 trainign loss: 7.9139 avg training loss: 6.1487
batch: [3010/3682] batch time: 0.057 trainign loss: 6.7041 avg training loss: 6.1512
batch: [3020/3682] batch time: 0.212 trainign loss: 7.3184 avg training loss: 6.1532
batch: [3030/3682] batch time: 0.057 trainign loss: 6.2660 avg training loss: 6.1555
batch: [3040/3682] batch time: 0.219 trainign loss: 1.2592 avg training loss: 6.1503
batch: [3050/3682] batch time: 0.057 trainign loss: 6.4021 avg training loss: 6.1499
batch: [3060/3682] batch time: 0.294 trainign loss: 6.5450 avg training loss: 6.1535
batch: [3070/3682] batch time: 0.057 trainign loss: 7.2942 avg training loss: 6.1554
batch: [3080/3682] batch time: 0.129 trainign loss: 6.9733 avg training loss: 6.1572
batch: [3090/3682] batch time: 0.057 trainign loss: 6.2212 avg training loss: 6.1587
batch: [3100/3682] batch time: 0.268 trainign loss: 7.2231 avg training loss: 6.1593
batch: [3110/3682] batch time: 0.058 trainign loss: 6.2057 avg training loss: 6.1607
batch: [3120/3682] batch time: 0.262 trainign loss: 7.3463 avg training loss: 6.1622
batch: [3130/3682] batch time: 0.057 trainign loss: 9.2084 avg training loss: 6.1572
batch: [3140/3682] batch time: 0.274 trainign loss: 7.3328 avg training loss: 6.1624
batch: [3150/3682] batch time: 0.056 trainign loss: 7.1665 avg training loss: 6.1640
batch: [3160/3682] batch time: 0.265 trainign loss: 6.3183 avg training loss: 6.1662
batch: [3170/3682] batch time: 0.057 trainign loss: 7.0552 avg training loss: 6.1684
batch: [3180/3682] batch time: 0.274 trainign loss: 4.3403 avg training loss: 6.1688
batch: [3190/3682] batch time: 0.056 trainign loss: 9.6199 avg training loss: 6.1659
batch: [3200/3682] batch time: 0.295 trainign loss: 4.2155 avg training loss: 6.1668
batch: [3210/3682] batch time: 0.056 trainign loss: 9.9011 avg training loss: 6.1637
batch: [3220/3682] batch time: 0.266 trainign loss: 7.2251 avg training loss: 6.1675
batch: [3230/3682] batch time: 0.056 trainign loss: 6.7971 avg training loss: 6.1691
batch: [3240/3682] batch time: 0.272 trainign loss: 1.3927 avg training loss: 6.1628
batch: [3250/3682] batch time: 0.056 trainign loss: 9.7536 avg training loss: 6.1627
batch: [3260/3682] batch time: 0.268 trainign loss: 7.1753 avg training loss: 6.1658
batch: [3270/3682] batch time: 0.056 trainign loss: 6.9076 avg training loss: 6.1675
batch: [3280/3682] batch time: 0.277 trainign loss: 7.5116 avg training loss: 6.1689
batch: [3290/3682] batch time: 0.056 trainign loss: 5.4994 avg training loss: 6.1701
batch: [3300/3682] batch time: 0.291 trainign loss: 6.6391 avg training loss: 6.1726
batch: [3310/3682] batch time: 0.056 trainign loss: 6.9217 avg training loss: 6.1750
batch: [3320/3682] batch time: 0.292 trainign loss: 7.3271 avg training loss: 6.1775
batch: [3330/3682] batch time: 0.056 trainign loss: 7.1955 avg training loss: 6.1799
batch: [3340/3682] batch time: 0.211 trainign loss: 6.8686 avg training loss: 6.1817
batch: [3350/3682] batch time: 0.056 trainign loss: 7.0612 avg training loss: 6.1841
batch: [3360/3682] batch time: 0.275 trainign loss: 7.2134 avg training loss: 6.1868
batch: [3370/3682] batch time: 0.056 trainign loss: 7.1827 avg training loss: 6.1889
batch: [3380/3682] batch time: 0.268 trainign loss: 3.8190 avg training loss: 6.1882
batch: [3390/3682] batch time: 0.057 trainign loss: 8.0196 avg training loss: 6.1891
batch: [3400/3682] batch time: 0.269 trainign loss: 6.9621 avg training loss: 6.1915
batch: [3410/3682] batch time: 0.056 trainign loss: 7.1415 avg training loss: 6.1911
batch: [3420/3682] batch time: 0.248 trainign loss: 7.5405 avg training loss: 6.1898
batch: [3430/3682] batch time: 0.056 trainign loss: 6.9881 avg training loss: 6.1928
batch: [3440/3682] batch time: 0.243 trainign loss: 4.8971 avg training loss: 6.1936
batch: [3450/3682] batch time: 0.056 trainign loss: 7.4010 avg training loss: 6.1946
batch: [3460/3682] batch time: 0.256 trainign loss: 6.5323 avg training loss: 6.1960
batch: [3470/3682] batch time: 0.056 trainign loss: 5.8355 avg training loss: 6.1952
batch: [3480/3682] batch time: 0.265 trainign loss: 6.9987 avg training loss: 6.1986
batch: [3490/3682] batch time: 0.056 trainign loss: 7.1175 avg training loss: 6.2005
batch: [3500/3682] batch time: 0.258 trainign loss: 5.2973 avg training loss: 6.2017
batch: [3510/3682] batch time: 0.057 trainign loss: 7.1703 avg training loss: 6.2022
batch: [3520/3682] batch time: 0.266 trainign loss: 6.6625 avg training loss: 6.2039
batch: [3530/3682] batch time: 0.057 trainign loss: 7.1388 avg training loss: 6.2059
batch: [3540/3682] batch time: 0.265 trainign loss: 7.1945 avg training loss: 6.2069
batch: [3550/3682] batch time: 0.057 trainign loss: 6.2838 avg training loss: 6.2088
batch: [3560/3682] batch time: 0.159 trainign loss: 0.0808 avg training loss: 6.1983
batch: [3570/3682] batch time: 0.057 trainign loss: 7.2500 avg training loss: 6.2034
batch: [3580/3682] batch time: 0.287 trainign loss: 7.0019 avg training loss: 6.2042
batch: [3590/3682] batch time: 0.057 trainign loss: 7.1508 avg training loss: 6.2069
batch: [3600/3682] batch time: 0.250 trainign loss: 6.7843 avg training loss: 6.2091
batch: [3610/3682] batch time: 0.057 trainign loss: 6.8895 avg training loss: 6.2112
batch: [3620/3682] batch time: 0.280 trainign loss: 6.4946 avg training loss: 6.2113
batch: [3630/3682] batch time: 0.057 trainign loss: 6.8950 avg training loss: 6.2125
batch: [3640/3682] batch time: 0.241 trainign loss: 7.1173 avg training loss: 6.2148
batch: [3650/3682] batch time: 0.057 trainign loss: 7.4440 avg training loss: 6.2168
batch: [3660/3682] batch time: 0.274 trainign loss: 7.1343 avg training loss: 6.2191
batch: [3670/3682] batch time: 0.057 trainign loss: 7.3544 avg training loss: 6.2200
batch: [3680/3682] batch time: 0.186 trainign loss: 6.7261 avg training loss: 6.2219
