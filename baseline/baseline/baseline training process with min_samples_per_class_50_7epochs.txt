total batches: 10547
Epoch: 1
----------------------------------------------------------------------
batch: [0/10547] batch time: 0.567 trainign loss: 8.8051 avg training loss: 8.8051
batch: [10/10547] batch time: 0.056 trainign loss: 8.7623 avg training loss: 8.7744
batch: [20/10547] batch time: 0.222 trainign loss: 8.8073 avg training loss: 8.8022
batch: [30/10547] batch time: 0.058 trainign loss: 8.8205 avg training loss: 8.8053
batch: [40/10547] batch time: 0.223 trainign loss: 8.8826 avg training loss: 8.8181
batch: [50/10547] batch time: 0.056 trainign loss: 8.8322 avg training loss: 8.8195
batch: [60/10547] batch time: 0.104 trainign loss: 8.9201 avg training loss: 8.8231
batch: [70/10547] batch time: 0.056 trainign loss: 8.8451 avg training loss: 8.8289
batch: [80/10547] batch time: 0.111 trainign loss: 8.8679 avg training loss: 8.8294
batch: [90/10547] batch time: 0.056 trainign loss: 8.7405 avg training loss: 8.8248
batch: [100/10547] batch time: 0.057 trainign loss: 8.6797 avg training loss: 8.8194
batch: [110/10547] batch time: 0.056 trainign loss: 8.2135 avg training loss: 8.7839
batch: [120/10547] batch time: 0.052 trainign loss: 8.8704 avg training loss: 8.7834
batch: [130/10547] batch time: 0.786 trainign loss: 8.7633 avg training loss: 8.7849
batch: [140/10547] batch time: 0.235 trainign loss: 8.8826 avg training loss: 8.7894
batch: [150/10547] batch time: 0.054 trainign loss: 8.8172 avg training loss: 8.7906
batch: [160/10547] batch time: 1.180 trainign loss: 8.7850 avg training loss: 8.7920
batch: [170/10547] batch time: 0.046 trainign loss: 8.8368 avg training loss: 8.7926
batch: [180/10547] batch time: 2.083 trainign loss: 8.8375 avg training loss: 8.7942
batch: [190/10547] batch time: 0.047 trainign loss: 8.7837 avg training loss: 8.7935
batch: [200/10547] batch time: 0.054 trainign loss: 8.7532 avg training loss: 8.7931
batch: [210/10547] batch time: 0.090 trainign loss: 8.8005 avg training loss: 8.7926
batch: [220/10547] batch time: 0.054 trainign loss: 8.8585 avg training loss: 8.7940
batch: [230/10547] batch time: 0.821 trainign loss: 8.8044 avg training loss: 8.7957
batch: [240/10547] batch time: 0.054 trainign loss: 8.8223 avg training loss: 8.7939
batch: [250/10547] batch time: 1.085 trainign loss: 8.7966 avg training loss: 8.7950
batch: [260/10547] batch time: 0.044 trainign loss: 8.8254 avg training loss: 8.7953
batch: [270/10547] batch time: 0.225 trainign loss: 8.7058 avg training loss: 8.7928
batch: [280/10547] batch time: 0.055 trainign loss: 8.7119 avg training loss: 8.7921
batch: [290/10547] batch time: 0.055 trainign loss: 8.7515 avg training loss: 8.7891
batch: [300/10547] batch time: 0.249 trainign loss: 8.7797 avg training loss: 8.7887
batch: [310/10547] batch time: 0.055 trainign loss: 8.7307 avg training loss: 8.7861
batch: [320/10547] batch time: 2.462 trainign loss: 8.8917 avg training loss: 8.7880
batch: [330/10547] batch time: 0.047 trainign loss: 8.7813 avg training loss: 8.7887
batch: [340/10547] batch time: 1.456 trainign loss: 8.7712 avg training loss: 8.7843
batch: [350/10547] batch time: 0.046 trainign loss: 8.8360 avg training loss: 8.7819
batch: [360/10547] batch time: 1.406 trainign loss: 8.8366 avg training loss: 8.7809
batch: [370/10547] batch time: 0.053 trainign loss: 8.7915 avg training loss: 8.7815
batch: [380/10547] batch time: 2.494 trainign loss: 8.7252 avg training loss: 8.7817
batch: [390/10547] batch time: 0.045 trainign loss: 8.8680 avg training loss: 8.7831
batch: [400/10547] batch time: 1.656 trainign loss: 8.7746 avg training loss: 8.7829
batch: [410/10547] batch time: 0.051 trainign loss: 8.9857 avg training loss: 8.7823
batch: [420/10547] batch time: 0.047 trainign loss: 8.9599 avg training loss: 8.7845
batch: [430/10547] batch time: 0.046 trainign loss: 8.8023 avg training loss: 8.7842
batch: [440/10547] batch time: 0.223 trainign loss: 8.5185 avg training loss: 8.7831
batch: [450/10547] batch time: 0.046 trainign loss: 8.7708 avg training loss: 8.7818
batch: [460/10547] batch time: 1.062 trainign loss: 8.7736 avg training loss: 8.7824
batch: [470/10547] batch time: 0.055 trainign loss: 8.1353 avg training loss: 8.7806
batch: [480/10547] batch time: 1.313 trainign loss: 8.7351 avg training loss: 8.7436
batch: [490/10547] batch time: 0.055 trainign loss: 9.1258 avg training loss: 8.7506
batch: [500/10547] batch time: 0.046 trainign loss: 8.9701 avg training loss: 8.7521
batch: [510/10547] batch time: 0.054 trainign loss: 8.9420 avg training loss: 8.7562
batch: [520/10547] batch time: 0.053 trainign loss: 8.8426 avg training loss: 8.7582
batch: [530/10547] batch time: 0.054 trainign loss: 7.5736 avg training loss: 8.7523
batch: [540/10547] batch time: 0.240 trainign loss: 8.7580 avg training loss: 8.7330
batch: [550/10547] batch time: 0.492 trainign loss: 9.0970 avg training loss: 8.7363
batch: [560/10547] batch time: 0.054 trainign loss: 9.0281 avg training loss: 8.7417
batch: [570/10547] batch time: 1.859 trainign loss: 8.9404 avg training loss: 8.7451
batch: [580/10547] batch time: 0.186 trainign loss: 8.8664 avg training loss: 8.7479
batch: [590/10547] batch time: 0.103 trainign loss: 8.9486 avg training loss: 8.7495
batch: [600/10547] batch time: 1.115 trainign loss: 8.6368 avg training loss: 8.7499
batch: [610/10547] batch time: 0.053 trainign loss: 8.8685 avg training loss: 8.7515
batch: [620/10547] batch time: 0.691 trainign loss: 8.8189 avg training loss: 8.7530
batch: [630/10547] batch time: 0.046 trainign loss: 8.8483 avg training loss: 8.7546
batch: [640/10547] batch time: 0.194 trainign loss: 8.5122 avg training loss: 8.7548
batch: [650/10547] batch time: 0.043 trainign loss: 8.7729 avg training loss: 8.7545
batch: [660/10547] batch time: 0.148 trainign loss: 8.8044 avg training loss: 8.7551
batch: [670/10547] batch time: 0.049 trainign loss: 8.7503 avg training loss: 8.7538
batch: [680/10547] batch time: 1.106 trainign loss: 8.8140 avg training loss: 8.7542
batch: [690/10547] batch time: 0.396 trainign loss: 8.7738 avg training loss: 8.7548
batch: [700/10547] batch time: 1.109 trainign loss: 8.8329 avg training loss: 8.7551
batch: [710/10547] batch time: 0.046 trainign loss: 8.7921 avg training loss: 8.7555
batch: [720/10547] batch time: 0.057 trainign loss: 8.7290 avg training loss: 8.7560
batch: [730/10547] batch time: 1.229 trainign loss: 8.5299 avg training loss: 8.7375
batch: [740/10547] batch time: 0.046 trainign loss: 8.0842 avg training loss: 8.7374
batch: [750/10547] batch time: 0.054 trainign loss: 8.4223 avg training loss: 8.7369
batch: [760/10547] batch time: 0.041 trainign loss: 6.9486 avg training loss: 8.7281
batch: [770/10547] batch time: 0.049 trainign loss: 8.7449 avg training loss: 8.7283
batch: [780/10547] batch time: 0.053 trainign loss: 8.8274 avg training loss: 8.7282
batch: [790/10547] batch time: 0.397 trainign loss: 8.6989 avg training loss: 8.7283
batch: [800/10547] batch time: 0.047 trainign loss: 8.8476 avg training loss: 8.7287
batch: [810/10547] batch time: 0.049 trainign loss: 8.3913 avg training loss: 8.7280
batch: [820/10547] batch time: 0.048 trainign loss: 8.7931 avg training loss: 8.7269
batch: [830/10547] batch time: 0.928 trainign loss: 9.0349 avg training loss: 8.7272
batch: [840/10547] batch time: 0.055 trainign loss: 8.5876 avg training loss: 8.7274
batch: [850/10547] batch time: 0.805 trainign loss: 8.8015 avg training loss: 8.7279
batch: [860/10547] batch time: 0.379 trainign loss: 8.7397 avg training loss: 8.7261
batch: [870/10547] batch time: 0.761 trainign loss: 8.8207 avg training loss: 8.7268
batch: [880/10547] batch time: 0.056 trainign loss: 8.6947 avg training loss: 8.7271
batch: [890/10547] batch time: 0.205 trainign loss: 8.2849 avg training loss: 8.7204
batch: [900/10547] batch time: 0.110 trainign loss: 8.6128 avg training loss: 8.7206
batch: [910/10547] batch time: 0.054 trainign loss: 8.7882 avg training loss: 8.7221
batch: [920/10547] batch time: 0.507 trainign loss: 8.8600 avg training loss: 8.7202
batch: [930/10547] batch time: 0.042 trainign loss: 8.4962 avg training loss: 8.7202
batch: [940/10547] batch time: 1.314 trainign loss: 8.8652 avg training loss: 8.7195
batch: [950/10547] batch time: 0.054 trainign loss: 8.9739 avg training loss: 8.7196
batch: [960/10547] batch time: 0.054 trainign loss: 6.5504 avg training loss: 8.7129
batch: [970/10547] batch time: 0.961 trainign loss: 9.1441 avg training loss: 8.7119
batch: [980/10547] batch time: 0.055 trainign loss: 8.7794 avg training loss: 8.7065
batch: [990/10547] batch time: 0.055 trainign loss: 6.6999 avg training loss: 8.6971
batch: [1000/10547] batch time: 0.043 trainign loss: 10.9715 avg training loss: 8.6555
batch: [1010/10547] batch time: 0.055 trainign loss: 8.9100 avg training loss: 8.6694
batch: [1020/10547] batch time: 0.055 trainign loss: 2.8962 avg training loss: 8.6442
batch: [1030/10547] batch time: 0.055 trainign loss: 0.0103 avg training loss: 8.5651
batch: [1040/10547] batch time: 0.046 trainign loss: 14.9812 avg training loss: 8.5323
batch: [1050/10547] batch time: 1.365 trainign loss: 9.3601 avg training loss: 8.5519
batch: [1060/10547] batch time: 0.047 trainign loss: 8.7360 avg training loss: 8.5559
batch: [1070/10547] batch time: 1.397 trainign loss: 9.1398 avg training loss: 8.5588
batch: [1080/10547] batch time: 0.045 trainign loss: 7.8345 avg training loss: 8.5591
batch: [1090/10547] batch time: 1.080 trainign loss: 8.5792 avg training loss: 8.5596
batch: [1100/10547] batch time: 0.259 trainign loss: 8.3345 avg training loss: 8.5594
batch: [1110/10547] batch time: 0.054 trainign loss: 8.8759 avg training loss: 8.5565
batch: [1120/10547] batch time: 0.054 trainign loss: 8.8352 avg training loss: 8.5538
batch: [1130/10547] batch time: 1.236 trainign loss: 8.8346 avg training loss: 8.5522
batch: [1140/10547] batch time: 1.695 trainign loss: 8.9039 avg training loss: 8.5547
batch: [1150/10547] batch time: 1.151 trainign loss: 8.5719 avg training loss: 8.5562
batch: [1160/10547] batch time: 0.047 trainign loss: 8.4974 avg training loss: 8.5558
batch: [1170/10547] batch time: 1.904 trainign loss: 8.5507 avg training loss: 8.5567
batch: [1180/10547] batch time: 0.559 trainign loss: 8.8424 avg training loss: 8.5591
batch: [1190/10547] batch time: 0.862 trainign loss: 8.8142 avg training loss: 8.5607
batch: [1200/10547] batch time: 0.046 trainign loss: 8.2276 avg training loss: 8.5609
batch: [1210/10547] batch time: 0.050 trainign loss: 8.8102 avg training loss: 8.5618
batch: [1220/10547] batch time: 0.056 trainign loss: 8.6839 avg training loss: 8.5616
batch: [1230/10547] batch time: 1.650 trainign loss: 8.7522 avg training loss: 8.5625
batch: [1240/10547] batch time: 0.049 trainign loss: 8.6408 avg training loss: 8.5635
batch: [1250/10547] batch time: 0.711 trainign loss: 8.8155 avg training loss: 8.5637
batch: [1260/10547] batch time: 0.046 trainign loss: 2.9636 avg training loss: 8.5474
batch: [1270/10547] batch time: 2.031 trainign loss: 9.2507 avg training loss: 8.5495
batch: [1280/10547] batch time: 0.042 trainign loss: 6.4505 avg training loss: 8.5473
batch: [1290/10547] batch time: 1.287 trainign loss: 8.9639 avg training loss: 8.5486
batch: [1300/10547] batch time: 0.049 trainign loss: 8.6904 avg training loss: 8.5499
batch: [1310/10547] batch time: 0.055 trainign loss: 8.4569 avg training loss: 8.5506
batch: [1320/10547] batch time: 0.055 trainign loss: 7.6436 avg training loss: 8.5488
batch: [1330/10547] batch time: 0.431 trainign loss: 8.6571 avg training loss: 8.5486
batch: [1340/10547] batch time: 0.049 trainign loss: 8.7498 avg training loss: 8.5499
batch: [1350/10547] batch time: 0.898 trainign loss: 8.7263 avg training loss: 8.5506
batch: [1360/10547] batch time: 0.046 trainign loss: 8.8753 avg training loss: 8.5524
batch: [1370/10547] batch time: 0.042 trainign loss: 8.4575 avg training loss: 8.5537
batch: [1380/10547] batch time: 0.054 trainign loss: 8.8112 avg training loss: 8.5531
batch: [1390/10547] batch time: 0.768 trainign loss: 9.0410 avg training loss: 8.5444
batch: [1400/10547] batch time: 0.049 trainign loss: 8.7038 avg training loss: 8.5476
batch: [1410/10547] batch time: 0.409 trainign loss: 8.7495 avg training loss: 8.5469
batch: [1420/10547] batch time: 1.010 trainign loss: 8.7731 avg training loss: 8.5480
batch: [1430/10547] batch time: 0.055 trainign loss: 8.7474 avg training loss: 8.5486
batch: [1440/10547] batch time: 0.597 trainign loss: 8.7269 avg training loss: 8.5499
batch: [1450/10547] batch time: 0.055 trainign loss: 6.8452 avg training loss: 8.5475
batch: [1460/10547] batch time: 0.051 trainign loss: 7.8866 avg training loss: 8.5415
batch: [1470/10547] batch time: 0.047 trainign loss: 8.9930 avg training loss: 8.5448
batch: [1480/10547] batch time: 0.055 trainign loss: 8.6261 avg training loss: 8.5407
batch: [1490/10547] batch time: 0.046 trainign loss: 8.6104 avg training loss: 8.5422
batch: [1500/10547] batch time: 0.043 trainign loss: 8.7104 avg training loss: 8.5436
batch: [1510/10547] batch time: 0.051 trainign loss: 8.8345 avg training loss: 8.5447
batch: [1520/10547] batch time: 1.721 trainign loss: 8.5562 avg training loss: 8.5450
batch: [1530/10547] batch time: 0.545 trainign loss: 8.8417 avg training loss: 8.5464
batch: [1540/10547] batch time: 0.221 trainign loss: 5.4400 avg training loss: 8.5415
batch: [1550/10547] batch time: 0.043 trainign loss: 8.9100 avg training loss: 8.5432
batch: [1560/10547] batch time: 0.046 trainign loss: 8.7517 avg training loss: 8.5426
batch: [1570/10547] batch time: 1.004 trainign loss: 8.5586 avg training loss: 8.5435
batch: [1580/10547] batch time: 0.041 trainign loss: 8.7608 avg training loss: 8.5436
batch: [1590/10547] batch time: 0.142 trainign loss: 7.3882 avg training loss: 8.5428
batch: [1600/10547] batch time: 0.046 trainign loss: 8.5871 avg training loss: 8.5438
batch: [1610/10547] batch time: 0.054 trainign loss: 5.9466 avg training loss: 8.5372
batch: [1620/10547] batch time: 0.047 trainign loss: 8.7531 avg training loss: 8.5387
batch: [1630/10547] batch time: 0.054 trainign loss: 6.2571 avg training loss: 8.5358
batch: [1640/10547] batch time: 0.053 trainign loss: 8.6533 avg training loss: 8.5327
batch: [1650/10547] batch time: 1.605 trainign loss: 8.3289 avg training loss: 8.5327
batch: [1660/10547] batch time: 0.506 trainign loss: 8.3519 avg training loss: 8.5311
batch: [1670/10547] batch time: 0.044 trainign loss: 7.1510 avg training loss: 8.5299
batch: [1680/10547] batch time: 0.439 trainign loss: 8.5355 avg training loss: 8.5305
batch: [1690/10547] batch time: 0.419 trainign loss: 8.7520 avg training loss: 8.5315
batch: [1700/10547] batch time: 0.049 trainign loss: 8.6913 avg training loss: 8.5309
batch: [1710/10547] batch time: 1.316 trainign loss: 8.7291 avg training loss: 8.5316
batch: [1720/10547] batch time: 0.041 trainign loss: 8.7609 avg training loss: 8.5327
batch: [1730/10547] batch time: 0.042 trainign loss: 8.7271 avg training loss: 8.5337
batch: [1740/10547] batch time: 0.041 trainign loss: 8.7119 avg training loss: 8.5350
batch: [1750/10547] batch time: 0.054 trainign loss: 8.8241 avg training loss: 8.5360
batch: [1760/10547] batch time: 0.043 trainign loss: 6.9372 avg training loss: 8.5330
batch: [1770/10547] batch time: 0.055 trainign loss: 8.7657 avg training loss: 8.5348
batch: [1780/10547] batch time: 0.055 trainign loss: 8.7451 avg training loss: 8.5332
batch: [1790/10547] batch time: 0.056 trainign loss: 8.7448 avg training loss: 8.5336
batch: [1800/10547] batch time: 2.812 trainign loss: 8.6226 avg training loss: 8.5342
batch: [1810/10547] batch time: 0.054 trainign loss: 8.6574 avg training loss: 8.5349
batch: [1820/10547] batch time: 1.399 trainign loss: 8.7083 avg training loss: 8.5353
batch: [1830/10547] batch time: 0.046 trainign loss: 8.4374 avg training loss: 8.5355
batch: [1840/10547] batch time: 1.281 trainign loss: 8.8908 avg training loss: 8.5368
batch: [1850/10547] batch time: 0.246 trainign loss: 8.3469 avg training loss: 8.5360
batch: [1860/10547] batch time: 0.055 trainign loss: 8.1561 avg training loss: 8.5362
batch: [1870/10547] batch time: 2.676 trainign loss: 8.8346 avg training loss: 8.5377
batch: [1880/10547] batch time: 0.047 trainign loss: 8.6258 avg training loss: 8.5393
batch: [1890/10547] batch time: 2.468 trainign loss: 8.6674 avg training loss: 8.5399
batch: [1900/10547] batch time: 0.056 trainign loss: 7.5784 avg training loss: 8.5401
batch: [1910/10547] batch time: 0.498 trainign loss: 1.9271 avg training loss: 8.5110
batch: [1920/10547] batch time: 0.046 trainign loss: 8.7906 avg training loss: 8.5209
batch: [1930/10547] batch time: 0.577 trainign loss: 8.4180 avg training loss: 8.5217
batch: [1940/10547] batch time: 0.046 trainign loss: 8.7734 avg training loss: 8.5226
batch: [1950/10547] batch time: 0.055 trainign loss: 8.8839 avg training loss: 8.5241
batch: [1960/10547] batch time: 1.303 trainign loss: 8.7969 avg training loss: 8.5232
batch: [1970/10547] batch time: 0.046 trainign loss: 8.5365 avg training loss: 8.5237
batch: [1980/10547] batch time: 0.780 trainign loss: 8.6853 avg training loss: 8.5241
batch: [1990/10547] batch time: 0.054 trainign loss: 1.6832 avg training loss: 8.5121
batch: [2000/10547] batch time: 0.858 trainign loss: 8.9433 avg training loss: 8.5064
batch: [2010/10547] batch time: 0.045 trainign loss: 8.9138 avg training loss: 8.5072
batch: [2020/10547] batch time: 2.235 trainign loss: 8.6634 avg training loss: 8.5080
batch: [2030/10547] batch time: 0.054 trainign loss: 8.6147 avg training loss: 8.5087
batch: [2040/10547] batch time: 2.156 trainign loss: 8.8883 avg training loss: 8.5069
batch: [2050/10547] batch time: 0.046 trainign loss: 8.6628 avg training loss: 8.5086
batch: [2060/10547] batch time: 0.415 trainign loss: 7.6341 avg training loss: 8.5089
batch: [2070/10547] batch time: 0.057 trainign loss: 7.6525 avg training loss: 8.5092
batch: [2080/10547] batch time: 0.056 trainign loss: 8.9310 avg training loss: 8.5056
batch: [2090/10547] batch time: 0.054 trainign loss: 8.6014 avg training loss: 8.5056
batch: [2100/10547] batch time: 0.252 trainign loss: 0.7468 avg training loss: 8.4887
batch: [2110/10547] batch time: 0.046 trainign loss: 8.6293 avg training loss: 8.4945
batch: [2120/10547] batch time: 0.042 trainign loss: 7.6474 avg training loss: 8.4946
batch: [2130/10547] batch time: 0.048 trainign loss: 8.7871 avg training loss: 8.4964
batch: [2140/10547] batch time: 0.042 trainign loss: 8.6429 avg training loss: 8.4971
batch: [2150/10547] batch time: 0.055 trainign loss: 7.9586 avg training loss: 8.4981
batch: [2160/10547] batch time: 0.053 trainign loss: 8.6357 avg training loss: 8.4961
batch: [2170/10547] batch time: 0.237 trainign loss: 8.8266 avg training loss: 8.4959
batch: [2180/10547] batch time: 0.055 trainign loss: 8.7423 avg training loss: 8.4971
batch: [2190/10547] batch time: 0.056 trainign loss: 3.0561 avg training loss: 8.4886
batch: [2200/10547] batch time: 1.031 trainign loss: 8.4013 avg training loss: 8.4841
batch: [2210/10547] batch time: 0.055 trainign loss: 7.2906 avg training loss: 8.4841
batch: [2220/10547] batch time: 1.267 trainign loss: 10.1466 avg training loss: 8.4789
batch: [2230/10547] batch time: 0.055 trainign loss: 7.9632 avg training loss: 8.4796
batch: [2240/10547] batch time: 0.425 trainign loss: 8.8791 avg training loss: 8.4793
batch: [2250/10547] batch time: 0.051 trainign loss: 8.7606 avg training loss: 8.4810
batch: [2260/10547] batch time: 0.126 trainign loss: 8.4851 avg training loss: 8.4815
batch: [2270/10547] batch time: 1.240 trainign loss: 8.7838 avg training loss: 8.4821
batch: [2280/10547] batch time: 0.055 trainign loss: 7.9973 avg training loss: 8.4829
batch: [2290/10547] batch time: 0.047 trainign loss: 8.9662 avg training loss: 8.4834
batch: [2300/10547] batch time: 0.055 trainign loss: 8.4787 avg training loss: 8.4846
batch: [2310/10547] batch time: 0.659 trainign loss: 8.7895 avg training loss: 8.4842
batch: [2320/10547] batch time: 0.335 trainign loss: 8.9086 avg training loss: 8.4848
batch: [2330/10547] batch time: 2.196 trainign loss: 8.7252 avg training loss: 8.4853
batch: [2340/10547] batch time: 0.055 trainign loss: 8.7161 avg training loss: 8.4864
batch: [2350/10547] batch time: 1.156 trainign loss: 8.4704 avg training loss: 8.4863
batch: [2360/10547] batch time: 0.046 trainign loss: 8.6236 avg training loss: 8.4831
batch: [2370/10547] batch time: 0.723 trainign loss: 8.2663 avg training loss: 8.4846
batch: [2380/10547] batch time: 0.055 trainign loss: 7.6441 avg training loss: 8.4846
batch: [2390/10547] batch time: 1.223 trainign loss: 11.6553 avg training loss: 8.4700
batch: [2400/10547] batch time: 0.054 trainign loss: 8.9001 avg training loss: 8.4759
batch: [2410/10547] batch time: 0.239 trainign loss: 8.2506 avg training loss: 8.4764
batch: [2420/10547] batch time: 0.055 trainign loss: 8.8636 avg training loss: 8.4765
batch: [2430/10547] batch time: 0.255 trainign loss: 8.8456 avg training loss: 8.4766
batch: [2440/10547] batch time: 0.047 trainign loss: 8.5654 avg training loss: 8.4772
batch: [2450/10547] batch time: 0.055 trainign loss: 8.7015 avg training loss: 8.4771
batch: [2460/10547] batch time: 0.055 trainign loss: 5.4968 avg training loss: 8.4746
batch: [2470/10547] batch time: 0.925 trainign loss: 12.5225 avg training loss: 8.4613
batch: [2480/10547] batch time: 0.054 trainign loss: 7.9861 avg training loss: 8.4635
batch: [2490/10547] batch time: 1.341 trainign loss: 8.7879 avg training loss: 8.4616
batch: [2500/10547] batch time: 0.966 trainign loss: 8.9111 avg training loss: 8.4628
batch: [2510/10547] batch time: 0.128 trainign loss: 4.4767 avg training loss: 8.4584
batch: [2520/10547] batch time: 0.233 trainign loss: 10.4748 avg training loss: 8.4546
batch: [2530/10547] batch time: 0.047 trainign loss: 8.5859 avg training loss: 8.4561
batch: [2540/10547] batch time: 0.259 trainign loss: 8.7813 avg training loss: 8.4572
batch: [2550/10547] batch time: 0.179 trainign loss: 8.8151 avg training loss: 8.4567
batch: [2560/10547] batch time: 0.047 trainign loss: 8.8843 avg training loss: 8.4580
batch: [2570/10547] batch time: 0.046 trainign loss: 8.7047 avg training loss: 8.4595
batch: [2580/10547] batch time: 0.046 trainign loss: 8.4951 avg training loss: 8.4603
batch: [2590/10547] batch time: 0.049 trainign loss: 8.8161 avg training loss: 8.4612
batch: [2600/10547] batch time: 0.048 trainign loss: 7.0364 avg training loss: 8.4612
batch: [2610/10547] batch time: 0.588 trainign loss: 8.9136 avg training loss: 8.4619
batch: [2620/10547] batch time: 0.049 trainign loss: 8.7390 avg training loss: 8.4628
batch: [2630/10547] batch time: 0.758 trainign loss: 8.7256 avg training loss: 8.4636
batch: [2640/10547] batch time: 0.041 trainign loss: 7.2800 avg training loss: 8.4636
batch: [2650/10547] batch time: 0.501 trainign loss: 8.9239 avg training loss: 8.4638
batch: [2660/10547] batch time: 0.046 trainign loss: 8.8845 avg training loss: 8.4647
batch: [2670/10547] batch time: 0.663 trainign loss: 8.5997 avg training loss: 8.4655
batch: [2680/10547] batch time: 0.046 trainign loss: 8.7662 avg training loss: 8.4666
batch: [2690/10547] batch time: 0.056 trainign loss: 8.4913 avg training loss: 8.4672
batch: [2700/10547] batch time: 0.046 trainign loss: 8.7242 avg training loss: 8.4668
batch: [2710/10547] batch time: 0.041 trainign loss: 8.7727 avg training loss: 8.4675
batch: [2720/10547] batch time: 0.046 trainign loss: 8.7788 avg training loss: 8.4686
batch: [2730/10547] batch time: 0.056 trainign loss: 8.8718 avg training loss: 8.4699
batch: [2740/10547] batch time: 0.420 trainign loss: 8.5559 avg training loss: 8.4668
batch: [2750/10547] batch time: 0.237 trainign loss: 8.5415 avg training loss: 8.4685
batch: [2760/10547] batch time: 1.038 trainign loss: 8.6755 avg training loss: 8.4689
batch: [2770/10547] batch time: 1.288 trainign loss: 4.2821 avg training loss: 8.4646
batch: [2780/10547] batch time: 0.217 trainign loss: 9.1195 avg training loss: 8.4661
batch: [2790/10547] batch time: 0.687 trainign loss: 8.9094 avg training loss: 8.4669
batch: [2800/10547] batch time: 0.122 trainign loss: 8.5501 avg training loss: 8.4669
batch: [2810/10547] batch time: 0.559 trainign loss: 8.2142 avg training loss: 8.4670
batch: [2820/10547] batch time: 1.564 trainign loss: 8.8684 avg training loss: 8.4681
batch: [2830/10547] batch time: 1.045 trainign loss: 8.6751 avg training loss: 8.4687
batch: [2840/10547] batch time: 1.517 trainign loss: 8.6445 avg training loss: 8.4682
batch: [2850/10547] batch time: 0.400 trainign loss: 8.9724 avg training loss: 8.4688
batch: [2860/10547] batch time: 1.246 trainign loss: 8.7363 avg training loss: 8.4697
batch: [2870/10547] batch time: 1.257 trainign loss: 8.7208 avg training loss: 8.4703
batch: [2880/10547] batch time: 0.214 trainign loss: 8.8179 avg training loss: 8.4709
batch: [2890/10547] batch time: 1.749 trainign loss: 8.8225 avg training loss: 8.4717
batch: [2900/10547] batch time: 0.046 trainign loss: 8.6140 avg training loss: 8.4724
batch: [2910/10547] batch time: 1.369 trainign loss: 8.7821 avg training loss: 8.4731
batch: [2920/10547] batch time: 0.048 trainign loss: 8.2355 avg training loss: 8.4737
batch: [2930/10547] batch time: 1.211 trainign loss: 8.1061 avg training loss: 8.4726
batch: [2940/10547] batch time: 0.049 trainign loss: 8.8320 avg training loss: 8.4725
batch: [2950/10547] batch time: 1.095 trainign loss: 8.1353 avg training loss: 8.4718
batch: [2960/10547] batch time: 0.345 trainign loss: 8.7066 avg training loss: 8.4720
batch: [2970/10547] batch time: 1.271 trainign loss: 8.8465 avg training loss: 8.4730
batch: [2980/10547] batch time: 0.470 trainign loss: 7.6563 avg training loss: 8.4727
batch: [2990/10547] batch time: 1.167 trainign loss: 8.9618 avg training loss: 8.4717
batch: [3000/10547] batch time: 0.127 trainign loss: 8.6619 avg training loss: 8.4722
batch: [3010/10547] batch time: 1.228 trainign loss: 8.3482 avg training loss: 8.4711
batch: [3020/10547] batch time: 0.046 trainign loss: 8.6288 avg training loss: 8.4712
batch: [3030/10547] batch time: 0.827 trainign loss: 8.9012 avg training loss: 8.4716
batch: [3040/10547] batch time: 0.054 trainign loss: 8.0150 avg training loss: 8.4718
batch: [3050/10547] batch time: 0.046 trainign loss: 8.8730 avg training loss: 8.4715
batch: [3060/10547] batch time: 0.046 trainign loss: 8.7759 avg training loss: 8.4725
batch: [3070/10547] batch time: 0.826 trainign loss: 9.0120 avg training loss: 8.4720
batch: [3080/10547] batch time: 0.122 trainign loss: 8.9603 avg training loss: 8.4733
batch: [3090/10547] batch time: 1.933 trainign loss: 8.7072 avg training loss: 8.4741
batch: [3100/10547] batch time: 0.046 trainign loss: 8.7668 avg training loss: 8.4747
batch: [3110/10547] batch time: 1.364 trainign loss: 8.7246 avg training loss: 8.4755
batch: [3120/10547] batch time: 0.049 trainign loss: 9.0144 avg training loss: 8.4759
batch: [3130/10547] batch time: 1.041 trainign loss: 8.5935 avg training loss: 8.4766
batch: [3140/10547] batch time: 0.046 trainign loss: 8.7678 avg training loss: 8.4771
batch: [3150/10547] batch time: 1.530 trainign loss: 8.7424 avg training loss: 8.4775
batch: [3160/10547] batch time: 0.046 trainign loss: 8.5625 avg training loss: 8.4773
batch: [3170/10547] batch time: 1.233 trainign loss: 8.5177 avg training loss: 8.4774
batch: [3180/10547] batch time: 0.046 trainign loss: 8.8649 avg training loss: 8.4778
batch: [3190/10547] batch time: 1.386 trainign loss: 6.9901 avg training loss: 8.4754
batch: [3200/10547] batch time: 0.055 trainign loss: 8.8237 avg training loss: 8.4771
batch: [3210/10547] batch time: 0.413 trainign loss: 8.9782 avg training loss: 8.4780
batch: [3220/10547] batch time: 0.399 trainign loss: 8.5732 avg training loss: 8.4787
batch: [3230/10547] batch time: 0.054 trainign loss: 8.5449 avg training loss: 8.4788
batch: [3240/10547] batch time: 1.133 trainign loss: 7.8840 avg training loss: 8.4792
batch: [3250/10547] batch time: 1.661 trainign loss: 8.5764 avg training loss: 8.4791
batch: [3260/10547] batch time: 1.088 trainign loss: 8.6302 avg training loss: 8.4797
batch: [3270/10547] batch time: 1.180 trainign loss: 8.4034 avg training loss: 8.4802
batch: [3280/10547] batch time: 1.462 trainign loss: 9.0105 avg training loss: 8.4803
batch: [3290/10547] batch time: 0.543 trainign loss: 8.6404 avg training loss: 8.4808
batch: [3300/10547] batch time: 1.032 trainign loss: 8.8246 avg training loss: 8.4815
batch: [3310/10547] batch time: 0.526 trainign loss: 8.0286 avg training loss: 8.4814
batch: [3320/10547] batch time: 0.646 trainign loss: 8.5436 avg training loss: 8.4816
batch: [3330/10547] batch time: 1.914 trainign loss: 8.6889 avg training loss: 8.4824
batch: [3340/10547] batch time: 0.048 trainign loss: 8.5271 avg training loss: 8.4829
batch: [3350/10547] batch time: 1.487 trainign loss: 8.7365 avg training loss: 8.4835
batch: [3360/10547] batch time: 0.564 trainign loss: 8.7922 avg training loss: 8.4842
batch: [3370/10547] batch time: 1.033 trainign loss: 5.4807 avg training loss: 8.4823
batch: [3380/10547] batch time: 0.292 trainign loss: 9.5255 avg training loss: 8.4816
batch: [3390/10547] batch time: 1.329 trainign loss: 8.8069 avg training loss: 8.4806
batch: [3400/10547] batch time: 0.957 trainign loss: 7.3016 avg training loss: 8.4802
batch: [3410/10547] batch time: 0.550 trainign loss: 8.9702 avg training loss: 8.4812
batch: [3420/10547] batch time: 0.421 trainign loss: 8.8729 avg training loss: 8.4821
batch: [3430/10547] batch time: 0.359 trainign loss: 7.2827 avg training loss: 8.4820
batch: [3440/10547] batch time: 0.054 trainign loss: 8.8885 avg training loss: 8.4823
batch: [3450/10547] batch time: 1.016 trainign loss: 7.5997 avg training loss: 8.4817
batch: [3460/10547] batch time: 0.047 trainign loss: 9.0222 avg training loss: 8.4827
batch: [3470/10547] batch time: 1.256 trainign loss: 8.6318 avg training loss: 8.4832
batch: [3480/10547] batch time: 0.055 trainign loss: 8.6512 avg training loss: 8.4839
batch: [3490/10547] batch time: 1.610 trainign loss: 9.0688 avg training loss: 8.4838
batch: [3500/10547] batch time: 0.054 trainign loss: 8.4804 avg training loss: 8.4846
batch: [3510/10547] batch time: 2.084 trainign loss: 7.7313 avg training loss: 8.4847
batch: [3520/10547] batch time: 0.047 trainign loss: 8.7524 avg training loss: 8.4830
batch: [3530/10547] batch time: 2.338 trainign loss: 8.7768 avg training loss: 8.4837
batch: [3540/10547] batch time: 0.046 trainign loss: 8.1792 avg training loss: 8.4839
batch: [3550/10547] batch time: 1.173 trainign loss: 8.8978 avg training loss: 8.4845
batch: [3560/10547] batch time: 0.048 trainign loss: 7.3632 avg training loss: 8.4850
batch: [3570/10547] batch time: 2.085 trainign loss: 8.9801 avg training loss: 8.4850
batch: [3580/10547] batch time: 0.048 trainign loss: 8.8876 avg training loss: 8.4854
batch: [3590/10547] batch time: 0.939 trainign loss: 8.4990 avg training loss: 8.4856
batch: [3600/10547] batch time: 0.046 trainign loss: 8.8537 avg training loss: 8.4858
batch: [3610/10547] batch time: 1.447 trainign loss: 8.7697 avg training loss: 8.4861
batch: [3620/10547] batch time: 0.046 trainign loss: 8.7539 avg training loss: 8.4865
batch: [3630/10547] batch time: 1.286 trainign loss: 8.7216 avg training loss: 8.4866
batch: [3640/10547] batch time: 0.054 trainign loss: 7.5716 avg training loss: 8.4868
batch: [3650/10547] batch time: 0.795 trainign loss: 8.9545 avg training loss: 8.4862
batch: [3660/10547] batch time: 0.046 trainign loss: 5.2208 avg training loss: 8.4844
batch: [3670/10547] batch time: 0.152 trainign loss: 8.9379 avg training loss: 8.4857
batch: [3680/10547] batch time: 0.047 trainign loss: 7.5036 avg training loss: 8.4858
batch: [3690/10547] batch time: 0.154 trainign loss: 6.8161 avg training loss: 8.4836
batch: [3700/10547] batch time: 0.046 trainign loss: 8.8272 avg training loss: 8.4847
batch: [3710/10547] batch time: 0.149 trainign loss: 8.8094 avg training loss: 8.4852
batch: [3720/10547] batch time: 0.046 trainign loss: 8.7958 avg training loss: 8.4857
batch: [3730/10547] batch time: 1.201 trainign loss: 8.7325 avg training loss: 8.4862
batch: [3740/10547] batch time: 0.048 trainign loss: 8.5173 avg training loss: 8.4865
batch: [3750/10547] batch time: 1.556 trainign loss: 8.3804 avg training loss: 8.4870
batch: [3760/10547] batch time: 0.055 trainign loss: 8.5470 avg training loss: 8.4872
batch: [3770/10547] batch time: 1.852 trainign loss: 7.4831 avg training loss: 8.4872
batch: [3780/10547] batch time: 0.046 trainign loss: 8.8448 avg training loss: 8.4876
batch: [3790/10547] batch time: 1.728 trainign loss: 8.2681 avg training loss: 8.4881
batch: [3800/10547] batch time: 0.046 trainign loss: 6.5336 avg training loss: 8.4871
batch: [3810/10547] batch time: 0.042 trainign loss: 6.6610 avg training loss: 8.4871
batch: [3820/10547] batch time: 0.046 trainign loss: 9.1983 avg training loss: 8.4839
batch: [3830/10547] batch time: 0.043 trainign loss: 8.7289 avg training loss: 8.4848
batch: [3840/10547] batch time: 0.048 trainign loss: 8.7773 avg training loss: 8.4850
batch: [3850/10547] batch time: 0.268 trainign loss: 8.7815 avg training loss: 8.4854
batch: [3860/10547] batch time: 0.287 trainign loss: 8.7193 avg training loss: 8.4859
batch: [3870/10547] batch time: 0.656 trainign loss: 8.8250 avg training loss: 8.4866
batch: [3880/10547] batch time: 0.443 trainign loss: 8.5091 avg training loss: 8.4870
batch: [3890/10547] batch time: 0.055 trainign loss: 8.5217 avg training loss: 8.4876
batch: [3900/10547] batch time: 0.871 trainign loss: 5.6931 avg training loss: 8.4854
batch: [3910/10547] batch time: 0.783 trainign loss: 8.6792 avg training loss: 8.4866
batch: [3920/10547] batch time: 1.075 trainign loss: 8.6752 avg training loss: 8.4872
batch: [3930/10547] batch time: 1.524 trainign loss: 8.8984 avg training loss: 8.4879
batch: [3940/10547] batch time: 0.046 trainign loss: 8.7021 avg training loss: 8.4881
batch: [3950/10547] batch time: 1.619 trainign loss: 8.6856 avg training loss: 8.4886
batch: [3960/10547] batch time: 0.046 trainign loss: 5.1176 avg training loss: 8.4867
batch: [3970/10547] batch time: 0.852 trainign loss: 8.9189 avg training loss: 8.4848
batch: [3980/10547] batch time: 0.046 trainign loss: 8.7462 avg training loss: 8.4852
batch: [3990/10547] batch time: 0.656 trainign loss: 8.7176 avg training loss: 8.4857
batch: [4000/10547] batch time: 0.051 trainign loss: 8.7105 avg training loss: 8.4861
batch: [4010/10547] batch time: 1.488 trainign loss: 8.7785 avg training loss: 8.4866
batch: [4020/10547] batch time: 0.047 trainign loss: 8.8955 avg training loss: 8.4872
batch: [4030/10547] batch time: 1.191 trainign loss: 7.9662 avg training loss: 8.4875
batch: [4040/10547] batch time: 0.053 trainign loss: 8.8563 avg training loss: 8.4882
batch: [4050/10547] batch time: 0.681 trainign loss: 8.2608 avg training loss: 8.4886
batch: [4060/10547] batch time: 0.046 trainign loss: 7.9600 avg training loss: 8.4891
batch: [4070/10547] batch time: 1.462 trainign loss: 8.6030 avg training loss: 8.4895
batch: [4080/10547] batch time: 0.046 trainign loss: 8.7921 avg training loss: 8.4901
batch: [4090/10547] batch time: 0.336 trainign loss: 8.8638 avg training loss: 8.4908
batch: [4100/10547] batch time: 0.046 trainign loss: 8.6868 avg training loss: 8.4912
batch: [4110/10547] batch time: 1.143 trainign loss: 8.6594 avg training loss: 8.4917
batch: [4120/10547] batch time: 0.046 trainign loss: 7.6372 avg training loss: 8.4917
batch: [4130/10547] batch time: 0.860 trainign loss: 3.7111 avg training loss: 8.4880
batch: [4140/10547] batch time: 0.046 trainign loss: 8.2260 avg training loss: 8.4886
batch: [4150/10547] batch time: 0.931 trainign loss: 9.3311 avg training loss: 8.4860
batch: [4160/10547] batch time: 0.046 trainign loss: 8.1447 avg training loss: 8.4861
batch: [4170/10547] batch time: 0.070 trainign loss: 8.6234 avg training loss: 8.4864
batch: [4180/10547] batch time: 0.046 trainign loss: 7.9863 avg training loss: 8.4864
batch: [4190/10547] batch time: 1.263 trainign loss: 8.8434 avg training loss: 8.4869
batch: [4200/10547] batch time: 0.051 trainign loss: 8.8311 avg training loss: 8.4874
batch: [4210/10547] batch time: 0.047 trainign loss: 8.5612 avg training loss: 8.4877
batch: [4220/10547] batch time: 0.047 trainign loss: 8.6707 avg training loss: 8.4882
batch: [4230/10547] batch time: 0.047 trainign loss: 8.9748 avg training loss: 8.4884
batch: [4240/10547] batch time: 0.049 trainign loss: 8.7348 avg training loss: 8.4886
batch: [4250/10547] batch time: 0.047 trainign loss: 8.8930 avg training loss: 8.4882
batch: [4260/10547] batch time: 0.047 trainign loss: 8.0325 avg training loss: 8.4885
batch: [4270/10547] batch time: 0.046 trainign loss: 8.5579 avg training loss: 8.4888
batch: [4280/10547] batch time: 0.047 trainign loss: 3.6107 avg training loss: 8.4855
batch: [4290/10547] batch time: 0.047 trainign loss: 0.0005 avg training loss: 8.4668
batch: [4300/10547] batch time: 0.047 trainign loss: 3.3136 avg training loss: 8.4479
batch: [4310/10547] batch time: 0.046 trainign loss: 7.8037 avg training loss: 8.4531
batch: [4320/10547] batch time: 0.046 trainign loss: 8.8445 avg training loss: 8.4536
batch: [4330/10547] batch time: 0.047 trainign loss: 7.9752 avg training loss: 8.4537
batch: [4340/10547] batch time: 0.240 trainign loss: 8.7657 avg training loss: 8.4539
batch: [4350/10547] batch time: 0.046 trainign loss: 8.7062 avg training loss: 8.4547
batch: [4360/10547] batch time: 0.336 trainign loss: 8.7514 avg training loss: 8.4551
batch: [4370/10547] batch time: 0.046 trainign loss: 8.5429 avg training loss: 8.4561
batch: [4380/10547] batch time: 0.285 trainign loss: 8.6869 avg training loss: 8.4567
batch: [4390/10547] batch time: 0.050 trainign loss: 8.6733 avg training loss: 8.4573
batch: [4400/10547] batch time: 0.042 trainign loss: 9.0146 avg training loss: 8.4568
batch: [4410/10547] batch time: 0.047 trainign loss: 8.5353 avg training loss: 8.4575
batch: [4420/10547] batch time: 0.212 trainign loss: 8.5931 avg training loss: 8.4582
batch: [4430/10547] batch time: 0.046 trainign loss: 8.8314 avg training loss: 8.4583
batch: [4440/10547] batch time: 0.342 trainign loss: 8.3225 avg training loss: 8.4554
batch: [4450/10547] batch time: 0.054 trainign loss: 8.6891 avg training loss: 8.4569
batch: [4460/10547] batch time: 0.942 trainign loss: 8.7665 avg training loss: 8.4567
batch: [4470/10547] batch time: 0.947 trainign loss: 8.9628 avg training loss: 8.4572
batch: [4480/10547] batch time: 0.349 trainign loss: 9.0905 avg training loss: 8.4575
batch: [4490/10547] batch time: 0.586 trainign loss: 8.9290 avg training loss: 8.4580
batch: [4500/10547] batch time: 1.678 trainign loss: 7.5880 avg training loss: 8.4583
batch: [4510/10547] batch time: 0.054 trainign loss: 8.7995 avg training loss: 8.4573
batch: [4520/10547] batch time: 2.254 trainign loss: 9.1938 avg training loss: 8.4581
batch: [4530/10547] batch time: 0.047 trainign loss: 8.2886 avg training loss: 8.4586
batch: [4540/10547] batch time: 2.199 trainign loss: 8.8908 avg training loss: 8.4584
batch: [4550/10547] batch time: 0.235 trainign loss: 8.6862 avg training loss: 8.4592
batch: [4560/10547] batch time: 1.368 trainign loss: 8.6999 avg training loss: 8.4596
batch: [4570/10547] batch time: 0.861 trainign loss: 8.6923 avg training loss: 8.4599
batch: [4580/10547] batch time: 1.546 trainign loss: 8.6671 avg training loss: 8.4607
batch: [4590/10547] batch time: 1.197 trainign loss: 8.7676 avg training loss: 8.4612
batch: [4600/10547] batch time: 1.365 trainign loss: 8.6981 avg training loss: 8.4610
batch: [4610/10547] batch time: 1.000 trainign loss: 8.7686 avg training loss: 8.4615
batch: [4620/10547] batch time: 2.005 trainign loss: 8.7745 avg training loss: 8.4621
batch: [4630/10547] batch time: 0.670 trainign loss: 8.6668 avg training loss: 8.4625
batch: [4640/10547] batch time: 2.137 trainign loss: 8.9250 avg training loss: 8.4631
batch: [4650/10547] batch time: 0.209 trainign loss: 8.7503 avg training loss: 8.4636
batch: [4660/10547] batch time: 1.888 trainign loss: 8.8435 avg training loss: 8.4637
batch: [4670/10547] batch time: 0.298 trainign loss: 8.3799 avg training loss: 8.4631
batch: [4680/10547] batch time: 1.773 trainign loss: 8.6764 avg training loss: 8.4636
batch: [4690/10547] batch time: 0.776 trainign loss: 8.7391 avg training loss: 8.4635
batch: [4700/10547] batch time: 1.027 trainign loss: 8.9484 avg training loss: 8.4637
batch: [4710/10547] batch time: 0.176 trainign loss: 8.9108 avg training loss: 8.4635
batch: [4720/10547] batch time: 2.114 trainign loss: 8.8028 avg training loss: 8.4635
batch: [4730/10547] batch time: 0.046 trainign loss: 8.7925 avg training loss: 8.4630
batch: [4740/10547] batch time: 2.080 trainign loss: 8.9383 avg training loss: 8.4633
batch: [4750/10547] batch time: 0.093 trainign loss: 8.7002 avg training loss: 8.4639
batch: [4760/10547] batch time: 0.694 trainign loss: 8.7327 avg training loss: 8.4643
batch: [4770/10547] batch time: 0.540 trainign loss: 8.8989 avg training loss: 8.4645
batch: [4780/10547] batch time: 0.046 trainign loss: 8.7687 avg training loss: 8.4652
batch: [4790/10547] batch time: 0.046 trainign loss: 8.9568 avg training loss: 8.4657
batch: [4800/10547] batch time: 0.270 trainign loss: 8.9236 avg training loss: 8.4663
batch: [4810/10547] batch time: 0.048 trainign loss: 8.8689 avg training loss: 8.4669
batch: [4820/10547] batch time: 0.047 trainign loss: 8.4333 avg training loss: 8.4670
batch: [4830/10547] batch time: 0.053 trainign loss: 8.8578 avg training loss: 8.4677
batch: [4840/10547] batch time: 0.049 trainign loss: 8.6680 avg training loss: 8.4680
batch: [4850/10547] batch time: 0.047 trainign loss: 7.6001 avg training loss: 8.4677
batch: [4860/10547] batch time: 0.258 trainign loss: 8.6534 avg training loss: 8.4676
batch: [4870/10547] batch time: 0.046 trainign loss: 8.7199 avg training loss: 8.4679
batch: [4880/10547] batch time: 1.354 trainign loss: 8.3212 avg training loss: 8.4682
batch: [4890/10547] batch time: 0.046 trainign loss: 8.8250 avg training loss: 8.4687
batch: [4900/10547] batch time: 1.857 trainign loss: 8.7630 avg training loss: 8.4694
batch: [4910/10547] batch time: 0.046 trainign loss: 8.8425 avg training loss: 8.4699
batch: [4920/10547] batch time: 1.648 trainign loss: 8.9004 avg training loss: 8.4706
batch: [4930/10547] batch time: 0.054 trainign loss: 8.6209 avg training loss: 8.4709
batch: [4940/10547] batch time: 1.484 trainign loss: 8.7646 avg training loss: 8.4714
batch: [4950/10547] batch time: 0.047 trainign loss: 8.8494 avg training loss: 8.4707
batch: [4960/10547] batch time: 1.318 trainign loss: 8.7982 avg training loss: 8.4712
batch: [4970/10547] batch time: 0.046 trainign loss: 8.8216 avg training loss: 8.4719
batch: [4980/10547] batch time: 1.907 trainign loss: 8.8052 avg training loss: 8.4726
batch: [4990/10547] batch time: 0.047 trainign loss: 8.6945 avg training loss: 8.4730
batch: [5000/10547] batch time: 2.181 trainign loss: 8.3222 avg training loss: 8.4736
batch: [5010/10547] batch time: 0.046 trainign loss: 8.4564 avg training loss: 8.4736
batch: [5020/10547] batch time: 2.099 trainign loss: 8.9666 avg training loss: 8.4737
batch: [5030/10547] batch time: 0.047 trainign loss: 8.8012 avg training loss: 8.4740
batch: [5040/10547] batch time: 2.367 trainign loss: 8.8071 avg training loss: 8.4743
batch: [5050/10547] batch time: 0.047 trainign loss: 8.8884 avg training loss: 8.4747
batch: [5060/10547] batch time: 1.998 trainign loss: 9.3959 avg training loss: 8.4732
batch: [5070/10547] batch time: 0.047 trainign loss: 8.7138 avg training loss: 8.4742
batch: [5080/10547] batch time: 2.134 trainign loss: 8.8100 avg training loss: 8.4743
batch: [5090/10547] batch time: 0.060 trainign loss: 8.9675 avg training loss: 8.4745
batch: [5100/10547] batch time: 2.225 trainign loss: 7.2255 avg training loss: 8.4747
batch: [5110/10547] batch time: 0.049 trainign loss: 8.9849 avg training loss: 8.4751
batch: [5120/10547] batch time: 2.453 trainign loss: 7.2485 avg training loss: 8.4753
batch: [5130/10547] batch time: 0.045 trainign loss: 8.3191 avg training loss: 8.4755
batch: [5140/10547] batch time: 1.663 trainign loss: 8.7906 avg training loss: 8.4759
batch: [5150/10547] batch time: 1.008 trainign loss: 8.8966 avg training loss: 8.4767
batch: [5160/10547] batch time: 1.595 trainign loss: 3.4568 avg training loss: 8.4741
batch: [5170/10547] batch time: 1.744 trainign loss: 8.8496 avg training loss: 8.4746
batch: [5180/10547] batch time: 0.056 trainign loss: 9.0331 avg training loss: 8.4750
batch: [5190/10547] batch time: 2.409 trainign loss: 8.9025 avg training loss: 8.4751
batch: [5200/10547] batch time: 0.051 trainign loss: 7.5108 avg training loss: 8.4754
batch: [5210/10547] batch time: 2.393 trainign loss: 8.7147 avg training loss: 8.4756
batch: [5220/10547] batch time: 0.368 trainign loss: 9.0062 avg training loss: 8.4759
batch: [5230/10547] batch time: 2.507 trainign loss: 8.1682 avg training loss: 8.4762
batch: [5240/10547] batch time: 0.502 trainign loss: 6.7732 avg training loss: 8.4747
batch: [5250/10547] batch time: 1.629 trainign loss: 8.7252 avg training loss: 8.4754
batch: [5260/10547] batch time: 0.758 trainign loss: 8.7017 avg training loss: 8.4759
batch: [5270/10547] batch time: 1.737 trainign loss: 8.5764 avg training loss: 8.4761
batch: [5280/10547] batch time: 0.738 trainign loss: 8.9050 avg training loss: 8.4765
batch: [5290/10547] batch time: 1.660 trainign loss: 8.8733 avg training loss: 8.4771
batch: [5300/10547] batch time: 0.942 trainign loss: 5.8506 avg training loss: 8.4760
batch: [5310/10547] batch time: 0.930 trainign loss: 9.0669 avg training loss: 8.4762
batch: [5320/10547] batch time: 1.457 trainign loss: 8.7453 avg training loss: 8.4768
batch: [5330/10547] batch time: 0.747 trainign loss: 8.7881 avg training loss: 8.4773
batch: [5340/10547] batch time: 0.661 trainign loss: 7.8425 avg training loss: 8.4773
batch: [5350/10547] batch time: 1.816 trainign loss: 8.7729 avg training loss: 8.4777
batch: [5360/10547] batch time: 0.855 trainign loss: 8.3196 avg training loss: 8.4776
batch: [5370/10547] batch time: 1.688 trainign loss: 8.4837 avg training loss: 8.4783
batch: [5380/10547] batch time: 1.207 trainign loss: 8.8055 avg training loss: 8.4783
batch: [5390/10547] batch time: 0.734 trainign loss: 7.5563 avg training loss: 8.4785
batch: [5400/10547] batch time: 1.498 trainign loss: 8.7878 avg training loss: 8.4786
batch: [5410/10547] batch time: 0.047 trainign loss: 8.7583 avg training loss: 8.4791
batch: [5420/10547] batch time: 2.116 trainign loss: 8.7859 avg training loss: 8.4796
batch: [5430/10547] batch time: 0.046 trainign loss: 8.8361 avg training loss: 8.4798
batch: [5440/10547] batch time: 2.169 trainign loss: 8.9076 avg training loss: 8.4806
batch: [5450/10547] batch time: 0.046 trainign loss: 8.7391 avg training loss: 8.4810
batch: [5460/10547] batch time: 2.377 trainign loss: 8.7164 avg training loss: 8.4812
batch: [5470/10547] batch time: 0.046 trainign loss: 6.3062 avg training loss: 8.4808
batch: [5480/10547] batch time: 2.063 trainign loss: 8.5896 avg training loss: 8.4796
batch: [5490/10547] batch time: 0.046 trainign loss: 8.9289 avg training loss: 8.4797
batch: [5500/10547] batch time: 2.134 trainign loss: 8.7610 avg training loss: 8.4803
batch: [5510/10547] batch time: 0.046 trainign loss: 8.7808 avg training loss: 8.4808
batch: [5520/10547] batch time: 2.616 trainign loss: 8.9011 avg training loss: 8.4788
batch: [5530/10547] batch time: 0.046 trainign loss: 8.5275 avg training loss: 8.4798
batch: [5540/10547] batch time: 2.588 trainign loss: 6.6271 avg training loss: 8.4795
batch: [5550/10547] batch time: 0.045 trainign loss: 9.3887 avg training loss: 8.4796
batch: [5560/10547] batch time: 2.252 trainign loss: 8.6166 avg training loss: 8.4801
batch: [5570/10547] batch time: 0.055 trainign loss: 8.9250 avg training loss: 8.4807
batch: [5580/10547] batch time: 2.529 trainign loss: 8.5402 avg training loss: 8.4811
batch: [5590/10547] batch time: 0.046 trainign loss: 9.4663 avg training loss: 8.4772
batch: [5600/10547] batch time: 2.328 trainign loss: 8.6418 avg training loss: 8.4788
batch: [5610/10547] batch time: 0.046 trainign loss: 8.9824 avg training loss: 8.4796
batch: [5620/10547] batch time: 2.575 trainign loss: 8.8389 avg training loss: 8.4802
batch: [5630/10547] batch time: 0.047 trainign loss: 8.6088 avg training loss: 8.4802
batch: [5640/10547] batch time: 2.116 trainign loss: 8.9622 avg training loss: 8.4797
batch: [5650/10547] batch time: 0.047 trainign loss: 8.8890 avg training loss: 8.4800
batch: [5660/10547] batch time: 2.507 trainign loss: 8.4700 avg training loss: 8.4805
batch: [5670/10547] batch time: 0.055 trainign loss: 7.6603 avg training loss: 8.4808
batch: [5680/10547] batch time: 2.596 trainign loss: 8.4229 avg training loss: 8.4809
batch: [5690/10547] batch time: 0.046 trainign loss: 8.8008 avg training loss: 8.4814
batch: [5700/10547] batch time: 2.532 trainign loss: 8.7647 avg training loss: 8.4819
batch: [5710/10547] batch time: 0.045 trainign loss: 8.7190 avg training loss: 8.4822
batch: [5720/10547] batch time: 2.151 trainign loss: 8.6714 avg training loss: 8.4827
batch: [5730/10547] batch time: 0.053 trainign loss: 8.7573 avg training loss: 8.4834
batch: [5740/10547] batch time: 2.469 trainign loss: 8.8886 avg training loss: 8.4840
batch: [5750/10547] batch time: 0.054 trainign loss: 8.2045 avg training loss: 8.4843
batch: [5760/10547] batch time: 2.368 trainign loss: 8.9378 avg training loss: 8.4842
batch: [5770/10547] batch time: 0.046 trainign loss: 8.7885 avg training loss: 8.4848
batch: [5780/10547] batch time: 2.626 trainign loss: 8.7585 avg training loss: 8.4852
batch: [5790/10547] batch time: 0.046 trainign loss: 1.2882 avg training loss: 8.4799
batch: [5800/10547] batch time: 1.997 trainign loss: 12.5347 avg training loss: 8.4732
batch: [5810/10547] batch time: 0.045 trainign loss: 8.9188 avg training loss: 8.4740
batch: [5820/10547] batch time: 2.639 trainign loss: 8.8909 avg training loss: 8.4739
batch: [5830/10547] batch time: 0.047 trainign loss: 8.7228 avg training loss: 8.4739
batch: [5840/10547] batch time: 2.079 trainign loss: 8.2447 avg training loss: 8.4738
batch: [5850/10547] batch time: 0.046 trainign loss: 9.6952 avg training loss: 8.4724
batch: [5860/10547] batch time: 2.302 trainign loss: 8.5676 avg training loss: 8.4730
batch: [5870/10547] batch time: 0.047 trainign loss: 8.2439 avg training loss: 8.4733
batch: [5880/10547] batch time: 1.985 trainign loss: 7.7989 avg training loss: 8.4730
batch: [5890/10547] batch time: 0.047 trainign loss: 8.4932 avg training loss: 8.4735
batch: [5900/10547] batch time: 2.591 trainign loss: 8.5294 avg training loss: 8.4740
batch: [5910/10547] batch time: 0.049 trainign loss: 8.8876 avg training loss: 8.4746
batch: [5920/10547] batch time: 1.962 trainign loss: 8.8624 avg training loss: 8.4753
batch: [5930/10547] batch time: 0.047 trainign loss: 7.4347 avg training loss: 8.4754
batch: [5940/10547] batch time: 2.020 trainign loss: 8.3864 avg training loss: 8.4735
batch: [5950/10547] batch time: 0.050 trainign loss: 7.9397 avg training loss: 8.4741
batch: [5960/10547] batch time: 2.264 trainign loss: 8.8986 avg training loss: 8.4748
batch: [5970/10547] batch time: 0.046 trainign loss: 9.2287 avg training loss: 8.4742
batch: [5980/10547] batch time: 2.257 trainign loss: 8.7938 avg training loss: 8.4743
batch: [5990/10547] batch time: 0.046 trainign loss: 8.9172 avg training loss: 8.4750
batch: [6000/10547] batch time: 1.961 trainign loss: 8.8469 avg training loss: 8.4754
batch: [6010/10547] batch time: 0.046 trainign loss: 8.8676 avg training loss: 8.4760
batch: [6020/10547] batch time: 1.367 trainign loss: 8.9470 avg training loss: 8.4767
batch: [6030/10547] batch time: 0.054 trainign loss: 8.6206 avg training loss: 8.4769
batch: [6040/10547] batch time: 1.547 trainign loss: 9.0724 avg training loss: 8.4774
batch: [6050/10547] batch time: 0.046 trainign loss: 8.8671 avg training loss: 8.4780
batch: [6060/10547] batch time: 1.198 trainign loss: 8.8188 avg training loss: 8.4786
batch: [6070/10547] batch time: 0.053 trainign loss: 8.8175 avg training loss: 8.4790
batch: [6080/10547] batch time: 1.107 trainign loss: 8.8881 avg training loss: 8.4797
batch: [6090/10547] batch time: 0.046 trainign loss: 8.9958 avg training loss: 8.4804
batch: [6100/10547] batch time: 1.172 trainign loss: 8.4780 avg training loss: 8.4807
batch: [6110/10547] batch time: 0.046 trainign loss: 8.8453 avg training loss: 8.4812
batch: [6120/10547] batch time: 0.679 trainign loss: 8.8091 avg training loss: 8.4817
batch: [6130/10547] batch time: 0.048 trainign loss: 7.9320 avg training loss: 8.4821
batch: [6140/10547] batch time: 0.425 trainign loss: 9.0069 avg training loss: 8.4823
batch: [6150/10547] batch time: 0.046 trainign loss: 8.8348 avg training loss: 8.4829
batch: [6160/10547] batch time: 1.268 trainign loss: 9.0130 avg training loss: 8.4832
batch: [6170/10547] batch time: 0.048 trainign loss: 3.4044 avg training loss: 8.4808
batch: [6180/10547] batch time: 1.847 trainign loss: 9.4683 avg training loss: 8.4776
batch: [6190/10547] batch time: 0.048 trainign loss: 8.8380 avg training loss: 8.4775
batch: [6200/10547] batch time: 1.349 trainign loss: 8.8109 avg training loss: 8.4777
batch: [6210/10547] batch time: 0.046 trainign loss: 8.8214 avg training loss: 8.4784
batch: [6220/10547] batch time: 1.685 trainign loss: 8.8340 avg training loss: 8.4791
batch: [6230/10547] batch time: 0.046 trainign loss: 8.1757 avg training loss: 8.4794
batch: [6240/10547] batch time: 2.379 trainign loss: 8.6832 avg training loss: 8.4796
batch: [6250/10547] batch time: 0.055 trainign loss: 8.6467 avg training loss: 8.4801
batch: [6260/10547] batch time: 2.620 trainign loss: 8.7575 avg training loss: 8.4803
batch: [6270/10547] batch time: 0.047 trainign loss: 7.7322 avg training loss: 8.4803
batch: [6280/10547] batch time: 2.149 trainign loss: 8.7199 avg training loss: 8.4805
batch: [6290/10547] batch time: 0.054 trainign loss: 8.8082 avg training loss: 8.4810
batch: [6300/10547] batch time: 2.109 trainign loss: 8.9771 avg training loss: 8.4814
batch: [6310/10547] batch time: 0.047 trainign loss: 7.5773 avg training loss: 8.4817
batch: [6320/10547] batch time: 1.977 trainign loss: 8.8879 avg training loss: 8.4823
batch: [6330/10547] batch time: 0.046 trainign loss: 8.5952 avg training loss: 8.4828
batch: [6340/10547] batch time: 2.688 trainign loss: 8.9595 avg training loss: 8.4832
batch: [6350/10547] batch time: 0.055 trainign loss: 8.9156 avg training loss: 8.4840
batch: [6360/10547] batch time: 2.525 trainign loss: 9.0106 avg training loss: 8.4845
batch: [6370/10547] batch time: 0.054 trainign loss: 9.0326 avg training loss: 8.4843
batch: [6380/10547] batch time: 2.371 trainign loss: 7.2279 avg training loss: 8.4846
batch: [6390/10547] batch time: 0.050 trainign loss: 8.9012 avg training loss: 8.4849
batch: [6400/10547] batch time: 2.542 trainign loss: 8.1052 avg training loss: 8.4850
batch: [6410/10547] batch time: 0.056 trainign loss: 8.8685 avg training loss: 8.4854
batch: [6420/10547] batch time: 1.756 trainign loss: 8.8586 avg training loss: 8.4850
batch: [6430/10547] batch time: 0.055 trainign loss: 8.8624 avg training loss: 8.4857
batch: [6440/10547] batch time: 1.486 trainign loss: 6.7736 avg training loss: 8.4851
batch: [6450/10547] batch time: 0.046 trainign loss: 9.2987 avg training loss: 8.4853
batch: [6460/10547] batch time: 1.433 trainign loss: 8.4683 avg training loss: 8.4860
batch: [6470/10547] batch time: 0.046 trainign loss: 9.3229 avg training loss: 8.4856
batch: [6480/10547] batch time: 1.375 trainign loss: 8.9836 avg training loss: 8.4864
batch: [6490/10547] batch time: 0.046 trainign loss: 8.6621 avg training loss: 8.4868
batch: [6500/10547] batch time: 0.916 trainign loss: 3.3086 avg training loss: 8.4840
batch: [6510/10547] batch time: 0.054 trainign loss: 0.0008 avg training loss: 8.4717
batch: [6520/10547] batch time: 0.697 trainign loss: 0.0000 avg training loss: 8.4587
batch: [6530/10547] batch time: 0.055 trainign loss: 10.5159 avg training loss: 8.4560
batch: [6540/10547] batch time: 1.059 trainign loss: 8.9365 avg training loss: 8.4568
batch: [6550/10547] batch time: 0.046 trainign loss: 8.6362 avg training loss: 8.4574
batch: [6560/10547] batch time: 1.131 trainign loss: 8.7065 avg training loss: 8.4581
batch: [6570/10547] batch time: 0.047 trainign loss: 7.4452 avg training loss: 8.4583
batch: [6580/10547] batch time: 1.926 trainign loss: 8.9301 avg training loss: 8.4585
batch: [6590/10547] batch time: 0.046 trainign loss: 8.9717 avg training loss: 8.4592
batch: [6600/10547] batch time: 1.397 trainign loss: 7.7441 avg training loss: 8.4594
batch: [6610/10547] batch time: 0.054 trainign loss: 7.8653 avg training loss: 8.4596
batch: [6620/10547] batch time: 0.646 trainign loss: 8.9415 avg training loss: 8.4604
batch: [6630/10547] batch time: 0.046 trainign loss: 8.9990 avg training loss: 8.4610
batch: [6640/10547] batch time: 0.651 trainign loss: 9.0357 avg training loss: 8.4610
batch: [6650/10547] batch time: 0.046 trainign loss: 7.7202 avg training loss: 8.4612
batch: [6660/10547] batch time: 0.069 trainign loss: 8.8565 avg training loss: 8.4616
batch: [6670/10547] batch time: 0.046 trainign loss: 8.9821 avg training loss: 8.4623
batch: [6680/10547] batch time: 0.046 trainign loss: 8.7350 avg training loss: 8.4625
batch: [6690/10547] batch time: 0.047 trainign loss: 8.9409 avg training loss: 8.4629
batch: [6700/10547] batch time: 0.047 trainign loss: 8.9831 avg training loss: 8.4635
batch: [6710/10547] batch time: 0.049 trainign loss: 8.8761 avg training loss: 8.4640
batch: [6720/10547] batch time: 0.047 trainign loss: 8.9149 avg training loss: 8.4646
batch: [6730/10547] batch time: 0.047 trainign loss: 9.0339 avg training loss: 8.4652
batch: [6740/10547] batch time: 0.047 trainign loss: 8.6005 avg training loss: 8.4659
batch: [6750/10547] batch time: 0.231 trainign loss: 8.7299 avg training loss: 8.4661
batch: [6760/10547] batch time: 0.046 trainign loss: 8.8498 avg training loss: 8.4666
batch: [6770/10547] batch time: 0.053 trainign loss: 8.9098 avg training loss: 8.4670
batch: [6780/10547] batch time: 0.047 trainign loss: 8.7597 avg training loss: 8.4673
batch: [6790/10547] batch time: 0.611 trainign loss: 8.9005 avg training loss: 8.4680
batch: [6800/10547] batch time: 0.046 trainign loss: 8.8799 avg training loss: 8.4686
batch: [6810/10547] batch time: 0.086 trainign loss: 8.9286 avg training loss: 8.4691
batch: [6820/10547] batch time: 0.046 trainign loss: 6.3172 avg training loss: 8.4675
batch: [6830/10547] batch time: 0.057 trainign loss: 8.0363 avg training loss: 8.4681
batch: [6840/10547] batch time: 0.046 trainign loss: 8.3517 avg training loss: 8.4685
batch: [6850/10547] batch time: 0.114 trainign loss: 7.2012 avg training loss: 8.4685
batch: [6860/10547] batch time: 0.043 trainign loss: 8.9754 avg training loss: 8.4687
batch: [6870/10547] batch time: 0.436 trainign loss: 8.9659 avg training loss: 8.4687
batch: [6880/10547] batch time: 0.054 trainign loss: 8.5390 avg training loss: 8.4692
batch: [6890/10547] batch time: 1.489 trainign loss: 8.9433 avg training loss: 8.4698
batch: [6900/10547] batch time: 0.050 trainign loss: 8.9879 avg training loss: 8.4702
batch: [6910/10547] batch time: 1.045 trainign loss: 8.8272 avg training loss: 8.4704
batch: [6920/10547] batch time: 0.048 trainign loss: 8.8259 avg training loss: 8.4707
batch: [6930/10547] batch time: 1.288 trainign loss: 8.9665 avg training loss: 8.4709
batch: [6940/10547] batch time: 0.047 trainign loss: 8.7875 avg training loss: 8.4715
batch: [6950/10547] batch time: 0.831 trainign loss: 8.5264 avg training loss: 8.4719
batch: [6960/10547] batch time: 0.145 trainign loss: 8.9948 avg training loss: 8.4719
batch: [6970/10547] batch time: 0.046 trainign loss: 8.7734 avg training loss: 8.4725
batch: [6980/10547] batch time: 0.046 trainign loss: 8.8587 avg training loss: 8.4730
batch: [6990/10547] batch time: 0.152 trainign loss: 8.7336 avg training loss: 8.4735
batch: [7000/10547] batch time: 0.118 trainign loss: 9.0835 avg training loss: 8.4734
batch: [7010/10547] batch time: 1.290 trainign loss: 7.2696 avg training loss: 8.4736
batch: [7020/10547] batch time: 0.047 trainign loss: 9.1514 avg training loss: 8.4733
batch: [7030/10547] batch time: 0.050 trainign loss: 9.0325 avg training loss: 8.4740
batch: [7040/10547] batch time: 0.046 trainign loss: 9.0168 avg training loss: 8.4744
batch: [7050/10547] batch time: 0.046 trainign loss: 8.9087 avg training loss: 8.4749
batch: [7060/10547] batch time: 0.055 trainign loss: 8.9256 avg training loss: 8.4756
batch: [7070/10547] batch time: 0.055 trainign loss: 9.1477 avg training loss: 8.4747
batch: [7080/10547] batch time: 0.399 trainign loss: 8.7916 avg training loss: 8.4754
batch: [7090/10547] batch time: 0.046 trainign loss: 9.0549 avg training loss: 8.4759
batch: [7100/10547] batch time: 0.046 trainign loss: 8.9293 avg training loss: 8.4765
batch: [7110/10547] batch time: 0.091 trainign loss: 8.9545 avg training loss: 8.4769
batch: [7120/10547] batch time: 0.046 trainign loss: 8.9632 avg training loss: 8.4775
batch: [7130/10547] batch time: 0.198 trainign loss: 8.5590 avg training loss: 8.4780
batch: [7140/10547] batch time: 0.048 trainign loss: 8.8480 avg training loss: 8.4782
batch: [7150/10547] batch time: 0.056 trainign loss: 5.3536 avg training loss: 8.4775
batch: [7160/10547] batch time: 0.046 trainign loss: 0.0033 avg training loss: 8.4674
batch: [7170/10547] batch time: 0.054 trainign loss: 0.0001 avg training loss: 8.4556
batch: [7180/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 8.4438
batch: [7190/10547] batch time: 0.054 trainign loss: 0.0000 avg training loss: 8.4321
batch: [7200/10547] batch time: 0.054 trainign loss: 0.0000 avg training loss: 8.4204
batch: [7210/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 8.4087
batch: [7220/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 8.3971
batch: [7230/10547] batch time: 0.055 trainign loss: 0.0000 avg training loss: 8.3855
batch: [7240/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 8.3739
batch: [7250/10547] batch time: 0.048 trainign loss: 8.8354 avg training loss: 8.3766
batch: [7260/10547] batch time: 0.048 trainign loss: 8.8482 avg training loss: 8.3771
batch: [7270/10547] batch time: 0.046 trainign loss: 8.5262 avg training loss: 8.3778
batch: [7280/10547] batch time: 0.054 trainign loss: 9.2185 avg training loss: 8.3787
batch: [7290/10547] batch time: 0.047 trainign loss: 9.1771 avg training loss: 8.3789
batch: [7300/10547] batch time: 0.055 trainign loss: 3.4497 avg training loss: 8.3770
batch: [7310/10547] batch time: 0.042 trainign loss: 13.7854 avg training loss: 8.3727
batch: [7320/10547] batch time: 0.402 trainign loss: 8.9418 avg training loss: 8.3739
batch: [7330/10547] batch time: 0.046 trainign loss: 8.8836 avg training loss: 8.3745
batch: [7340/10547] batch time: 1.022 trainign loss: 8.9988 avg training loss: 8.3755
batch: [7350/10547] batch time: 0.047 trainign loss: 8.9118 avg training loss: 8.3761
batch: [7360/10547] batch time: 0.219 trainign loss: 8.9731 avg training loss: 8.3769
batch: [7370/10547] batch time: 0.046 trainign loss: 8.9899 avg training loss: 8.3777
batch: [7380/10547] batch time: 0.046 trainign loss: 8.9412 avg training loss: 8.3784
batch: [7390/10547] batch time: 0.054 trainign loss: 8.9343 avg training loss: 8.3791
batch: [7400/10547] batch time: 0.053 trainign loss: 9.0220 avg training loss: 8.3795
batch: [7410/10547] batch time: 0.046 trainign loss: 8.8481 avg training loss: 8.3800
batch: [7420/10547] batch time: 0.047 trainign loss: 9.0798 avg training loss: 8.3808
batch: [7430/10547] batch time: 0.054 trainign loss: 8.6379 avg training loss: 8.3807
batch: [7440/10547] batch time: 0.518 trainign loss: 8.5887 avg training loss: 8.3813
batch: [7450/10547] batch time: 0.053 trainign loss: 9.0226 avg training loss: 8.3820
batch: [7460/10547] batch time: 0.046 trainign loss: 8.8772 avg training loss: 8.3826
batch: [7470/10547] batch time: 0.049 trainign loss: 8.9716 avg training loss: 8.3829
batch: [7480/10547] batch time: 0.046 trainign loss: 9.0577 avg training loss: 8.3836
batch: [7490/10547] batch time: 0.041 trainign loss: 5.8582 avg training loss: 8.3832
batch: [7500/10547] batch time: 0.046 trainign loss: 9.3503 avg training loss: 8.3836
batch: [7510/10547] batch time: 0.047 trainign loss: 9.0640 avg training loss: 8.3834
batch: [7520/10547] batch time: 0.046 trainign loss: 8.7613 avg training loss: 8.3837
batch: [7530/10547] batch time: 0.042 trainign loss: 9.0560 avg training loss: 8.3846
batch: [7540/10547] batch time: 0.050 trainign loss: 8.9096 avg training loss: 8.3850
batch: [7550/10547] batch time: 0.046 trainign loss: 7.0101 avg training loss: 8.3851
batch: [7560/10547] batch time: 0.054 trainign loss: 9.0078 avg training loss: 8.3859
batch: [7570/10547] batch time: 0.055 trainign loss: 8.9038 avg training loss: 8.3867
batch: [7580/10547] batch time: 0.054 trainign loss: 9.0639 avg training loss: 8.3869
batch: [7590/10547] batch time: 0.445 trainign loss: 9.1731 avg training loss: 8.3874
batch: [7600/10547] batch time: 0.046 trainign loss: 8.1946 avg training loss: 8.3878
batch: [7610/10547] batch time: 0.047 trainign loss: 8.9956 avg training loss: 8.3883
batch: [7620/10547] batch time: 0.047 trainign loss: 9.1917 avg training loss: 8.3889
batch: [7630/10547] batch time: 0.046 trainign loss: 8.7355 avg training loss: 8.3895
batch: [7640/10547] batch time: 0.046 trainign loss: 9.0202 avg training loss: 8.3896
batch: [7650/10547] batch time: 0.047 trainign loss: 9.0935 avg training loss: 8.3904
batch: [7660/10547] batch time: 0.054 trainign loss: 8.8872 avg training loss: 8.3910
batch: [7670/10547] batch time: 0.043 trainign loss: 8.2462 avg training loss: 8.3916
batch: [7680/10547] batch time: 0.046 trainign loss: 8.8440 avg training loss: 8.3922
batch: [7690/10547] batch time: 0.047 trainign loss: 8.8944 avg training loss: 8.3927
batch: [7700/10547] batch time: 0.047 trainign loss: 9.0457 avg training loss: 8.3931
batch: [7710/10547] batch time: 0.046 trainign loss: 9.1663 avg training loss: 8.3921
batch: [7720/10547] batch time: 0.048 trainign loss: 8.9025 avg training loss: 8.3930
batch: [7730/10547] batch time: 0.051 trainign loss: 8.5970 avg training loss: 8.3933
batch: [7740/10547] batch time: 0.046 trainign loss: 8.7255 avg training loss: 8.3940
batch: [7750/10547] batch time: 0.046 trainign loss: 9.0263 avg training loss: 8.3946
batch: [7760/10547] batch time: 0.399 trainign loss: 9.1089 avg training loss: 8.3953
batch: [7770/10547] batch time: 0.046 trainign loss: 9.0052 avg training loss: 8.3960
batch: [7780/10547] batch time: 0.046 trainign loss: 9.0532 avg training loss: 8.3966
batch: [7790/10547] batch time: 0.047 trainign loss: 8.9817 avg training loss: 8.3973
batch: [7800/10547] batch time: 0.048 trainign loss: 8.9734 avg training loss: 8.3979
batch: [7810/10547] batch time: 0.046 trainign loss: 4.9010 avg training loss: 8.3969
batch: [7820/10547] batch time: 0.047 trainign loss: 14.1080 avg training loss: 8.3912
batch: [7830/10547] batch time: 0.054 trainign loss: 8.9592 avg training loss: 8.3930
batch: [7840/10547] batch time: 0.049 trainign loss: 8.9747 avg training loss: 8.3936
batch: [7850/10547] batch time: 0.048 trainign loss: 7.7252 avg training loss: 8.3941
batch: [7860/10547] batch time: 0.329 trainign loss: 9.1510 avg training loss: 8.3947
batch: [7870/10547] batch time: 0.053 trainign loss: 9.0102 avg training loss: 8.3955
batch: [7880/10547] batch time: 1.467 trainign loss: 9.0457 avg training loss: 8.3960
batch: [7890/10547] batch time: 0.046 trainign loss: 9.0785 avg training loss: 8.3968
batch: [7900/10547] batch time: 1.705 trainign loss: 7.5653 avg training loss: 8.3971
batch: [7910/10547] batch time: 0.046 trainign loss: 9.0265 avg training loss: 8.3973
batch: [7920/10547] batch time: 0.874 trainign loss: 8.9439 avg training loss: 8.3979
batch: [7930/10547] batch time: 0.046 trainign loss: 8.7988 avg training loss: 8.3986
batch: [7940/10547] batch time: 0.628 trainign loss: 8.3531 avg training loss: 8.3992
batch: [7950/10547] batch time: 0.054 trainign loss: 9.8871 avg training loss: 8.3985
batch: [7960/10547] batch time: 0.046 trainign loss: 8.8783 avg training loss: 8.3993
batch: [7970/10547] batch time: 0.046 trainign loss: 9.0148 avg training loss: 8.3999
batch: [7980/10547] batch time: 0.047 trainign loss: 9.0467 avg training loss: 8.4007
batch: [7990/10547] batch time: 0.054 trainign loss: 9.1242 avg training loss: 8.4014
batch: [8000/10547] batch time: 0.148 trainign loss: 9.0013 avg training loss: 8.4019
batch: [8010/10547] batch time: 0.049 trainign loss: 8.8002 avg training loss: 8.4024
batch: [8020/10547] batch time: 0.467 trainign loss: 9.0351 avg training loss: 8.4029
batch: [8030/10547] batch time: 0.303 trainign loss: 9.0105 avg training loss: 8.4035
batch: [8040/10547] batch time: 1.602 trainign loss: 8.7671 avg training loss: 8.4041
batch: [8050/10547] batch time: 0.674 trainign loss: 8.7515 avg training loss: 8.4047
batch: [8060/10547] batch time: 0.860 trainign loss: 8.9845 avg training loss: 8.4054
batch: [8070/10547] batch time: 0.533 trainign loss: 8.8353 avg training loss: 8.4059
batch: [8080/10547] batch time: 1.200 trainign loss: 9.0054 avg training loss: 8.4066
batch: [8090/10547] batch time: 0.666 trainign loss: 8.8134 avg training loss: 8.4067
batch: [8100/10547] batch time: 1.061 trainign loss: 8.8106 avg training loss: 8.4069
batch: [8110/10547] batch time: 0.054 trainign loss: 8.7757 avg training loss: 8.4073
batch: [8120/10547] batch time: 0.808 trainign loss: 8.9244 avg training loss: 8.4080
batch: [8130/10547] batch time: 0.047 trainign loss: 8.6413 avg training loss: 8.4086
batch: [8140/10547] batch time: 1.240 trainign loss: 8.8816 avg training loss: 8.4093
batch: [8150/10547] batch time: 0.047 trainign loss: 8.9292 avg training loss: 8.4099
batch: [8160/10547] batch time: 0.093 trainign loss: 9.1020 avg training loss: 8.4104
batch: [8170/10547] batch time: 0.046 trainign loss: 8.0141 avg training loss: 8.4108
batch: [8180/10547] batch time: 0.046 trainign loss: 8.9325 avg training loss: 8.4113
batch: [8190/10547] batch time: 0.053 trainign loss: 8.7928 avg training loss: 8.4119
batch: [8200/10547] batch time: 0.053 trainign loss: 8.9538 avg training loss: 8.4124
batch: [8210/10547] batch time: 0.049 trainign loss: 6.1676 avg training loss: 8.4113
batch: [8220/10547] batch time: 1.548 trainign loss: 8.7964 avg training loss: 8.4117
batch: [8230/10547] batch time: 0.046 trainign loss: 8.7800 avg training loss: 8.4123
batch: [8240/10547] batch time: 0.421 trainign loss: 8.8099 avg training loss: 8.4127
batch: [8250/10547] batch time: 0.054 trainign loss: 9.0788 avg training loss: 8.4132
batch: [8260/10547] batch time: 0.047 trainign loss: 8.8313 avg training loss: 8.4139
batch: [8270/10547] batch time: 0.046 trainign loss: 8.9474 avg training loss: 8.4146
batch: [8280/10547] batch time: 0.046 trainign loss: 9.0611 avg training loss: 8.4152
batch: [8290/10547] batch time: 0.046 trainign loss: 8.6563 avg training loss: 8.4154
batch: [8300/10547] batch time: 0.049 trainign loss: 8.8571 avg training loss: 8.4157
batch: [8310/10547] batch time: 0.049 trainign loss: 8.9178 avg training loss: 8.4162
batch: [8320/10547] batch time: 0.048 trainign loss: 9.0772 avg training loss: 8.4169
batch: [8330/10547] batch time: 0.047 trainign loss: 9.0723 avg training loss: 8.4173
batch: [8340/10547] batch time: 0.055 trainign loss: 8.4608 avg training loss: 8.4178
batch: [8350/10547] batch time: 0.049 trainign loss: 0.2972 avg training loss: 8.4127
batch: [8360/10547] batch time: 0.047 trainign loss: 8.9904 avg training loss: 8.4133
batch: [8370/10547] batch time: 0.054 trainign loss: 9.0767 avg training loss: 8.4136
batch: [8380/10547] batch time: 1.077 trainign loss: 8.4969 avg training loss: 8.4138
batch: [8390/10547] batch time: 0.049 trainign loss: 7.1065 avg training loss: 8.4140
batch: [8400/10547] batch time: 1.337 trainign loss: 9.2605 avg training loss: 8.4140
batch: [8410/10547] batch time: 0.054 trainign loss: 9.0359 avg training loss: 8.4144
batch: [8420/10547] batch time: 1.671 trainign loss: 5.4543 avg training loss: 8.4111
batch: [8430/10547] batch time: 0.055 trainign loss: 7.9437 avg training loss: 8.4126
batch: [8440/10547] batch time: 2.316 trainign loss: 9.0423 avg training loss: 8.4128
batch: [8450/10547] batch time: 0.046 trainign loss: 9.1440 avg training loss: 8.4129
batch: [8460/10547] batch time: 2.165 trainign loss: 8.8412 avg training loss: 8.4130
batch: [8470/10547] batch time: 0.047 trainign loss: 9.1289 avg training loss: 8.4136
batch: [8480/10547] batch time: 1.412 trainign loss: 8.6721 avg training loss: 8.4142
batch: [8490/10547] batch time: 0.046 trainign loss: 8.8589 avg training loss: 8.4140
batch: [8500/10547] batch time: 1.450 trainign loss: 7.9406 avg training loss: 8.4144
batch: [8510/10547] batch time: 0.050 trainign loss: 8.7304 avg training loss: 8.4148
batch: [8520/10547] batch time: 2.421 trainign loss: 7.8649 avg training loss: 8.4149
batch: [8530/10547] batch time: 0.046 trainign loss: 7.2542 avg training loss: 8.4150
batch: [8540/10547] batch time: 2.548 trainign loss: 9.0013 avg training loss: 8.4158
batch: [8550/10547] batch time: 0.046 trainign loss: 8.9280 avg training loss: 8.4162
batch: [8560/10547] batch time: 2.195 trainign loss: 6.1717 avg training loss: 8.4159
batch: [8570/10547] batch time: 0.047 trainign loss: 10.5289 avg training loss: 8.4140
batch: [8580/10547] batch time: 1.688 trainign loss: 8.8888 avg training loss: 8.4138
batch: [8590/10547] batch time: 0.054 trainign loss: 9.2956 avg training loss: 8.4145
batch: [8600/10547] batch time: 2.297 trainign loss: 8.9466 avg training loss: 8.4152
batch: [8610/10547] batch time: 0.046 trainign loss: 9.0821 avg training loss: 8.4158
batch: [8620/10547] batch time: 2.159 trainign loss: 8.8447 avg training loss: 8.4165
batch: [8630/10547] batch time: 0.055 trainign loss: 8.0601 avg training loss: 8.4168
batch: [8640/10547] batch time: 1.744 trainign loss: 9.1005 avg training loss: 8.4174
batch: [8650/10547] batch time: 0.048 trainign loss: 9.0556 avg training loss: 8.4179
batch: [8660/10547] batch time: 2.649 trainign loss: 8.8360 avg training loss: 8.4184
batch: [8670/10547] batch time: 0.048 trainign loss: 8.7565 avg training loss: 8.4191
batch: [8680/10547] batch time: 1.921 trainign loss: 8.7550 avg training loss: 8.4195
batch: [8690/10547] batch time: 0.046 trainign loss: 1.2624 avg training loss: 8.4155
batch: [8700/10547] batch time: 2.159 trainign loss: 9.2474 avg training loss: 8.4153
batch: [8710/10547] batch time: 0.054 trainign loss: 8.4912 avg training loss: 8.4159
batch: [8720/10547] batch time: 0.928 trainign loss: 9.1534 avg training loss: 8.4166
batch: [8730/10547] batch time: 0.046 trainign loss: 9.1110 avg training loss: 8.4173
batch: [8740/10547] batch time: 2.146 trainign loss: 8.9089 avg training loss: 8.4180
batch: [8750/10547] batch time: 0.048 trainign loss: 8.9324 avg training loss: 8.4185
batch: [8760/10547] batch time: 1.855 trainign loss: 9.3045 avg training loss: 8.4188
batch: [8770/10547] batch time: 0.046 trainign loss: 9.0861 avg training loss: 8.4196
batch: [8780/10547] batch time: 2.126 trainign loss: 9.0799 avg training loss: 8.4202
batch: [8790/10547] batch time: 0.054 trainign loss: 9.1912 avg training loss: 8.4209
batch: [8800/10547] batch time: 0.750 trainign loss: 8.9931 avg training loss: 8.4216
batch: [8810/10547] batch time: 0.046 trainign loss: 8.9949 avg training loss: 8.4221
batch: [8820/10547] batch time: 0.280 trainign loss: 8.8857 avg training loss: 8.4227
batch: [8830/10547] batch time: 0.046 trainign loss: 8.9977 avg training loss: 8.4233
batch: [8840/10547] batch time: 1.057 trainign loss: 8.7544 avg training loss: 8.4238
batch: [8850/10547] batch time: 0.048 trainign loss: 9.0882 avg training loss: 8.4244
batch: [8860/10547] batch time: 0.850 trainign loss: 8.8842 avg training loss: 8.4250
batch: [8870/10547] batch time: 0.049 trainign loss: 8.9701 avg training loss: 8.4252
batch: [8880/10547] batch time: 2.025 trainign loss: 8.2491 avg training loss: 8.4256
batch: [8890/10547] batch time: 0.046 trainign loss: 8.9959 avg training loss: 8.4260
batch: [8900/10547] batch time: 2.259 trainign loss: 6.8292 avg training loss: 8.4262
batch: [8910/10547] batch time: 0.048 trainign loss: 9.0472 avg training loss: 8.4257
batch: [8920/10547] batch time: 2.750 trainign loss: 8.5586 avg training loss: 8.4263
batch: [8930/10547] batch time: 0.476 trainign loss: 9.0189 avg training loss: 8.4264
batch: [8940/10547] batch time: 1.972 trainign loss: 8.8235 avg training loss: 8.4270
batch: [8950/10547] batch time: 0.432 trainign loss: 8.7059 avg training loss: 8.4275
batch: [8960/10547] batch time: 2.048 trainign loss: 8.7490 avg training loss: 8.4280
batch: [8970/10547] batch time: 0.046 trainign loss: 8.3978 avg training loss: 8.4285
batch: [8980/10547] batch time: 1.330 trainign loss: 8.9234 avg training loss: 8.4282
batch: [8990/10547] batch time: 0.046 trainign loss: 8.9860 avg training loss: 8.4289
batch: [9000/10547] batch time: 2.093 trainign loss: 5.5026 avg training loss: 8.4284
batch: [9010/10547] batch time: 0.048 trainign loss: 9.7080 avg training loss: 8.4279
batch: [9020/10547] batch time: 2.176 trainign loss: 8.1351 avg training loss: 8.4282
batch: [9030/10547] batch time: 0.192 trainign loss: 8.5683 avg training loss: 8.4287
batch: [9040/10547] batch time: 2.045 trainign loss: 8.9840 avg training loss: 8.4294
batch: [9050/10547] batch time: 0.399 trainign loss: 8.9817 avg training loss: 8.4295
batch: [9060/10547] batch time: 0.668 trainign loss: 9.0424 avg training loss: 8.4299
batch: [9070/10547] batch time: 0.726 trainign loss: 8.0729 avg training loss: 8.4303
batch: [9080/10547] batch time: 0.444 trainign loss: 8.9389 avg training loss: 8.4307
batch: [9090/10547] batch time: 0.337 trainign loss: 8.6016 avg training loss: 8.4314
batch: [9100/10547] batch time: 1.365 trainign loss: 8.4461 avg training loss: 8.4319
batch: [9110/10547] batch time: 0.274 trainign loss: 8.9575 avg training loss: 8.4324
batch: [9120/10547] batch time: 1.118 trainign loss: 9.2241 avg training loss: 8.4329
batch: [9130/10547] batch time: 0.318 trainign loss: 3.4869 avg training loss: 8.4313
batch: [9140/10547] batch time: 1.016 trainign loss: 9.2760 avg training loss: 8.4301
batch: [9150/10547] batch time: 1.126 trainign loss: 9.0192 avg training loss: 8.4306
batch: [9160/10547] batch time: 1.726 trainign loss: 7.6941 avg training loss: 8.4309
batch: [9170/10547] batch time: 0.746 trainign loss: 0.0873 avg training loss: 8.4251
batch: [9180/10547] batch time: 1.912 trainign loss: 9.2984 avg training loss: 8.4269
batch: [9190/10547] batch time: 1.483 trainign loss: 9.1059 avg training loss: 8.4274
batch: [9200/10547] batch time: 0.321 trainign loss: 9.0518 avg training loss: 8.4277
batch: [9210/10547] batch time: 1.619 trainign loss: 8.9351 avg training loss: 8.4282
batch: [9220/10547] batch time: 0.424 trainign loss: 8.1450 avg training loss: 8.4288
batch: [9230/10547] batch time: 1.686 trainign loss: 9.0684 avg training loss: 8.4287
batch: [9240/10547] batch time: 0.047 trainign loss: 8.9188 avg training loss: 8.4295
batch: [9250/10547] batch time: 1.130 trainign loss: 3.6474 avg training loss: 8.4280
batch: [9260/10547] batch time: 0.051 trainign loss: 15.5759 avg training loss: 8.4229
batch: [9270/10547] batch time: 1.688 trainign loss: 9.1225 avg training loss: 8.4247
batch: [9280/10547] batch time: 0.168 trainign loss: 9.1379 avg training loss: 8.4253
batch: [9290/10547] batch time: 2.078 trainign loss: 9.1055 avg training loss: 8.4259
batch: [9300/10547] batch time: 0.047 trainign loss: 9.0275 avg training loss: 8.4264
batch: [9310/10547] batch time: 1.585 trainign loss: 9.1600 avg training loss: 8.4267
batch: [9320/10547] batch time: 0.046 trainign loss: 9.0953 avg training loss: 8.4271
batch: [9330/10547] batch time: 1.460 trainign loss: 9.0465 avg training loss: 8.4276
batch: [9340/10547] batch time: 0.045 trainign loss: 7.5885 avg training loss: 8.4279
batch: [9350/10547] batch time: 1.904 trainign loss: 9.1423 avg training loss: 8.4284
batch: [9360/10547] batch time: 0.053 trainign loss: 9.1246 avg training loss: 8.4284
batch: [9370/10547] batch time: 1.924 trainign loss: 9.0576 avg training loss: 8.4292
batch: [9380/10547] batch time: 0.046 trainign loss: 8.8342 avg training loss: 8.4298
batch: [9390/10547] batch time: 1.423 trainign loss: 9.1697 avg training loss: 8.4300
batch: [9400/10547] batch time: 0.047 trainign loss: 6.4329 avg training loss: 8.4300
batch: [9410/10547] batch time: 0.739 trainign loss: 9.0570 avg training loss: 8.4307
batch: [9420/10547] batch time: 0.054 trainign loss: 9.0369 avg training loss: 8.4313
batch: [9430/10547] batch time: 0.746 trainign loss: 8.9838 avg training loss: 8.4317
batch: [9440/10547] batch time: 0.054 trainign loss: 8.7363 avg training loss: 8.4323
batch: [9450/10547] batch time: 0.047 trainign loss: 8.4542 avg training loss: 8.4325
batch: [9460/10547] batch time: 0.046 trainign loss: 9.1657 avg training loss: 8.4332
batch: [9470/10547] batch time: 0.052 trainign loss: 7.8338 avg training loss: 8.4336
batch: [9480/10547] batch time: 0.043 trainign loss: 8.4470 avg training loss: 8.4340
batch: [9490/10547] batch time: 0.046 trainign loss: 9.2420 avg training loss: 8.4347
batch: [9500/10547] batch time: 0.047 trainign loss: 8.7561 avg training loss: 8.4353
batch: [9510/10547] batch time: 0.049 trainign loss: 9.1598 avg training loss: 8.4355
batch: [9520/10547] batch time: 0.048 trainign loss: 8.9304 avg training loss: 8.4361
batch: [9530/10547] batch time: 0.048 trainign loss: 9.1580 avg training loss: 8.4364
batch: [9540/10547] batch time: 0.047 trainign loss: 7.3711 avg training loss: 8.4366
batch: [9550/10547] batch time: 0.046 trainign loss: 9.2456 avg training loss: 8.4367
batch: [9560/10547] batch time: 0.045 trainign loss: 9.1110 avg training loss: 8.4374
batch: [9570/10547] batch time: 0.047 trainign loss: 9.0730 avg training loss: 8.4380
batch: [9580/10547] batch time: 0.054 trainign loss: 9.0853 avg training loss: 8.4386
batch: [9590/10547] batch time: 0.195 trainign loss: 7.9135 avg training loss: 8.4390
batch: [9600/10547] batch time: 0.055 trainign loss: 8.7747 avg training loss: 8.4394
batch: [9610/10547] batch time: 0.048 trainign loss: 8.9438 avg training loss: 8.4400
batch: [9620/10547] batch time: 0.046 trainign loss: 8.8061 avg training loss: 8.4405
batch: [9630/10547] batch time: 0.345 trainign loss: 9.0659 avg training loss: 8.4412
batch: [9640/10547] batch time: 0.046 trainign loss: 9.0750 avg training loss: 8.4418
batch: [9650/10547] batch time: 0.424 trainign loss: 9.1263 avg training loss: 8.4423
batch: [9660/10547] batch time: 0.046 trainign loss: 9.0945 avg training loss: 8.4428
batch: [9670/10547] batch time: 0.282 trainign loss: 8.9936 avg training loss: 8.4433
batch: [9680/10547] batch time: 0.045 trainign loss: 8.9686 avg training loss: 8.4439
batch: [9690/10547] batch time: 0.396 trainign loss: 9.0030 avg training loss: 8.4442
batch: [9700/10547] batch time: 0.047 trainign loss: 8.6673 avg training loss: 8.4448
batch: [9710/10547] batch time: 0.046 trainign loss: 8.9976 avg training loss: 8.4454
batch: [9720/10547] batch time: 0.047 trainign loss: 9.0582 avg training loss: 8.4459
batch: [9730/10547] batch time: 0.046 trainign loss: 4.7469 avg training loss: 8.4451
batch: [9740/10547] batch time: 0.046 trainign loss: 9.3201 avg training loss: 8.4442
batch: [9750/10547] batch time: 0.404 trainign loss: 9.0283 avg training loss: 8.4448
batch: [9760/10547] batch time: 0.051 trainign loss: 9.0829 avg training loss: 8.4453
batch: [9770/10547] batch time: 0.441 trainign loss: 9.0608 avg training loss: 8.4459
batch: [9780/10547] batch time: 0.048 trainign loss: 9.1080 avg training loss: 8.4466
batch: [9790/10547] batch time: 0.046 trainign loss: 9.1796 avg training loss: 8.4471
batch: [9800/10547] batch time: 0.046 trainign loss: 9.0676 avg training loss: 8.4473
batch: [9810/10547] batch time: 0.047 trainign loss: 9.2357 avg training loss: 8.4480
batch: [9820/10547] batch time: 0.046 trainign loss: 9.2367 avg training loss: 8.4486
batch: [9830/10547] batch time: 0.382 trainign loss: 9.0654 avg training loss: 8.4491
batch: [9840/10547] batch time: 0.051 trainign loss: 5.1825 avg training loss: 8.4485
batch: [9850/10547] batch time: 0.984 trainign loss: 3.5252 avg training loss: 8.4464
batch: [9860/10547] batch time: 0.046 trainign loss: 8.8856 avg training loss: 8.4470
batch: [9870/10547] batch time: 0.301 trainign loss: 8.1253 avg training loss: 8.4471
batch: [9880/10547] batch time: 0.047 trainign loss: 9.1509 avg training loss: 8.4475
batch: [9890/10547] batch time: 1.190 trainign loss: 9.1911 avg training loss: 8.4482
batch: [9900/10547] batch time: 0.047 trainign loss: 9.0229 avg training loss: 8.4488
batch: [9910/10547] batch time: 0.053 trainign loss: 6.6257 avg training loss: 8.4488
batch: [9920/10547] batch time: 0.046 trainign loss: 8.9059 avg training loss: 8.4478
batch: [9930/10547] batch time: 0.047 trainign loss: 9.0290 avg training loss: 8.4481
batch: [9940/10547] batch time: 0.042 trainign loss: 9.1770 avg training loss: 8.4476
batch: [9950/10547] batch time: 0.047 trainign loss: 9.0826 avg training loss: 8.4482
batch: [9960/10547] batch time: 0.053 trainign loss: 8.5382 avg training loss: 8.4488
batch: [9970/10547] batch time: 0.046 trainign loss: 9.5995 avg training loss: 8.4472
batch: [9980/10547] batch time: 0.748 trainign loss: 9.1993 avg training loss: 8.4482
batch: [9990/10547] batch time: 0.046 trainign loss: 8.9587 avg training loss: 8.4487
batch: [10000/10547] batch time: 0.665 trainign loss: 9.2536 avg training loss: 8.4493
batch: [10010/10547] batch time: 0.047 trainign loss: 9.0244 avg training loss: 8.4495
batch: [10020/10547] batch time: 0.350 trainign loss: 9.2052 avg training loss: 8.4499
batch: [10030/10547] batch time: 0.047 trainign loss: 9.1304 avg training loss: 8.4504
batch: [10040/10547] batch time: 0.046 trainign loss: 8.0772 avg training loss: 8.4508
batch: [10050/10547] batch time: 0.048 trainign loss: 8.6509 avg training loss: 8.4513
batch: [10060/10547] batch time: 0.045 trainign loss: 9.7727 avg training loss: 8.4500
batch: [10070/10547] batch time: 0.050 trainign loss: 9.0886 avg training loss: 8.4508
batch: [10080/10547] batch time: 0.046 trainign loss: 8.8518 avg training loss: 8.4511
batch: [10090/10547] batch time: 0.047 trainign loss: 9.3691 avg training loss: 8.4511
batch: [10100/10547] batch time: 0.041 trainign loss: 8.2931 avg training loss: 8.4517
batch: [10110/10547] batch time: 0.048 trainign loss: 8.9615 avg training loss: 8.4521
batch: [10120/10547] batch time: 0.046 trainign loss: 8.5264 avg training loss: 8.4525
batch: [10130/10547] batch time: 0.046 trainign loss: 7.9942 avg training loss: 8.4523
batch: [10140/10547] batch time: 0.046 trainign loss: 8.2969 avg training loss: 8.4526
batch: [10150/10547] batch time: 0.046 trainign loss: 8.9575 avg training loss: 8.4532
batch: [10160/10547] batch time: 0.042 trainign loss: 0.9820 avg training loss: 8.4499
batch: [10170/10547] batch time: 0.047 trainign loss: 9.1269 avg training loss: 8.4472
batch: [10180/10547] batch time: 0.046 trainign loss: 9.1303 avg training loss: 8.4479
batch: [10190/10547] batch time: 0.047 trainign loss: 6.3443 avg training loss: 8.4476
batch: [10200/10547] batch time: 0.042 trainign loss: 9.1603 avg training loss: 8.4483
batch: [10210/10547] batch time: 0.054 trainign loss: 8.9938 avg training loss: 8.4485
batch: [10220/10547] batch time: 0.044 trainign loss: 8.9138 avg training loss: 8.4490
batch: [10230/10547] batch time: 0.046 trainign loss: 9.0118 avg training loss: 8.4497
batch: [10240/10547] batch time: 0.047 trainign loss: 8.9443 avg training loss: 8.4501
batch: [10250/10547] batch time: 0.046 trainign loss: 8.7764 avg training loss: 8.4506
batch: [10260/10547] batch time: 0.042 trainign loss: 8.8973 avg training loss: 8.4510
batch: [10270/10547] batch time: 0.047 trainign loss: 8.9730 avg training loss: 8.4515
batch: [10280/10547] batch time: 0.046 trainign loss: 9.0739 avg training loss: 8.4521
batch: [10290/10547] batch time: 0.046 trainign loss: 8.8717 avg training loss: 8.4526
batch: [10300/10547] batch time: 0.047 trainign loss: 9.1306 avg training loss: 8.4532
batch: [10310/10547] batch time: 0.047 trainign loss: 9.0527 avg training loss: 8.4537
batch: [10320/10547] batch time: 0.046 trainign loss: 9.0951 avg training loss: 8.4543
batch: [10330/10547] batch time: 0.050 trainign loss: 8.7632 avg training loss: 8.4546
batch: [10340/10547] batch time: 0.042 trainign loss: 9.2194 avg training loss: 8.4551
batch: [10350/10547] batch time: 0.050 trainign loss: 8.9849 avg training loss: 8.4557
batch: [10360/10547] batch time: 0.054 trainign loss: 8.9001 avg training loss: 8.4559
batch: [10370/10547] batch time: 0.054 trainign loss: 5.4519 avg training loss: 8.4555
batch: [10380/10547] batch time: 0.048 trainign loss: 8.7517 avg training loss: 8.4553
batch: [10390/10547] batch time: 0.046 trainign loss: 9.1066 avg training loss: 8.4555
batch: [10400/10547] batch time: 0.157 trainign loss: 9.0337 avg training loss: 8.4560
batch: [10410/10547] batch time: 0.047 trainign loss: 9.0272 avg training loss: 8.4565
batch: [10420/10547] batch time: 0.041 trainign loss: 9.0557 avg training loss: 8.4570
batch: [10430/10547] batch time: 0.046 trainign loss: 8.8990 avg training loss: 8.4575
batch: [10440/10547] batch time: 0.048 trainign loss: 8.8203 avg training loss: 8.4578
batch: [10450/10547] batch time: 0.046 trainign loss: 9.1207 avg training loss: 8.4581
batch: [10460/10547] batch time: 0.046 trainign loss: 9.0080 avg training loss: 8.4584
batch: [10470/10547] batch time: 0.047 trainign loss: 8.8372 avg training loss: 8.4590
batch: [10480/10547] batch time: 0.046 trainign loss: 7.7435 avg training loss: 8.4593
batch: [10490/10547] batch time: 0.051 trainign loss: 9.2029 avg training loss: 8.4591
batch: [10500/10547] batch time: 0.046 trainign loss: 9.1053 avg training loss: 8.4596
batch: [10510/10547] batch time: 0.512 trainign loss: 9.1758 avg training loss: 8.4601
batch: [10520/10547] batch time: 0.046 trainign loss: 8.9870 avg training loss: 8.4607
batch: [10530/10547] batch time: 0.393 trainign loss: 9.1135 avg training loss: 8.4608
batch: [10540/10547] batch time: 0.046 trainign loss: 9.1368 avg training loss: 8.4612
Epoch: 2
----------------------------------------------------------------------
batch: [0/10547] batch time: 2.964 trainign loss: 8.8161 avg training loss: 8.4616
batch: [10/10547] batch time: 0.057 trainign loss: 9.0360 avg training loss: 8.4607
batch: [20/10547] batch time: 0.219 trainign loss: 8.7251 avg training loss: 8.4610
batch: [30/10547] batch time: 0.056 trainign loss: 8.6215 avg training loss: 8.4608
batch: [40/10547] batch time: 0.182 trainign loss: 8.6322 avg training loss: 8.4609
batch: [50/10547] batch time: 0.057 trainign loss: 8.7335 avg training loss: 8.4609
batch: [60/10547] batch time: 0.272 trainign loss: 8.2877 avg training loss: 8.4609
batch: [70/10547] batch time: 0.055 trainign loss: 8.4615 avg training loss: 8.4608
batch: [80/10547] batch time: 0.077 trainign loss: 8.7886 avg training loss: 8.4605
batch: [90/10547] batch time: 0.055 trainign loss: 8.3440 avg training loss: 8.4606
batch: [100/10547] batch time: 0.055 trainign loss: 6.7335 avg training loss: 8.4602
batch: [110/10547] batch time: 0.054 trainign loss: 0.0257 avg training loss: 8.4545
batch: [120/10547] batch time: 0.842 trainign loss: 8.7144 avg training loss: 8.4557
batch: [130/10547] batch time: 1.510 trainign loss: 8.7255 avg training loss: 8.4558
batch: [140/10547] batch time: 1.207 trainign loss: 8.5579 avg training loss: 8.4560
batch: [150/10547] batch time: 0.774 trainign loss: 7.6342 avg training loss: 8.4557
batch: [160/10547] batch time: 0.350 trainign loss: 8.5620 avg training loss: 8.4553
batch: [170/10547] batch time: 0.304 trainign loss: 7.9870 avg training loss: 8.4551
batch: [180/10547] batch time: 0.896 trainign loss: 8.4942 avg training loss: 8.4551
batch: [190/10547] batch time: 0.046 trainign loss: 8.5344 avg training loss: 8.4550
batch: [200/10547] batch time: 1.247 trainign loss: 8.2235 avg training loss: 8.4548
batch: [210/10547] batch time: 0.046 trainign loss: 8.4017 avg training loss: 8.4543
batch: [220/10547] batch time: 0.467 trainign loss: 8.5421 avg training loss: 8.4543
batch: [230/10547] batch time: 0.046 trainign loss: 8.5553 avg training loss: 8.4542
batch: [240/10547] batch time: 0.049 trainign loss: 8.6101 avg training loss: 8.4538
batch: [250/10547] batch time: 0.046 trainign loss: 8.4164 avg training loss: 8.4537
batch: [260/10547] batch time: 0.048 trainign loss: 8.3938 avg training loss: 8.4536
batch: [270/10547] batch time: 0.046 trainign loss: 8.0605 avg training loss: 8.4530
batch: [280/10547] batch time: 0.046 trainign loss: 7.7452 avg training loss: 8.4528
batch: [290/10547] batch time: 0.052 trainign loss: 7.9911 avg training loss: 8.4521
batch: [300/10547] batch time: 0.046 trainign loss: 7.8204 avg training loss: 8.4517
batch: [310/10547] batch time: 0.042 trainign loss: 8.2391 avg training loss: 8.4512
batch: [320/10547] batch time: 0.046 trainign loss: 8.5311 avg training loss: 8.4512
batch: [330/10547] batch time: 0.046 trainign loss: 8.2544 avg training loss: 8.4512
batch: [340/10547] batch time: 0.046 trainign loss: 8.3598 avg training loss: 8.4505
batch: [350/10547] batch time: 0.044 trainign loss: 8.7111 avg training loss: 8.4496
batch: [360/10547] batch time: 0.048 trainign loss: 8.5160 avg training loss: 8.4494
batch: [370/10547] batch time: 0.041 trainign loss: 8.5556 avg training loss: 8.4492
batch: [380/10547] batch time: 0.048 trainign loss: 8.3104 avg training loss: 8.4492
batch: [390/10547] batch time: 0.046 trainign loss: 8.4926 avg training loss: 8.4492
batch: [400/10547] batch time: 0.049 trainign loss: 8.5289 avg training loss: 8.4488
batch: [410/10547] batch time: 0.047 trainign loss: 8.3922 avg training loss: 8.4488
batch: [420/10547] batch time: 0.054 trainign loss: 8.3756 avg training loss: 8.4486
batch: [430/10547] batch time: 0.051 trainign loss: 8.3603 avg training loss: 8.4484
batch: [440/10547] batch time: 0.052 trainign loss: 6.8418 avg training loss: 8.4479
batch: [450/10547] batch time: 0.047 trainign loss: 8.6883 avg training loss: 8.4475
batch: [460/10547] batch time: 0.831 trainign loss: 8.2462 avg training loss: 8.4474
batch: [470/10547] batch time: 0.050 trainign loss: 4.8125 avg training loss: 8.4465
batch: [480/10547] batch time: 0.046 trainign loss: 14.1340 avg training loss: 8.4417
batch: [490/10547] batch time: 0.046 trainign loss: 8.6968 avg training loss: 8.4434
batch: [500/10547] batch time: 0.055 trainign loss: 8.5450 avg training loss: 8.4431
batch: [510/10547] batch time: 0.049 trainign loss: 8.1505 avg training loss: 8.4431
batch: [520/10547] batch time: 0.046 trainign loss: 8.5486 avg training loss: 8.4429
batch: [530/10547] batch time: 0.046 trainign loss: 3.3117 avg training loss: 8.4412
batch: [540/10547] batch time: 0.051 trainign loss: 9.7655 avg training loss: 8.4392
batch: [550/10547] batch time: 0.046 trainign loss: 8.4779 avg training loss: 8.4394
batch: [560/10547] batch time: 0.048 trainign loss: 8.6333 avg training loss: 8.4394
batch: [570/10547] batch time: 0.046 trainign loss: 8.2995 avg training loss: 8.4394
batch: [580/10547] batch time: 0.054 trainign loss: 8.2201 avg training loss: 8.4395
batch: [590/10547] batch time: 0.046 trainign loss: 8.6436 avg training loss: 8.4393
batch: [600/10547] batch time: 0.046 trainign loss: 7.4770 avg training loss: 8.4389
batch: [610/10547] batch time: 0.047 trainign loss: 8.3366 avg training loss: 8.4388
batch: [620/10547] batch time: 0.388 trainign loss: 7.8480 avg training loss: 8.4386
batch: [630/10547] batch time: 0.046 trainign loss: 8.1674 avg training loss: 8.4383
batch: [640/10547] batch time: 0.047 trainign loss: 5.8968 avg training loss: 8.4376
batch: [650/10547] batch time: 0.048 trainign loss: 7.7345 avg training loss: 8.4370
batch: [660/10547] batch time: 0.048 trainign loss: 7.1361 avg training loss: 8.4363
batch: [670/10547] batch time: 0.054 trainign loss: 7.8581 avg training loss: 8.4349
batch: [680/10547] batch time: 0.046 trainign loss: 7.7947 avg training loss: 8.4344
batch: [690/10547] batch time: 0.048 trainign loss: 7.4453 avg training loss: 8.4336
batch: [700/10547] batch time: 1.101 trainign loss: 7.5292 avg training loss: 8.4328
batch: [710/10547] batch time: 0.047 trainign loss: 8.0078 avg training loss: 8.4322
batch: [720/10547] batch time: 0.427 trainign loss: 5.4571 avg training loss: 8.4311
batch: [730/10547] batch time: 0.048 trainign loss: 11.3343 avg training loss: 8.4260
batch: [740/10547] batch time: 0.501 trainign loss: 6.4235 avg training loss: 8.4271
batch: [750/10547] batch time: 0.046 trainign loss: 6.8758 avg training loss: 8.4265
batch: [760/10547] batch time: 0.379 trainign loss: 3.7475 avg training loss: 8.4241
batch: [770/10547] batch time: 0.046 trainign loss: 7.8511 avg training loss: 8.4242
batch: [780/10547] batch time: 0.391 trainign loss: 7.9601 avg training loss: 8.4236
batch: [790/10547] batch time: 0.054 trainign loss: 7.6636 avg training loss: 8.4231
batch: [800/10547] batch time: 0.439 trainign loss: 7.9078 avg training loss: 8.4226
batch: [810/10547] batch time: 0.046 trainign loss: 6.0901 avg training loss: 8.4216
batch: [820/10547] batch time: 1.641 trainign loss: 7.9676 avg training loss: 8.4208
batch: [830/10547] batch time: 0.046 trainign loss: 8.0635 avg training loss: 8.4203
batch: [840/10547] batch time: 1.094 trainign loss: 6.9760 avg training loss: 8.4194
batch: [850/10547] batch time: 0.054 trainign loss: 7.5056 avg training loss: 8.4187
batch: [860/10547] batch time: 0.974 trainign loss: 7.5615 avg training loss: 8.4172
batch: [870/10547] batch time: 0.046 trainign loss: 7.5309 avg training loss: 8.4168
batch: [880/10547] batch time: 0.462 trainign loss: 6.6885 avg training loss: 8.4160
batch: [890/10547] batch time: 0.046 trainign loss: 6.4993 avg training loss: 8.4134
batch: [900/10547] batch time: 0.048 trainign loss: 7.2450 avg training loss: 8.4132
batch: [910/10547] batch time: 0.049 trainign loss: 6.5185 avg training loss: 8.4124
batch: [920/10547] batch time: 0.046 trainign loss: 7.8671 avg training loss: 8.4111
batch: [930/10547] batch time: 0.047 trainign loss: 6.4064 avg training loss: 8.4101
batch: [940/10547] batch time: 0.054 trainign loss: 7.6991 avg training loss: 8.4090
batch: [950/10547] batch time: 0.042 trainign loss: 6.9484 avg training loss: 8.4080
batch: [960/10547] batch time: 0.049 trainign loss: 2.0994 avg training loss: 8.4051
batch: [970/10547] batch time: 0.046 trainign loss: 7.4136 avg training loss: 8.4047
batch: [980/10547] batch time: 0.381 trainign loss: 6.3542 avg training loss: 8.4025
batch: [990/10547] batch time: 0.054 trainign loss: 3.1682 avg training loss: 8.3996
batch: [1000/10547] batch time: 0.211 trainign loss: 11.7149 avg training loss: 8.3944
batch: [1010/10547] batch time: 0.260 trainign loss: 7.7867 avg training loss: 8.3947
batch: [1020/10547] batch time: 0.049 trainign loss: 0.2843 avg training loss: 8.3906
batch: [1030/10547] batch time: 0.051 trainign loss: 0.0002 avg training loss: 8.3833
batch: [1040/10547] batch time: 0.046 trainign loss: 16.3562 avg training loss: 8.3810
batch: [1050/10547] batch time: 0.047 trainign loss: 8.7016 avg training loss: 8.3820
batch: [1060/10547] batch time: 0.046 trainign loss: 8.0861 avg training loss: 8.3819
batch: [1070/10547] batch time: 0.043 trainign loss: 8.3916 avg training loss: 8.3816
batch: [1080/10547] batch time: 0.046 trainign loss: 5.4325 avg training loss: 8.3805
batch: [1090/10547] batch time: 0.046 trainign loss: 7.3741 avg training loss: 8.3798
batch: [1100/10547] batch time: 0.046 trainign loss: 5.7968 avg training loss: 8.3784
batch: [1110/10547] batch time: 0.046 trainign loss: 6.8806 avg training loss: 8.3761
batch: [1120/10547] batch time: 0.376 trainign loss: 7.0440 avg training loss: 8.3738
batch: [1130/10547] batch time: 0.055 trainign loss: 7.3626 avg training loss: 8.3718
batch: [1140/10547] batch time: 0.934 trainign loss: 7.4344 avg training loss: 8.3711
batch: [1150/10547] batch time: 0.048 trainign loss: 6.9025 avg training loss: 8.3702
batch: [1160/10547] batch time: 0.046 trainign loss: 6.5250 avg training loss: 8.3688
batch: [1170/10547] batch time: 0.047 trainign loss: 6.9124 avg training loss: 8.3676
batch: [1180/10547] batch time: 0.190 trainign loss: 7.2862 avg training loss: 8.3667
batch: [1190/10547] batch time: 1.226 trainign loss: 7.1320 avg training loss: 8.3658
batch: [1200/10547] batch time: 0.227 trainign loss: 5.8114 avg training loss: 8.3644
batch: [1210/10547] batch time: 1.004 trainign loss: 5.8411 avg training loss: 8.3632
batch: [1220/10547] batch time: 0.046 trainign loss: 6.7611 avg training loss: 8.3618
batch: [1230/10547] batch time: 0.645 trainign loss: 6.8912 avg training loss: 8.3605
batch: [1240/10547] batch time: 0.046 trainign loss: 7.1714 avg training loss: 8.3595
batch: [1250/10547] batch time: 1.111 trainign loss: 7.1887 avg training loss: 8.3581
batch: [1260/10547] batch time: 0.046 trainign loss: 0.1524 avg training loss: 8.3538
batch: [1270/10547] batch time: 0.428 trainign loss: 7.7029 avg training loss: 8.3536
batch: [1280/10547] batch time: 0.493 trainign loss: 4.0740 avg training loss: 8.3522
batch: [1290/10547] batch time: 0.045 trainign loss: 7.6915 avg training loss: 8.3516
batch: [1300/10547] batch time: 0.352 trainign loss: 6.8483 avg training loss: 8.3502
batch: [1310/10547] batch time: 0.047 trainign loss: 6.0370 avg training loss: 8.3489
batch: [1320/10547] batch time: 0.066 trainign loss: 5.0279 avg training loss: 8.3467
batch: [1330/10547] batch time: 0.042 trainign loss: 6.7354 avg training loss: 8.3449
batch: [1340/10547] batch time: 0.049 trainign loss: 7.3878 avg training loss: 8.3436
batch: [1350/10547] batch time: 0.054 trainign loss: 7.3202 avg training loss: 8.3426
batch: [1360/10547] batch time: 0.709 trainign loss: 7.3530 avg training loss: 8.3418
batch: [1370/10547] batch time: 0.047 trainign loss: 5.5542 avg training loss: 8.3406
batch: [1380/10547] batch time: 0.091 trainign loss: 6.3984 avg training loss: 8.3391
batch: [1390/10547] batch time: 0.046 trainign loss: 9.8576 avg training loss: 8.3358
batch: [1400/10547] batch time: 0.049 trainign loss: 7.6377 avg training loss: 8.3355
batch: [1410/10547] batch time: 0.047 trainign loss: 7.6237 avg training loss: 8.3343
batch: [1420/10547] batch time: 0.046 trainign loss: 7.0935 avg training loss: 8.3336
batch: [1430/10547] batch time: 0.047 trainign loss: 7.0200 avg training loss: 8.3325
batch: [1440/10547] batch time: 0.046 trainign loss: 6.8016 avg training loss: 8.3315
batch: [1450/10547] batch time: 0.046 trainign loss: 3.2985 avg training loss: 8.3289
batch: [1460/10547] batch time: 0.048 trainign loss: 5.5783 avg training loss: 8.3269
batch: [1470/10547] batch time: 0.047 trainign loss: 6.6428 avg training loss: 8.3260
batch: [1480/10547] batch time: 0.047 trainign loss: 7.8014 avg training loss: 8.3239
batch: [1490/10547] batch time: 0.050 trainign loss: 6.6680 avg training loss: 8.3230
batch: [1500/10547] batch time: 0.046 trainign loss: 7.4450 avg training loss: 8.3220
batch: [1510/10547] batch time: 0.046 trainign loss: 7.1440 avg training loss: 8.3207
batch: [1520/10547] batch time: 0.046 trainign loss: 6.5758 avg training loss: 8.3195
batch: [1530/10547] batch time: 0.046 trainign loss: 6.6826 avg training loss: 8.3183
batch: [1540/10547] batch time: 0.053 trainign loss: 1.1567 avg training loss: 8.3152
batch: [1550/10547] batch time: 0.868 trainign loss: 5.9995 avg training loss: 8.3147
batch: [1560/10547] batch time: 0.049 trainign loss: 8.1979 avg training loss: 8.3136
batch: [1570/10547] batch time: 0.880 trainign loss: 6.8694 avg training loss: 8.3128
batch: [1580/10547] batch time: 0.050 trainign loss: 7.6597 avg training loss: 8.3116
batch: [1590/10547] batch time: 1.461 trainign loss: 3.9614 avg training loss: 8.3097
batch: [1600/10547] batch time: 0.055 trainign loss: 6.0644 avg training loss: 8.3086
batch: [1610/10547] batch time: 2.145 trainign loss: 2.2776 avg training loss: 8.3049
batch: [1620/10547] batch time: 0.056 trainign loss: 6.8516 avg training loss: 8.3043
batch: [1630/10547] batch time: 0.874 trainign loss: 3.1638 avg training loss: 8.3023
batch: [1640/10547] batch time: 0.049 trainign loss: 6.6464 avg training loss: 8.3009
batch: [1650/10547] batch time: 1.675 trainign loss: 6.5633 avg training loss: 8.2995
batch: [1660/10547] batch time: 0.050 trainign loss: 6.1795 avg training loss: 8.2975
batch: [1670/10547] batch time: 0.829 trainign loss: 3.6082 avg training loss: 8.2952
batch: [1680/10547] batch time: 0.048 trainign loss: 6.6492 avg training loss: 8.2938
batch: [1690/10547] batch time: 1.269 trainign loss: 7.1106 avg training loss: 8.2927
batch: [1700/10547] batch time: 0.055 trainign loss: 7.3938 avg training loss: 8.2909
batch: [1710/10547] batch time: 1.499 trainign loss: 6.8819 avg training loss: 8.2897
batch: [1720/10547] batch time: 0.048 trainign loss: 7.0201 avg training loss: 8.2885
batch: [1730/10547] batch time: 1.194 trainign loss: 7.5874 avg training loss: 8.2872
batch: [1740/10547] batch time: 0.046 trainign loss: 7.2556 avg training loss: 8.2866
batch: [1750/10547] batch time: 0.827 trainign loss: 6.2687 avg training loss: 8.2854
batch: [1760/10547] batch time: 0.046 trainign loss: 3.8077 avg training loss: 8.2827
batch: [1770/10547] batch time: 0.959 trainign loss: 7.3497 avg training loss: 8.2823
batch: [1780/10547] batch time: 0.050 trainign loss: 7.3875 avg training loss: 8.2802
batch: [1790/10547] batch time: 1.348 trainign loss: 6.3162 avg training loss: 8.2792
batch: [1800/10547] batch time: 0.047 trainign loss: 6.9650 avg training loss: 8.2780
batch: [1810/10547] batch time: 1.616 trainign loss: 7.0870 avg training loss: 8.2772
batch: [1820/10547] batch time: 0.055 trainign loss: 7.0875 avg training loss: 8.2758
batch: [1830/10547] batch time: 0.916 trainign loss: 5.9859 avg training loss: 8.2743
batch: [1840/10547] batch time: 0.054 trainign loss: 7.4977 avg training loss: 8.2734
batch: [1850/10547] batch time: 1.008 trainign loss: 6.5402 avg training loss: 8.2716
batch: [1860/10547] batch time: 0.055 trainign loss: 5.4171 avg training loss: 8.2701
batch: [1870/10547] batch time: 1.260 trainign loss: 7.5331 avg training loss: 8.2690
batch: [1880/10547] batch time: 0.046 trainign loss: 6.4914 avg training loss: 8.2681
batch: [1890/10547] batch time: 0.058 trainign loss: 7.2641 avg training loss: 8.2670
batch: [1900/10547] batch time: 0.044 trainign loss: 3.5534 avg training loss: 8.2654
batch: [1910/10547] batch time: 0.046 trainign loss: 1.7717 avg training loss: 8.2594
batch: [1920/10547] batch time: 0.046 trainign loss: 8.1362 avg training loss: 8.2610
batch: [1930/10547] batch time: 0.046 trainign loss: 7.2526 avg training loss: 8.2607
batch: [1940/10547] batch time: 0.054 trainign loss: 7.0196 avg training loss: 8.2598
batch: [1950/10547] batch time: 0.050 trainign loss: 7.2598 avg training loss: 8.2588
batch: [1960/10547] batch time: 0.050 trainign loss: 7.4123 avg training loss: 8.2567
batch: [1970/10547] batch time: 0.050 trainign loss: 6.5001 avg training loss: 8.2557
batch: [1980/10547] batch time: 0.047 trainign loss: 6.8103 avg training loss: 8.2545
batch: [1990/10547] batch time: 0.046 trainign loss: 0.0549 avg training loss: 8.2500
batch: [2000/10547] batch time: 0.046 trainign loss: 8.3972 avg training loss: 8.2484
batch: [2010/10547] batch time: 0.047 trainign loss: 7.0088 avg training loss: 8.2476
batch: [2020/10547] batch time: 0.043 trainign loss: 7.1726 avg training loss: 8.2467
batch: [2030/10547] batch time: 0.055 trainign loss: 5.0406 avg training loss: 8.2453
batch: [2040/10547] batch time: 0.045 trainign loss: 6.6687 avg training loss: 8.2431
batch: [2050/10547] batch time: 0.046 trainign loss: 7.2726 avg training loss: 8.2421
batch: [2060/10547] batch time: 0.049 trainign loss: 5.2071 avg training loss: 8.2411
batch: [2070/10547] batch time: 0.048 trainign loss: 3.7909 avg training loss: 8.2395
batch: [2080/10547] batch time: 0.046 trainign loss: 5.3293 avg training loss: 8.2372
batch: [2090/10547] batch time: 0.048 trainign loss: 6.9596 avg training loss: 8.2358
batch: [2100/10547] batch time: 0.044 trainign loss: 0.0765 avg training loss: 8.2316
batch: [2110/10547] batch time: 0.046 trainign loss: 7.5452 avg training loss: 8.2315
batch: [2120/10547] batch time: 0.043 trainign loss: 6.0219 avg training loss: 8.2307
batch: [2130/10547] batch time: 0.055 trainign loss: 6.7319 avg training loss: 8.2300
batch: [2140/10547] batch time: 0.045 trainign loss: 6.5473 avg training loss: 8.2283
batch: [2150/10547] batch time: 0.048 trainign loss: 4.5240 avg training loss: 8.2269
batch: [2160/10547] batch time: 0.041 trainign loss: 7.6753 avg training loss: 8.2246
batch: [2170/10547] batch time: 0.046 trainign loss: 7.5770 avg training loss: 8.2232
batch: [2180/10547] batch time: 0.042 trainign loss: 7.2420 avg training loss: 8.2225
batch: [2190/10547] batch time: 0.047 trainign loss: 0.1316 avg training loss: 8.2186
batch: [2200/10547] batch time: 0.046 trainign loss: 9.2499 avg training loss: 8.2182
batch: [2210/10547] batch time: 0.047 trainign loss: 5.8462 avg training loss: 8.2177
batch: [2220/10547] batch time: 0.047 trainign loss: 8.5216 avg training loss: 8.2159
batch: [2230/10547] batch time: 0.048 trainign loss: 5.2362 avg training loss: 8.2143
batch: [2240/10547] batch time: 0.045 trainign loss: 7.6303 avg training loss: 8.2129
batch: [2250/10547] batch time: 0.054 trainign loss: 6.5352 avg training loss: 8.2123
batch: [2260/10547] batch time: 0.042 trainign loss: 6.8478 avg training loss: 8.2110
batch: [2270/10547] batch time: 0.047 trainign loss: 7.1838 avg training loss: 8.2099
batch: [2280/10547] batch time: 0.047 trainign loss: 4.2981 avg training loss: 8.2085
batch: [2290/10547] batch time: 0.046 trainign loss: 7.4920 avg training loss: 8.2072
batch: [2300/10547] batch time: 0.042 trainign loss: 4.7784 avg training loss: 8.2058
batch: [2310/10547] batch time: 0.054 trainign loss: 7.6061 avg training loss: 8.2043
batch: [2320/10547] batch time: 0.046 trainign loss: 7.8403 avg training loss: 8.2027
batch: [2330/10547] batch time: 0.046 trainign loss: 7.0987 avg training loss: 8.2014
batch: [2340/10547] batch time: 0.052 trainign loss: 6.2424 avg training loss: 8.2004
batch: [2350/10547] batch time: 0.046 trainign loss: 6.3926 avg training loss: 8.1987
batch: [2360/10547] batch time: 0.046 trainign loss: 7.7618 avg training loss: 8.1961
batch: [2370/10547] batch time: 0.046 trainign loss: 6.3107 avg training loss: 8.1955
batch: [2380/10547] batch time: 0.047 trainign loss: 4.2628 avg training loss: 8.1940
batch: [2390/10547] batch time: 0.047 trainign loss: 12.3865 avg training loss: 8.1899
batch: [2400/10547] batch time: 0.048 trainign loss: 7.6776 avg training loss: 8.1902
batch: [2410/10547] batch time: 0.046 trainign loss: 6.2234 avg training loss: 8.1895
batch: [2420/10547] batch time: 0.046 trainign loss: 7.8358 avg training loss: 8.1881
batch: [2430/10547] batch time: 0.046 trainign loss: 6.5659 avg training loss: 8.1864
batch: [2440/10547] batch time: 0.047 trainign loss: 6.0712 avg training loss: 8.1849
batch: [2450/10547] batch time: 0.053 trainign loss: 7.2890 avg training loss: 8.1833
batch: [2460/10547] batch time: 0.047 trainign loss: 0.8183 avg training loss: 8.1806
batch: [2470/10547] batch time: 0.046 trainign loss: 11.9159 avg training loss: 8.1771
batch: [2480/10547] batch time: 0.055 trainign loss: 6.4868 avg training loss: 8.1772
batch: [2490/10547] batch time: 0.046 trainign loss: 7.4369 avg training loss: 8.1758
batch: [2500/10547] batch time: 0.054 trainign loss: 6.8925 avg training loss: 8.1746
batch: [2510/10547] batch time: 0.050 trainign loss: 1.1542 avg training loss: 8.1720
batch: [2520/10547] batch time: 0.047 trainign loss: 9.0867 avg training loss: 8.1706
batch: [2530/10547] batch time: 0.047 trainign loss: 7.6014 avg training loss: 8.1702
batch: [2540/10547] batch time: 0.047 trainign loss: 7.8212 avg training loss: 8.1699
batch: [2550/10547] batch time: 0.046 trainign loss: 7.0162 avg training loss: 8.1684
batch: [2560/10547] batch time: 0.047 trainign loss: 6.8555 avg training loss: 8.1676
batch: [2570/10547] batch time: 0.047 trainign loss: 6.4798 avg training loss: 8.1666
batch: [2580/10547] batch time: 0.054 trainign loss: 6.5930 avg training loss: 8.1656
batch: [2590/10547] batch time: 0.046 trainign loss: 7.1953 avg training loss: 8.1646
batch: [2600/10547] batch time: 0.047 trainign loss: 3.0127 avg training loss: 8.1629
batch: [2610/10547] batch time: 0.046 trainign loss: 7.4585 avg training loss: 8.1621
batch: [2620/10547] batch time: 0.337 trainign loss: 6.8880 avg training loss: 8.1608
batch: [2630/10547] batch time: 0.046 trainign loss: 7.3997 avg training loss: 8.1598
batch: [2640/10547] batch time: 0.046 trainign loss: 3.4613 avg training loss: 8.1581
batch: [2650/10547] batch time: 0.047 trainign loss: 6.9458 avg training loss: 8.1565
batch: [2660/10547] batch time: 0.047 trainign loss: 7.3409 avg training loss: 8.1552
batch: [2670/10547] batch time: 0.048 trainign loss: 6.5369 avg training loss: 8.1540
batch: [2680/10547] batch time: 0.047 trainign loss: 6.6193 avg training loss: 8.1529
batch: [2690/10547] batch time: 0.050 trainign loss: 5.4931 avg training loss: 8.1515
batch: [2700/10547] batch time: 0.243 trainign loss: 7.2978 avg training loss: 8.1495
batch: [2710/10547] batch time: 0.046 trainign loss: 6.2289 avg training loss: 8.1483
batch: [2720/10547] batch time: 0.315 trainign loss: 7.1528 avg training loss: 8.1473
batch: [2730/10547] batch time: 0.046 trainign loss: 7.4627 avg training loss: 8.1468
batch: [2740/10547] batch time: 0.279 trainign loss: 8.1857 avg training loss: 8.1442
batch: [2750/10547] batch time: 0.047 trainign loss: 6.5900 avg training loss: 8.1436
batch: [2760/10547] batch time: 0.953 trainign loss: 6.9940 avg training loss: 8.1423
batch: [2770/10547] batch time: 0.047 trainign loss: 0.8408 avg training loss: 8.1394
batch: [2780/10547] batch time: 0.606 trainign loss: 7.0875 avg training loss: 8.1387
batch: [2790/10547] batch time: 0.047 trainign loss: 6.6401 avg training loss: 8.1378
batch: [2800/10547] batch time: 0.048 trainign loss: 6.8668 avg training loss: 8.1366
batch: [2810/10547] batch time: 0.048 trainign loss: 5.7884 avg training loss: 8.1351
batch: [2820/10547] batch time: 0.046 trainign loss: 7.1599 avg training loss: 8.1341
batch: [2830/10547] batch time: 0.046 trainign loss: 6.6759 avg training loss: 8.1330
batch: [2840/10547] batch time: 0.046 trainign loss: 6.3896 avg training loss: 8.1310
batch: [2850/10547] batch time: 0.054 trainign loss: 6.9166 avg training loss: 8.1294
batch: [2860/10547] batch time: 0.048 trainign loss: 7.3302 avg training loss: 8.1288
batch: [2870/10547] batch time: 0.043 trainign loss: 6.9785 avg training loss: 8.1279
batch: [2880/10547] batch time: 0.047 trainign loss: 7.0342 avg training loss: 8.1267
batch: [2890/10547] batch time: 0.046 trainign loss: 6.4279 avg training loss: 8.1255
batch: [2900/10547] batch time: 0.047 trainign loss: 6.6467 avg training loss: 8.1244
batch: [2910/10547] batch time: 0.107 trainign loss: 6.9205 avg training loss: 8.1233
batch: [2920/10547] batch time: 0.046 trainign loss: 4.0869 avg training loss: 8.1219
batch: [2930/10547] batch time: 0.546 trainign loss: 4.5919 avg training loss: 8.1198
batch: [2940/10547] batch time: 0.046 trainign loss: 6.6396 avg training loss: 8.1183
batch: [2950/10547] batch time: 1.043 trainign loss: 5.5483 avg training loss: 8.1167
batch: [2960/10547] batch time: 0.054 trainign loss: 7.3392 avg training loss: 8.1152
batch: [2970/10547] batch time: 1.804 trainign loss: 7.1537 avg training loss: 8.1147
batch: [2980/10547] batch time: 0.048 trainign loss: 4.0035 avg training loss: 8.1129
batch: [2990/10547] batch time: 2.072 trainign loss: 7.8970 avg training loss: 8.1109
batch: [3000/10547] batch time: 0.046 trainign loss: 7.1296 avg training loss: 8.1097
batch: [3010/10547] batch time: 2.128 trainign loss: 6.2066 avg training loss: 8.1082
batch: [3020/10547] batch time: 0.054 trainign loss: 7.0818 avg training loss: 8.1068
batch: [3030/10547] batch time: 2.500 trainign loss: 7.5971 avg training loss: 8.1059
batch: [3040/10547] batch time: 0.047 trainign loss: 4.0295 avg training loss: 8.1044
batch: [3050/10547] batch time: 1.407 trainign loss: 6.7200 avg training loss: 8.1029
batch: [3060/10547] batch time: 0.054 trainign loss: 7.3585 avg training loss: 8.1021
batch: [3070/10547] batch time: 0.049 trainign loss: 7.9118 avg training loss: 8.1008
batch: [3080/10547] batch time: 0.048 trainign loss: 7.3710 avg training loss: 8.0999
batch: [3090/10547] batch time: 0.046 trainign loss: 6.7228 avg training loss: 8.0991
batch: [3100/10547] batch time: 0.046 trainign loss: 6.8332 avg training loss: 8.0979
batch: [3110/10547] batch time: 0.047 trainign loss: 6.1289 avg training loss: 8.0967
batch: [3120/10547] batch time: 0.049 trainign loss: 7.6205 avg training loss: 8.0953
batch: [3130/10547] batch time: 0.054 trainign loss: 5.8455 avg training loss: 8.0939
batch: [3140/10547] batch time: 0.046 trainign loss: 7.3090 avg training loss: 8.0924
batch: [3150/10547] batch time: 0.042 trainign loss: 6.9058 avg training loss: 8.0912
batch: [3160/10547] batch time: 0.047 trainign loss: 6.9172 avg training loss: 8.0894
batch: [3170/10547] batch time: 0.046 trainign loss: 5.3101 avg training loss: 8.0879
batch: [3180/10547] batch time: 0.047 trainign loss: 5.6229 avg training loss: 8.0864
batch: [3190/10547] batch time: 0.051 trainign loss: 4.3229 avg training loss: 8.0838
batch: [3200/10547] batch time: 0.047 trainign loss: 7.2229 avg training loss: 8.0832
batch: [3210/10547] batch time: 0.046 trainign loss: 7.1142 avg training loss: 8.0825
batch: [3220/10547] batch time: 0.171 trainign loss: 6.5558 avg training loss: 8.0814
batch: [3230/10547] batch time: 0.054 trainign loss: 6.6925 avg training loss: 8.0799
batch: [3240/10547] batch time: 1.470 trainign loss: 4.0779 avg training loss: 8.0784
batch: [3250/10547] batch time: 0.046 trainign loss: 6.6310 avg training loss: 8.0770
batch: [3260/10547] batch time: 1.714 trainign loss: 6.5895 avg training loss: 8.0757
batch: [3270/10547] batch time: 0.047 trainign loss: 4.9664 avg training loss: 8.0747
batch: [3280/10547] batch time: 2.083 trainign loss: 6.0568 avg training loss: 8.0735
batch: [3290/10547] batch time: 0.054 trainign loss: 5.5199 avg training loss: 8.0719
batch: [3300/10547] batch time: 0.544 trainign loss: 6.7418 avg training loss: 8.0707
batch: [3310/10547] batch time: 0.047 trainign loss: 5.0069 avg training loss: 8.0690
batch: [3320/10547] batch time: 1.699 trainign loss: 6.0601 avg training loss: 8.0676
batch: [3330/10547] batch time: 0.053 trainign loss: 7.0995 avg training loss: 8.0667
batch: [3340/10547] batch time: 1.894 trainign loss: 6.0747 avg training loss: 8.0656
batch: [3350/10547] batch time: 0.348 trainign loss: 6.5145 avg training loss: 8.0644
batch: [3360/10547] batch time: 2.178 trainign loss: 7.1277 avg training loss: 8.0633
batch: [3370/10547] batch time: 0.047 trainign loss: 1.7861 avg training loss: 8.0609
batch: [3380/10547] batch time: 2.391 trainign loss: 6.3225 avg training loss: 8.0598
batch: [3390/10547] batch time: 0.051 trainign loss: 8.0287 avg training loss: 8.0583
batch: [3400/10547] batch time: 2.542 trainign loss: 4.7391 avg training loss: 8.0571
batch: [3410/10547] batch time: 0.046 trainign loss: 7.3955 avg training loss: 8.0564
batch: [3420/10547] batch time: 2.329 trainign loss: 7.2817 avg training loss: 8.0557
batch: [3430/10547] batch time: 0.055 trainign loss: 3.8476 avg training loss: 8.0542
batch: [3440/10547] batch time: 2.207 trainign loss: 7.2045 avg training loss: 8.0533
batch: [3450/10547] batch time: 0.049 trainign loss: 4.5478 avg training loss: 8.0510
batch: [3460/10547] batch time: 2.272 trainign loss: 7.5918 avg training loss: 8.0506
batch: [3470/10547] batch time: 0.049 trainign loss: 6.1362 avg training loss: 8.0496
batch: [3480/10547] batch time: 2.188 trainign loss: 5.8374 avg training loss: 8.0487
batch: [3490/10547] batch time: 0.192 trainign loss: 7.6438 avg training loss: 8.0469
batch: [3500/10547] batch time: 2.022 trainign loss: 6.3050 avg training loss: 8.0459
batch: [3510/10547] batch time: 0.538 trainign loss: 5.4817 avg training loss: 8.0449
batch: [3520/10547] batch time: 1.773 trainign loss: 7.1363 avg training loss: 8.0430
batch: [3530/10547] batch time: 0.418 trainign loss: 6.7855 avg training loss: 8.0418
batch: [3540/10547] batch time: 1.519 trainign loss: 5.9825 avg training loss: 8.0410
batch: [3550/10547] batch time: 0.624 trainign loss: 6.6968 avg training loss: 8.0398
batch: [3560/10547] batch time: 1.860 trainign loss: 2.9644 avg training loss: 8.0383
batch: [3570/10547] batch time: 0.437 trainign loss: 6.5863 avg training loss: 8.0370
batch: [3580/10547] batch time: 1.334 trainign loss: 7.2033 avg training loss: 8.0356
batch: [3590/10547] batch time: 0.047 trainign loss: 5.1859 avg training loss: 8.0342
batch: [3600/10547] batch time: 1.526 trainign loss: 7.2742 avg training loss: 8.0329
batch: [3610/10547] batch time: 0.050 trainign loss: 7.1588 avg training loss: 8.0314
batch: [3620/10547] batch time: 0.224 trainign loss: 5.7129 avg training loss: 8.0302
batch: [3630/10547] batch time: 0.055 trainign loss: 6.8840 avg training loss: 8.0289
batch: [3640/10547] batch time: 0.045 trainign loss: 3.6297 avg training loss: 8.0273
batch: [3650/10547] batch time: 0.046 trainign loss: 7.1411 avg training loss: 8.0258
batch: [3660/10547] batch time: 0.044 trainign loss: 1.6807 avg training loss: 8.0237
batch: [3670/10547] batch time: 0.046 trainign loss: 6.9643 avg training loss: 8.0233
batch: [3680/10547] batch time: 0.046 trainign loss: 4.8567 avg training loss: 8.0223
batch: [3690/10547] batch time: 0.046 trainign loss: 2.6307 avg training loss: 8.0200
batch: [3700/10547] batch time: 0.046 trainign loss: 7.1632 avg training loss: 8.0192
batch: [3710/10547] batch time: 0.047 trainign loss: 7.2259 avg training loss: 8.0184
batch: [3720/10547] batch time: 0.046 trainign loss: 6.6961 avg training loss: 8.0174
batch: [3730/10547] batch time: 0.046 trainign loss: 5.8887 avg training loss: 8.0160
batch: [3740/10547] batch time: 0.046 trainign loss: 6.2672 avg training loss: 8.0147
batch: [3750/10547] batch time: 0.047 trainign loss: 6.1143 avg training loss: 8.0137
batch: [3760/10547] batch time: 0.047 trainign loss: 6.0180 avg training loss: 8.0124
batch: [3770/10547] batch time: 0.048 trainign loss: 3.3312 avg training loss: 8.0108
batch: [3780/10547] batch time: 0.046 trainign loss: 6.5968 avg training loss: 8.0097
batch: [3790/10547] batch time: 0.047 trainign loss: 5.6582 avg training loss: 8.0084
batch: [3800/10547] batch time: 0.093 trainign loss: 3.1293 avg training loss: 8.0065
batch: [3810/10547] batch time: 0.046 trainign loss: 2.6976 avg training loss: 8.0050
batch: [3820/10547] batch time: 0.046 trainign loss: 8.8490 avg training loss: 8.0026
batch: [3830/10547] batch time: 0.047 trainign loss: 8.0300 avg training loss: 8.0027
batch: [3840/10547] batch time: 0.042 trainign loss: 6.9697 avg training loss: 8.0021
batch: [3850/10547] batch time: 0.046 trainign loss: 6.8648 avg training loss: 8.0011
batch: [3860/10547] batch time: 0.046 trainign loss: 6.8741 avg training loss: 7.9999
batch: [3870/10547] batch time: 0.048 trainign loss: 7.0020 avg training loss: 7.9991
batch: [3880/10547] batch time: 0.048 trainign loss: 6.1656 avg training loss: 7.9979
batch: [3890/10547] batch time: 0.047 trainign loss: 5.5809 avg training loss: 7.9971
batch: [3900/10547] batch time: 0.047 trainign loss: 2.1984 avg training loss: 7.9945
batch: [3910/10547] batch time: 0.047 trainign loss: 6.6145 avg training loss: 7.9943
batch: [3920/10547] batch time: 0.054 trainign loss: 7.1498 avg training loss: 7.9939
batch: [3930/10547] batch time: 0.048 trainign loss: 6.6863 avg training loss: 7.9932
batch: [3940/10547] batch time: 0.046 trainign loss: 5.4255 avg training loss: 7.9916
batch: [3950/10547] batch time: 0.047 trainign loss: 5.6238 avg training loss: 7.9904
batch: [3960/10547] batch time: 0.055 trainign loss: 1.0460 avg training loss: 7.9879
batch: [3970/10547] batch time: 0.061 trainign loss: 6.2270 avg training loss: 7.9863
batch: [3980/10547] batch time: 0.054 trainign loss: 7.3571 avg training loss: 7.9857
batch: [3990/10547] batch time: 0.605 trainign loss: 6.2170 avg training loss: 7.9849
batch: [4000/10547] batch time: 0.046 trainign loss: 5.5554 avg training loss: 7.9837
batch: [4010/10547] batch time: 0.055 trainign loss: 6.7387 avg training loss: 7.9823
batch: [4020/10547] batch time: 0.047 trainign loss: 6.6627 avg training loss: 7.9813
batch: [4030/10547] batch time: 0.048 trainign loss: 4.7113 avg training loss: 7.9801
batch: [4040/10547] batch time: 0.047 trainign loss: 6.7088 avg training loss: 7.9791
batch: [4050/10547] batch time: 0.047 trainign loss: 5.3501 avg training loss: 7.9778
batch: [4060/10547] batch time: 0.046 trainign loss: 3.6826 avg training loss: 7.9766
batch: [4070/10547] batch time: 0.049 trainign loss: 6.3502 avg training loss: 7.9755
batch: [4080/10547] batch time: 0.053 trainign loss: 7.2200 avg training loss: 7.9748
batch: [4090/10547] batch time: 0.301 trainign loss: 5.9401 avg training loss: 7.9741
batch: [4100/10547] batch time: 0.042 trainign loss: 5.5759 avg training loss: 7.9727
batch: [4110/10547] batch time: 0.258 trainign loss: 5.7323 avg training loss: 7.9713
batch: [4120/10547] batch time: 0.047 trainign loss: 4.3468 avg training loss: 7.9698
batch: [4130/10547] batch time: 0.831 trainign loss: 0.1880 avg training loss: 7.9665
batch: [4140/10547] batch time: 0.051 trainign loss: 6.4297 avg training loss: 7.9659
batch: [4150/10547] batch time: 0.890 trainign loss: 7.0026 avg training loss: 7.9641
batch: [4160/10547] batch time: 0.046 trainign loss: 5.9873 avg training loss: 7.9632
batch: [4170/10547] batch time: 0.058 trainign loss: 5.2770 avg training loss: 7.9621
batch: [4180/10547] batch time: 0.054 trainign loss: 4.5457 avg training loss: 7.9606
batch: [4190/10547] batch time: 0.897 trainign loss: 7.4767 avg training loss: 7.9593
batch: [4200/10547] batch time: 0.047 trainign loss: 6.8043 avg training loss: 7.9583
batch: [4210/10547] batch time: 0.766 trainign loss: 6.1044 avg training loss: 7.9572
batch: [4220/10547] batch time: 0.046 trainign loss: 6.1704 avg training loss: 7.9564
batch: [4230/10547] batch time: 0.329 trainign loss: 7.5325 avg training loss: 7.9549
batch: [4240/10547] batch time: 0.047 trainign loss: 6.5736 avg training loss: 7.9536
batch: [4250/10547] batch time: 0.046 trainign loss: 7.2489 avg training loss: 7.9518
batch: [4260/10547] batch time: 0.047 trainign loss: 5.0321 avg training loss: 7.9507
batch: [4270/10547] batch time: 0.046 trainign loss: 5.8200 avg training loss: 7.9496
batch: [4280/10547] batch time: 0.054 trainign loss: 0.0546 avg training loss: 7.9462
batch: [4290/10547] batch time: 0.047 trainign loss: 0.0005 avg training loss: 7.9408
batch: [4300/10547] batch time: 0.046 trainign loss: 2.7450 avg training loss: 7.9356
batch: [4310/10547] batch time: 0.047 trainign loss: 7.4437 avg training loss: 7.9387
batch: [4320/10547] batch time: 0.047 trainign loss: 6.5733 avg training loss: 7.9384
batch: [4330/10547] batch time: 0.046 trainign loss: 4.3971 avg training loss: 7.9371
batch: [4340/10547] batch time: 0.042 trainign loss: 6.8166 avg training loss: 7.9361
batch: [4350/10547] batch time: 0.046 trainign loss: 7.0624 avg training loss: 7.9357
batch: [4360/10547] batch time: 0.046 trainign loss: 6.9991 avg training loss: 7.9348
batch: [4370/10547] batch time: 0.046 trainign loss: 6.4470 avg training loss: 7.9343
batch: [4380/10547] batch time: 0.047 trainign loss: 6.5315 avg training loss: 7.9334
batch: [4390/10547] batch time: 0.046 trainign loss: 5.8437 avg training loss: 7.9325
batch: [4400/10547] batch time: 0.046 trainign loss: 7.5668 avg training loss: 7.9306
batch: [4410/10547] batch time: 0.048 trainign loss: 6.1907 avg training loss: 7.9298
batch: [4420/10547] batch time: 0.046 trainign loss: 5.9609 avg training loss: 7.9292
batch: [4430/10547] batch time: 0.048 trainign loss: 7.0329 avg training loss: 7.9280
batch: [4440/10547] batch time: 0.047 trainign loss: 6.9619 avg training loss: 7.9250
batch: [4450/10547] batch time: 0.046 trainign loss: 7.1757 avg training loss: 7.9248
batch: [4460/10547] batch time: 0.046 trainign loss: 7.7526 avg training loss: 7.9240
batch: [4470/10547] batch time: 0.049 trainign loss: 6.6678 avg training loss: 7.9230
batch: [4480/10547] batch time: 0.047 trainign loss: 7.7919 avg training loss: 7.9215
batch: [4490/10547] batch time: 0.047 trainign loss: 7.2421 avg training loss: 7.9205
batch: [4500/10547] batch time: 0.046 trainign loss: 4.3761 avg training loss: 7.9194
batch: [4510/10547] batch time: 0.046 trainign loss: 7.7335 avg training loss: 7.9175
batch: [4520/10547] batch time: 0.399 trainign loss: 7.4008 avg training loss: 7.9164
batch: [4530/10547] batch time: 0.048 trainign loss: 5.4588 avg training loss: 7.9158
batch: [4540/10547] batch time: 0.046 trainign loss: 6.6297 avg training loss: 7.9146
batch: [4550/10547] batch time: 0.047 trainign loss: 6.3304 avg training loss: 7.9136
batch: [4560/10547] batch time: 0.043 trainign loss: 5.9630 avg training loss: 7.9125
batch: [4570/10547] batch time: 0.055 trainign loss: 6.1750 avg training loss: 7.9112
batch: [4580/10547] batch time: 0.046 trainign loss: 5.2060 avg training loss: 7.9101
batch: [4590/10547] batch time: 0.047 trainign loss: 6.5137 avg training loss: 7.9090
batch: [4600/10547] batch time: 0.048 trainign loss: 6.9496 avg training loss: 7.9076
batch: [4610/10547] batch time: 0.047 trainign loss: 6.9834 avg training loss: 7.9065
batch: [4620/10547] batch time: 1.119 trainign loss: 6.5685 avg training loss: 7.9058
batch: [4630/10547] batch time: 0.047 trainign loss: 5.5096 avg training loss: 7.9047
batch: [4640/10547] batch time: 1.171 trainign loss: 7.3857 avg training loss: 7.9039
batch: [4650/10547] batch time: 0.049 trainign loss: 5.7868 avg training loss: 7.9031
batch: [4660/10547] batch time: 1.195 trainign loss: 7.1287 avg training loss: 7.9019
batch: [4670/10547] batch time: 0.049 trainign loss: 6.2798 avg training loss: 7.8999
batch: [4680/10547] batch time: 2.817 trainign loss: 7.1065 avg training loss: 7.8993
batch: [4690/10547] batch time: 0.047 trainign loss: 6.7469 avg training loss: 7.8985
batch: [4700/10547] batch time: 2.376 trainign loss: 7.0781 avg training loss: 7.8973
batch: [4710/10547] batch time: 0.047 trainign loss: 7.5614 avg training loss: 7.8959
batch: [4720/10547] batch time: 2.235 trainign loss: 7.7418 avg training loss: 7.8946
batch: [4730/10547] batch time: 0.046 trainign loss: 7.6497 avg training loss: 7.8935
batch: [4740/10547] batch time: 2.779 trainign loss: 6.8446 avg training loss: 7.8923
batch: [4750/10547] batch time: 0.050 trainign loss: 7.1694 avg training loss: 7.8917
batch: [4760/10547] batch time: 2.206 trainign loss: 7.2961 avg training loss: 7.8910
batch: [4770/10547] batch time: 0.293 trainign loss: 7.7855 avg training loss: 7.8901
batch: [4780/10547] batch time: 1.241 trainign loss: 7.3612 avg training loss: 7.8898
batch: [4790/10547] batch time: 0.752 trainign loss: 6.2287 avg training loss: 7.8889
batch: [4800/10547] batch time: 1.133 trainign loss: 7.6095 avg training loss: 7.8881
batch: [4810/10547] batch time: 0.573 trainign loss: 7.2211 avg training loss: 7.8872
batch: [4820/10547] batch time: 0.876 trainign loss: 5.8837 avg training loss: 7.8859
batch: [4830/10547] batch time: 0.739 trainign loss: 6.7454 avg training loss: 7.8851
batch: [4840/10547] batch time: 0.048 trainign loss: 6.3603 avg training loss: 7.8838
batch: [4850/10547] batch time: 1.492 trainign loss: 4.6836 avg training loss: 7.8822
batch: [4860/10547] batch time: 0.049 trainign loss: 5.6004 avg training loss: 7.8806
batch: [4870/10547] batch time: 2.339 trainign loss: 6.7520 avg training loss: 7.8799
batch: [4880/10547] batch time: 0.046 trainign loss: 6.0089 avg training loss: 7.8791
batch: [4890/10547] batch time: 2.392 trainign loss: 7.0869 avg training loss: 7.8781
batch: [4900/10547] batch time: 0.054 trainign loss: 7.1587 avg training loss: 7.8776
batch: [4910/10547] batch time: 2.168 trainign loss: 7.6486 avg training loss: 7.8771
batch: [4920/10547] batch time: 0.306 trainign loss: 6.6093 avg training loss: 7.8764
batch: [4930/10547] batch time: 2.432 trainign loss: 5.3340 avg training loss: 7.8748
batch: [4940/10547] batch time: 0.140 trainign loss: 5.5756 avg training loss: 7.8738
batch: [4950/10547] batch time: 2.458 trainign loss: 7.2083 avg training loss: 7.8721
batch: [4960/10547] batch time: 0.047 trainign loss: 7.7452 avg training loss: 7.8716
batch: [4970/10547] batch time: 1.842 trainign loss: 7.6060 avg training loss: 7.8714
batch: [4980/10547] batch time: 1.321 trainign loss: 6.4294 avg training loss: 7.8708
batch: [4990/10547] batch time: 1.076 trainign loss: 6.0577 avg training loss: 7.8695
batch: [5000/10547] batch time: 0.575 trainign loss: 4.5985 avg training loss: 7.8686
batch: [5010/10547] batch time: 2.272 trainign loss: 5.4229 avg training loss: 7.8673
batch: [5020/10547] batch time: 0.054 trainign loss: 7.1018 avg training loss: 7.8658
batch: [5030/10547] batch time: 2.012 trainign loss: 7.1576 avg training loss: 7.8648
batch: [5040/10547] batch time: 0.094 trainign loss: 7.0159 avg training loss: 7.8639
batch: [5050/10547] batch time: 1.477 trainign loss: 5.3440 avg training loss: 7.8628
batch: [5060/10547] batch time: 0.856 trainign loss: 8.0879 avg training loss: 7.8608
batch: [5070/10547] batch time: 1.152 trainign loss: 7.4246 avg training loss: 7.8604
batch: [5080/10547] batch time: 1.207 trainign loss: 6.4856 avg training loss: 7.8596
batch: [5090/10547] batch time: 1.181 trainign loss: 6.7302 avg training loss: 7.8585
batch: [5100/10547] batch time: 1.121 trainign loss: 3.4650 avg training loss: 7.8574
batch: [5110/10547] batch time: 1.380 trainign loss: 6.9364 avg training loss: 7.8567
batch: [5120/10547] batch time: 1.293 trainign loss: 3.5923 avg training loss: 7.8555
batch: [5130/10547] batch time: 0.568 trainign loss: 5.0003 avg training loss: 7.8547
batch: [5140/10547] batch time: 1.546 trainign loss: 6.9789 avg training loss: 7.8537
batch: [5150/10547] batch time: 0.800 trainign loss: 7.1438 avg training loss: 7.8533
batch: [5160/10547] batch time: 0.769 trainign loss: 0.3217 avg training loss: 7.8507
batch: [5170/10547] batch time: 1.562 trainign loss: 7.7520 avg training loss: 7.8507
batch: [5180/10547] batch time: 0.707 trainign loss: 7.2219 avg training loss: 7.8504
batch: [5190/10547] batch time: 1.255 trainign loss: 7.5813 avg training loss: 7.8493
batch: [5200/10547] batch time: 1.289 trainign loss: 3.4362 avg training loss: 7.8480
batch: [5210/10547] batch time: 0.933 trainign loss: 6.7728 avg training loss: 7.8472
batch: [5220/10547] batch time: 1.142 trainign loss: 7.3646 avg training loss: 7.8461
batch: [5230/10547] batch time: 1.732 trainign loss: 5.5317 avg training loss: 7.8449
batch: [5240/10547] batch time: 1.076 trainign loss: 3.2973 avg training loss: 7.8425
batch: [5250/10547] batch time: 0.732 trainign loss: 7.6185 avg training loss: 7.8419
batch: [5260/10547] batch time: 2.066 trainign loss: 6.9103 avg training loss: 7.8416
batch: [5270/10547] batch time: 0.675 trainign loss: 5.7784 avg training loss: 7.8406
batch: [5280/10547] batch time: 2.149 trainign loss: 6.8719 avg training loss: 7.8395
batch: [5290/10547] batch time: 0.642 trainign loss: 6.7772 avg training loss: 7.8387
batch: [5300/10547] batch time: 1.695 trainign loss: 2.0498 avg training loss: 7.8367
batch: [5310/10547] batch time: 0.589 trainign loss: 7.1853 avg training loss: 7.8360
batch: [5320/10547] batch time: 1.302 trainign loss: 7.1385 avg training loss: 7.8356
batch: [5330/10547] batch time: 0.650 trainign loss: 6.3707 avg training loss: 7.8349
batch: [5340/10547] batch time: 1.847 trainign loss: 4.5422 avg training loss: 7.8335
batch: [5350/10547] batch time: 1.137 trainign loss: 6.2415 avg training loss: 7.8325
batch: [5360/10547] batch time: 0.486 trainign loss: 6.2213 avg training loss: 7.8309
batch: [5370/10547] batch time: 2.220 trainign loss: 4.9563 avg training loss: 7.8303
batch: [5380/10547] batch time: 0.045 trainign loss: 7.1954 avg training loss: 7.8293
batch: [5390/10547] batch time: 2.361 trainign loss: 3.3602 avg training loss: 7.8280
batch: [5400/10547] batch time: 0.047 trainign loss: 6.0390 avg training loss: 7.8270
batch: [5410/10547] batch time: 2.102 trainign loss: 7.7997 avg training loss: 7.8263
batch: [5420/10547] batch time: 0.045 trainign loss: 7.1720 avg training loss: 7.8259
batch: [5430/10547] batch time: 2.099 trainign loss: 7.0134 avg training loss: 7.8248
batch: [5440/10547] batch time: 0.078 trainign loss: 6.5918 avg training loss: 7.8242
batch: [5450/10547] batch time: 2.503 trainign loss: 7.0949 avg training loss: 7.8233
batch: [5460/10547] batch time: 1.119 trainign loss: 7.0433 avg training loss: 7.8223
batch: [5470/10547] batch time: 2.044 trainign loss: 2.1160 avg training loss: 7.8210
batch: [5480/10547] batch time: 0.266 trainign loss: 4.7012 avg training loss: 7.8197
batch: [5490/10547] batch time: 2.431 trainign loss: 7.7350 avg training loss: 7.8189
batch: [5500/10547] batch time: 0.047 trainign loss: 6.3731 avg training loss: 7.8184
batch: [5510/10547] batch time: 2.127 trainign loss: 6.7999 avg training loss: 7.8177
batch: [5520/10547] batch time: 0.444 trainign loss: 8.5020 avg training loss: 7.8152
batch: [5530/10547] batch time: 1.975 trainign loss: 6.7589 avg training loss: 7.8151
batch: [5540/10547] batch time: 1.061 trainign loss: 3.8735 avg training loss: 7.8142
batch: [5550/10547] batch time: 1.664 trainign loss: 7.1318 avg training loss: 7.8131
batch: [5560/10547] batch time: 0.844 trainign loss: 6.5091 avg training loss: 7.8124
batch: [5570/10547] batch time: 1.199 trainign loss: 7.2262 avg training loss: 7.8119
batch: [5580/10547] batch time: 2.701 trainign loss: 5.0552 avg training loss: 7.8107
batch: [5590/10547] batch time: 0.051 trainign loss: 11.4857 avg training loss: 7.8075
batch: [5600/10547] batch time: 2.137 trainign loss: 8.0455 avg training loss: 7.8081
batch: [5610/10547] batch time: 0.222 trainign loss: 8.1088 avg training loss: 7.8083
batch: [5620/10547] batch time: 2.361 trainign loss: 5.3634 avg training loss: 7.8078
batch: [5630/10547] batch time: 0.055 trainign loss: 3.9913 avg training loss: 7.8064
batch: [5640/10547] batch time: 2.120 trainign loss: 7.0855 avg training loss: 7.8051
batch: [5650/10547] batch time: 0.051 trainign loss: 7.3516 avg training loss: 7.8045
batch: [5660/10547] batch time: 2.201 trainign loss: 5.0182 avg training loss: 7.8037
batch: [5670/10547] batch time: 0.054 trainign loss: 3.3864 avg training loss: 7.8025
batch: [5680/10547] batch time: 2.562 trainign loss: 5.8532 avg training loss: 7.8013
batch: [5690/10547] batch time: 0.047 trainign loss: 7.4527 avg training loss: 7.8008
batch: [5700/10547] batch time: 2.472 trainign loss: 6.5654 avg training loss: 7.7999
batch: [5710/10547] batch time: 0.051 trainign loss: 6.0222 avg training loss: 7.7986
batch: [5720/10547] batch time: 1.988 trainign loss: 6.6454 avg training loss: 7.7980
batch: [5730/10547] batch time: 0.531 trainign loss: 6.8131 avg training loss: 7.7976
batch: [5740/10547] batch time: 2.046 trainign loss: 5.5743 avg training loss: 7.7966
batch: [5750/10547] batch time: 0.047 trainign loss: 4.1260 avg training loss: 7.7953
batch: [5760/10547] batch time: 1.524 trainign loss: 5.6791 avg training loss: 7.7938
batch: [5770/10547] batch time: 0.423 trainign loss: 7.4692 avg training loss: 7.7932
batch: [5780/10547] batch time: 2.544 trainign loss: 4.6563 avg training loss: 7.7926
batch: [5790/10547] batch time: 0.047 trainign loss: 0.0060 avg training loss: 7.7884
batch: [5800/10547] batch time: 2.053 trainign loss: 12.3825 avg training loss: 7.7863
batch: [5810/10547] batch time: 0.346 trainign loss: 7.9102 avg training loss: 7.7867
batch: [5820/10547] batch time: 1.643 trainign loss: 7.5834 avg training loss: 7.7861
batch: [5830/10547] batch time: 0.799 trainign loss: 5.5585 avg training loss: 7.7847
batch: [5840/10547] batch time: 2.115 trainign loss: 4.7926 avg training loss: 7.7832
batch: [5850/10547] batch time: 0.046 trainign loss: 7.6208 avg training loss: 7.7816
batch: [5860/10547] batch time: 2.141 trainign loss: 6.3072 avg training loss: 7.7812
batch: [5870/10547] batch time: 0.046 trainign loss: 4.4327 avg training loss: 7.7803
batch: [5880/10547] batch time: 2.159 trainign loss: 3.5769 avg training loss: 7.7789
batch: [5890/10547] batch time: 0.412 trainign loss: 6.1212 avg training loss: 7.7781
batch: [5900/10547] batch time: 2.070 trainign loss: 6.3192 avg training loss: 7.7774
batch: [5910/10547] batch time: 0.046 trainign loss: 7.2373 avg training loss: 7.7770
batch: [5920/10547] batch time: 2.213 trainign loss: 6.0947 avg training loss: 7.7762
batch: [5930/10547] batch time: 0.188 trainign loss: 3.0908 avg training loss: 7.7748
batch: [5940/10547] batch time: 2.090 trainign loss: 6.5397 avg training loss: 7.7724
batch: [5950/10547] batch time: 0.046 trainign loss: 5.7686 avg training loss: 7.7718
batch: [5960/10547] batch time: 2.361 trainign loss: 6.1453 avg training loss: 7.7716
batch: [5970/10547] batch time: 0.046 trainign loss: 8.4491 avg training loss: 7.7697
batch: [5980/10547] batch time: 1.806 trainign loss: 6.8193 avg training loss: 7.7683
batch: [5990/10547] batch time: 0.046 trainign loss: 8.1314 avg training loss: 7.7681
batch: [6000/10547] batch time: 2.431 trainign loss: 7.2901 avg training loss: 7.7678
batch: [6010/10547] batch time: 0.047 trainign loss: 6.9091 avg training loss: 7.7672
batch: [6020/10547] batch time: 2.321 trainign loss: 6.7765 avg training loss: 7.7666
batch: [6030/10547] batch time: 0.264 trainign loss: 5.2099 avg training loss: 7.7651
batch: [6040/10547] batch time: 2.091 trainign loss: 7.4016 avg training loss: 7.7643
batch: [6050/10547] batch time: 0.135 trainign loss: 7.3033 avg training loss: 7.7638
batch: [6060/10547] batch time: 2.195 trainign loss: 6.4439 avg training loss: 7.7633
batch: [6070/10547] batch time: 0.170 trainign loss: 6.3576 avg training loss: 7.7621
batch: [6080/10547] batch time: 2.381 trainign loss: 5.6653 avg training loss: 7.7612
batch: [6090/10547] batch time: 1.075 trainign loss: 7.0083 avg training loss: 7.7604
batch: [6100/10547] batch time: 1.848 trainign loss: 5.0603 avg training loss: 7.7591
batch: [6110/10547] batch time: 0.140 trainign loss: 7.2527 avg training loss: 7.7584
batch: [6120/10547] batch time: 2.160 trainign loss: 6.3677 avg training loss: 7.7575
batch: [6130/10547] batch time: 0.054 trainign loss: 3.3650 avg training loss: 7.7563
batch: [6140/10547] batch time: 2.388 trainign loss: 6.5824 avg training loss: 7.7554
batch: [6150/10547] batch time: 0.056 trainign loss: 7.3228 avg training loss: 7.7548
batch: [6160/10547] batch time: 2.424 trainign loss: 7.4281 avg training loss: 7.7543
batch: [6170/10547] batch time: 0.047 trainign loss: 0.0784 avg training loss: 7.7516
batch: [6180/10547] batch time: 2.393 trainign loss: 10.7897 avg training loss: 7.7507
batch: [6190/10547] batch time: 0.047 trainign loss: 6.8488 avg training loss: 7.7503
batch: [6200/10547] batch time: 2.256 trainign loss: 6.8741 avg training loss: 7.7494
batch: [6210/10547] batch time: 0.047 trainign loss: 6.7462 avg training loss: 7.7489
batch: [6220/10547] batch time: 2.051 trainign loss: 6.0848 avg training loss: 7.7484
batch: [6230/10547] batch time: 0.046 trainign loss: 3.9563 avg training loss: 7.7472
batch: [6240/10547] batch time: 2.490 trainign loss: 6.0121 avg training loss: 7.7461
batch: [6250/10547] batch time: 0.047 trainign loss: 5.6347 avg training loss: 7.7452
batch: [6260/10547] batch time: 2.307 trainign loss: 6.2095 avg training loss: 7.7439
batch: [6270/10547] batch time: 0.046 trainign loss: 2.8267 avg training loss: 7.7420
batch: [6280/10547] batch time: 2.104 trainign loss: 6.5093 avg training loss: 7.7410
batch: [6290/10547] batch time: 0.047 trainign loss: 6.9637 avg training loss: 7.7403
batch: [6300/10547] batch time: 2.382 trainign loss: 6.9407 avg training loss: 7.7395
batch: [6310/10547] batch time: 0.046 trainign loss: 3.2580 avg training loss: 7.7382
batch: [6320/10547] batch time: 2.221 trainign loss: 6.9660 avg training loss: 7.7378
batch: [6330/10547] batch time: 0.046 trainign loss: 6.0718 avg training loss: 7.7371
batch: [6340/10547] batch time: 2.328 trainign loss: 6.8011 avg training loss: 7.7361
batch: [6350/10547] batch time: 0.050 trainign loss: 6.9028 avg training loss: 7.7354
batch: [6360/10547] batch time: 2.159 trainign loss: 5.9609 avg training loss: 7.7347
batch: [6370/10547] batch time: 0.046 trainign loss: 7.8831 avg training loss: 7.7334
batch: [6380/10547] batch time: 2.330 trainign loss: 2.9419 avg training loss: 7.7322
batch: [6390/10547] batch time: 0.045 trainign loss: 6.2874 avg training loss: 7.7317
batch: [6400/10547] batch time: 2.015 trainign loss: 5.5968 avg training loss: 7.7305
batch: [6410/10547] batch time: 0.046 trainign loss: 6.4137 avg training loss: 7.7299
batch: [6420/10547] batch time: 2.145 trainign loss: 7.2037 avg training loss: 7.7285
batch: [6430/10547] batch time: 0.046 trainign loss: 7.6406 avg training loss: 7.7281
batch: [6440/10547] batch time: 1.992 trainign loss: 3.7132 avg training loss: 7.7266
batch: [6450/10547] batch time: 0.049 trainign loss: 7.9018 avg training loss: 7.7256
batch: [6460/10547] batch time: 2.525 trainign loss: 5.3465 avg training loss: 7.7252
batch: [6470/10547] batch time: 0.050 trainign loss: 8.1296 avg training loss: 7.7241
batch: [6480/10547] batch time: 1.565 trainign loss: 7.2711 avg training loss: 7.7234
batch: [6490/10547] batch time: 0.049 trainign loss: 5.5119 avg training loss: 7.7226
batch: [6500/10547] batch time: 0.222 trainign loss: 0.0189 avg training loss: 7.7189
batch: [6510/10547] batch time: 0.042 trainign loss: 0.0006 avg training loss: 7.7144
batch: [6520/10547] batch time: 0.471 trainign loss: 0.0001 avg training loss: 7.7099
batch: [6530/10547] batch time: 0.046 trainign loss: 12.3179 avg training loss: 7.7089
batch: [6540/10547] batch time: 1.615 trainign loss: 8.2206 avg training loss: 7.7094
batch: [6550/10547] batch time: 0.054 trainign loss: 6.3007 avg training loss: 7.7092
batch: [6560/10547] batch time: 1.568 trainign loss: 5.9142 avg training loss: 7.7087
batch: [6570/10547] batch time: 0.054 trainign loss: 2.0100 avg training loss: 7.7070
batch: [6580/10547] batch time: 2.022 trainign loss: 6.0084 avg training loss: 7.7063
batch: [6590/10547] batch time: 0.048 trainign loss: 7.0030 avg training loss: 7.7058
batch: [6600/10547] batch time: 2.467 trainign loss: 3.2786 avg training loss: 7.7044
batch: [6610/10547] batch time: 0.046 trainign loss: 3.9140 avg training loss: 7.7033
batch: [6620/10547] batch time: 2.450 trainign loss: 7.7700 avg training loss: 7.7030
batch: [6630/10547] batch time: 0.046 trainign loss: 6.4285 avg training loss: 7.7026
batch: [6640/10547] batch time: 1.812 trainign loss: 5.8926 avg training loss: 7.7013
batch: [6650/10547] batch time: 0.047 trainign loss: 3.6408 avg training loss: 7.6999
batch: [6660/10547] batch time: 0.884 trainign loss: 7.2321 avg training loss: 7.6992
batch: [6670/10547] batch time: 0.047 trainign loss: 7.5727 avg training loss: 7.6988
batch: [6680/10547] batch time: 1.976 trainign loss: 7.0114 avg training loss: 7.6977
batch: [6690/10547] batch time: 0.048 trainign loss: 6.2967 avg training loss: 7.6967
batch: [6700/10547] batch time: 1.969 trainign loss: 6.5216 avg training loss: 7.6960
batch: [6710/10547] batch time: 0.162 trainign loss: 7.2137 avg training loss: 7.6956
batch: [6720/10547] batch time: 1.746 trainign loss: 6.9301 avg training loss: 7.6950
batch: [6730/10547] batch time: 0.385 trainign loss: 7.0353 avg training loss: 7.6943
batch: [6740/10547] batch time: 1.220 trainign loss: 5.2874 avg training loss: 7.6937
batch: [6750/10547] batch time: 0.929 trainign loss: 6.5023 avg training loss: 7.6926
batch: [6760/10547] batch time: 0.678 trainign loss: 5.6637 avg training loss: 7.6918
batch: [6770/10547] batch time: 1.811 trainign loss: 7.0896 avg training loss: 7.6909
batch: [6780/10547] batch time: 0.263 trainign loss: 6.7559 avg training loss: 7.6900
batch: [6790/10547] batch time: 1.070 trainign loss: 6.8699 avg training loss: 7.6895
batch: [6800/10547] batch time: 0.046 trainign loss: 6.3072 avg training loss: 7.6891
batch: [6810/10547] batch time: 0.652 trainign loss: 6.7675 avg training loss: 7.6882
batch: [6820/10547] batch time: 0.156 trainign loss: 3.1481 avg training loss: 7.6858
batch: [6830/10547] batch time: 0.499 trainign loss: 4.9772 avg training loss: 7.6852
batch: [6840/10547] batch time: 1.235 trainign loss: 5.3400 avg training loss: 7.6848
batch: [6850/10547] batch time: 0.088 trainign loss: 2.6756 avg training loss: 7.6835
batch: [6860/10547] batch time: 0.610 trainign loss: 4.9516 avg training loss: 7.6823
batch: [6870/10547] batch time: 0.048 trainign loss: 7.3197 avg training loss: 7.6808
batch: [6880/10547] batch time: 1.109 trainign loss: 6.0506 avg training loss: 7.6801
batch: [6890/10547] batch time: 0.048 trainign loss: 6.7160 avg training loss: 7.6797
batch: [6900/10547] batch time: 1.528 trainign loss: 5.3000 avg training loss: 7.6786
batch: [6910/10547] batch time: 0.282 trainign loss: 5.4049 avg training loss: 7.6773
batch: [6920/10547] batch time: 0.454 trainign loss: 5.1944 avg training loss: 7.6759
batch: [6930/10547] batch time: 2.056 trainign loss: 7.2066 avg training loss: 7.6750
batch: [6940/10547] batch time: 0.671 trainign loss: 6.3436 avg training loss: 7.6746
batch: [6950/10547] batch time: 1.006 trainign loss: 5.3158 avg training loss: 7.6735
batch: [6960/10547] batch time: 0.049 trainign loss: 7.0840 avg training loss: 7.6720
batch: [6970/10547] batch time: 1.422 trainign loss: 6.4046 avg training loss: 7.6712
batch: [6980/10547] batch time: 0.055 trainign loss: 7.0721 avg training loss: 7.6707
batch: [6990/10547] batch time: 0.660 trainign loss: 5.8047 avg training loss: 7.6700
batch: [7000/10547] batch time: 0.046 trainign loss: 8.4835 avg training loss: 7.6687
batch: [7010/10547] batch time: 0.267 trainign loss: 2.9606 avg training loss: 7.6676
batch: [7020/10547] batch time: 0.046 trainign loss: 7.6477 avg training loss: 7.6668
batch: [7030/10547] batch time: 0.046 trainign loss: 7.5461 avg training loss: 7.6667
batch: [7040/10547] batch time: 0.046 trainign loss: 7.0750 avg training loss: 7.6661
batch: [7050/10547] batch time: 0.046 trainign loss: 7.0016 avg training loss: 7.6654
batch: [7060/10547] batch time: 0.046 trainign loss: 6.8646 avg training loss: 7.6650
batch: [7070/10547] batch time: 0.046 trainign loss: 10.0380 avg training loss: 7.6633
batch: [7080/10547] batch time: 0.046 trainign loss: 7.2333 avg training loss: 7.6630
batch: [7090/10547] batch time: 0.042 trainign loss: 7.7143 avg training loss: 7.6626
batch: [7100/10547] batch time: 0.333 trainign loss: 7.1772 avg training loss: 7.6620
batch: [7110/10547] batch time: 0.240 trainign loss: 6.3647 avg training loss: 7.6608
batch: [7120/10547] batch time: 0.336 trainign loss: 7.1473 avg training loss: 7.6602
batch: [7130/10547] batch time: 1.135 trainign loss: 5.7968 avg training loss: 7.6596
batch: [7140/10547] batch time: 0.046 trainign loss: 6.5070 avg training loss: 7.6584
batch: [7150/10547] batch time: 1.216 trainign loss: 0.1501 avg training loss: 7.6564
batch: [7160/10547] batch time: 0.056 trainign loss: 0.0002 avg training loss: 7.6521
batch: [7170/10547] batch time: 2.081 trainign loss: 0.0001 avg training loss: 7.6478
batch: [7180/10547] batch time: 0.055 trainign loss: 0.0000 avg training loss: 7.6435
batch: [7190/10547] batch time: 1.689 trainign loss: 0.0000 avg training loss: 7.6392
batch: [7200/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 7.6349
batch: [7210/10547] batch time: 2.004 trainign loss: 0.0000 avg training loss: 7.6306
batch: [7220/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 7.6263
batch: [7230/10547] batch time: 2.085 trainign loss: 0.0000 avg training loss: 7.6220
batch: [7240/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 7.6177
batch: [7250/10547] batch time: 2.363 trainign loss: 8.3615 avg training loss: 7.6196
batch: [7260/10547] batch time: 0.047 trainign loss: 7.2096 avg training loss: 7.6196
batch: [7270/10547] batch time: 2.410 trainign loss: 5.7052 avg training loss: 7.6192
batch: [7280/10547] batch time: 0.049 trainign loss: 5.6196 avg training loss: 7.6187
batch: [7290/10547] batch time: 2.358 trainign loss: 8.7608 avg training loss: 7.6178
batch: [7300/10547] batch time: 0.051 trainign loss: 0.1600 avg training loss: 7.6154
batch: [7310/10547] batch time: 2.408 trainign loss: 11.8388 avg training loss: 7.6134
batch: [7320/10547] batch time: 0.046 trainign loss: 8.5137 avg training loss: 7.6143
batch: [7330/10547] batch time: 2.097 trainign loss: 6.8806 avg training loss: 7.6143
batch: [7340/10547] batch time: 0.054 trainign loss: 7.6085 avg training loss: 7.6142
batch: [7350/10547] batch time: 1.810 trainign loss: 7.6330 avg training loss: 7.6138
batch: [7360/10547] batch time: 0.046 trainign loss: 7.4298 avg training loss: 7.6136
batch: [7370/10547] batch time: 1.870 trainign loss: 6.6374 avg training loss: 7.6133
batch: [7380/10547] batch time: 0.047 trainign loss: 7.1920 avg training loss: 7.6129
batch: [7390/10547] batch time: 1.036 trainign loss: 6.7615 avg training loss: 7.6123
batch: [7400/10547] batch time: 0.047 trainign loss: 7.1016 avg training loss: 7.6112
batch: [7410/10547] batch time: 0.409 trainign loss: 6.7310 avg training loss: 7.6106
batch: [7420/10547] batch time: 0.048 trainign loss: 7.1327 avg training loss: 7.6101
batch: [7430/10547] batch time: 0.312 trainign loss: 5.9759 avg training loss: 7.6088
batch: [7440/10547] batch time: 0.046 trainign loss: 6.0442 avg training loss: 7.6080
batch: [7450/10547] batch time: 0.760 trainign loss: 6.8645 avg training loss: 7.6075
batch: [7460/10547] batch time: 0.054 trainign loss: 6.9089 avg training loss: 7.6071
batch: [7470/10547] batch time: 1.828 trainign loss: 7.2071 avg training loss: 7.6061
batch: [7480/10547] batch time: 0.048 trainign loss: 6.8382 avg training loss: 7.6054
batch: [7490/10547] batch time: 1.343 trainign loss: 1.0089 avg training loss: 7.6038
batch: [7500/10547] batch time: 0.049 trainign loss: 7.3495 avg training loss: 7.6036
batch: [7510/10547] batch time: 0.668 trainign loss: 7.4065 avg training loss: 7.6025
batch: [7520/10547] batch time: 0.046 trainign loss: 5.9616 avg training loss: 7.6014
batch: [7530/10547] batch time: 0.112 trainign loss: 7.0784 avg training loss: 7.6011
batch: [7540/10547] batch time: 0.046 trainign loss: 7.4786 avg training loss: 7.6002
batch: [7550/10547] batch time: 0.749 trainign loss: 2.6098 avg training loss: 7.5989
batch: [7560/10547] batch time: 0.049 trainign loss: 6.5482 avg training loss: 7.5985
batch: [7570/10547] batch time: 0.715 trainign loss: 6.7019 avg training loss: 7.5980
batch: [7580/10547] batch time: 0.050 trainign loss: 7.3141 avg training loss: 7.5969
batch: [7590/10547] batch time: 0.240 trainign loss: 6.9938 avg training loss: 7.5960
batch: [7600/10547] batch time: 0.047 trainign loss: 4.2770 avg training loss: 7.5949
batch: [7610/10547] batch time: 0.043 trainign loss: 7.2266 avg training loss: 7.5943
batch: [7620/10547] batch time: 0.048 trainign loss: 7.7370 avg training loss: 7.5939
batch: [7630/10547] batch time: 0.046 trainign loss: 4.5814 avg training loss: 7.5931
batch: [7640/10547] batch time: 0.054 trainign loss: 6.5911 avg training loss: 7.5921
batch: [7650/10547] batch time: 0.043 trainign loss: 7.8439 avg training loss: 7.5918
batch: [7660/10547] batch time: 0.049 trainign loss: 6.9330 avg training loss: 7.5915
batch: [7670/10547] batch time: 0.046 trainign loss: 4.2389 avg training loss: 7.5908
batch: [7680/10547] batch time: 0.049 trainign loss: 5.6585 avg training loss: 7.5900
batch: [7690/10547] batch time: 0.049 trainign loss: 6.8063 avg training loss: 7.5893
batch: [7700/10547] batch time: 0.046 trainign loss: 7.3416 avg training loss: 7.5885
batch: [7710/10547] batch time: 0.047 trainign loss: 8.6517 avg training loss: 7.5868
batch: [7720/10547] batch time: 0.234 trainign loss: 7.9614 avg training loss: 7.5864
batch: [7730/10547] batch time: 0.539 trainign loss: 6.1768 avg training loss: 7.5858
batch: [7740/10547] batch time: 0.758 trainign loss: 6.1210 avg training loss: 7.5854
batch: [7750/10547] batch time: 0.556 trainign loss: 6.9510 avg training loss: 7.5846
batch: [7760/10547] batch time: 0.390 trainign loss: 7.2976 avg training loss: 7.5843
batch: [7770/10547] batch time: 0.481 trainign loss: 6.3991 avg training loss: 7.5837
batch: [7780/10547] batch time: 1.489 trainign loss: 6.9159 avg training loss: 7.5830
batch: [7790/10547] batch time: 0.474 trainign loss: 6.4788 avg training loss: 7.5824
batch: [7800/10547] batch time: 1.297 trainign loss: 6.4009 avg training loss: 7.5815
batch: [7810/10547] batch time: 0.617 trainign loss: 0.1861 avg training loss: 7.5792
batch: [7820/10547] batch time: 1.191 trainign loss: 12.6987 avg training loss: 7.5765
batch: [7830/10547] batch time: 1.061 trainign loss: 8.1242 avg training loss: 7.5776
batch: [7840/10547] batch time: 1.472 trainign loss: 7.4713 avg training loss: 7.5776
batch: [7850/10547] batch time: 0.046 trainign loss: 3.8350 avg training loss: 7.5769
batch: [7860/10547] batch time: 2.588 trainign loss: 6.7284 avg training loss: 7.5763
batch: [7870/10547] batch time: 0.055 trainign loss: 6.8622 avg training loss: 7.5757
batch: [7880/10547] batch time: 1.719 trainign loss: 6.7110 avg training loss: 7.5750
batch: [7890/10547] batch time: 0.046 trainign loss: 6.4588 avg training loss: 7.5746
batch: [7900/10547] batch time: 1.823 trainign loss: 3.1999 avg training loss: 7.5733
batch: [7910/10547] batch time: 0.049 trainign loss: 7.4442 avg training loss: 7.5725
batch: [7920/10547] batch time: 2.351 trainign loss: 7.2524 avg training loss: 7.5719
batch: [7930/10547] batch time: 0.049 trainign loss: 6.6665 avg training loss: 7.5716
batch: [7940/10547] batch time: 2.322 trainign loss: 3.9656 avg training loss: 7.5708
batch: [7950/10547] batch time: 0.046 trainign loss: 8.9816 avg training loss: 7.5697
batch: [7960/10547] batch time: 2.435 trainign loss: 5.9294 avg training loss: 7.5689
batch: [7970/10547] batch time: 0.048 trainign loss: 7.9124 avg training loss: 7.5686
batch: [7980/10547] batch time: 2.091 trainign loss: 7.2175 avg training loss: 7.5682
batch: [7990/10547] batch time: 0.047 trainign loss: 7.1643 avg training loss: 7.5677
batch: [8000/10547] batch time: 1.293 trainign loss: 6.1980 avg training loss: 7.5668
batch: [8010/10547] batch time: 0.047 trainign loss: 6.2557 avg training loss: 7.5662
batch: [8020/10547] batch time: 2.547 trainign loss: 7.0724 avg training loss: 7.5653
batch: [8030/10547] batch time: 0.046 trainign loss: 7.0437 avg training loss: 7.5650
batch: [8040/10547] batch time: 1.865 trainign loss: 6.4157 avg training loss: 7.5645
batch: [8050/10547] batch time: 0.348 trainign loss: 6.4470 avg training loss: 7.5640
batch: [8060/10547] batch time: 1.815 trainign loss: 7.4910 avg training loss: 7.5637
batch: [8070/10547] batch time: 0.054 trainign loss: 6.3965 avg training loss: 7.5631
batch: [8080/10547] batch time: 2.616 trainign loss: 6.4869 avg training loss: 7.5627
batch: [8090/10547] batch time: 0.046 trainign loss: 7.0373 avg training loss: 7.5614
batch: [8100/10547] batch time: 2.663 trainign loss: 6.5890 avg training loss: 7.5602
batch: [8110/10547] batch time: 0.047 trainign loss: 6.6663 avg training loss: 7.5595
batch: [8120/10547] batch time: 2.317 trainign loss: 6.9024 avg training loss: 7.5593
batch: [8130/10547] batch time: 0.046 trainign loss: 6.2122 avg training loss: 7.5588
batch: [8140/10547] batch time: 2.294 trainign loss: 5.8158 avg training loss: 7.5584
batch: [8150/10547] batch time: 0.409 trainign loss: 6.8596 avg training loss: 7.5578
batch: [8160/10547] batch time: 1.269 trainign loss: 7.1681 avg training loss: 7.5572
batch: [8170/10547] batch time: 0.049 trainign loss: 3.5895 avg training loss: 7.5562
batch: [8180/10547] batch time: 1.848 trainign loss: 6.9564 avg training loss: 7.5556
batch: [8190/10547] batch time: 0.049 trainign loss: 6.7952 avg training loss: 7.5553
batch: [8200/10547] batch time: 2.001 trainign loss: 4.7392 avg training loss: 7.5547
batch: [8210/10547] batch time: 0.050 trainign loss: 1.9769 avg training loss: 7.5523
batch: [8220/10547] batch time: 2.035 trainign loss: 7.2273 avg training loss: 7.5515
batch: [8230/10547] batch time: 0.054 trainign loss: 6.8571 avg training loss: 7.5513
batch: [8240/10547] batch time: 1.372 trainign loss: 6.8670 avg training loss: 7.5506
batch: [8250/10547] batch time: 0.047 trainign loss: 6.8885 avg training loss: 7.5500
batch: [8260/10547] batch time: 1.463 trainign loss: 5.6584 avg training loss: 7.5496
batch: [8270/10547] batch time: 0.047 trainign loss: 7.2414 avg training loss: 7.5493
batch: [8280/10547] batch time: 0.483 trainign loss: 7.1108 avg training loss: 7.5491
batch: [8290/10547] batch time: 0.046 trainign loss: 4.4332 avg training loss: 7.5476
batch: [8300/10547] batch time: 1.348 trainign loss: 6.3086 avg training loss: 7.5470
batch: [8310/10547] batch time: 0.046 trainign loss: 7.2917 avg training loss: 7.5465
batch: [8320/10547] batch time: 0.232 trainign loss: 6.5108 avg training loss: 7.5463
batch: [8330/10547] batch time: 0.048 trainign loss: 7.7117 avg training loss: 7.5455
batch: [8340/10547] batch time: 0.047 trainign loss: 3.2914 avg training loss: 7.5447
batch: [8350/10547] batch time: 0.054 trainign loss: 0.0051 avg training loss: 7.5410
batch: [8360/10547] batch time: 0.214 trainign loss: 8.1495 avg training loss: 7.5425
batch: [8370/10547] batch time: 0.046 trainign loss: 7.6152 avg training loss: 7.5422
batch: [8380/10547] batch time: 0.096 trainign loss: 6.3806 avg training loss: 7.5415
batch: [8390/10547] batch time: 0.048 trainign loss: 2.7700 avg training loss: 7.5406
batch: [8400/10547] batch time: 0.689 trainign loss: 6.7769 avg training loss: 7.5394
batch: [8410/10547] batch time: 0.047 trainign loss: 5.8507 avg training loss: 7.5386
batch: [8420/10547] batch time: 0.046 trainign loss: 5.3438 avg training loss: 7.5360
batch: [8430/10547] batch time: 0.046 trainign loss: 6.3744 avg training loss: 7.5366
batch: [8440/10547] batch time: 0.047 trainign loss: 5.6008 avg training loss: 7.5361
batch: [8450/10547] batch time: 0.048 trainign loss: 6.5920 avg training loss: 7.5354
batch: [8460/10547] batch time: 0.048 trainign loss: 7.3068 avg training loss: 7.5344
batch: [8470/10547] batch time: 0.048 trainign loss: 7.4122 avg training loss: 7.5343
batch: [8480/10547] batch time: 0.050 trainign loss: 6.2462 avg training loss: 7.5340
batch: [8490/10547] batch time: 0.046 trainign loss: 7.1695 avg training loss: 7.5325
batch: [8500/10547] batch time: 0.048 trainign loss: 4.3208 avg training loss: 7.5318
batch: [8510/10547] batch time: 0.047 trainign loss: 6.8449 avg training loss: 7.5315
batch: [8520/10547] batch time: 0.046 trainign loss: 4.6213 avg training loss: 7.5305
batch: [8530/10547] batch time: 1.401 trainign loss: 3.7604 avg training loss: 7.5294
batch: [8540/10547] batch time: 0.046 trainign loss: 7.5117 avg training loss: 7.5295
batch: [8550/10547] batch time: 1.197 trainign loss: 7.6959 avg training loss: 7.5292
batch: [8560/10547] batch time: 0.046 trainign loss: 1.0460 avg training loss: 7.5275
batch: [8570/10547] batch time: 0.705 trainign loss: 9.0616 avg training loss: 7.5265
batch: [8580/10547] batch time: 0.051 trainign loss: 6.9606 avg training loss: 7.5258
batch: [8590/10547] batch time: 0.817 trainign loss: 7.8116 avg training loss: 7.5258
batch: [8600/10547] batch time: 0.047 trainign loss: 7.0386 avg training loss: 7.5256
batch: [8610/10547] batch time: 1.052 trainign loss: 7.2737 avg training loss: 7.5250
batch: [8620/10547] batch time: 0.047 trainign loss: 5.7410 avg training loss: 7.5245
batch: [8630/10547] batch time: 1.308 trainign loss: 3.6860 avg training loss: 7.5231
batch: [8640/10547] batch time: 0.047 trainign loss: 6.9555 avg training loss: 7.5226
batch: [8650/10547] batch time: 1.080 trainign loss: 6.6020 avg training loss: 7.5218
batch: [8660/10547] batch time: 0.046 trainign loss: 6.3731 avg training loss: 7.5212
batch: [8670/10547] batch time: 2.459 trainign loss: 6.2172 avg training loss: 7.5208
batch: [8680/10547] batch time: 0.046 trainign loss: 4.4109 avg training loss: 7.5198
batch: [8690/10547] batch time: 2.514 trainign loss: 0.0048 avg training loss: 7.5164
batch: [8700/10547] batch time: 0.054 trainign loss: 8.2918 avg training loss: 7.5160
batch: [8710/10547] batch time: 2.292 trainign loss: 6.8685 avg training loss: 7.5161
batch: [8720/10547] batch time: 0.053 trainign loss: 7.5926 avg training loss: 7.5160
batch: [8730/10547] batch time: 2.419 trainign loss: 6.5617 avg training loss: 7.5155
batch: [8740/10547] batch time: 0.047 trainign loss: 5.0510 avg training loss: 7.5147
batch: [8750/10547] batch time: 3.096 trainign loss: 7.1152 avg training loss: 7.5142
batch: [8760/10547] batch time: 0.047 trainign loss: 8.2493 avg training loss: 7.5134
batch: [8770/10547] batch time: 2.315 trainign loss: 7.4622 avg training loss: 7.5133
batch: [8780/10547] batch time: 0.054 trainign loss: 7.1561 avg training loss: 7.5129
batch: [8790/10547] batch time: 1.704 trainign loss: 7.6863 avg training loss: 7.5127
batch: [8800/10547] batch time: 0.923 trainign loss: 6.8739 avg training loss: 7.5126
batch: [8810/10547] batch time: 1.367 trainign loss: 6.8548 avg training loss: 7.5123
batch: [8820/10547] batch time: 1.032 trainign loss: 6.3714 avg training loss: 7.5119
batch: [8830/10547] batch time: 1.845 trainign loss: 6.7205 avg training loss: 7.5113
batch: [8840/10547] batch time: 0.663 trainign loss: 6.1583 avg training loss: 7.5105
batch: [8850/10547] batch time: 1.190 trainign loss: 6.6114 avg training loss: 7.5102
batch: [8860/10547] batch time: 1.507 trainign loss: 5.3704 avg training loss: 7.5098
batch: [8870/10547] batch time: 1.192 trainign loss: 6.8885 avg training loss: 7.5091
batch: [8880/10547] batch time: 1.630 trainign loss: 5.2344 avg training loss: 7.5084
batch: [8890/10547] batch time: 0.826 trainign loss: 6.7960 avg training loss: 7.5078
batch: [8900/10547] batch time: 1.457 trainign loss: 2.4256 avg training loss: 7.5068
batch: [8910/10547] batch time: 1.284 trainign loss: 7.5623 avg training loss: 7.5058
batch: [8920/10547] batch time: 0.459 trainign loss: 6.0397 avg training loss: 7.5057
batch: [8930/10547] batch time: 0.927 trainign loss: 7.2614 avg training loss: 7.5051
batch: [8940/10547] batch time: 1.194 trainign loss: 6.1522 avg training loss: 7.5044
batch: [8950/10547] batch time: 0.723 trainign loss: 6.2200 avg training loss: 7.5040
batch: [8960/10547] batch time: 2.338 trainign loss: 6.0633 avg training loss: 7.5036
batch: [8970/10547] batch time: 0.343 trainign loss: 4.5636 avg training loss: 7.5030
batch: [8980/10547] batch time: 1.283 trainign loss: 5.7882 avg training loss: 7.5019
batch: [8990/10547] batch time: 1.504 trainign loss: 6.8786 avg training loss: 7.5014
batch: [9000/10547] batch time: 0.836 trainign loss: 1.0523 avg training loss: 7.5000
batch: [9010/10547] batch time: 1.970 trainign loss: 7.6594 avg training loss: 7.4996
batch: [9020/10547] batch time: 1.130 trainign loss: 5.8401 avg training loss: 7.4991
batch: [9030/10547] batch time: 1.653 trainign loss: 5.9954 avg training loss: 7.4989
batch: [9040/10547] batch time: 0.047 trainign loss: 4.8985 avg training loss: 7.4985
batch: [9050/10547] batch time: 2.627 trainign loss: 5.9200 avg training loss: 7.4975
batch: [9060/10547] batch time: 0.046 trainign loss: 5.9388 avg training loss: 7.4967
batch: [9070/10547] batch time: 2.449 trainign loss: 4.2904 avg training loss: 7.4961
batch: [9080/10547] batch time: 0.054 trainign loss: 6.9672 avg training loss: 7.4957
batch: [9090/10547] batch time: 2.226 trainign loss: 5.2260 avg training loss: 7.4953
batch: [9100/10547] batch time: 0.053 trainign loss: 5.0971 avg training loss: 7.4948
batch: [9110/10547] batch time: 2.482 trainign loss: 6.0213 avg training loss: 7.4945
batch: [9120/10547] batch time: 0.047 trainign loss: 7.6450 avg training loss: 7.4938
batch: [9130/10547] batch time: 2.313 trainign loss: 0.0520 avg training loss: 7.4913
batch: [9140/10547] batch time: 0.047 trainign loss: 8.8734 avg training loss: 7.4912
batch: [9150/10547] batch time: 2.473 trainign loss: 7.9667 avg training loss: 7.4914
batch: [9160/10547] batch time: 0.046 trainign loss: 2.3175 avg training loss: 7.4905
batch: [9170/10547] batch time: 2.413 trainign loss: 0.0014 avg training loss: 7.4868
batch: [9180/10547] batch time: 0.046 trainign loss: 8.3323 avg training loss: 7.4879
batch: [9190/10547] batch time: 2.643 trainign loss: 6.5077 avg training loss: 7.4878
batch: [9200/10547] batch time: 0.046 trainign loss: 7.0627 avg training loss: 7.4871
batch: [9210/10547] batch time: 2.287 trainign loss: 6.5544 avg training loss: 7.4864
batch: [9220/10547] batch time: 0.046 trainign loss: 4.2388 avg training loss: 7.4859
batch: [9230/10547] batch time: 2.118 trainign loss: 7.5663 avg training loss: 7.4852
batch: [9240/10547] batch time: 0.046 trainign loss: 7.1281 avg training loss: 7.4850
batch: [9250/10547] batch time: 2.210 trainign loss: 0.0592 avg training loss: 7.4826
batch: [9260/10547] batch time: 0.046 trainign loss: 13.9933 avg training loss: 7.4803
batch: [9270/10547] batch time: 2.523 trainign loss: 8.5262 avg training loss: 7.4818
batch: [9280/10547] batch time: 0.045 trainign loss: 8.0975 avg training loss: 7.4821
batch: [9290/10547] batch time: 2.401 trainign loss: 7.5283 avg training loss: 7.4820
batch: [9300/10547] batch time: 0.051 trainign loss: 5.5353 avg training loss: 7.4814
batch: [9310/10547] batch time: 2.660 trainign loss: 6.0453 avg training loss: 7.4804
batch: [9320/10547] batch time: 0.047 trainign loss: 7.6124 avg training loss: 7.4799
batch: [9330/10547] batch time: 2.189 trainign loss: 7.5549 avg training loss: 7.4797
batch: [9340/10547] batch time: 0.046 trainign loss: 3.4038 avg training loss: 7.4788
batch: [9350/10547] batch time: 1.980 trainign loss: 7.1352 avg training loss: 7.4783
batch: [9360/10547] batch time: 0.047 trainign loss: 9.1220 avg training loss: 7.4773
batch: [9370/10547] batch time: 2.582 trainign loss: 7.5249 avg training loss: 7.4774
batch: [9380/10547] batch time: 0.046 trainign loss: 6.5864 avg training loss: 7.4773
batch: [9390/10547] batch time: 2.007 trainign loss: 7.4682 avg training loss: 7.4765
batch: [9400/10547] batch time: 0.045 trainign loss: 1.2296 avg training loss: 7.4753
batch: [9410/10547] batch time: 2.382 trainign loss: 6.5009 avg training loss: 7.4752
batch: [9420/10547] batch time: 0.046 trainign loss: 7.4818 avg training loss: 7.4751
batch: [9430/10547] batch time: 2.254 trainign loss: 6.1145 avg training loss: 7.4746
batch: [9440/10547] batch time: 0.046 trainign loss: 5.0136 avg training loss: 7.4741
batch: [9450/10547] batch time: 1.728 trainign loss: 5.4509 avg training loss: 7.4729
batch: [9460/10547] batch time: 0.046 trainign loss: 7.2427 avg training loss: 7.4728
batch: [9470/10547] batch time: 2.021 trainign loss: 4.0460 avg training loss: 7.4723
batch: [9480/10547] batch time: 0.047 trainign loss: 5.0404 avg training loss: 7.4718
batch: [9490/10547] batch time: 2.360 trainign loss: 7.4634 avg training loss: 7.4715
batch: [9500/10547] batch time: 0.054 trainign loss: 6.2009 avg training loss: 7.4713
batch: [9510/10547] batch time: 2.351 trainign loss: 6.3307 avg training loss: 7.4706
batch: [9520/10547] batch time: 0.047 trainign loss: 4.5495 avg training loss: 7.4697
batch: [9530/10547] batch time: 2.118 trainign loss: 6.9661 avg training loss: 7.4689
batch: [9540/10547] batch time: 0.047 trainign loss: 2.6210 avg training loss: 7.4679
batch: [9550/10547] batch time: 1.103 trainign loss: 7.2836 avg training loss: 7.4672
batch: [9560/10547] batch time: 0.047 trainign loss: 7.6252 avg training loss: 7.4669
batch: [9570/10547] batch time: 1.236 trainign loss: 7.3391 avg training loss: 7.4668
batch: [9580/10547] batch time: 0.047 trainign loss: 7.0980 avg training loss: 7.4664
batch: [9590/10547] batch time: 1.230 trainign loss: 3.3313 avg training loss: 7.4657
batch: [9600/10547] batch time: 0.046 trainign loss: 5.8962 avg training loss: 7.4651
batch: [9610/10547] batch time: 1.619 trainign loss: 6.4811 avg training loss: 7.4646
batch: [9620/10547] batch time: 0.047 trainign loss: 6.2541 avg training loss: 7.4641
batch: [9630/10547] batch time: 1.793 trainign loss: 6.5631 avg training loss: 7.4638
batch: [9640/10547] batch time: 0.048 trainign loss: 6.9496 avg training loss: 7.4633
batch: [9650/10547] batch time: 0.784 trainign loss: 6.4499 avg training loss: 7.4628
batch: [9660/10547] batch time: 0.048 trainign loss: 6.6686 avg training loss: 7.4622
batch: [9670/10547] batch time: 1.172 trainign loss: 6.7300 avg training loss: 7.4614
batch: [9680/10547] batch time: 0.054 trainign loss: 5.9613 avg training loss: 7.4611
batch: [9690/10547] batch time: 0.734 trainign loss: 6.9928 avg training loss: 7.4602
batch: [9700/10547] batch time: 0.046 trainign loss: 5.7690 avg training loss: 7.4600
batch: [9710/10547] batch time: 0.046 trainign loss: 6.9636 avg training loss: 7.4597
batch: [9720/10547] batch time: 0.047 trainign loss: 7.1526 avg training loss: 7.4594
batch: [9730/10547] batch time: 0.042 trainign loss: 0.1788 avg training loss: 7.4573
batch: [9740/10547] batch time: 0.053 trainign loss: 8.9476 avg training loss: 7.4571
batch: [9750/10547] batch time: 0.044 trainign loss: 7.8534 avg training loss: 7.4575
batch: [9760/10547] batch time: 0.046 trainign loss: 7.7139 avg training loss: 7.4574
batch: [9770/10547] batch time: 0.048 trainign loss: 6.3870 avg training loss: 7.4569
batch: [9780/10547] batch time: 0.047 trainign loss: 7.2895 avg training loss: 7.4568
batch: [9790/10547] batch time: 0.045 trainign loss: 6.8185 avg training loss: 7.4561
batch: [9800/10547] batch time: 0.047 trainign loss: 6.9667 avg training loss: 7.4552
batch: [9810/10547] batch time: 0.047 trainign loss: 6.8519 avg training loss: 7.4550
batch: [9820/10547] batch time: 0.047 trainign loss: 7.7974 avg training loss: 7.4546
batch: [9830/10547] batch time: 0.047 trainign loss: 6.8024 avg training loss: 7.4544
batch: [9840/10547] batch time: 0.046 trainign loss: 0.5900 avg training loss: 7.4527
batch: [9850/10547] batch time: 0.047 trainign loss: 0.0729 avg training loss: 7.4504
batch: [9860/10547] batch time: 0.048 trainign loss: 7.6452 avg training loss: 7.4507
batch: [9870/10547] batch time: 0.047 trainign loss: 6.0861 avg training loss: 7.4504
batch: [9880/10547] batch time: 0.046 trainign loss: 6.2844 avg training loss: 7.4499
batch: [9890/10547] batch time: 0.042 trainign loss: 7.4504 avg training loss: 7.4496
batch: [9900/10547] batch time: 0.048 trainign loss: 7.1247 avg training loss: 7.4493
batch: [9910/10547] batch time: 0.046 trainign loss: 1.2501 avg training loss: 7.4481
batch: [9920/10547] batch time: 0.048 trainign loss: 6.4469 avg training loss: 7.4473
batch: [9930/10547] batch time: 0.045 trainign loss: 7.4192 avg training loss: 7.4466
batch: [9940/10547] batch time: 0.046 trainign loss: 8.2479 avg training loss: 7.4456
batch: [9950/10547] batch time: 0.043 trainign loss: 7.1444 avg training loss: 7.4451
batch: [9960/10547] batch time: 0.047 trainign loss: 6.0424 avg training loss: 7.4449
batch: [9970/10547] batch time: 0.044 trainign loss: 9.4349 avg training loss: 7.4435
batch: [9980/10547] batch time: 0.046 trainign loss: 7.6577 avg training loss: 7.4436
batch: [9990/10547] batch time: 0.044 trainign loss: 7.2738 avg training loss: 7.4435
batch: [10000/10547] batch time: 0.046 trainign loss: 7.2789 avg training loss: 7.4432
batch: [10010/10547] batch time: 0.046 trainign loss: 5.2810 avg training loss: 7.4423
batch: [10020/10547] batch time: 0.814 trainign loss: 7.5800 avg training loss: 7.4415
batch: [10030/10547] batch time: 0.046 trainign loss: 6.9073 avg training loss: 7.4411
batch: [10040/10547] batch time: 0.409 trainign loss: 4.5928 avg training loss: 7.4404
batch: [10050/10547] batch time: 0.047 trainign loss: 4.6736 avg training loss: 7.4400
batch: [10060/10547] batch time: 0.357 trainign loss: 9.9941 avg training loss: 7.4384
batch: [10070/10547] batch time: 0.047 trainign loss: 7.2339 avg training loss: 7.4381
batch: [10080/10547] batch time: 0.046 trainign loss: 5.3950 avg training loss: 7.4377
batch: [10090/10547] batch time: 0.045 trainign loss: 8.2143 avg training loss: 7.4370
batch: [10100/10547] batch time: 0.054 trainign loss: 4.6536 avg training loss: 7.4364
batch: [10110/10547] batch time: 0.053 trainign loss: 7.0316 avg training loss: 7.4359
batch: [10120/10547] batch time: 0.664 trainign loss: 5.8706 avg training loss: 7.4354
batch: [10130/10547] batch time: 0.053 trainign loss: 5.8228 avg training loss: 7.4341
batch: [10140/10547] batch time: 0.591 trainign loss: 4.8703 avg training loss: 7.4333
batch: [10150/10547] batch time: 0.046 trainign loss: 5.0998 avg training loss: 7.4332
batch: [10160/10547] batch time: 1.926 trainign loss: 0.0047 avg training loss: 7.4302
batch: [10170/10547] batch time: 0.047 trainign loss: 11.9573 avg training loss: 7.4296
batch: [10180/10547] batch time: 2.317 trainign loss: 7.8724 avg training loss: 7.4302
batch: [10190/10547] batch time: 0.046 trainign loss: 2.5880 avg training loss: 7.4292
batch: [10200/10547] batch time: 2.782 trainign loss: 5.0648 avg training loss: 7.4294
batch: [10210/10547] batch time: 0.046 trainign loss: 7.9662 avg training loss: 7.4287
batch: [10220/10547] batch time: 2.125 trainign loss: 7.0902 avg training loss: 7.4285
batch: [10230/10547] batch time: 0.046 trainign loss: 7.4551 avg training loss: 7.4286
batch: [10240/10547] batch time: 2.262 trainign loss: 6.5448 avg training loss: 7.4283
batch: [10250/10547] batch time: 0.046 trainign loss: 6.1204 avg training loss: 7.4277
batch: [10260/10547] batch time: 1.924 trainign loss: 6.8411 avg training loss: 7.4270
batch: [10270/10547] batch time: 0.050 trainign loss: 6.9717 avg training loss: 7.4269
batch: [10280/10547] batch time: 2.212 trainign loss: 7.6435 avg training loss: 7.4269
batch: [10290/10547] batch time: 0.046 trainign loss: 5.8961 avg training loss: 7.4265
batch: [10300/10547] batch time: 1.251 trainign loss: 7.2094 avg training loss: 7.4261
batch: [10310/10547] batch time: 0.806 trainign loss: 6.9694 avg training loss: 7.4258
batch: [10320/10547] batch time: 0.603 trainign loss: 7.6259 avg training loss: 7.4257
batch: [10330/10547] batch time: 0.936 trainign loss: 5.7833 avg training loss: 7.4251
batch: [10340/10547] batch time: 0.047 trainign loss: 6.8816 avg training loss: 7.4245
batch: [10350/10547] batch time: 0.047 trainign loss: 6.9892 avg training loss: 7.4242
batch: [10360/10547] batch time: 0.046 trainign loss: 6.4881 avg training loss: 7.4233
batch: [10370/10547] batch time: 0.047 trainign loss: 0.7283 avg training loss: 7.4218
batch: [10380/10547] batch time: 0.047 trainign loss: 4.9648 avg training loss: 7.4203
batch: [10390/10547] batch time: 0.467 trainign loss: 7.2026 avg training loss: 7.4199
batch: [10400/10547] batch time: 0.047 trainign loss: 6.8240 avg training loss: 7.4197
batch: [10410/10547] batch time: 0.795 trainign loss: 7.0928 avg training loss: 7.4192
batch: [10420/10547] batch time: 0.050 trainign loss: 6.9405 avg training loss: 7.4188
batch: [10430/10547] batch time: 0.555 trainign loss: 5.5880 avg training loss: 7.4184
batch: [10440/10547] batch time: 0.047 trainign loss: 6.0170 avg training loss: 7.4172
batch: [10450/10547] batch time: 0.767 trainign loss: 5.0584 avg training loss: 7.4164
batch: [10460/10547] batch time: 0.054 trainign loss: 6.6716 avg training loss: 7.4156
batch: [10470/10547] batch time: 0.820 trainign loss: 6.7193 avg training loss: 7.4155
batch: [10480/10547] batch time: 0.118 trainign loss: 2.3643 avg training loss: 7.4146
batch: [10490/10547] batch time: 1.863 trainign loss: 6.8166 avg training loss: 7.4140
batch: [10500/10547] batch time: 0.055 trainign loss: 6.9194 avg training loss: 7.4136
batch: [10510/10547] batch time: 1.859 trainign loss: 7.1176 avg training loss: 7.4132
batch: [10520/10547] batch time: 0.782 trainign loss: 6.1477 avg training loss: 7.4130
batch: [10530/10547] batch time: 0.825 trainign loss: 7.0976 avg training loss: 7.4120
batch: [10540/10547] batch time: 1.136 trainign loss: 5.9904 avg training loss: 7.4114
Epoch: 3
----------------------------------------------------------------------
batch: [0/10547] batch time: 2.875 trainign loss: 5.3417 avg training loss: 7.4112
batch: [10/10547] batch time: 0.047 trainign loss: 9.7869 avg training loss: 7.4099
batch: [20/10547] batch time: 2.002 trainign loss: 7.7551 avg training loss: 7.4100
batch: [30/10547] batch time: 0.579 trainign loss: 6.8935 avg training loss: 7.4097
batch: [40/10547] batch time: 1.538 trainign loss: 7.2935 avg training loss: 7.4094
batch: [50/10547] batch time: 0.806 trainign loss: 7.4812 avg training loss: 7.4089
batch: [60/10547] batch time: 2.407 trainign loss: 5.4772 avg training loss: 7.4085
batch: [70/10547] batch time: 0.047 trainign loss: 6.8900 avg training loss: 7.4079
batch: [80/10547] batch time: 1.075 trainign loss: 7.3561 avg training loss: 7.4070
batch: [90/10547] batch time: 0.602 trainign loss: 6.5059 avg training loss: 7.4067
batch: [100/10547] batch time: 0.363 trainign loss: 2.0357 avg training loss: 7.4057
batch: [110/10547] batch time: 0.077 trainign loss: 0.0013 avg training loss: 7.4023
batch: [120/10547] batch time: 1.114 trainign loss: 8.3508 avg training loss: 7.4038
batch: [130/10547] batch time: 0.054 trainign loss: 7.5080 avg training loss: 7.4040
batch: [140/10547] batch time: 1.232 trainign loss: 6.4047 avg training loss: 7.4038
batch: [150/10547] batch time: 0.046 trainign loss: 3.9552 avg training loss: 7.4029
batch: [160/10547] batch time: 0.659 trainign loss: 7.1354 avg training loss: 7.4022
batch: [170/10547] batch time: 0.046 trainign loss: 6.6451 avg training loss: 7.4019
batch: [180/10547] batch time: 1.706 trainign loss: 6.9372 avg training loss: 7.4017
batch: [190/10547] batch time: 0.046 trainign loss: 6.8757 avg training loss: 7.4012
batch: [200/10547] batch time: 1.892 trainign loss: 5.2674 avg training loss: 7.4006
batch: [210/10547] batch time: 0.046 trainign loss: 6.5516 avg training loss: 7.3996
batch: [220/10547] batch time: 1.739 trainign loss: 7.4733 avg training loss: 7.3995
batch: [230/10547] batch time: 0.046 trainign loss: 6.3536 avg training loss: 7.3991
batch: [240/10547] batch time: 1.961 trainign loss: 7.4355 avg training loss: 7.3981
batch: [250/10547] batch time: 0.054 trainign loss: 7.0778 avg training loss: 7.3978
batch: [260/10547] batch time: 1.729 trainign loss: 6.7044 avg training loss: 7.3976
batch: [270/10547] batch time: 0.047 trainign loss: 5.9912 avg training loss: 7.3966
batch: [280/10547] batch time: 1.580 trainign loss: 5.7065 avg training loss: 7.3962
batch: [290/10547] batch time: 0.052 trainign loss: 5.2261 avg training loss: 7.3955
batch: [300/10547] batch time: 2.140 trainign loss: 5.7130 avg training loss: 7.3948
batch: [310/10547] batch time: 0.048 trainign loss: 6.4034 avg training loss: 7.3942
batch: [320/10547] batch time: 1.689 trainign loss: 7.4097 avg training loss: 7.3940
batch: [330/10547] batch time: 0.054 trainign loss: 6.3319 avg training loss: 7.3939
batch: [340/10547] batch time: 1.660 trainign loss: 6.4375 avg training loss: 7.3930
batch: [350/10547] batch time: 0.055 trainign loss: 7.9432 avg training loss: 7.3918
batch: [360/10547] batch time: 0.262 trainign loss: 7.2747 avg training loss: 7.3916
batch: [370/10547] batch time: 0.046 trainign loss: 7.1684 avg training loss: 7.3913
batch: [380/10547] batch time: 0.049 trainign loss: 6.6828 avg training loss: 7.3910
batch: [390/10547] batch time: 0.046 trainign loss: 6.5600 avg training loss: 7.3906
batch: [400/10547] batch time: 0.101 trainign loss: 6.7375 avg training loss: 7.3898
batch: [410/10547] batch time: 0.047 trainign loss: 6.1114 avg training loss: 7.3893
batch: [420/10547] batch time: 0.576 trainign loss: 6.6459 avg training loss: 7.3889
batch: [430/10547] batch time: 0.047 trainign loss: 6.9592 avg training loss: 7.3884
batch: [440/10547] batch time: 0.051 trainign loss: 3.6570 avg training loss: 7.3876
batch: [450/10547] batch time: 0.046 trainign loss: 7.3416 avg training loss: 7.3870
batch: [460/10547] batch time: 0.803 trainign loss: 6.8114 avg training loss: 7.3868
batch: [470/10547] batch time: 0.051 trainign loss: 1.0502 avg training loss: 7.3855
batch: [480/10547] batch time: 1.806 trainign loss: 11.8085 avg training loss: 7.3830
batch: [490/10547] batch time: 0.045 trainign loss: 7.5722 avg training loss: 7.3840
batch: [500/10547] batch time: 1.816 trainign loss: 7.8617 avg training loss: 7.3837
batch: [510/10547] batch time: 0.046 trainign loss: 5.9227 avg training loss: 7.3836
batch: [520/10547] batch time: 1.235 trainign loss: 6.9688 avg training loss: 7.3831
batch: [530/10547] batch time: 0.047 trainign loss: 0.4188 avg training loss: 7.3814
batch: [540/10547] batch time: 0.992 trainign loss: 9.1690 avg training loss: 7.3806
batch: [550/10547] batch time: 0.216 trainign loss: 7.1610 avg training loss: 7.3805
batch: [560/10547] batch time: 0.361 trainign loss: 7.4551 avg training loss: 7.3803
batch: [570/10547] batch time: 1.443 trainign loss: 6.5329 avg training loss: 7.3800
batch: [580/10547] batch time: 0.669 trainign loss: 6.1685 avg training loss: 7.3796
batch: [590/10547] batch time: 0.103 trainign loss: 7.3410 avg training loss: 7.3789
batch: [600/10547] batch time: 1.827 trainign loss: 5.1983 avg training loss: 7.3783
batch: [610/10547] batch time: 0.687 trainign loss: 6.8905 avg training loss: 7.3780
batch: [620/10547] batch time: 1.732 trainign loss: 6.2083 avg training loss: 7.3778
batch: [630/10547] batch time: 0.664 trainign loss: 6.3792 avg training loss: 7.3774
batch: [640/10547] batch time: 1.570 trainign loss: 2.6458 avg training loss: 7.3764
batch: [650/10547] batch time: 1.785 trainign loss: 5.9503 avg training loss: 7.3759
batch: [660/10547] batch time: 0.977 trainign loss: 5.9784 avg training loss: 7.3755
batch: [670/10547] batch time: 1.615 trainign loss: 5.9775 avg training loss: 7.3746
batch: [680/10547] batch time: 0.796 trainign loss: 7.0378 avg training loss: 7.3742
batch: [690/10547] batch time: 0.962 trainign loss: 6.0693 avg training loss: 7.3739
batch: [700/10547] batch time: 1.110 trainign loss: 5.7403 avg training loss: 7.3732
batch: [710/10547] batch time: 1.061 trainign loss: 6.4706 avg training loss: 7.3727
batch: [720/10547] batch time: 1.560 trainign loss: 4.5239 avg training loss: 7.3719
batch: [730/10547] batch time: 0.119 trainign loss: 9.6191 avg training loss: 7.3695
batch: [740/10547] batch time: 2.247 trainign loss: 5.5007 avg training loss: 7.3700
batch: [750/10547] batch time: 0.381 trainign loss: 5.6052 avg training loss: 7.3699
batch: [760/10547] batch time: 2.092 trainign loss: 1.9627 avg training loss: 7.3682
batch: [770/10547] batch time: 0.631 trainign loss: 7.3389 avg training loss: 7.3685
batch: [780/10547] batch time: 1.444 trainign loss: 7.4811 avg training loss: 7.3685
batch: [790/10547] batch time: 0.296 trainign loss: 6.4424 avg training loss: 7.3683
batch: [800/10547] batch time: 1.312 trainign loss: 6.5758 avg training loss: 7.3680
batch: [810/10547] batch time: 0.440 trainign loss: 4.9937 avg training loss: 7.3673
batch: [820/10547] batch time: 2.461 trainign loss: 6.2534 avg training loss: 7.3668
batch: [830/10547] batch time: 0.258 trainign loss: 7.2416 avg training loss: 7.3665
batch: [840/10547] batch time: 1.237 trainign loss: 6.2748 avg training loss: 7.3661
batch: [850/10547] batch time: 0.622 trainign loss: 6.1407 avg training loss: 7.3657
batch: [860/10547] batch time: 1.756 trainign loss: 6.0540 avg training loss: 7.3649
batch: [870/10547] batch time: 0.667 trainign loss: 6.5848 avg training loss: 7.3646
batch: [880/10547] batch time: 1.726 trainign loss: 6.0064 avg training loss: 7.3643
batch: [890/10547] batch time: 0.780 trainign loss: 4.7055 avg training loss: 7.3632
batch: [900/10547] batch time: 0.047 trainign loss: 6.0985 avg training loss: 7.3628
batch: [910/10547] batch time: 2.151 trainign loss: 6.4731 avg training loss: 7.3626
batch: [920/10547] batch time: 0.047 trainign loss: 6.4537 avg training loss: 7.3620
batch: [930/10547] batch time: 2.370 trainign loss: 5.3088 avg training loss: 7.3613
batch: [940/10547] batch time: 0.055 trainign loss: 6.3189 avg training loss: 7.3607
batch: [950/10547] batch time: 2.485 trainign loss: 5.4786 avg training loss: 7.3600
batch: [960/10547] batch time: 0.054 trainign loss: 1.5409 avg training loss: 7.3586
batch: [970/10547] batch time: 2.568 trainign loss: 7.2688 avg training loss: 7.3583
batch: [980/10547] batch time: 0.054 trainign loss: 5.7365 avg training loss: 7.3577
batch: [990/10547] batch time: 2.362 trainign loss: 1.2454 avg training loss: 7.3560
batch: [1000/10547] batch time: 0.046 trainign loss: 10.0134 avg training loss: 7.3535
batch: [1010/10547] batch time: 2.846 trainign loss: 6.8659 avg training loss: 7.3536
batch: [1020/10547] batch time: 0.047 trainign loss: 0.0250 avg training loss: 7.3512
batch: [1030/10547] batch time: 2.295 trainign loss: 0.0002 avg training loss: 7.3479
batch: [1040/10547] batch time: 0.046 trainign loss: 14.2480 avg training loss: 7.3468
batch: [1050/10547] batch time: 2.755 trainign loss: 8.1102 avg training loss: 7.3476
batch: [1060/10547] batch time: 0.047 trainign loss: 6.9704 avg training loss: 7.3476
batch: [1070/10547] batch time: 2.161 trainign loss: 7.2832 avg training loss: 7.3474
batch: [1080/10547] batch time: 0.046 trainign loss: 3.3015 avg training loss: 7.3464
batch: [1090/10547] batch time: 2.661 trainign loss: 6.8998 avg training loss: 7.3462
batch: [1100/10547] batch time: 0.046 trainign loss: 5.0166 avg training loss: 7.3457
batch: [1110/10547] batch time: 2.482 trainign loss: 7.1691 avg training loss: 7.3446
batch: [1120/10547] batch time: 0.046 trainign loss: 6.2920 avg training loss: 7.3437
batch: [1130/10547] batch time: 2.175 trainign loss: 6.5739 avg training loss: 7.3431
batch: [1140/10547] batch time: 0.523 trainign loss: 7.0003 avg training loss: 7.3428
batch: [1150/10547] batch time: 1.931 trainign loss: 6.0021 avg training loss: 7.3425
batch: [1160/10547] batch time: 0.054 trainign loss: 5.2139 avg training loss: 7.3417
batch: [1170/10547] batch time: 2.194 trainign loss: 6.6346 avg training loss: 7.3412
batch: [1180/10547] batch time: 0.046 trainign loss: 6.5301 avg training loss: 7.3409
batch: [1190/10547] batch time: 2.393 trainign loss: 6.2357 avg training loss: 7.3405
batch: [1200/10547] batch time: 0.045 trainign loss: 4.8573 avg training loss: 7.3398
batch: [1210/10547] batch time: 2.205 trainign loss: 5.9556 avg training loss: 7.3393
batch: [1220/10547] batch time: 0.045 trainign loss: 6.2560 avg training loss: 7.3388
batch: [1230/10547] batch time: 2.108 trainign loss: 6.2595 avg training loss: 7.3384
batch: [1240/10547] batch time: 0.047 trainign loss: 6.2744 avg training loss: 7.3379
batch: [1250/10547] batch time: 2.304 trainign loss: 6.4285 avg training loss: 7.3373
batch: [1260/10547] batch time: 0.053 trainign loss: 0.2864 avg training loss: 7.3355
batch: [1270/10547] batch time: 2.380 trainign loss: 7.4599 avg training loss: 7.3355
batch: [1280/10547] batch time: 0.047 trainign loss: 4.3306 avg training loss: 7.3351
batch: [1290/10547] batch time: 2.307 trainign loss: 7.1609 avg training loss: 7.3349
batch: [1300/10547] batch time: 0.046 trainign loss: 6.2847 avg training loss: 7.3345
batch: [1310/10547] batch time: 2.457 trainign loss: 5.2940 avg training loss: 7.3339
batch: [1320/10547] batch time: 0.046 trainign loss: 3.8571 avg training loss: 7.3329
batch: [1330/10547] batch time: 2.366 trainign loss: 6.5139 avg training loss: 7.3320
batch: [1340/10547] batch time: 0.048 trainign loss: 7.0815 avg training loss: 7.3318
batch: [1350/10547] batch time: 2.556 trainign loss: 6.3856 avg training loss: 7.3313
batch: [1360/10547] batch time: 0.046 trainign loss: 6.5976 avg training loss: 7.3310
batch: [1370/10547] batch time: 1.897 trainign loss: 5.4444 avg training loss: 7.3305
batch: [1380/10547] batch time: 0.056 trainign loss: 5.8994 avg training loss: 7.3298
batch: [1390/10547] batch time: 0.548 trainign loss: 8.1226 avg training loss: 7.3283
batch: [1400/10547] batch time: 0.302 trainign loss: 7.3709 avg training loss: 7.3283
batch: [1410/10547] batch time: 0.469 trainign loss: 6.5799 avg training loss: 7.3279
batch: [1420/10547] batch time: 0.053 trainign loss: 6.2245 avg training loss: 7.3275
batch: [1430/10547] batch time: 0.102 trainign loss: 6.2211 avg training loss: 7.3270
batch: [1440/10547] batch time: 0.046 trainign loss: 6.0961 avg training loss: 7.3265
batch: [1450/10547] batch time: 0.046 trainign loss: 3.0016 avg training loss: 7.3254
batch: [1460/10547] batch time: 0.047 trainign loss: 4.2269 avg training loss: 7.3242
batch: [1470/10547] batch time: 0.049 trainign loss: 6.7712 avg training loss: 7.3240
batch: [1480/10547] batch time: 0.049 trainign loss: 6.1253 avg training loss: 7.3232
batch: [1490/10547] batch time: 0.046 trainign loss: 6.1903 avg training loss: 7.3228
batch: [1500/10547] batch time: 0.052 trainign loss: 6.7951 avg training loss: 7.3225
batch: [1510/10547] batch time: 0.056 trainign loss: 6.3569 avg training loss: 7.3220
batch: [1520/10547] batch time: 0.050 trainign loss: 5.8474 avg training loss: 7.3214
batch: [1530/10547] batch time: 0.599 trainign loss: 5.9386 avg training loss: 7.3208
batch: [1540/10547] batch time: 0.055 trainign loss: 1.9172 avg training loss: 7.3197
batch: [1550/10547] batch time: 0.047 trainign loss: 5.2573 avg training loss: 7.3191
batch: [1560/10547] batch time: 0.046 trainign loss: 7.1767 avg training loss: 7.3186
batch: [1570/10547] batch time: 0.272 trainign loss: 6.4547 avg training loss: 7.3183
batch: [1580/10547] batch time: 0.046 trainign loss: 6.8111 avg training loss: 7.3178
batch: [1590/10547] batch time: 1.227 trainign loss: 3.3882 avg training loss: 7.3169
batch: [1600/10547] batch time: 0.046 trainign loss: 6.0331 avg training loss: 7.3164
batch: [1610/10547] batch time: 1.023 trainign loss: 2.5410 avg training loss: 7.3152
batch: [1620/10547] batch time: 0.054 trainign loss: 6.2296 avg training loss: 7.3146
batch: [1630/10547] batch time: 0.817 trainign loss: 2.9626 avg training loss: 7.3137
batch: [1640/10547] batch time: 0.046 trainign loss: 5.6624 avg training loss: 7.3131
batch: [1650/10547] batch time: 1.294 trainign loss: 6.7213 avg training loss: 7.3129
batch: [1660/10547] batch time: 0.046 trainign loss: 5.2114 avg training loss: 7.3122
batch: [1670/10547] batch time: 0.244 trainign loss: 3.1931 avg training loss: 7.3109
batch: [1680/10547] batch time: 0.043 trainign loss: 6.3910 avg training loss: 7.3102
batch: [1690/10547] batch time: 0.046 trainign loss: 6.7743 avg training loss: 7.3100
batch: [1700/10547] batch time: 0.382 trainign loss: 6.3657 avg training loss: 7.3094
batch: [1710/10547] batch time: 0.289 trainign loss: 6.6155 avg training loss: 7.3090
batch: [1720/10547] batch time: 1.153 trainign loss: 6.2219 avg training loss: 7.3085
batch: [1730/10547] batch time: 0.345 trainign loss: 7.0580 avg training loss: 7.3080
batch: [1740/10547] batch time: 0.755 trainign loss: 6.6872 avg training loss: 7.3078
batch: [1750/10547] batch time: 1.648 trainign loss: 5.8671 avg training loss: 7.3073
batch: [1760/10547] batch time: 0.054 trainign loss: 2.4278 avg training loss: 7.3059
batch: [1770/10547] batch time: 2.171 trainign loss: 6.8707 avg training loss: 7.3060
batch: [1780/10547] batch time: 0.048 trainign loss: 5.6045 avg training loss: 7.3053
batch: [1790/10547] batch time: 1.742 trainign loss: 5.7458 avg training loss: 7.3046
batch: [1800/10547] batch time: 0.047 trainign loss: 5.6761 avg training loss: 7.3040
batch: [1810/10547] batch time: 0.958 trainign loss: 7.0688 avg training loss: 7.3038
batch: [1820/10547] batch time: 0.867 trainign loss: 6.5608 avg training loss: 7.3034
batch: [1830/10547] batch time: 0.772 trainign loss: 5.3581 avg training loss: 7.3028
batch: [1840/10547] batch time: 0.937 trainign loss: 6.9253 avg training loss: 7.3024
batch: [1850/10547] batch time: 0.169 trainign loss: 5.3312 avg training loss: 7.3017
batch: [1860/10547] batch time: 1.623 trainign loss: 4.7530 avg training loss: 7.3009
batch: [1870/10547] batch time: 0.228 trainign loss: 6.9569 avg training loss: 7.3005
batch: [1880/10547] batch time: 2.679 trainign loss: 5.9381 avg training loss: 7.3002
batch: [1890/10547] batch time: 0.054 trainign loss: 6.1710 avg training loss: 7.2997
batch: [1900/10547] batch time: 1.851 trainign loss: 3.6251 avg training loss: 7.2988
batch: [1910/10547] batch time: 0.046 trainign loss: 1.4412 avg training loss: 7.2960
batch: [1920/10547] batch time: 1.204 trainign loss: 7.8260 avg training loss: 7.2966
batch: [1930/10547] batch time: 0.046 trainign loss: 7.1894 avg training loss: 7.2968
batch: [1940/10547] batch time: 2.268 trainign loss: 6.8374 avg training loss: 7.2967
batch: [1950/10547] batch time: 0.054 trainign loss: 6.4534 avg training loss: 7.2963
batch: [1960/10547] batch time: 1.591 trainign loss: 6.3618 avg training loss: 7.2953
batch: [1970/10547] batch time: 0.046 trainign loss: 6.2961 avg training loss: 7.2951
batch: [1980/10547] batch time: 2.637 trainign loss: 6.0551 avg training loss: 7.2946
batch: [1990/10547] batch time: 0.046 trainign loss: 0.2744 avg training loss: 7.2929
batch: [2000/10547] batch time: 2.091 trainign loss: 10.3054 avg training loss: 7.2920
batch: [2010/10547] batch time: 0.048 trainign loss: 7.2335 avg training loss: 7.2921
batch: [2020/10547] batch time: 1.882 trainign loss: 6.6595 avg training loss: 7.2919
batch: [2030/10547] batch time: 0.046 trainign loss: 5.1979 avg training loss: 7.2914
batch: [2040/10547] batch time: 0.267 trainign loss: 5.7974 avg training loss: 7.2902
batch: [2050/10547] batch time: 0.046 trainign loss: 6.1420 avg training loss: 7.2898
batch: [2060/10547] batch time: 0.444 trainign loss: 4.3595 avg training loss: 7.2893
batch: [2070/10547] batch time: 0.046 trainign loss: 3.8535 avg training loss: 7.2886
batch: [2080/10547] batch time: 0.045 trainign loss: 5.8473 avg training loss: 7.2876
batch: [2090/10547] batch time: 0.046 trainign loss: 5.6478 avg training loss: 7.2869
batch: [2100/10547] batch time: 0.046 trainign loss: 0.0639 avg training loss: 7.2848
batch: [2110/10547] batch time: 0.048 trainign loss: 7.2018 avg training loss: 7.2849
batch: [2120/10547] batch time: 0.047 trainign loss: 5.6850 avg training loss: 7.2847
batch: [2130/10547] batch time: 0.051 trainign loss: 6.1478 avg training loss: 7.2844
batch: [2140/10547] batch time: 0.046 trainign loss: 5.8530 avg training loss: 7.2838
batch: [2150/10547] batch time: 0.048 trainign loss: 4.2469 avg training loss: 7.2831
batch: [2160/10547] batch time: 0.042 trainign loss: 5.8440 avg training loss: 7.2819
batch: [2170/10547] batch time: 0.046 trainign loss: 7.0591 avg training loss: 7.2812
batch: [2180/10547] batch time: 0.046 trainign loss: 6.1898 avg training loss: 7.2808
batch: [2190/10547] batch time: 0.046 trainign loss: 0.4226 avg training loss: 7.2792
batch: [2200/10547] batch time: 0.047 trainign loss: 6.4256 avg training loss: 7.2785
batch: [2210/10547] batch time: 0.047 trainign loss: 4.9734 avg training loss: 7.2782
batch: [2220/10547] batch time: 0.046 trainign loss: 7.6811 avg training loss: 7.2774
batch: [2230/10547] batch time: 0.308 trainign loss: 4.9113 avg training loss: 7.2768
batch: [2240/10547] batch time: 0.055 trainign loss: 6.8803 avg training loss: 7.2759
batch: [2250/10547] batch time: 0.148 trainign loss: 6.8372 avg training loss: 7.2757
batch: [2260/10547] batch time: 0.046 trainign loss: 5.7871 avg training loss: 7.2753
batch: [2270/10547] batch time: 1.254 trainign loss: 6.2152 avg training loss: 7.2746
batch: [2280/10547] batch time: 0.048 trainign loss: 4.4952 avg training loss: 7.2740
batch: [2290/10547] batch time: 1.821 trainign loss: 7.0213 avg training loss: 7.2734
batch: [2300/10547] batch time: 0.046 trainign loss: 5.6250 avg training loss: 7.2729
batch: [2310/10547] batch time: 2.072 trainign loss: 6.2754 avg training loss: 7.2721
batch: [2320/10547] batch time: 0.047 trainign loss: 7.0141 avg training loss: 7.2715
batch: [2330/10547] batch time: 2.486 trainign loss: 6.3671 avg training loss: 7.2708
batch: [2340/10547] batch time: 0.055 trainign loss: 6.2804 avg training loss: 7.2705
batch: [2350/10547] batch time: 2.373 trainign loss: 5.1219 avg training loss: 7.2697
batch: [2360/10547] batch time: 0.046 trainign loss: 5.6662 avg training loss: 7.2685
batch: [2370/10547] batch time: 2.050 trainign loss: 5.8672 avg training loss: 7.2681
batch: [2380/10547] batch time: 0.054 trainign loss: 3.6412 avg training loss: 7.2673
batch: [2390/10547] batch time: 1.995 trainign loss: 11.3241 avg training loss: 7.2653
batch: [2400/10547] batch time: 0.046 trainign loss: 7.1403 avg training loss: 7.2654
batch: [2410/10547] batch time: 2.509 trainign loss: 5.5139 avg training loss: 7.2651
batch: [2420/10547] batch time: 0.053 trainign loss: 7.0389 avg training loss: 7.2645
batch: [2430/10547] batch time: 2.029 trainign loss: 5.9231 avg training loss: 7.2637
batch: [2440/10547] batch time: 0.047 trainign loss: 5.9794 avg training loss: 7.2632
batch: [2450/10547] batch time: 1.301 trainign loss: 5.8922 avg training loss: 7.2625
batch: [2460/10547] batch time: 0.046 trainign loss: 1.7056 avg training loss: 7.2615
batch: [2470/10547] batch time: 0.043 trainign loss: 9.8108 avg training loss: 7.2597
batch: [2480/10547] batch time: 0.048 trainign loss: 5.2718 avg training loss: 7.2594
batch: [2490/10547] batch time: 1.873 trainign loss: 7.0817 avg training loss: 7.2588
batch: [2500/10547] batch time: 0.048 trainign loss: 6.4882 avg training loss: 7.2584
batch: [2510/10547] batch time: 2.190 trainign loss: 1.2241 avg training loss: 7.2573
batch: [2520/10547] batch time: 0.046 trainign loss: 7.6973 avg training loss: 7.2566
batch: [2530/10547] batch time: 1.971 trainign loss: 7.1161 avg training loss: 7.2565
batch: [2540/10547] batch time: 0.957 trainign loss: 7.2421 avg training loss: 7.2565
batch: [2550/10547] batch time: 1.770 trainign loss: 6.3234 avg training loss: 7.2560
batch: [2560/10547] batch time: 0.181 trainign loss: 6.0994 avg training loss: 7.2556
batch: [2570/10547] batch time: 1.664 trainign loss: 5.6419 avg training loss: 7.2551
batch: [2580/10547] batch time: 1.507 trainign loss: 5.6191 avg training loss: 7.2546
batch: [2590/10547] batch time: 1.747 trainign loss: 6.4040 avg training loss: 7.2541
batch: [2600/10547] batch time: 0.665 trainign loss: 3.7956 avg training loss: 7.2534
batch: [2610/10547] batch time: 1.288 trainign loss: 6.5159 avg training loss: 7.2529
batch: [2620/10547] batch time: 0.049 trainign loss: 6.3552 avg training loss: 7.2525
batch: [2630/10547] batch time: 1.374 trainign loss: 6.8128 avg training loss: 7.2520
batch: [2640/10547] batch time: 1.080 trainign loss: 4.3109 avg training loss: 7.2515
batch: [2650/10547] batch time: 1.549 trainign loss: 5.7709 avg training loss: 7.2505
batch: [2660/10547] batch time: 0.048 trainign loss: 7.1331 avg training loss: 7.2499
batch: [2670/10547] batch time: 1.332 trainign loss: 5.6303 avg training loss: 7.2494
batch: [2680/10547] batch time: 0.832 trainign loss: 6.0166 avg training loss: 7.2489
batch: [2690/10547] batch time: 1.007 trainign loss: 5.0150 avg training loss: 7.2483
batch: [2700/10547] batch time: 0.553 trainign loss: 6.2964 avg training loss: 7.2473
batch: [2710/10547] batch time: 0.198 trainign loss: 5.8303 avg training loss: 7.2467
batch: [2720/10547] batch time: 0.735 trainign loss: 6.1934 avg training loss: 7.2463
batch: [2730/10547] batch time: 0.554 trainign loss: 6.6903 avg training loss: 7.2460
batch: [2740/10547] batch time: 0.186 trainign loss: 6.6934 avg training loss: 7.2448
batch: [2750/10547] batch time: 0.629 trainign loss: 6.1246 avg training loss: 7.2445
batch: [2760/10547] batch time: 0.049 trainign loss: 5.8256 avg training loss: 7.2439
batch: [2770/10547] batch time: 0.571 trainign loss: 0.2218 avg training loss: 7.2421
batch: [2780/10547] batch time: 0.299 trainign loss: 7.6128 avg training loss: 7.2422
batch: [2790/10547] batch time: 0.055 trainign loss: 6.3116 avg training loss: 7.2421
batch: [2800/10547] batch time: 0.182 trainign loss: 5.6523 avg training loss: 7.2414
batch: [2810/10547] batch time: 0.047 trainign loss: 5.0910 avg training loss: 7.2406
batch: [2820/10547] batch time: 1.365 trainign loss: 6.4838 avg training loss: 7.2400
batch: [2830/10547] batch time: 0.045 trainign loss: 5.4635 avg training loss: 7.2395
batch: [2840/10547] batch time: 1.317 trainign loss: 5.2403 avg training loss: 7.2384
batch: [2850/10547] batch time: 0.047 trainign loss: 6.8991 avg training loss: 7.2377
batch: [2860/10547] batch time: 1.602 trainign loss: 6.7369 avg training loss: 7.2376
batch: [2870/10547] batch time: 0.046 trainign loss: 6.4120 avg training loss: 7.2372
batch: [2880/10547] batch time: 0.185 trainign loss: 6.3891 avg training loss: 7.2367
batch: [2890/10547] batch time: 0.046 trainign loss: 5.6938 avg training loss: 7.2362
batch: [2900/10547] batch time: 0.980 trainign loss: 5.6416 avg training loss: 7.2355
batch: [2910/10547] batch time: 0.054 trainign loss: 6.4601 avg training loss: 7.2351
batch: [2920/10547] batch time: 0.804 trainign loss: 5.2297 avg training loss: 7.2346
batch: [2930/10547] batch time: 0.047 trainign loss: 3.7209 avg training loss: 7.2335
batch: [2940/10547] batch time: 0.975 trainign loss: 6.6517 avg training loss: 7.2327
batch: [2950/10547] batch time: 0.054 trainign loss: 4.2891 avg training loss: 7.2319
batch: [2960/10547] batch time: 1.258 trainign loss: 6.2818 avg training loss: 7.2311
batch: [2970/10547] batch time: 0.046 trainign loss: 6.5967 avg training loss: 7.2309
batch: [2980/10547] batch time: 0.329 trainign loss: 3.6785 avg training loss: 7.2301
batch: [2990/10547] batch time: 0.042 trainign loss: 5.9001 avg training loss: 7.2290
batch: [3000/10547] batch time: 0.047 trainign loss: 6.2486 avg training loss: 7.2286
batch: [3010/10547] batch time: 0.048 trainign loss: 5.2317 avg training loss: 7.2277
batch: [3020/10547] batch time: 0.046 trainign loss: 6.6233 avg training loss: 7.2270
batch: [3030/10547] batch time: 0.042 trainign loss: 6.8836 avg training loss: 7.2268
batch: [3040/10547] batch time: 0.046 trainign loss: 4.0379 avg training loss: 7.2260
batch: [3050/10547] batch time: 0.045 trainign loss: 6.0196 avg training loss: 7.2251
batch: [3060/10547] batch time: 0.046 trainign loss: 6.4901 avg training loss: 7.2248
batch: [3070/10547] batch time: 0.937 trainign loss: 7.3761 avg training loss: 7.2240
batch: [3080/10547] batch time: 0.046 trainign loss: 6.9455 avg training loss: 7.2238
batch: [3090/10547] batch time: 0.929 trainign loss: 5.5768 avg training loss: 7.2234
batch: [3100/10547] batch time: 0.054 trainign loss: 5.8543 avg training loss: 7.2228
batch: [3110/10547] batch time: 1.059 trainign loss: 5.4175 avg training loss: 7.2221
batch: [3120/10547] batch time: 0.048 trainign loss: 6.8428 avg training loss: 7.2214
batch: [3130/10547] batch time: 0.550 trainign loss: 5.2948 avg training loss: 7.2208
batch: [3140/10547] batch time: 0.047 trainign loss: 6.5712 avg training loss: 7.2200
batch: [3150/10547] batch time: 0.047 trainign loss: 6.1794 avg training loss: 7.2197
batch: [3160/10547] batch time: 0.047 trainign loss: 5.9215 avg training loss: 7.2188
batch: [3170/10547] batch time: 0.046 trainign loss: 5.0702 avg training loss: 7.2181
batch: [3180/10547] batch time: 0.112 trainign loss: 4.9246 avg training loss: 7.2173
batch: [3190/10547] batch time: 0.047 trainign loss: 3.3280 avg training loss: 7.2157
batch: [3200/10547] batch time: 0.047 trainign loss: 7.3464 avg training loss: 7.2158
batch: [3210/10547] batch time: 0.050 trainign loss: 6.2545 avg training loss: 7.2156
batch: [3220/10547] batch time: 0.046 trainign loss: 5.7332 avg training loss: 7.2150
batch: [3230/10547] batch time: 0.054 trainign loss: 6.0133 avg training loss: 7.2142
batch: [3240/10547] batch time: 0.050 trainign loss: 4.4695 avg training loss: 7.2136
batch: [3250/10547] batch time: 0.047 trainign loss: 6.1951 avg training loss: 7.2129
batch: [3260/10547] batch time: 0.046 trainign loss: 5.9952 avg training loss: 7.2124
batch: [3270/10547] batch time: 0.055 trainign loss: 5.1088 avg training loss: 7.2120
batch: [3280/10547] batch time: 0.047 trainign loss: 6.2991 avg training loss: 7.2114
batch: [3290/10547] batch time: 0.668 trainign loss: 5.0054 avg training loss: 7.2108
batch: [3300/10547] batch time: 0.478 trainign loss: 6.0077 avg training loss: 7.2101
batch: [3310/10547] batch time: 0.845 trainign loss: 4.9423 avg training loss: 7.2096
batch: [3320/10547] batch time: 0.955 trainign loss: 4.8518 avg training loss: 7.2086
batch: [3330/10547] batch time: 1.242 trainign loss: 6.6139 avg training loss: 7.2082
batch: [3340/10547] batch time: 1.164 trainign loss: 5.6668 avg training loss: 7.2077
batch: [3350/10547] batch time: 1.370 trainign loss: 6.0498 avg training loss: 7.2072
batch: [3360/10547] batch time: 1.456 trainign loss: 6.3290 avg training loss: 7.2067
batch: [3370/10547] batch time: 0.402 trainign loss: 1.4325 avg training loss: 7.2054
batch: [3380/10547] batch time: 1.589 trainign loss: 6.3394 avg training loss: 7.2047
batch: [3390/10547] batch time: 1.181 trainign loss: 6.5552 avg training loss: 7.2041
batch: [3400/10547] batch time: 1.733 trainign loss: 4.5483 avg training loss: 7.2034
batch: [3410/10547] batch time: 0.053 trainign loss: 6.7117 avg training loss: 7.2030
batch: [3420/10547] batch time: 1.643 trainign loss: 6.2884 avg training loss: 7.2027
batch: [3430/10547] batch time: 0.424 trainign loss: 3.9008 avg training loss: 7.2020
batch: [3440/10547] batch time: 1.623 trainign loss: 6.2673 avg training loss: 7.2015
batch: [3450/10547] batch time: 0.054 trainign loss: 3.4735 avg training loss: 7.2005
batch: [3460/10547] batch time: 2.211 trainign loss: 6.9345 avg training loss: 7.2002
batch: [3470/10547] batch time: 0.046 trainign loss: 5.8168 avg training loss: 7.1999
batch: [3480/10547] batch time: 1.259 trainign loss: 4.9913 avg training loss: 7.1994
batch: [3490/10547] batch time: 0.055 trainign loss: 6.7622 avg training loss: 7.1985
batch: [3500/10547] batch time: 1.605 trainign loss: 5.6084 avg training loss: 7.1981
batch: [3510/10547] batch time: 0.047 trainign loss: 4.0923 avg training loss: 7.1973
batch: [3520/10547] batch time: 0.423 trainign loss: 6.1364 avg training loss: 7.1962
batch: [3530/10547] batch time: 0.047 trainign loss: 6.6767 avg training loss: 7.1957
batch: [3540/10547] batch time: 0.449 trainign loss: 5.6430 avg training loss: 7.1954
batch: [3550/10547] batch time: 0.046 trainign loss: 6.5127 avg training loss: 7.1950
batch: [3560/10547] batch time: 0.364 trainign loss: 4.9770 avg training loss: 7.1947
batch: [3570/10547] batch time: 0.049 trainign loss: 5.9981 avg training loss: 7.1939
batch: [3580/10547] batch time: 0.046 trainign loss: 6.2853 avg training loss: 7.1933
batch: [3590/10547] batch time: 0.046 trainign loss: 4.9542 avg training loss: 7.1925
batch: [3600/10547] batch time: 0.047 trainign loss: 7.0526 avg training loss: 7.1920
batch: [3610/10547] batch time: 0.054 trainign loss: 6.0326 avg training loss: 7.1915
batch: [3620/10547] batch time: 0.047 trainign loss: 5.6896 avg training loss: 7.1908
batch: [3630/10547] batch time: 0.046 trainign loss: 6.2884 avg training loss: 7.1902
batch: [3640/10547] batch time: 0.042 trainign loss: 4.7737 avg training loss: 7.1897
batch: [3650/10547] batch time: 0.046 trainign loss: 6.1934 avg training loss: 7.1889
batch: [3660/10547] batch time: 0.046 trainign loss: 1.9629 avg training loss: 7.1879
batch: [3670/10547] batch time: 0.047 trainign loss: 6.2895 avg training loss: 7.1875
batch: [3680/10547] batch time: 0.044 trainign loss: 3.9683 avg training loss: 7.1869
batch: [3690/10547] batch time: 0.048 trainign loss: 2.4796 avg training loss: 7.1858
batch: [3700/10547] batch time: 0.046 trainign loss: 6.6192 avg training loss: 7.1852
batch: [3710/10547] batch time: 0.046 trainign loss: 6.1664 avg training loss: 7.1848
batch: [3720/10547] batch time: 0.046 trainign loss: 5.7257 avg training loss: 7.1842
batch: [3730/10547] batch time: 0.054 trainign loss: 4.9254 avg training loss: 7.1835
batch: [3740/10547] batch time: 0.046 trainign loss: 5.4607 avg training loss: 7.1827
batch: [3750/10547] batch time: 0.046 trainign loss: 4.9256 avg training loss: 7.1822
batch: [3760/10547] batch time: 0.054 trainign loss: 5.8918 avg training loss: 7.1815
batch: [3770/10547] batch time: 0.047 trainign loss: 3.3621 avg training loss: 7.1808
batch: [3780/10547] batch time: 0.044 trainign loss: 6.7716 avg training loss: 7.1803
batch: [3790/10547] batch time: 0.047 trainign loss: 5.3424 avg training loss: 7.1798
batch: [3800/10547] batch time: 0.055 trainign loss: 3.0134 avg training loss: 7.1789
batch: [3810/10547] batch time: 0.047 trainign loss: 4.1517 avg training loss: 7.1785
batch: [3820/10547] batch time: 0.046 trainign loss: 8.5354 avg training loss: 7.1772
batch: [3830/10547] batch time: 0.046 trainign loss: 7.7170 avg training loss: 7.1775
batch: [3840/10547] batch time: 0.048 trainign loss: 6.5990 avg training loss: 7.1774
batch: [3850/10547] batch time: 0.049 trainign loss: 5.8818 avg training loss: 7.1770
batch: [3860/10547] batch time: 0.047 trainign loss: 5.9096 avg training loss: 7.1762
batch: [3870/10547] batch time: 0.050 trainign loss: 6.7002 avg training loss: 7.1759
batch: [3880/10547] batch time: 0.208 trainign loss: 5.5686 avg training loss: 7.1754
batch: [3890/10547] batch time: 0.054 trainign loss: 5.1861 avg training loss: 7.1750
batch: [3900/10547] batch time: 1.250 trainign loss: 2.5026 avg training loss: 7.1739
batch: [3910/10547] batch time: 0.046 trainign loss: 6.3097 avg training loss: 7.1735
batch: [3920/10547] batch time: 0.220 trainign loss: 6.6800 avg training loss: 7.1735
batch: [3930/10547] batch time: 0.047 trainign loss: 5.9382 avg training loss: 7.1732
batch: [3940/10547] batch time: 0.048 trainign loss: 4.6920 avg training loss: 7.1725
batch: [3950/10547] batch time: 0.487 trainign loss: 5.6477 avg training loss: 7.1719
batch: [3960/10547] batch time: 0.056 trainign loss: 0.9632 avg training loss: 7.1706
batch: [3970/10547] batch time: 0.620 trainign loss: 5.5574 avg training loss: 7.1697
batch: [3980/10547] batch time: 0.056 trainign loss: 6.5689 avg training loss: 7.1694
batch: [3990/10547] batch time: 0.669 trainign loss: 5.5630 avg training loss: 7.1690
batch: [4000/10547] batch time: 0.047 trainign loss: 5.2138 avg training loss: 7.1684
batch: [4010/10547] batch time: 0.680 trainign loss: 6.3859 avg training loss: 7.1678
batch: [4020/10547] batch time: 0.047 trainign loss: 6.0434 avg training loss: 7.1672
batch: [4030/10547] batch time: 1.355 trainign loss: 4.9414 avg training loss: 7.1667
batch: [4040/10547] batch time: 0.047 trainign loss: 6.3182 avg training loss: 7.1663
batch: [4050/10547] batch time: 1.715 trainign loss: 4.7764 avg training loss: 7.1656
batch: [4060/10547] batch time: 0.047 trainign loss: 5.1761 avg training loss: 7.1652
batch: [4070/10547] batch time: 2.098 trainign loss: 5.9272 avg training loss: 7.1646
batch: [4080/10547] batch time: 0.047 trainign loss: 6.4586 avg training loss: 7.1643
batch: [4090/10547] batch time: 1.904 trainign loss: 6.5381 avg training loss: 7.1641
batch: [4100/10547] batch time: 0.046 trainign loss: 5.1951 avg training loss: 7.1635
batch: [4110/10547] batch time: 1.710 trainign loss: 5.5131 avg training loss: 7.1629
batch: [4120/10547] batch time: 0.047 trainign loss: 4.2644 avg training loss: 7.1622
batch: [4130/10547] batch time: 0.803 trainign loss: 0.1725 avg training loss: 7.1605
batch: [4140/10547] batch time: 0.055 trainign loss: 6.2170 avg training loss: 7.1603
batch: [4150/10547] batch time: 0.715 trainign loss: 5.7548 avg training loss: 7.1594
batch: [4160/10547] batch time: 0.046 trainign loss: 5.7761 avg training loss: 7.1589
batch: [4170/10547] batch time: 0.046 trainign loss: 5.6927 avg training loss: 7.1585
batch: [4180/10547] batch time: 0.046 trainign loss: 4.0194 avg training loss: 7.1577
batch: [4190/10547] batch time: 0.546 trainign loss: 7.0365 avg training loss: 7.1571
batch: [4200/10547] batch time: 0.045 trainign loss: 6.4015 avg training loss: 7.1567
batch: [4210/10547] batch time: 1.319 trainign loss: 5.6243 avg training loss: 7.1562
batch: [4220/10547] batch time: 0.047 trainign loss: 5.8991 avg training loss: 7.1559
batch: [4230/10547] batch time: 1.657 trainign loss: 6.5452 avg training loss: 7.1551
batch: [4240/10547] batch time: 0.046 trainign loss: 6.1399 avg training loss: 7.1544
batch: [4250/10547] batch time: 1.648 trainign loss: 6.2142 avg training loss: 7.1536
batch: [4260/10547] batch time: 0.048 trainign loss: 5.0910 avg training loss: 7.1531
batch: [4270/10547] batch time: 2.139 trainign loss: 4.6524 avg training loss: 7.1525
batch: [4280/10547] batch time: 0.048 trainign loss: 0.3632 avg training loss: 7.1510
batch: [4290/10547] batch time: 2.283 trainign loss: 0.0017 avg training loss: 7.1482
batch: [4300/10547] batch time: 0.046 trainign loss: 2.2348 avg training loss: 7.1455
batch: [4310/10547] batch time: 2.080 trainign loss: 6.9096 avg training loss: 7.1465
batch: [4320/10547] batch time: 0.046 trainign loss: 6.6445 avg training loss: 7.1465
batch: [4330/10547] batch time: 2.138 trainign loss: 4.7503 avg training loss: 7.1460
batch: [4340/10547] batch time: 0.047 trainign loss: 5.8162 avg training loss: 7.1454
batch: [4350/10547] batch time: 2.379 trainign loss: 6.0346 avg training loss: 7.1451
batch: [4360/10547] batch time: 0.047 trainign loss: 6.1810 avg training loss: 7.1445
batch: [4370/10547] batch time: 2.583 trainign loss: 5.9755 avg training loss: 7.1443
batch: [4380/10547] batch time: 0.047 trainign loss: 5.8974 avg training loss: 7.1438
batch: [4390/10547] batch time: 2.159 trainign loss: 5.5775 avg training loss: 7.1433
batch: [4400/10547] batch time: 0.046 trainign loss: 5.8544 avg training loss: 7.1425
batch: [4410/10547] batch time: 2.253 trainign loss: 5.8630 avg training loss: 7.1420
batch: [4420/10547] batch time: 0.054 trainign loss: 5.3310 avg training loss: 7.1416
batch: [4430/10547] batch time: 2.389 trainign loss: 6.2712 avg training loss: 7.1410
batch: [4440/10547] batch time: 0.046 trainign loss: 4.8256 avg training loss: 7.1396
batch: [4450/10547] batch time: 2.543 trainign loss: 6.9874 avg training loss: 7.1393
batch: [4460/10547] batch time: 0.047 trainign loss: 6.9560 avg training loss: 7.1390
batch: [4470/10547] batch time: 2.104 trainign loss: 6.2626 avg training loss: 7.1386
batch: [4480/10547] batch time: 0.046 trainign loss: 6.9503 avg training loss: 7.1379
batch: [4490/10547] batch time: 2.379 trainign loss: 6.9066 avg training loss: 7.1373
batch: [4500/10547] batch time: 0.046 trainign loss: 4.1443 avg training loss: 7.1368
batch: [4510/10547] batch time: 2.457 trainign loss: 7.1212 avg training loss: 7.1358
batch: [4520/10547] batch time: 0.055 trainign loss: 7.2603 avg training loss: 7.1353
batch: [4530/10547] batch time: 2.580 trainign loss: 5.0734 avg training loss: 7.1350
batch: [4540/10547] batch time: 0.052 trainign loss: 6.1059 avg training loss: 7.1344
batch: [4550/10547] batch time: 2.117 trainign loss: 5.4899 avg training loss: 7.1340
batch: [4560/10547] batch time: 0.046 trainign loss: 5.4767 avg training loss: 7.1333
batch: [4570/10547] batch time: 2.517 trainign loss: 5.3440 avg training loss: 7.1327
batch: [4580/10547] batch time: 0.046 trainign loss: 5.3226 avg training loss: 7.1322
batch: [4590/10547] batch time: 2.604 trainign loss: 6.3036 avg training loss: 7.1316
batch: [4600/10547] batch time: 0.054 trainign loss: 5.8014 avg training loss: 7.1310
batch: [4610/10547] batch time: 2.492 trainign loss: 6.1791 avg training loss: 7.1304
batch: [4620/10547] batch time: 0.045 trainign loss: 6.1324 avg training loss: 7.1299
batch: [4630/10547] batch time: 2.220 trainign loss: 5.0918 avg training loss: 7.1294
batch: [4640/10547] batch time: 0.046 trainign loss: 6.9123 avg training loss: 7.1290
batch: [4650/10547] batch time: 2.381 trainign loss: 5.4200 avg training loss: 7.1286
batch: [4660/10547] batch time: 0.047 trainign loss: 6.4308 avg training loss: 7.1280
batch: [4670/10547] batch time: 1.717 trainign loss: 4.0866 avg training loss: 7.1271
batch: [4680/10547] batch time: 0.046 trainign loss: 6.1086 avg training loss: 7.1266
batch: [4690/10547] batch time: 2.157 trainign loss: 6.0649 avg training loss: 7.1261
batch: [4700/10547] batch time: 1.063 trainign loss: 6.5589 avg training loss: 7.1255
batch: [4710/10547] batch time: 1.705 trainign loss: 6.2232 avg training loss: 7.1248
batch: [4720/10547] batch time: 0.523 trainign loss: 6.6698 avg training loss: 7.1240
batch: [4730/10547] batch time: 2.092 trainign loss: 6.8210 avg training loss: 7.1234
batch: [4740/10547] batch time: 0.355 trainign loss: 5.7322 avg training loss: 7.1228
batch: [4750/10547] batch time: 1.377 trainign loss: 6.5288 avg training loss: 7.1224
batch: [4760/10547] batch time: 0.055 trainign loss: 6.3849 avg training loss: 7.1220
batch: [4770/10547] batch time: 1.695 trainign loss: 7.0735 avg training loss: 7.1215
batch: [4780/10547] batch time: 0.048 trainign loss: 6.7044 avg training loss: 7.1214
batch: [4790/10547] batch time: 1.940 trainign loss: 5.4232 avg training loss: 7.1209
batch: [4800/10547] batch time: 0.046 trainign loss: 6.6831 avg training loss: 7.1204
batch: [4810/10547] batch time: 1.700 trainign loss: 6.3677 avg training loss: 7.1200
batch: [4820/10547] batch time: 0.048 trainign loss: 4.9677 avg training loss: 7.1192
batch: [4830/10547] batch time: 1.647 trainign loss: 6.1272 avg training loss: 7.1188
batch: [4840/10547] batch time: 0.046 trainign loss: 4.9799 avg training loss: 7.1180
batch: [4850/10547] batch time: 0.740 trainign loss: 4.7216 avg training loss: 7.1173
batch: [4860/10547] batch time: 0.046 trainign loss: 4.4483 avg training loss: 7.1164
batch: [4870/10547] batch time: 1.185 trainign loss: 6.5312 avg training loss: 7.1160
batch: [4880/10547] batch time: 0.584 trainign loss: 5.6461 avg training loss: 7.1157
batch: [4890/10547] batch time: 0.853 trainign loss: 6.1633 avg training loss: 7.1152
batch: [4900/10547] batch time: 1.782 trainign loss: 6.6262 avg training loss: 7.1149
batch: [4910/10547] batch time: 1.027 trainign loss: 7.4382 avg training loss: 7.1146
batch: [4920/10547] batch time: 1.045 trainign loss: 6.4072 avg training loss: 7.1144
batch: [4930/10547] batch time: 2.627 trainign loss: 4.4269 avg training loss: 7.1137
batch: [4940/10547] batch time: 0.048 trainign loss: 5.9607 avg training loss: 7.1132
batch: [4950/10547] batch time: 2.362 trainign loss: 6.3602 avg training loss: 7.1124
batch: [4960/10547] batch time: 0.054 trainign loss: 6.7450 avg training loss: 7.1121
batch: [4970/10547] batch time: 2.631 trainign loss: 7.1077 avg training loss: 7.1120
batch: [4980/10547] batch time: 0.045 trainign loss: 6.1006 avg training loss: 7.1118
batch: [4990/10547] batch time: 1.866 trainign loss: 5.3877 avg training loss: 7.1112
batch: [5000/10547] batch time: 0.046 trainign loss: 5.4288 avg training loss: 7.1108
batch: [5010/10547] batch time: 2.341 trainign loss: 4.8892 avg training loss: 7.1101
batch: [5020/10547] batch time: 0.041 trainign loss: 6.4055 avg training loss: 7.1093
batch: [5030/10547] batch time: 2.153 trainign loss: 6.5287 avg training loss: 7.1089
batch: [5040/10547] batch time: 0.053 trainign loss: 6.4428 avg training loss: 7.1084
batch: [5050/10547] batch time: 2.476 trainign loss: 5.6261 avg training loss: 7.1080
batch: [5060/10547] batch time: 0.047 trainign loss: 6.5812 avg training loss: 7.1069
batch: [5070/10547] batch time: 1.562 trainign loss: 6.6970 avg training loss: 7.1067
batch: [5080/10547] batch time: 0.202 trainign loss: 6.0318 avg training loss: 7.1062
batch: [5090/10547] batch time: 1.801 trainign loss: 5.7577 avg training loss: 7.1056
batch: [5100/10547] batch time: 0.053 trainign loss: 4.4433 avg training loss: 7.1052
batch: [5110/10547] batch time: 1.819 trainign loss: 6.1245 avg training loss: 7.1047
batch: [5120/10547] batch time: 0.394 trainign loss: 3.8831 avg training loss: 7.1042
batch: [5130/10547] batch time: 1.343 trainign loss: 4.6265 avg training loss: 7.1036
batch: [5140/10547] batch time: 0.598 trainign loss: 6.4922 avg training loss: 7.1031
batch: [5150/10547] batch time: 2.102 trainign loss: 6.5643 avg training loss: 7.1030
batch: [5160/10547] batch time: 0.046 trainign loss: 0.5794 avg training loss: 7.1018
batch: [5170/10547] batch time: 2.297 trainign loss: 7.2519 avg training loss: 7.1016
batch: [5180/10547] batch time: 0.047 trainign loss: 6.8053 avg training loss: 7.1015
batch: [5190/10547] batch time: 1.938 trainign loss: 6.6248 avg training loss: 7.1010
batch: [5200/10547] batch time: 0.217 trainign loss: 4.3759 avg training loss: 7.1005
batch: [5210/10547] batch time: 1.229 trainign loss: 6.1111 avg training loss: 7.1000
batch: [5220/10547] batch time: 0.048 trainign loss: 6.2153 avg training loss: 7.0994
batch: [5230/10547] batch time: 2.823 trainign loss: 4.5342 avg training loss: 7.0987
batch: [5240/10547] batch time: 0.047 trainign loss: 2.1691 avg training loss: 7.0973
batch: [5250/10547] batch time: 2.126 trainign loss: 7.1617 avg training loss: 7.0971
batch: [5260/10547] batch time: 0.805 trainign loss: 6.3813 avg training loss: 7.0970
batch: [5270/10547] batch time: 2.010 trainign loss: 4.8659 avg training loss: 7.0964
batch: [5280/10547] batch time: 0.047 trainign loss: 6.5073 avg training loss: 7.0958
batch: [5290/10547] batch time: 2.024 trainign loss: 5.8604 avg training loss: 7.0954
batch: [5300/10547] batch time: 0.054 trainign loss: 1.3668 avg training loss: 7.0942
batch: [5310/10547] batch time: 1.698 trainign loss: 6.9064 avg training loss: 7.0937
batch: [5320/10547] batch time: 0.516 trainign loss: 6.1498 avg training loss: 7.0936
batch: [5330/10547] batch time: 1.853 trainign loss: 5.4844 avg training loss: 7.0931
batch: [5340/10547] batch time: 0.047 trainign loss: 4.2152 avg training loss: 7.0924
batch: [5350/10547] batch time: 1.152 trainign loss: 5.3423 avg training loss: 7.0916
batch: [5360/10547] batch time: 0.056 trainign loss: 5.5534 avg training loss: 7.0908
batch: [5370/10547] batch time: 1.198 trainign loss: 5.7199 avg training loss: 7.0905
batch: [5380/10547] batch time: 0.046 trainign loss: 6.2713 avg training loss: 7.0899
batch: [5390/10547] batch time: 0.431 trainign loss: 3.7938 avg training loss: 7.0893
batch: [5400/10547] batch time: 0.048 trainign loss: 5.4795 avg training loss: 7.0886
batch: [5410/10547] batch time: 0.048 trainign loss: 6.9312 avg training loss: 7.0883
batch: [5420/10547] batch time: 0.054 trainign loss: 6.7084 avg training loss: 7.0880
batch: [5430/10547] batch time: 0.408 trainign loss: 6.3369 avg training loss: 7.0876
batch: [5440/10547] batch time: 0.048 trainign loss: 5.7913 avg training loss: 7.0872
batch: [5450/10547] batch time: 0.315 trainign loss: 5.8041 avg training loss: 7.0866
batch: [5460/10547] batch time: 1.137 trainign loss: 5.9302 avg training loss: 7.0859
batch: [5470/10547] batch time: 0.047 trainign loss: 2.5399 avg training loss: 7.0853
batch: [5480/10547] batch time: 0.046 trainign loss: 4.1029 avg training loss: 7.0844
batch: [5490/10547] batch time: 0.671 trainign loss: 7.1444 avg training loss: 7.0840
batch: [5500/10547] batch time: 0.046 trainign loss: 6.2491 avg training loss: 7.0839
batch: [5510/10547] batch time: 1.052 trainign loss: 5.5223 avg training loss: 7.0834
batch: [5520/10547] batch time: 0.047 trainign loss: 6.1031 avg training loss: 7.0823
batch: [5530/10547] batch time: 0.752 trainign loss: 5.7783 avg training loss: 7.0818
batch: [5540/10547] batch time: 0.612 trainign loss: 3.9853 avg training loss: 7.0815
batch: [5550/10547] batch time: 0.046 trainign loss: 6.4099 avg training loss: 7.0807
batch: [5560/10547] batch time: 0.620 trainign loss: 6.0784 avg training loss: 7.0804
batch: [5570/10547] batch time: 0.199 trainign loss: 6.7096 avg training loss: 7.0801
batch: [5580/10547] batch time: 0.813 trainign loss: 4.3427 avg training loss: 7.0794
batch: [5590/10547] batch time: 0.045 trainign loss: 9.7919 avg training loss: 7.0778
batch: [5600/10547] batch time: 0.896 trainign loss: 7.0919 avg training loss: 7.0780
batch: [5610/10547] batch time: 0.046 trainign loss: 7.2798 avg training loss: 7.0781
batch: [5620/10547] batch time: 0.712 trainign loss: 5.5360 avg training loss: 7.0778
batch: [5630/10547] batch time: 0.046 trainign loss: 4.5391 avg training loss: 7.0771
batch: [5640/10547] batch time: 0.758 trainign loss: 6.1395 avg training loss: 7.0764
batch: [5650/10547] batch time: 0.054 trainign loss: 6.7547 avg training loss: 7.0762
batch: [5660/10547] batch time: 0.414 trainign loss: 5.0877 avg training loss: 7.0758
batch: [5670/10547] batch time: 0.049 trainign loss: 3.4256 avg training loss: 7.0751
batch: [5680/10547] batch time: 0.048 trainign loss: 5.1297 avg training loss: 7.0743
batch: [5690/10547] batch time: 0.048 trainign loss: 6.7339 avg training loss: 7.0741
batch: [5700/10547] batch time: 0.049 trainign loss: 6.0134 avg training loss: 7.0737
batch: [5710/10547] batch time: 0.042 trainign loss: 5.2004 avg training loss: 7.0729
batch: [5720/10547] batch time: 0.047 trainign loss: 6.2918 avg training loss: 7.0727
batch: [5730/10547] batch time: 0.046 trainign loss: 6.0992 avg training loss: 7.0724
batch: [5740/10547] batch time: 0.054 trainign loss: 5.3084 avg training loss: 7.0719
batch: [5750/10547] batch time: 0.047 trainign loss: 4.4477 avg training loss: 7.0713
batch: [5760/10547] batch time: 0.054 trainign loss: 5.3321 avg training loss: 7.0705
batch: [5770/10547] batch time: 0.047 trainign loss: 6.6641 avg training loss: 7.0701
batch: [5780/10547] batch time: 0.047 trainign loss: 4.9653 avg training loss: 7.0696
batch: [5790/10547] batch time: 0.047 trainign loss: 0.0189 avg training loss: 7.0676
batch: [5800/10547] batch time: 0.048 trainign loss: 9.4328 avg training loss: 7.0662
batch: [5810/10547] batch time: 0.312 trainign loss: 7.2079 avg training loss: 7.0663
batch: [5820/10547] batch time: 0.047 trainign loss: 6.9455 avg training loss: 7.0659
batch: [5830/10547] batch time: 0.446 trainign loss: 4.7997 avg training loss: 7.0651
batch: [5840/10547] batch time: 0.047 trainign loss: 3.8720 avg training loss: 7.0642
batch: [5850/10547] batch time: 1.133 trainign loss: 7.4661 avg training loss: 7.0633
batch: [5860/10547] batch time: 0.046 trainign loss: 6.5540 avg training loss: 7.0634
batch: [5870/10547] batch time: 1.871 trainign loss: 4.8099 avg training loss: 7.0629
batch: [5880/10547] batch time: 0.050 trainign loss: 2.8745 avg training loss: 7.0621
batch: [5890/10547] batch time: 1.491 trainign loss: 5.8581 avg training loss: 7.0616
batch: [5900/10547] batch time: 0.046 trainign loss: 5.7458 avg training loss: 7.0613
batch: [5910/10547] batch time: 2.492 trainign loss: 6.6991 avg training loss: 7.0611
batch: [5920/10547] batch time: 0.046 trainign loss: 5.6139 avg training loss: 7.0608
batch: [5930/10547] batch time: 2.443 trainign loss: 3.0249 avg training loss: 7.0600
batch: [5940/10547] batch time: 0.047 trainign loss: 5.1906 avg training loss: 7.0586
batch: [5950/10547] batch time: 2.414 trainign loss: 5.5456 avg training loss: 7.0582
batch: [5960/10547] batch time: 0.049 trainign loss: 5.9644 avg training loss: 7.0582
batch: [5970/10547] batch time: 1.589 trainign loss: 6.6833 avg training loss: 7.0573
batch: [5980/10547] batch time: 0.046 trainign loss: 5.4191 avg training loss: 7.0563
batch: [5990/10547] batch time: 2.241 trainign loss: 7.5988 avg training loss: 7.0563
batch: [6000/10547] batch time: 0.046 trainign loss: 6.6425 avg training loss: 7.0561
batch: [6010/10547] batch time: 1.539 trainign loss: 5.9122 avg training loss: 7.0558
batch: [6020/10547] batch time: 0.046 trainign loss: 5.8555 avg training loss: 7.0554
batch: [6030/10547] batch time: 2.040 trainign loss: 4.2674 avg training loss: 7.0545
batch: [6040/10547] batch time: 0.046 trainign loss: 7.0911 avg training loss: 7.0541
batch: [6050/10547] batch time: 0.656 trainign loss: 6.3879 avg training loss: 7.0538
batch: [6060/10547] batch time: 0.046 trainign loss: 6.1519 avg training loss: 7.0535
batch: [6070/10547] batch time: 0.191 trainign loss: 5.4545 avg training loss: 7.0530
batch: [6080/10547] batch time: 0.046 trainign loss: 5.7272 avg training loss: 7.0526
batch: [6090/10547] batch time: 0.530 trainign loss: 6.3322 avg training loss: 7.0522
batch: [6100/10547] batch time: 0.052 trainign loss: 4.7834 avg training loss: 7.0517
batch: [6110/10547] batch time: 0.107 trainign loss: 6.4970 avg training loss: 7.0511
batch: [6120/10547] batch time: 0.047 trainign loss: 5.6477 avg training loss: 7.0507
batch: [6130/10547] batch time: 0.078 trainign loss: 4.0470 avg training loss: 7.0501
batch: [6140/10547] batch time: 0.046 trainign loss: 6.2777 avg training loss: 7.0495
batch: [6150/10547] batch time: 0.714 trainign loss: 6.0146 avg training loss: 7.0492
batch: [6160/10547] batch time: 0.046 trainign loss: 6.6929 avg training loss: 7.0487
batch: [6170/10547] batch time: 0.050 trainign loss: 0.2132 avg training loss: 7.0473
batch: [6180/10547] batch time: 0.049 trainign loss: 7.7800 avg training loss: 7.0465
batch: [6190/10547] batch time: 0.044 trainign loss: 5.9264 avg training loss: 7.0463
batch: [6200/10547] batch time: 0.046 trainign loss: 6.7511 avg training loss: 7.0458
batch: [6210/10547] batch time: 0.046 trainign loss: 6.2446 avg training loss: 7.0457
batch: [6220/10547] batch time: 0.046 trainign loss: 5.6489 avg training loss: 7.0453
batch: [6230/10547] batch time: 0.047 trainign loss: 4.2521 avg training loss: 7.0448
batch: [6240/10547] batch time: 0.046 trainign loss: 5.6239 avg training loss: 7.0441
batch: [6250/10547] batch time: 0.046 trainign loss: 5.1038 avg training loss: 7.0436
batch: [6260/10547] batch time: 0.054 trainign loss: 5.5812 avg training loss: 7.0430
batch: [6270/10547] batch time: 0.053 trainign loss: 2.8920 avg training loss: 7.0420
batch: [6280/10547] batch time: 0.047 trainign loss: 6.3325 avg training loss: 7.0417
batch: [6290/10547] batch time: 0.359 trainign loss: 6.3354 avg training loss: 7.0413
batch: [6300/10547] batch time: 0.046 trainign loss: 6.2853 avg training loss: 7.0409
batch: [6310/10547] batch time: 0.048 trainign loss: 3.0846 avg training loss: 7.0402
batch: [6320/10547] batch time: 0.046 trainign loss: 6.8647 avg training loss: 7.0400
batch: [6330/10547] batch time: 0.047 trainign loss: 5.4705 avg training loss: 7.0397
batch: [6340/10547] batch time: 0.046 trainign loss: 6.1990 avg training loss: 7.0392
batch: [6350/10547] batch time: 0.047 trainign loss: 6.2422 avg training loss: 7.0389
batch: [6360/10547] batch time: 0.046 trainign loss: 5.7788 avg training loss: 7.0384
batch: [6370/10547] batch time: 0.043 trainign loss: 6.3006 avg training loss: 7.0377
batch: [6380/10547] batch time: 0.047 trainign loss: 3.2564 avg training loss: 7.0371
batch: [6390/10547] batch time: 0.047 trainign loss: 6.4723 avg training loss: 7.0369
batch: [6400/10547] batch time: 0.048 trainign loss: 4.9622 avg training loss: 7.0364
batch: [6410/10547] batch time: 0.052 trainign loss: 5.4892 avg training loss: 7.0359
batch: [6420/10547] batch time: 0.049 trainign loss: 5.1845 avg training loss: 7.0351
batch: [6430/10547] batch time: 0.046 trainign loss: 6.9200 avg training loss: 7.0348
batch: [6440/10547] batch time: 0.046 trainign loss: 3.1868 avg training loss: 7.0339
batch: [6450/10547] batch time: 0.042 trainign loss: 7.8622 avg training loss: 7.0333
batch: [6460/10547] batch time: 0.046 trainign loss: 5.5874 avg training loss: 7.0332
batch: [6470/10547] batch time: 1.246 trainign loss: 7.1841 avg training loss: 7.0326
batch: [6480/10547] batch time: 0.047 trainign loss: 6.0864 avg training loss: 7.0321
batch: [6490/10547] batch time: 0.046 trainign loss: 4.0899 avg training loss: 7.0315
batch: [6500/10547] batch time: 0.553 trainign loss: 0.0498 avg training loss: 7.0296
batch: [6510/10547] batch time: 0.054 trainign loss: 0.0005 avg training loss: 7.0270
batch: [6520/10547] batch time: 0.842 trainign loss: 0.0001 avg training loss: 7.0245
batch: [6530/10547] batch time: 0.046 trainign loss: 8.7823 avg training loss: 7.0237
batch: [6540/10547] batch time: 0.969 trainign loss: 7.5897 avg training loss: 7.0239
batch: [6550/10547] batch time: 0.047 trainign loss: 5.8372 avg training loss: 7.0238
batch: [6560/10547] batch time: 0.325 trainign loss: 5.4537 avg training loss: 7.0235
batch: [6570/10547] batch time: 0.047 trainign loss: 2.7859 avg training loss: 7.0227
batch: [6580/10547] batch time: 0.672 trainign loss: 5.9123 avg training loss: 7.0222
batch: [6590/10547] batch time: 0.045 trainign loss: 6.2329 avg training loss: 7.0219
batch: [6600/10547] batch time: 1.388 trainign loss: 3.9430 avg training loss: 7.0213
batch: [6610/10547] batch time: 0.046 trainign loss: 3.2698 avg training loss: 7.0205
batch: [6620/10547] batch time: 2.261 trainign loss: 7.1314 avg training loss: 7.0203
batch: [6630/10547] batch time: 0.046 trainign loss: 5.3874 avg training loss: 7.0201
batch: [6640/10547] batch time: 2.024 trainign loss: 5.4512 avg training loss: 7.0193
batch: [6650/10547] batch time: 0.054 trainign loss: 3.6049 avg training loss: 7.0185
batch: [6660/10547] batch time: 1.200 trainign loss: 6.5280 avg training loss: 7.0180
batch: [6670/10547] batch time: 0.046 trainign loss: 6.8707 avg training loss: 7.0178
batch: [6680/10547] batch time: 0.598 trainign loss: 6.1712 avg training loss: 7.0173
batch: [6690/10547] batch time: 0.046 trainign loss: 5.7824 avg training loss: 7.0168
batch: [6700/10547] batch time: 0.854 trainign loss: 5.7285 avg training loss: 7.0164
batch: [6710/10547] batch time: 0.056 trainign loss: 6.3340 avg training loss: 7.0161
batch: [6720/10547] batch time: 1.070 trainign loss: 5.9733 avg training loss: 7.0156
batch: [6730/10547] batch time: 0.053 trainign loss: 6.4932 avg training loss: 7.0152
batch: [6740/10547] batch time: 1.177 trainign loss: 5.1646 avg training loss: 7.0149
batch: [6750/10547] batch time: 0.046 trainign loss: 5.1911 avg training loss: 7.0141
batch: [6760/10547] batch time: 2.018 trainign loss: 5.5323 avg training loss: 7.0137
batch: [6770/10547] batch time: 0.047 trainign loss: 5.8046 avg training loss: 7.0130
batch: [6780/10547] batch time: 2.133 trainign loss: 6.2097 avg training loss: 7.0125
batch: [6790/10547] batch time: 0.053 trainign loss: 6.3912 avg training loss: 7.0122
batch: [6800/10547] batch time: 2.159 trainign loss: 5.6854 avg training loss: 7.0119
batch: [6810/10547] batch time: 0.049 trainign loss: 5.7963 avg training loss: 7.0113
batch: [6820/10547] batch time: 2.448 trainign loss: 2.5070 avg training loss: 7.0102
batch: [6830/10547] batch time: 0.047 trainign loss: 5.1620 avg training loss: 7.0098
batch: [6840/10547] batch time: 2.061 trainign loss: 4.7760 avg training loss: 7.0095
batch: [6850/10547] batch time: 0.048 trainign loss: 3.0276 avg training loss: 7.0088
batch: [6860/10547] batch time: 1.559 trainign loss: 5.2030 avg training loss: 7.0082
batch: [6870/10547] batch time: 0.048 trainign loss: 6.0778 avg training loss: 7.0073
batch: [6880/10547] batch time: 1.916 trainign loss: 6.0691 avg training loss: 7.0071
batch: [6890/10547] batch time: 0.046 trainign loss: 5.9669 avg training loss: 7.0068
batch: [6900/10547] batch time: 1.525 trainign loss: 5.0724 avg training loss: 7.0061
batch: [6910/10547] batch time: 0.047 trainign loss: 5.2497 avg training loss: 7.0054
batch: [6920/10547] batch time: 1.498 trainign loss: 4.4651 avg training loss: 7.0046
batch: [6930/10547] batch time: 0.046 trainign loss: 6.7557 avg training loss: 7.0040
batch: [6940/10547] batch time: 2.042 trainign loss: 6.0806 avg training loss: 7.0039
batch: [6950/10547] batch time: 0.046 trainign loss: 4.7321 avg training loss: 7.0034
batch: [6960/10547] batch time: 1.406 trainign loss: 6.1584 avg training loss: 7.0025
batch: [6970/10547] batch time: 0.047 trainign loss: 5.6402 avg training loss: 7.0019
batch: [6980/10547] batch time: 1.885 trainign loss: 6.3424 avg training loss: 7.0015
batch: [6990/10547] batch time: 0.047 trainign loss: 5.2926 avg training loss: 7.0012
batch: [7000/10547] batch time: 1.811 trainign loss: 6.9578 avg training loss: 7.0005
batch: [7010/10547] batch time: 0.049 trainign loss: 4.3123 avg training loss: 7.0001
batch: [7020/10547] batch time: 2.240 trainign loss: 6.0718 avg training loss: 6.9994
batch: [7030/10547] batch time: 0.055 trainign loss: 6.5745 avg training loss: 6.9993
batch: [7040/10547] batch time: 2.448 trainign loss: 5.9658 avg training loss: 6.9989
batch: [7050/10547] batch time: 0.046 trainign loss: 6.1066 avg training loss: 6.9984
batch: [7060/10547] batch time: 2.157 trainign loss: 6.1376 avg training loss: 6.9980
batch: [7070/10547] batch time: 0.054 trainign loss: 7.6298 avg training loss: 6.9971
batch: [7080/10547] batch time: 2.265 trainign loss: 6.5466 avg training loss: 6.9968
batch: [7090/10547] batch time: 0.046 trainign loss: 6.7367 avg training loss: 6.9966
batch: [7100/10547] batch time: 2.536 trainign loss: 6.4203 avg training loss: 6.9962
batch: [7110/10547] batch time: 0.051 trainign loss: 5.3358 avg training loss: 6.9954
batch: [7120/10547] batch time: 2.116 trainign loss: 6.6437 avg training loss: 6.9950
batch: [7130/10547] batch time: 0.047 trainign loss: 5.2130 avg training loss: 6.9947
batch: [7140/10547] batch time: 2.114 trainign loss: 5.0740 avg training loss: 6.9939
batch: [7150/10547] batch time: 0.051 trainign loss: 0.1659 avg training loss: 6.9927
batch: [7160/10547] batch time: 1.823 trainign loss: 0.0005 avg training loss: 6.9902
batch: [7170/10547] batch time: 0.048 trainign loss: 0.0001 avg training loss: 6.9877
batch: [7180/10547] batch time: 1.941 trainign loss: 0.0001 avg training loss: 6.9852
batch: [7190/10547] batch time: 0.055 trainign loss: 0.0001 avg training loss: 6.9828
batch: [7200/10547] batch time: 1.733 trainign loss: 0.0001 avg training loss: 6.9803
batch: [7210/10547] batch time: 0.046 trainign loss: 0.0001 avg training loss: 6.9778
batch: [7220/10547] batch time: 1.953 trainign loss: 0.0001 avg training loss: 6.9754
batch: [7230/10547] batch time: 0.047 trainign loss: 0.0001 avg training loss: 6.9729
batch: [7240/10547] batch time: 2.698 trainign loss: 0.0001 avg training loss: 6.9705
batch: [7250/10547] batch time: 0.046 trainign loss: 7.8673 avg training loss: 6.9711
batch: [7260/10547] batch time: 2.241 trainign loss: 6.9445 avg training loss: 6.9713
batch: [7270/10547] batch time: 0.047 trainign loss: 5.7435 avg training loss: 6.9712
batch: [7280/10547] batch time: 2.347 trainign loss: 5.9499 avg training loss: 6.9709
batch: [7290/10547] batch time: 0.046 trainign loss: 7.0299 avg training loss: 6.9703
batch: [7300/10547] batch time: 2.533 trainign loss: 0.1387 avg training loss: 6.9689
batch: [7310/10547] batch time: 0.047 trainign loss: 9.8808 avg training loss: 6.9677
batch: [7320/10547] batch time: 1.859 trainign loss: 7.5144 avg training loss: 6.9679
batch: [7330/10547] batch time: 0.047 trainign loss: 6.3266 avg training loss: 6.9678
batch: [7340/10547] batch time: 1.566 trainign loss: 6.6980 avg training loss: 6.9677
batch: [7350/10547] batch time: 0.046 trainign loss: 6.8233 avg training loss: 6.9674
batch: [7360/10547] batch time: 1.604 trainign loss: 6.6322 avg training loss: 6.9672
batch: [7370/10547] batch time: 0.404 trainign loss: 6.1165 avg training loss: 6.9670
batch: [7380/10547] batch time: 0.046 trainign loss: 6.5021 avg training loss: 6.9668
batch: [7390/10547] batch time: 1.649 trainign loss: 5.8011 avg training loss: 6.9665
batch: [7400/10547] batch time: 0.055 trainign loss: 5.9302 avg training loss: 6.9658
batch: [7410/10547] batch time: 0.451 trainign loss: 6.0508 avg training loss: 6.9653
batch: [7420/10547] batch time: 0.047 trainign loss: 6.2442 avg training loss: 6.9650
batch: [7430/10547] batch time: 0.812 trainign loss: 4.5747 avg training loss: 6.9643
batch: [7440/10547] batch time: 0.046 trainign loss: 5.2423 avg training loss: 6.9637
batch: [7450/10547] batch time: 0.046 trainign loss: 6.4101 avg training loss: 6.9634
batch: [7460/10547] batch time: 0.047 trainign loss: 6.0205 avg training loss: 6.9631
batch: [7470/10547] batch time: 0.048 trainign loss: 6.0684 avg training loss: 6.9625
batch: [7480/10547] batch time: 0.046 trainign loss: 6.2856 avg training loss: 6.9620
batch: [7490/10547] batch time: 0.046 trainign loss: 0.7079 avg training loss: 6.9610
batch: [7500/10547] batch time: 0.046 trainign loss: 6.7158 avg training loss: 6.9608
batch: [7510/10547] batch time: 0.046 trainign loss: 6.4614 avg training loss: 6.9603
batch: [7520/10547] batch time: 0.047 trainign loss: 5.0544 avg training loss: 6.9595
batch: [7530/10547] batch time: 0.046 trainign loss: 6.5863 avg training loss: 6.9593
batch: [7540/10547] batch time: 0.041 trainign loss: 6.1341 avg training loss: 6.9588
batch: [7550/10547] batch time: 0.047 trainign loss: 1.9976 avg training loss: 6.9579
batch: [7560/10547] batch time: 0.042 trainign loss: 6.2833 avg training loss: 6.9576
batch: [7570/10547] batch time: 0.049 trainign loss: 5.7968 avg training loss: 6.9574
batch: [7580/10547] batch time: 0.046 trainign loss: 6.4031 avg training loss: 6.9567
batch: [7590/10547] batch time: 0.052 trainign loss: 6.4887 avg training loss: 6.9561
batch: [7600/10547] batch time: 0.046 trainign loss: 3.5310 avg training loss: 6.9554
batch: [7610/10547] batch time: 0.046 trainign loss: 6.7906 avg training loss: 6.9550
batch: [7620/10547] batch time: 0.054 trainign loss: 6.7281 avg training loss: 6.9549
batch: [7630/10547] batch time: 0.261 trainign loss: 4.3022 avg training loss: 6.9542
batch: [7640/10547] batch time: 0.047 trainign loss: 6.0292 avg training loss: 6.9536
batch: [7650/10547] batch time: 0.572 trainign loss: 7.1047 avg training loss: 6.9535
batch: [7660/10547] batch time: 0.054 trainign loss: 6.3120 avg training loss: 6.9532
batch: [7670/10547] batch time: 0.055 trainign loss: 4.4439 avg training loss: 6.9528
batch: [7680/10547] batch time: 0.046 trainign loss: 5.5420 avg training loss: 6.9523
batch: [7690/10547] batch time: 0.046 trainign loss: 5.6591 avg training loss: 6.9518
batch: [7700/10547] batch time: 0.047 trainign loss: 6.3851 avg training loss: 6.9513
batch: [7710/10547] batch time: 0.047 trainign loss: 7.5851 avg training loss: 6.9502
batch: [7720/10547] batch time: 0.047 trainign loss: 7.2254 avg training loss: 6.9499
batch: [7730/10547] batch time: 0.049 trainign loss: 5.4843 avg training loss: 6.9495
batch: [7740/10547] batch time: 0.047 trainign loss: 6.0745 avg training loss: 6.9494
batch: [7750/10547] batch time: 0.050 trainign loss: 5.8350 avg training loss: 6.9490
batch: [7760/10547] batch time: 0.047 trainign loss: 6.7146 avg training loss: 6.9488
batch: [7770/10547] batch time: 0.046 trainign loss: 6.1090 avg training loss: 6.9485
batch: [7780/10547] batch time: 0.047 trainign loss: 6.2333 avg training loss: 6.9481
batch: [7790/10547] batch time: 0.053 trainign loss: 5.6141 avg training loss: 6.9476
batch: [7800/10547] batch time: 0.052 trainign loss: 5.5015 avg training loss: 6.9471
batch: [7810/10547] batch time: 0.047 trainign loss: 0.3401 avg training loss: 6.9458
batch: [7820/10547] batch time: 0.046 trainign loss: 10.5547 avg training loss: 6.9441
batch: [7830/10547] batch time: 0.046 trainign loss: 7.4880 avg training loss: 6.9445
batch: [7840/10547] batch time: 0.047 trainign loss: 6.7862 avg training loss: 6.9445
batch: [7850/10547] batch time: 0.048 trainign loss: 4.4589 avg training loss: 6.9442
batch: [7860/10547] batch time: 0.046 trainign loss: 6.0227 avg training loss: 6.9438
batch: [7870/10547] batch time: 0.053 trainign loss: 6.2390 avg training loss: 6.9435
batch: [7880/10547] batch time: 0.047 trainign loss: 5.7726 avg training loss: 6.9430
batch: [7890/10547] batch time: 0.048 trainign loss: 6.0989 avg training loss: 6.9427
batch: [7900/10547] batch time: 0.046 trainign loss: 4.0471 avg training loss: 6.9422
batch: [7910/10547] batch time: 0.046 trainign loss: 6.3955 avg training loss: 6.9416
batch: [7920/10547] batch time: 0.227 trainign loss: 6.6964 avg training loss: 6.9412
batch: [7930/10547] batch time: 0.055 trainign loss: 6.0635 avg training loss: 6.9410
batch: [7940/10547] batch time: 0.911 trainign loss: 3.8660 avg training loss: 6.9405
batch: [7950/10547] batch time: 0.046 trainign loss: 8.1136 avg training loss: 6.9398
batch: [7960/10547] batch time: 1.106 trainign loss: 6.1534 avg training loss: 6.9396
batch: [7970/10547] batch time: 0.054 trainign loss: 6.8053 avg training loss: 6.9393
batch: [7980/10547] batch time: 1.281 trainign loss: 6.4422 avg training loss: 6.9391
batch: [7990/10547] batch time: 0.049 trainign loss: 6.3707 avg training loss: 6.9388
batch: [8000/10547] batch time: 2.047 trainign loss: 5.2800 avg training loss: 6.9383
batch: [8010/10547] batch time: 0.047 trainign loss: 5.3684 avg training loss: 6.9377
batch: [8020/10547] batch time: 1.017 trainign loss: 6.1663 avg training loss: 6.9373
batch: [8030/10547] batch time: 0.048 trainign loss: 6.3556 avg training loss: 6.9370
batch: [8040/10547] batch time: 2.706 trainign loss: 5.5348 avg training loss: 6.9366
batch: [8050/10547] batch time: 0.055 trainign loss: 5.5744 avg training loss: 6.9363
batch: [8060/10547] batch time: 2.266 trainign loss: 6.3852 avg training loss: 6.9359
batch: [8070/10547] batch time: 0.046 trainign loss: 5.8362 avg training loss: 6.9355
batch: [8080/10547] batch time: 2.328 trainign loss: 5.7100 avg training loss: 6.9352
batch: [8090/10547] batch time: 0.047 trainign loss: 5.4638 avg training loss: 6.9345
batch: [8100/10547] batch time: 2.306 trainign loss: 5.5948 avg training loss: 6.9336
batch: [8110/10547] batch time: 0.045 trainign loss: 5.5577 avg training loss: 6.9332
batch: [8120/10547] batch time: 2.154 trainign loss: 6.2558 avg training loss: 6.9329
batch: [8130/10547] batch time: 0.045 trainign loss: 5.3870 avg training loss: 6.9325
batch: [8140/10547] batch time: 2.224 trainign loss: 5.8396 avg training loss: 6.9323
batch: [8150/10547] batch time: 0.046 trainign loss: 5.6496 avg training loss: 6.9318
batch: [8160/10547] batch time: 2.495 trainign loss: 6.2568 avg training loss: 6.9314
batch: [8170/10547] batch time: 0.046 trainign loss: 3.4629 avg training loss: 6.9307
batch: [8180/10547] batch time: 2.118 trainign loss: 6.2415 avg training loss: 6.9301
batch: [8190/10547] batch time: 0.046 trainign loss: 5.7577 avg training loss: 6.9300
batch: [8200/10547] batch time: 2.103 trainign loss: 4.8923 avg training loss: 6.9294
batch: [8210/10547] batch time: 0.054 trainign loss: 1.2818 avg training loss: 6.9281
batch: [8220/10547] batch time: 2.108 trainign loss: 6.8135 avg training loss: 6.9275
batch: [8230/10547] batch time: 0.055 trainign loss: 6.5498 avg training loss: 6.9275
batch: [8240/10547] batch time: 1.919 trainign loss: 5.7966 avg training loss: 6.9272
batch: [8250/10547] batch time: 0.055 trainign loss: 6.1486 avg training loss: 6.9267
batch: [8260/10547] batch time: 1.172 trainign loss: 5.0438 avg training loss: 6.9264
batch: [8270/10547] batch time: 0.047 trainign loss: 6.4516 avg training loss: 6.9261
batch: [8280/10547] batch time: 0.513 trainign loss: 6.2467 avg training loss: 6.9259
batch: [8290/10547] batch time: 0.046 trainign loss: 4.6982 avg training loss: 6.9253
batch: [8300/10547] batch time: 1.311 trainign loss: 4.8544 avg training loss: 6.9247
batch: [8310/10547] batch time: 0.047 trainign loss: 6.3911 avg training loss: 6.9243
batch: [8320/10547] batch time: 0.837 trainign loss: 6.2042 avg training loss: 6.9241
batch: [8330/10547] batch time: 0.048 trainign loss: 6.5329 avg training loss: 6.9235
batch: [8340/10547] batch time: 0.078 trainign loss: 4.5225 avg training loss: 6.9230
batch: [8350/10547] batch time: 0.053 trainign loss: 0.0063 avg training loss: 6.9210
batch: [8360/10547] batch time: 0.860 trainign loss: 6.9220 avg training loss: 6.9212
batch: [8370/10547] batch time: 0.047 trainign loss: 7.2990 avg training loss: 6.9209
batch: [8380/10547] batch time: 0.621 trainign loss: 5.5168 avg training loss: 6.9204
batch: [8390/10547] batch time: 0.046 trainign loss: 2.8132 avg training loss: 6.9198
batch: [8400/10547] batch time: 0.048 trainign loss: 5.9319 avg training loss: 6.9190
batch: [8410/10547] batch time: 0.046 trainign loss: 5.1755 avg training loss: 6.9184
batch: [8420/10547] batch time: 0.046 trainign loss: 4.9404 avg training loss: 6.9167
batch: [8430/10547] batch time: 0.046 trainign loss: 5.2641 avg training loss: 6.9169
batch: [8440/10547] batch time: 0.047 trainign loss: 4.8898 avg training loss: 6.9165
batch: [8450/10547] batch time: 0.046 trainign loss: 5.7101 avg training loss: 6.9158
batch: [8460/10547] batch time: 0.046 trainign loss: 5.6461 avg training loss: 6.9152
batch: [8470/10547] batch time: 0.043 trainign loss: 6.3807 avg training loss: 6.9149
batch: [8480/10547] batch time: 0.527 trainign loss: 5.0578 avg training loss: 6.9146
batch: [8490/10547] batch time: 0.055 trainign loss: 5.2160 avg training loss: 6.9138
batch: [8500/10547] batch time: 0.492 trainign loss: 3.5525 avg training loss: 6.9131
batch: [8510/10547] batch time: 0.054 trainign loss: 5.8596 avg training loss: 6.9128
batch: [8520/10547] batch time: 0.527 trainign loss: 3.9174 avg training loss: 6.9121
batch: [8530/10547] batch time: 0.054 trainign loss: 2.2933 avg training loss: 6.9112
batch: [8540/10547] batch time: 0.046 trainign loss: 7.0115 avg training loss: 6.9113
batch: [8550/10547] batch time: 0.055 trainign loss: 6.6336 avg training loss: 6.9110
batch: [8560/10547] batch time: 0.048 trainign loss: 0.7369 avg training loss: 6.9098
batch: [8570/10547] batch time: 0.046 trainign loss: 6.0947 avg training loss: 6.9090
batch: [8580/10547] batch time: 0.230 trainign loss: 5.9231 avg training loss: 6.9084
batch: [8590/10547] batch time: 0.054 trainign loss: 7.1920 avg training loss: 6.9082
batch: [8600/10547] batch time: 0.046 trainign loss: 6.5909 avg training loss: 6.9082
batch: [8610/10547] batch time: 0.046 trainign loss: 6.5820 avg training loss: 6.9079
batch: [8620/10547] batch time: 0.222 trainign loss: 5.1660 avg training loss: 6.9075
batch: [8630/10547] batch time: 0.054 trainign loss: 3.4368 avg training loss: 6.9068
batch: [8640/10547] batch time: 0.643 trainign loss: 6.8962 avg training loss: 6.9065
batch: [8650/10547] batch time: 0.050 trainign loss: 5.7240 avg training loss: 6.9062
batch: [8660/10547] batch time: 0.472 trainign loss: 5.9867 avg training loss: 6.9058
batch: [8670/10547] batch time: 0.852 trainign loss: 5.5573 avg training loss: 6.9056
batch: [8680/10547] batch time: 0.768 trainign loss: 4.4953 avg training loss: 6.9049
batch: [8690/10547] batch time: 1.099 trainign loss: 0.0101 avg training loss: 6.9030
batch: [8700/10547] batch time: 0.732 trainign loss: 7.1554 avg training loss: 6.9025
batch: [8710/10547] batch time: 0.537 trainign loss: 6.1709 avg training loss: 6.9025
batch: [8720/10547] batch time: 1.717 trainign loss: 7.1840 avg training loss: 6.9025
batch: [8730/10547] batch time: 0.999 trainign loss: 6.1365 avg training loss: 6.9023
batch: [8740/10547] batch time: 1.482 trainign loss: 5.2910 avg training loss: 6.9019
batch: [8750/10547] batch time: 1.049 trainign loss: 5.8860 avg training loss: 6.9015
batch: [8760/10547] batch time: 1.136 trainign loss: 6.8144 avg training loss: 6.9010
batch: [8770/10547] batch time: 1.598 trainign loss: 6.8754 avg training loss: 6.9008
batch: [8780/10547] batch time: 0.045 trainign loss: 5.7242 avg training loss: 6.9004
batch: [8790/10547] batch time: 2.196 trainign loss: 6.6127 avg training loss: 6.9002
batch: [8800/10547] batch time: 0.880 trainign loss: 5.8107 avg training loss: 6.9000
batch: [8810/10547] batch time: 2.316 trainign loss: 6.0597 avg training loss: 6.8996
batch: [8820/10547] batch time: 0.164 trainign loss: 6.1646 avg training loss: 6.8994
batch: [8830/10547] batch time: 2.030 trainign loss: 5.6797 avg training loss: 6.8990
batch: [8840/10547] batch time: 0.055 trainign loss: 5.3957 avg training loss: 6.8985
batch: [8850/10547] batch time: 1.631 trainign loss: 5.7844 avg training loss: 6.8982
batch: [8860/10547] batch time: 0.046 trainign loss: 4.8922 avg training loss: 6.8978
batch: [8870/10547] batch time: 2.431 trainign loss: 5.9358 avg training loss: 6.8972
batch: [8880/10547] batch time: 0.046 trainign loss: 5.0437 avg training loss: 6.8970
batch: [8890/10547] batch time: 1.667 trainign loss: 5.7563 avg training loss: 6.8964
batch: [8900/10547] batch time: 0.559 trainign loss: 3.5378 avg training loss: 6.8959
batch: [8910/10547] batch time: 1.816 trainign loss: 6.1815 avg training loss: 6.8951
batch: [8920/10547] batch time: 1.063 trainign loss: 6.2929 avg training loss: 6.8951
batch: [8930/10547] batch time: 0.155 trainign loss: 6.5411 avg training loss: 6.8948
batch: [8940/10547] batch time: 1.109 trainign loss: 5.5005 avg training loss: 6.8944
batch: [8950/10547] batch time: 0.936 trainign loss: 5.5474 avg training loss: 6.8941
batch: [8960/10547] batch time: 1.941 trainign loss: 5.0718 avg training loss: 6.8937
batch: [8970/10547] batch time: 0.802 trainign loss: 4.5878 avg training loss: 6.8932
batch: [8980/10547] batch time: 0.826 trainign loss: 5.1116 avg training loss: 6.8925
batch: [8990/10547] batch time: 1.540 trainign loss: 5.8045 avg training loss: 6.8921
batch: [9000/10547] batch time: 0.723 trainign loss: 0.5450 avg training loss: 6.8911
batch: [9010/10547] batch time: 2.076 trainign loss: 6.4199 avg training loss: 6.8907
batch: [9020/10547] batch time: 1.075 trainign loss: 4.7410 avg training loss: 6.8903
batch: [9030/10547] batch time: 0.750 trainign loss: 5.2578 avg training loss: 6.8901
batch: [9040/10547] batch time: 1.164 trainign loss: 5.6479 avg training loss: 6.8898
batch: [9050/10547] batch time: 2.002 trainign loss: 4.7521 avg training loss: 6.8891
batch: [9060/10547] batch time: 0.636 trainign loss: 5.7786 avg training loss: 6.8886
batch: [9070/10547] batch time: 1.941 trainign loss: 3.8476 avg training loss: 6.8882
batch: [9080/10547] batch time: 0.328 trainign loss: 6.4687 avg training loss: 6.8879
batch: [9090/10547] batch time: 2.494 trainign loss: 5.1541 avg training loss: 6.8877
batch: [9100/10547] batch time: 0.053 trainign loss: 3.9685 avg training loss: 6.8872
batch: [9110/10547] batch time: 2.580 trainign loss: 5.6017 avg training loss: 6.8869
batch: [9120/10547] batch time: 0.047 trainign loss: 6.4325 avg training loss: 6.8864
batch: [9130/10547] batch time: 2.230 trainign loss: 0.0576 avg training loss: 6.8849
batch: [9140/10547] batch time: 0.045 trainign loss: 7.2725 avg training loss: 6.8845
batch: [9150/10547] batch time: 2.182 trainign loss: 7.2172 avg training loss: 6.8847
batch: [9160/10547] batch time: 0.046 trainign loss: 2.3723 avg training loss: 6.8841
batch: [9170/10547] batch time: 2.767 trainign loss: 0.0028 avg training loss: 6.8819
batch: [9180/10547] batch time: 0.046 trainign loss: 7.9222 avg training loss: 6.8826
batch: [9190/10547] batch time: 2.615 trainign loss: 6.0275 avg training loss: 6.8825
batch: [9200/10547] batch time: 0.047 trainign loss: 6.0344 avg training loss: 6.8820
batch: [9210/10547] batch time: 2.336 trainign loss: 5.8271 avg training loss: 6.8816
batch: [9220/10547] batch time: 0.054 trainign loss: 4.4118 avg training loss: 6.8812
batch: [9230/10547] batch time: 2.264 trainign loss: 6.2236 avg training loss: 6.8807
batch: [9240/10547] batch time: 0.054 trainign loss: 5.7947 avg training loss: 6.8804
batch: [9250/10547] batch time: 2.255 trainign loss: 0.1144 avg training loss: 6.8790
batch: [9260/10547] batch time: 0.046 trainign loss: 11.8338 avg training loss: 6.8775
batch: [9270/10547] batch time: 2.427 trainign loss: 7.8595 avg training loss: 6.8780
batch: [9280/10547] batch time: 0.045 trainign loss: 7.4518 avg training loss: 6.8782
batch: [9290/10547] batch time: 2.337 trainign loss: 6.8062 avg training loss: 6.8781
batch: [9300/10547] batch time: 0.046 trainign loss: 5.3252 avg training loss: 6.8777
batch: [9310/10547] batch time: 2.551 trainign loss: 4.9049 avg training loss: 6.8770
batch: [9320/10547] batch time: 0.046 trainign loss: 6.4575 avg training loss: 6.8764
batch: [9330/10547] batch time: 2.428 trainign loss: 6.4293 avg training loss: 6.8761
batch: [9340/10547] batch time: 0.046 trainign loss: 3.1121 avg training loss: 6.8755
batch: [9350/10547] batch time: 1.999 trainign loss: 6.2740 avg training loss: 6.8750
batch: [9360/10547] batch time: 0.055 trainign loss: 7.8846 avg training loss: 6.8744
batch: [9370/10547] batch time: 1.788 trainign loss: 6.5822 avg training loss: 6.8744
batch: [9380/10547] batch time: 0.626 trainign loss: 5.4929 avg training loss: 6.8742
batch: [9390/10547] batch time: 0.968 trainign loss: 6.2329 avg training loss: 6.8737
batch: [9400/10547] batch time: 1.200 trainign loss: 1.9876 avg training loss: 6.8731
batch: [9410/10547] batch time: 0.368 trainign loss: 5.7169 avg training loss: 6.8728
batch: [9420/10547] batch time: 1.667 trainign loss: 6.6126 avg training loss: 6.8727
batch: [9430/10547] batch time: 0.047 trainign loss: 5.7372 avg training loss: 6.8724
batch: [9440/10547] batch time: 1.246 trainign loss: 5.0762 avg training loss: 6.8720
batch: [9450/10547] batch time: 0.200 trainign loss: 4.4683 avg training loss: 6.8713
batch: [9460/10547] batch time: 1.331 trainign loss: 6.8350 avg training loss: 6.8713
batch: [9470/10547] batch time: 0.084 trainign loss: 4.3902 avg training loss: 6.8710
batch: [9480/10547] batch time: 0.636 trainign loss: 4.3935 avg training loss: 6.8705
batch: [9490/10547] batch time: 0.405 trainign loss: 6.6208 avg training loss: 6.8702
batch: [9500/10547] batch time: 0.347 trainign loss: 5.4953 avg training loss: 6.8700
batch: [9510/10547] batch time: 1.181 trainign loss: 5.6995 avg training loss: 6.8694
batch: [9520/10547] batch time: 0.052 trainign loss: 4.8105 avg training loss: 6.8690
batch: [9530/10547] batch time: 1.157 trainign loss: 6.7424 avg training loss: 6.8685
batch: [9540/10547] batch time: 0.810 trainign loss: 4.1525 avg training loss: 6.8682
batch: [9550/10547] batch time: 0.135 trainign loss: 5.9583 avg training loss: 6.8675
batch: [9560/10547] batch time: 0.680 trainign loss: 7.1193 avg training loss: 6.8673
batch: [9570/10547] batch time: 0.054 trainign loss: 6.5069 avg training loss: 6.8672
batch: [9580/10547] batch time: 1.347 trainign loss: 5.9289 avg training loss: 6.8669
batch: [9590/10547] batch time: 0.957 trainign loss: 3.6372 avg training loss: 6.8664
batch: [9600/10547] batch time: 0.883 trainign loss: 5.4836 avg training loss: 6.8659
batch: [9610/10547] batch time: 1.597 trainign loss: 5.3402 avg training loss: 6.8656
batch: [9620/10547] batch time: 1.317 trainign loss: 5.5311 avg training loss: 6.8651
batch: [9630/10547] batch time: 0.929 trainign loss: 6.1760 avg training loss: 6.8650
batch: [9640/10547] batch time: 0.798 trainign loss: 6.0307 avg training loss: 6.8646
batch: [9650/10547] batch time: 1.094 trainign loss: 5.9103 avg training loss: 6.8643
batch: [9660/10547] batch time: 1.292 trainign loss: 5.6407 avg training loss: 6.8639
batch: [9670/10547] batch time: 0.960 trainign loss: 5.7044 avg training loss: 6.8633
batch: [9680/10547] batch time: 1.351 trainign loss: 5.3452 avg training loss: 6.8630
batch: [9690/10547] batch time: 0.356 trainign loss: 5.6469 avg training loss: 6.8625
batch: [9700/10547] batch time: 1.877 trainign loss: 5.5518 avg training loss: 6.8622
batch: [9710/10547] batch time: 0.046 trainign loss: 5.8759 avg training loss: 6.8620
batch: [9720/10547] batch time: 1.612 trainign loss: 5.7677 avg training loss: 6.8615
batch: [9730/10547] batch time: 0.046 trainign loss: 0.5424 avg training loss: 6.8605
batch: [9740/10547] batch time: 1.090 trainign loss: 6.5235 avg training loss: 6.8601
batch: [9750/10547] batch time: 0.047 trainign loss: 7.2953 avg training loss: 6.8603
batch: [9760/10547] batch time: 0.935 trainign loss: 6.7852 avg training loss: 6.8602
batch: [9770/10547] batch time: 0.054 trainign loss: 5.5947 avg training loss: 6.8599
batch: [9780/10547] batch time: 1.527 trainign loss: 6.0330 avg training loss: 6.8596
batch: [9790/10547] batch time: 0.054 trainign loss: 5.8832 avg training loss: 6.8592
batch: [9800/10547] batch time: 1.442 trainign loss: 5.9208 avg training loss: 6.8585
batch: [9810/10547] batch time: 0.047 trainign loss: 6.1783 avg training loss: 6.8583
batch: [9820/10547] batch time: 1.349 trainign loss: 6.6910 avg training loss: 6.8580
batch: [9830/10547] batch time: 0.046 trainign loss: 5.2463 avg training loss: 6.8576
batch: [9840/10547] batch time: 1.591 trainign loss: 0.5188 avg training loss: 6.8565
batch: [9850/10547] batch time: 0.046 trainign loss: 0.0391 avg training loss: 6.8551
batch: [9860/10547] batch time: 0.723 trainign loss: 6.8219 avg training loss: 6.8552
batch: [9870/10547] batch time: 0.047 trainign loss: 5.0507 avg training loss: 6.8549
batch: [9880/10547] batch time: 0.238 trainign loss: 5.5686 avg training loss: 6.8545
batch: [9890/10547] batch time: 0.046 trainign loss: 6.8483 avg training loss: 6.8543
batch: [9900/10547] batch time: 1.682 trainign loss: 6.0476 avg training loss: 6.8540
batch: [9910/10547] batch time: 0.053 trainign loss: 1.9528 avg training loss: 6.8534
batch: [9920/10547] batch time: 2.597 trainign loss: 4.6906 avg training loss: 6.8527
batch: [9930/10547] batch time: 0.046 trainign loss: 6.2599 avg training loss: 6.8521
batch: [9940/10547] batch time: 1.413 trainign loss: 7.3428 avg training loss: 6.8513
batch: [9950/10547] batch time: 0.047 trainign loss: 6.3246 avg training loss: 6.8509
batch: [9960/10547] batch time: 0.137 trainign loss: 5.0397 avg training loss: 6.8507
batch: [9970/10547] batch time: 0.046 trainign loss: 8.6457 avg training loss: 6.8497
batch: [9980/10547] batch time: 0.173 trainign loss: 7.1483 avg training loss: 6.8498
batch: [9990/10547] batch time: 0.047 trainign loss: 6.2642 avg training loss: 6.8496
batch: [10000/10547] batch time: 0.271 trainign loss: 6.4652 avg training loss: 6.8494
batch: [10010/10547] batch time: 0.046 trainign loss: 5.2101 avg training loss: 6.8490
batch: [10020/10547] batch time: 0.048 trainign loss: 6.6064 avg training loss: 6.8484
batch: [10030/10547] batch time: 0.048 trainign loss: 6.1583 avg training loss: 6.8482
batch: [10040/10547] batch time: 0.054 trainign loss: 3.5931 avg training loss: 6.8475
batch: [10050/10547] batch time: 0.046 trainign loss: 4.8748 avg training loss: 6.8472
batch: [10060/10547] batch time: 0.042 trainign loss: 8.1441 avg training loss: 6.8462
batch: [10070/10547] batch time: 0.046 trainign loss: 6.5959 avg training loss: 6.8459
batch: [10080/10547] batch time: 0.048 trainign loss: 4.8739 avg training loss: 6.8455
batch: [10090/10547] batch time: 0.046 trainign loss: 6.8083 avg training loss: 6.8450
batch: [10100/10547] batch time: 0.047 trainign loss: 4.5038 avg training loss: 6.8447
batch: [10110/10547] batch time: 0.048 trainign loss: 5.7950 avg training loss: 6.8441
batch: [10120/10547] batch time: 0.042 trainign loss: 5.1728 avg training loss: 6.8438
batch: [10130/10547] batch time: 0.046 trainign loss: 5.0227 avg training loss: 6.8431
batch: [10140/10547] batch time: 0.041 trainign loss: 3.8168 avg training loss: 6.8424
batch: [10150/10547] batch time: 0.046 trainign loss: 4.8010 avg training loss: 6.8423
batch: [10160/10547] batch time: 0.042 trainign loss: 0.0075 avg training loss: 6.8404
batch: [10170/10547] batch time: 0.046 trainign loss: 10.6873 avg training loss: 6.8399
batch: [10180/10547] batch time: 0.046 trainign loss: 7.7707 avg training loss: 6.8405
batch: [10190/10547] batch time: 0.048 trainign loss: 2.8379 avg training loss: 6.8401
batch: [10200/10547] batch time: 0.047 trainign loss: 6.2637 avg training loss: 6.8400
batch: [10210/10547] batch time: 0.048 trainign loss: 6.3310 avg training loss: 6.8396
batch: [10220/10547] batch time: 0.043 trainign loss: 5.8852 avg training loss: 6.8393
batch: [10230/10547] batch time: 0.046 trainign loss: 6.1345 avg training loss: 6.8391
batch: [10240/10547] batch time: 0.048 trainign loss: 5.6342 avg training loss: 6.8388
batch: [10250/10547] batch time: 0.046 trainign loss: 5.2391 avg training loss: 6.8383
batch: [10260/10547] batch time: 0.049 trainign loss: 5.7991 avg training loss: 6.8378
batch: [10270/10547] batch time: 0.049 trainign loss: 6.3769 avg training loss: 6.8376
batch: [10280/10547] batch time: 0.049 trainign loss: 6.7063 avg training loss: 6.8375
batch: [10290/10547] batch time: 0.047 trainign loss: 5.2289 avg training loss: 6.8372
batch: [10300/10547] batch time: 0.046 trainign loss: 6.0315 avg training loss: 6.8368
batch: [10310/10547] batch time: 0.048 trainign loss: 5.9911 avg training loss: 6.8364
batch: [10320/10547] batch time: 0.046 trainign loss: 6.7876 avg training loss: 6.8362
batch: [10330/10547] batch time: 0.046 trainign loss: 4.9374 avg training loss: 6.8357
batch: [10340/10547] batch time: 0.046 trainign loss: 5.8942 avg training loss: 6.8354
batch: [10350/10547] batch time: 0.046 trainign loss: 6.1437 avg training loss: 6.8351
batch: [10360/10547] batch time: 0.054 trainign loss: 5.5554 avg training loss: 6.8344
batch: [10370/10547] batch time: 0.047 trainign loss: 0.7806 avg training loss: 6.8335
batch: [10380/10547] batch time: 0.046 trainign loss: 3.9022 avg training loss: 6.8326
batch: [10390/10547] batch time: 0.054 trainign loss: 6.6169 avg training loss: 6.8323
batch: [10400/10547] batch time: 0.046 trainign loss: 6.3230 avg training loss: 6.8321
batch: [10410/10547] batch time: 0.044 trainign loss: 6.1322 avg training loss: 6.8318
batch: [10420/10547] batch time: 0.046 trainign loss: 6.3042 avg training loss: 6.8314
batch: [10430/10547] batch time: 0.046 trainign loss: 5.8759 avg training loss: 6.8312
batch: [10440/10547] batch time: 0.046 trainign loss: 4.6787 avg training loss: 6.8306
batch: [10450/10547] batch time: 0.053 trainign loss: 4.6564 avg training loss: 6.8299
batch: [10460/10547] batch time: 0.046 trainign loss: 6.2167 avg training loss: 6.8295
batch: [10470/10547] batch time: 0.483 trainign loss: 5.6614 avg training loss: 6.8293
batch: [10480/10547] batch time: 0.048 trainign loss: 3.1749 avg training loss: 6.8287
batch: [10490/10547] batch time: 0.046 trainign loss: 6.3728 avg training loss: 6.8283
batch: [10500/10547] batch time: 0.049 trainign loss: 6.2882 avg training loss: 6.8281
batch: [10510/10547] batch time: 0.050 trainign loss: 6.1926 avg training loss: 6.8279
batch: [10520/10547] batch time: 0.120 trainign loss: 5.3460 avg training loss: 6.8275
batch: [10530/10547] batch time: 0.046 trainign loss: 5.7781 avg training loss: 6.8270
batch: [10540/10547] batch time: 0.459 trainign loss: 4.9930 avg training loss: 6.8265
Epoch: 4
----------------------------------------------------------------------
batch: [0/10547] batch time: 3.067 trainign loss: 5.8438 avg training loss: 6.8262
batch: [10/10547] batch time: 0.047 trainign loss: 7.9903 avg training loss: 6.8255
batch: [20/10547] batch time: 2.611 trainign loss: 6.8581 avg training loss: 6.8254
batch: [30/10547] batch time: 0.046 trainign loss: 5.9774 avg training loss: 6.8251
batch: [40/10547] batch time: 2.580 trainign loss: 6.5024 avg training loss: 6.8249
batch: [50/10547] batch time: 0.046 trainign loss: 6.5643 avg training loss: 6.8246
batch: [60/10547] batch time: 2.265 trainign loss: 5.2676 avg training loss: 6.8242
batch: [70/10547] batch time: 0.050 trainign loss: 5.7206 avg training loss: 6.8237
batch: [80/10547] batch time: 2.362 trainign loss: 6.4975 avg training loss: 6.8231
batch: [90/10547] batch time: 0.046 trainign loss: 5.7660 avg training loss: 6.8229
batch: [100/10547] batch time: 2.238 trainign loss: 2.7302 avg training loss: 6.8223
batch: [110/10547] batch time: 0.046 trainign loss: 0.0027 avg training loss: 6.8203
batch: [120/10547] batch time: 2.579 trainign loss: 7.5058 avg training loss: 6.8207
batch: [130/10547] batch time: 0.046 trainign loss: 6.8343 avg training loss: 6.8208
batch: [140/10547] batch time: 2.149 trainign loss: 5.8185 avg training loss: 6.8206
batch: [150/10547] batch time: 0.046 trainign loss: 4.6180 avg training loss: 6.8202
batch: [160/10547] batch time: 2.441 trainign loss: 5.5100 avg training loss: 6.8195
batch: [170/10547] batch time: 0.054 trainign loss: 5.9894 avg training loss: 6.8192
batch: [180/10547] batch time: 1.362 trainign loss: 6.1994 avg training loss: 6.8190
batch: [190/10547] batch time: 0.047 trainign loss: 6.0098 avg training loss: 6.8186
batch: [200/10547] batch time: 1.272 trainign loss: 5.0397 avg training loss: 6.8182
batch: [210/10547] batch time: 0.046 trainign loss: 5.8077 avg training loss: 6.8174
batch: [220/10547] batch time: 1.469 trainign loss: 6.9690 avg training loss: 6.8174
batch: [230/10547] batch time: 0.047 trainign loss: 5.9924 avg training loss: 6.8172
batch: [240/10547] batch time: 1.003 trainign loss: 5.9454 avg training loss: 6.8166
batch: [250/10547] batch time: 0.047 trainign loss: 6.4833 avg training loss: 6.8163
batch: [260/10547] batch time: 1.292 trainign loss: 6.0016 avg training loss: 6.8162
batch: [270/10547] batch time: 0.046 trainign loss: 4.8945 avg training loss: 6.8156
batch: [280/10547] batch time: 1.199 trainign loss: 5.0841 avg training loss: 6.8152
batch: [290/10547] batch time: 0.047 trainign loss: 4.8646 avg training loss: 6.8147
batch: [300/10547] batch time: 0.710 trainign loss: 5.5865 avg training loss: 6.8143
batch: [310/10547] batch time: 0.453 trainign loss: 5.4389 avg training loss: 6.8138
batch: [320/10547] batch time: 0.653 trainign loss: 6.7438 avg training loss: 6.8136
batch: [330/10547] batch time: 1.089 trainign loss: 5.8770 avg training loss: 6.8135
batch: [340/10547] batch time: 0.473 trainign loss: 5.4982 avg training loss: 6.8130
batch: [350/10547] batch time: 0.779 trainign loss: 6.8205 avg training loss: 6.8122
batch: [360/10547] batch time: 0.607 trainign loss: 6.8848 avg training loss: 6.8121
batch: [370/10547] batch time: 0.715 trainign loss: 6.5662 avg training loss: 6.8119
batch: [380/10547] batch time: 0.561 trainign loss: 6.2748 avg training loss: 6.8118
batch: [390/10547] batch time: 0.318 trainign loss: 6.2920 avg training loss: 6.8116
batch: [400/10547] batch time: 0.048 trainign loss: 5.9287 avg training loss: 6.8112
batch: [410/10547] batch time: 0.167 trainign loss: 5.3742 avg training loss: 6.8109
batch: [420/10547] batch time: 0.049 trainign loss: 6.0013 avg training loss: 6.8105
batch: [430/10547] batch time: 0.054 trainign loss: 6.1870 avg training loss: 6.8102
batch: [440/10547] batch time: 0.055 trainign loss: 3.6832 avg training loss: 6.8097
batch: [450/10547] batch time: 0.048 trainign loss: 5.8160 avg training loss: 6.8092
batch: [460/10547] batch time: 0.054 trainign loss: 5.7011 avg training loss: 6.8089
batch: [470/10547] batch time: 0.047 trainign loss: 0.9489 avg training loss: 6.8080
batch: [480/10547] batch time: 0.046 trainign loss: 10.5002 avg training loss: 6.8065
batch: [490/10547] batch time: 0.046 trainign loss: 7.2095 avg training loss: 6.8070
batch: [500/10547] batch time: 0.046 trainign loss: 7.0130 avg training loss: 6.8067
batch: [510/10547] batch time: 0.046 trainign loss: 6.1785 avg training loss: 6.8067
batch: [520/10547] batch time: 0.045 trainign loss: 5.8530 avg training loss: 6.8064
batch: [530/10547] batch time: 0.046 trainign loss: 0.4695 avg training loss: 6.8053
batch: [540/10547] batch time: 0.046 trainign loss: 7.6080 avg training loss: 6.8047
batch: [550/10547] batch time: 0.046 trainign loss: 6.9373 avg training loss: 6.8046
batch: [560/10547] batch time: 0.202 trainign loss: 6.8420 avg training loss: 6.8045
batch: [570/10547] batch time: 0.051 trainign loss: 5.8689 avg training loss: 6.8043
batch: [580/10547] batch time: 0.743 trainign loss: 5.6902 avg training loss: 6.8041
batch: [590/10547] batch time: 0.047 trainign loss: 6.3808 avg training loss: 6.8037
batch: [600/10547] batch time: 0.899 trainign loss: 4.8104 avg training loss: 6.8033
batch: [610/10547] batch time: 0.046 trainign loss: 6.1309 avg training loss: 6.8030
batch: [620/10547] batch time: 1.330 trainign loss: 5.5482 avg training loss: 6.8029
batch: [630/10547] batch time: 0.046 trainign loss: 5.7260 avg training loss: 6.8025
batch: [640/10547] batch time: 1.679 trainign loss: 3.4296 avg training loss: 6.8021
batch: [650/10547] batch time: 0.047 trainign loss: 5.1767 avg training loss: 6.8015
batch: [660/10547] batch time: 1.705 trainign loss: 5.4118 avg training loss: 6.8012
batch: [670/10547] batch time: 0.046 trainign loss: 4.9446 avg training loss: 6.8005
batch: [680/10547] batch time: 1.521 trainign loss: 6.2139 avg training loss: 6.8002
batch: [690/10547] batch time: 0.049 trainign loss: 5.7320 avg training loss: 6.8000
batch: [700/10547] batch time: 1.616 trainign loss: 4.9976 avg training loss: 6.7996
batch: [710/10547] batch time: 0.049 trainign loss: 5.9925 avg training loss: 6.7992
batch: [720/10547] batch time: 1.050 trainign loss: 3.8260 avg training loss: 6.7987
batch: [730/10547] batch time: 0.046 trainign loss: 8.9370 avg training loss: 6.7971
batch: [740/10547] batch time: 1.296 trainign loss: 4.2572 avg training loss: 6.7972
batch: [750/10547] batch time: 0.046 trainign loss: 4.9538 avg training loss: 6.7970
batch: [760/10547] batch time: 1.274 trainign loss: 1.8309 avg training loss: 6.7960
batch: [770/10547] batch time: 0.051 trainign loss: 6.8350 avg training loss: 6.7961
batch: [780/10547] batch time: 0.994 trainign loss: 6.9688 avg training loss: 6.7961
batch: [790/10547] batch time: 0.054 trainign loss: 6.1410 avg training loss: 6.7960
batch: [800/10547] batch time: 0.974 trainign loss: 6.0729 avg training loss: 6.7958
batch: [810/10547] batch time: 0.047 trainign loss: 4.2018 avg training loss: 6.7954
batch: [820/10547] batch time: 0.357 trainign loss: 6.0463 avg training loss: 6.7950
batch: [830/10547] batch time: 0.048 trainign loss: 6.8763 avg training loss: 6.7949
batch: [840/10547] batch time: 1.156 trainign loss: 5.4863 avg training loss: 6.7947
batch: [850/10547] batch time: 0.047 trainign loss: 5.8335 avg training loss: 6.7945
batch: [860/10547] batch time: 0.042 trainign loss: 5.5733 avg training loss: 6.7939
batch: [870/10547] batch time: 0.054 trainign loss: 6.4785 avg training loss: 6.7937
batch: [880/10547] batch time: 0.046 trainign loss: 5.0803 avg training loss: 6.7935
batch: [890/10547] batch time: 0.046 trainign loss: 4.7314 avg training loss: 6.7926
batch: [900/10547] batch time: 0.046 trainign loss: 5.6408 avg training loss: 6.7923
batch: [910/10547] batch time: 0.046 trainign loss: 5.8501 avg training loss: 6.7922
batch: [920/10547] batch time: 0.053 trainign loss: 5.9455 avg training loss: 6.7918
batch: [930/10547] batch time: 0.048 trainign loss: 4.4397 avg training loss: 6.7914
batch: [940/10547] batch time: 0.046 trainign loss: 5.9328 avg training loss: 6.7909
batch: [950/10547] batch time: 0.046 trainign loss: 5.1372 avg training loss: 6.7906
batch: [960/10547] batch time: 0.046 trainign loss: 0.4816 avg training loss: 6.7895
batch: [970/10547] batch time: 0.049 trainign loss: 6.6828 avg training loss: 6.7894
batch: [980/10547] batch time: 0.047 trainign loss: 4.7602 avg training loss: 6.7888
batch: [990/10547] batch time: 0.049 trainign loss: 0.8086 avg training loss: 6.7876
batch: [1000/10547] batch time: 0.050 trainign loss: 9.3073 avg training loss: 6.7860
batch: [1010/10547] batch time: 0.046 trainign loss: 7.0009 avg training loss: 6.7863
batch: [1020/10547] batch time: 0.050 trainign loss: 0.0250 avg training loss: 6.7849
batch: [1030/10547] batch time: 0.053 trainign loss: 0.0008 avg training loss: 6.7828
batch: [1040/10547] batch time: 0.046 trainign loss: 12.7414 avg training loss: 6.7821
batch: [1050/10547] batch time: 0.046 trainign loss: 7.6961 avg training loss: 6.7825
batch: [1060/10547] batch time: 0.043 trainign loss: 6.3938 avg training loss: 6.7825
batch: [1070/10547] batch time: 0.046 trainign loss: 6.6649 avg training loss: 6.7823
batch: [1080/10547] batch time: 0.049 trainign loss: 3.3232 avg training loss: 6.7817
batch: [1090/10547] batch time: 0.054 trainign loss: 5.8156 avg training loss: 6.7815
batch: [1100/10547] batch time: 0.046 trainign loss: 4.2310 avg training loss: 6.7809
batch: [1110/10547] batch time: 0.046 trainign loss: 5.6682 avg training loss: 6.7802
batch: [1120/10547] batch time: 0.046 trainign loss: 5.6814 avg training loss: 6.7794
batch: [1130/10547] batch time: 0.046 trainign loss: 6.0476 avg training loss: 6.7788
batch: [1140/10547] batch time: 0.047 trainign loss: 6.3802 avg training loss: 6.7786
batch: [1150/10547] batch time: 0.047 trainign loss: 5.4788 avg training loss: 6.7784
batch: [1160/10547] batch time: 0.049 trainign loss: 4.5948 avg training loss: 6.7779
batch: [1170/10547] batch time: 0.046 trainign loss: 6.1123 avg training loss: 6.7775
batch: [1180/10547] batch time: 0.046 trainign loss: 5.9772 avg training loss: 6.7773
batch: [1190/10547] batch time: 0.046 trainign loss: 5.7334 avg training loss: 6.7771
batch: [1200/10547] batch time: 0.048 trainign loss: 4.0150 avg training loss: 6.7766
batch: [1210/10547] batch time: 0.047 trainign loss: 4.8414 avg training loss: 6.7762
batch: [1220/10547] batch time: 0.047 trainign loss: 5.6120 avg training loss: 6.7758
batch: [1230/10547] batch time: 0.046 trainign loss: 5.5555 avg training loss: 6.7754
batch: [1240/10547] batch time: 0.046 trainign loss: 5.6005 avg training loss: 6.7751
batch: [1250/10547] batch time: 0.337 trainign loss: 5.2374 avg training loss: 6.7746
batch: [1260/10547] batch time: 0.049 trainign loss: 0.1715 avg training loss: 6.7734
batch: [1270/10547] batch time: 0.843 trainign loss: 6.6811 avg training loss: 6.7732
batch: [1280/10547] batch time: 0.049 trainign loss: 3.4676 avg training loss: 6.7730
batch: [1290/10547] batch time: 0.505 trainign loss: 6.6286 avg training loss: 6.7729
batch: [1300/10547] batch time: 0.048 trainign loss: 5.6262 avg training loss: 6.7726
batch: [1310/10547] batch time: 0.805 trainign loss: 4.7482 avg training loss: 6.7722
batch: [1320/10547] batch time: 0.046 trainign loss: 3.6766 avg training loss: 6.7716
batch: [1330/10547] batch time: 0.394 trainign loss: 6.6112 avg training loss: 6.7711
batch: [1340/10547] batch time: 0.046 trainign loss: 6.4905 avg training loss: 6.7710
batch: [1350/10547] batch time: 1.810 trainign loss: 5.8062 avg training loss: 6.7707
batch: [1360/10547] batch time: 0.047 trainign loss: 6.2015 avg training loss: 6.7705
batch: [1370/10547] batch time: 1.101 trainign loss: 4.9088 avg training loss: 6.7703
batch: [1380/10547] batch time: 0.048 trainign loss: 5.5937 avg training loss: 6.7698
batch: [1390/10547] batch time: 0.432 trainign loss: 7.5250 avg training loss: 6.7689
batch: [1400/10547] batch time: 0.607 trainign loss: 6.9191 avg training loss: 6.7689
batch: [1410/10547] batch time: 0.867 trainign loss: 5.9037 avg training loss: 6.7685
batch: [1420/10547] batch time: 0.047 trainign loss: 5.6514 avg training loss: 6.7683
batch: [1430/10547] batch time: 0.441 trainign loss: 5.9897 avg training loss: 6.7680
batch: [1440/10547] batch time: 0.046 trainign loss: 5.7394 avg training loss: 6.7677
batch: [1450/10547] batch time: 0.054 trainign loss: 2.3687 avg training loss: 6.7670
batch: [1460/10547] batch time: 0.170 trainign loss: 3.4859 avg training loss: 6.7662
batch: [1470/10547] batch time: 0.049 trainign loss: 6.2623 avg training loss: 6.7661
batch: [1480/10547] batch time: 0.047 trainign loss: 6.0482 avg training loss: 6.7656
batch: [1490/10547] batch time: 0.046 trainign loss: 6.2426 avg training loss: 6.7654
batch: [1500/10547] batch time: 0.046 trainign loss: 6.5958 avg training loss: 6.7654
batch: [1510/10547] batch time: 0.048 trainign loss: 5.8410 avg training loss: 6.7651
batch: [1520/10547] batch time: 0.041 trainign loss: 5.5631 avg training loss: 6.7648
batch: [1530/10547] batch time: 0.046 trainign loss: 5.4528 avg training loss: 6.7645
batch: [1540/10547] batch time: 0.050 trainign loss: 1.0039 avg training loss: 6.7636
batch: [1550/10547] batch time: 0.053 trainign loss: 5.0416 avg training loss: 6.7634
batch: [1560/10547] batch time: 0.048 trainign loss: 6.8733 avg training loss: 6.7631
batch: [1570/10547] batch time: 0.050 trainign loss: 6.0777 avg training loss: 6.7630
batch: [1580/10547] batch time: 0.046 trainign loss: 6.1971 avg training loss: 6.7626
batch: [1590/10547] batch time: 0.046 trainign loss: 3.0064 avg training loss: 6.7620
batch: [1600/10547] batch time: 0.048 trainign loss: 5.8027 avg training loss: 6.7617
batch: [1610/10547] batch time: 0.046 trainign loss: 1.9003 avg training loss: 6.7608
batch: [1620/10547] batch time: 0.053 trainign loss: 5.5600 avg training loss: 6.7605
batch: [1630/10547] batch time: 0.047 trainign loss: 2.3333 avg training loss: 6.7599
batch: [1640/10547] batch time: 0.047 trainign loss: 4.8872 avg training loss: 6.7594
batch: [1650/10547] batch time: 0.046 trainign loss: 6.0056 avg training loss: 6.7592
batch: [1660/10547] batch time: 0.046 trainign loss: 4.9190 avg training loss: 6.7588
batch: [1670/10547] batch time: 0.055 trainign loss: 2.6007 avg training loss: 6.7580
batch: [1680/10547] batch time: 0.046 trainign loss: 5.7020 avg training loss: 6.7575
batch: [1690/10547] batch time: 0.047 trainign loss: 5.9888 avg training loss: 6.7573
batch: [1700/10547] batch time: 0.043 trainign loss: 5.8821 avg training loss: 6.7567
batch: [1710/10547] batch time: 0.046 trainign loss: 5.9561 avg training loss: 6.7564
batch: [1720/10547] batch time: 0.048 trainign loss: 5.5108 avg training loss: 6.7560
batch: [1730/10547] batch time: 0.054 trainign loss: 6.2755 avg training loss: 6.7556
batch: [1740/10547] batch time: 0.047 trainign loss: 6.3866 avg training loss: 6.7555
batch: [1750/10547] batch time: 0.047 trainign loss: 5.1253 avg training loss: 6.7551
batch: [1760/10547] batch time: 0.045 trainign loss: 2.1441 avg training loss: 6.7542
batch: [1770/10547] batch time: 0.046 trainign loss: 6.6159 avg training loss: 6.7541
batch: [1780/10547] batch time: 0.046 trainign loss: 5.2613 avg training loss: 6.7537
batch: [1790/10547] batch time: 0.046 trainign loss: 4.3484 avg training loss: 6.7532
batch: [1800/10547] batch time: 0.046 trainign loss: 4.9210 avg training loss: 6.7527
batch: [1810/10547] batch time: 0.051 trainign loss: 6.1770 avg training loss: 6.7524
batch: [1820/10547] batch time: 0.047 trainign loss: 5.7340 avg training loss: 6.7520
batch: [1830/10547] batch time: 0.054 trainign loss: 4.3752 avg training loss: 6.7515
batch: [1840/10547] batch time: 0.044 trainign loss: 6.2840 avg training loss: 6.7512
batch: [1850/10547] batch time: 0.053 trainign loss: 4.9891 avg training loss: 6.7506
batch: [1860/10547] batch time: 0.048 trainign loss: 4.1531 avg training loss: 6.7501
batch: [1870/10547] batch time: 0.047 trainign loss: 6.4681 avg training loss: 6.7498
batch: [1880/10547] batch time: 0.054 trainign loss: 5.3805 avg training loss: 6.7496
batch: [1890/10547] batch time: 0.049 trainign loss: 5.5334 avg training loss: 6.7492
batch: [1900/10547] batch time: 0.046 trainign loss: 3.1118 avg training loss: 6.7486
batch: [1910/10547] batch time: 0.047 trainign loss: 1.4177 avg training loss: 6.7468
batch: [1920/10547] batch time: 0.054 trainign loss: 7.0126 avg training loss: 6.7471
batch: [1930/10547] batch time: 0.049 trainign loss: 6.3463 avg training loss: 6.7472
batch: [1940/10547] batch time: 0.046 trainign loss: 6.1941 avg training loss: 6.7470
batch: [1950/10547] batch time: 0.047 trainign loss: 5.7886 avg training loss: 6.7468
batch: [1960/10547] batch time: 0.053 trainign loss: 5.3279 avg training loss: 6.7461
batch: [1970/10547] batch time: 0.046 trainign loss: 5.6824 avg training loss: 6.7458
batch: [1980/10547] batch time: 0.049 trainign loss: 5.0983 avg training loss: 6.7455
batch: [1990/10547] batch time: 0.048 trainign loss: 0.1190 avg training loss: 6.7442
batch: [2000/10547] batch time: 0.041 trainign loss: 8.9507 avg training loss: 6.7436
batch: [2010/10547] batch time: 0.046 trainign loss: 6.6627 avg training loss: 6.7437
batch: [2020/10547] batch time: 0.047 trainign loss: 5.8665 avg training loss: 6.7434
batch: [2030/10547] batch time: 0.048 trainign loss: 4.6927 avg training loss: 6.7431
batch: [2040/10547] batch time: 0.046 trainign loss: 5.0889 avg training loss: 6.7423
batch: [2050/10547] batch time: 0.046 trainign loss: 5.7787 avg training loss: 6.7421
batch: [2060/10547] batch time: 0.046 trainign loss: 3.1953 avg training loss: 6.7417
batch: [2070/10547] batch time: 0.048 trainign loss: 3.4582 avg training loss: 6.7412
batch: [2080/10547] batch time: 0.045 trainign loss: 5.0093 avg training loss: 6.7405
batch: [2090/10547] batch time: 0.046 trainign loss: 5.2775 avg training loss: 6.7400
batch: [2100/10547] batch time: 0.054 trainign loss: 0.0508 avg training loss: 6.7386
batch: [2110/10547] batch time: 0.046 trainign loss: 7.0103 avg training loss: 6.7387
batch: [2120/10547] batch time: 0.047 trainign loss: 4.9786 avg training loss: 6.7387
batch: [2130/10547] batch time: 0.055 trainign loss: 5.7180 avg training loss: 6.7385
batch: [2140/10547] batch time: 0.050 trainign loss: 5.4727 avg training loss: 6.7381
batch: [2150/10547] batch time: 0.048 trainign loss: 4.1168 avg training loss: 6.7377
batch: [2160/10547] batch time: 0.047 trainign loss: 6.0602 avg training loss: 6.7369
batch: [2170/10547] batch time: 0.049 trainign loss: 6.4360 avg training loss: 6.7366
batch: [2180/10547] batch time: 0.575 trainign loss: 5.6113 avg training loss: 6.7364
batch: [2190/10547] batch time: 0.055 trainign loss: 0.1067 avg training loss: 6.7352
batch: [2200/10547] batch time: 0.050 trainign loss: 5.8489 avg training loss: 6.7347
batch: [2210/10547] batch time: 0.046 trainign loss: 4.0131 avg training loss: 6.7345
batch: [2220/10547] batch time: 0.169 trainign loss: 7.3390 avg training loss: 6.7339
batch: [2230/10547] batch time: 0.046 trainign loss: 4.5898 avg training loss: 6.7335
batch: [2240/10547] batch time: 0.046 trainign loss: 5.9641 avg training loss: 6.7330
batch: [2250/10547] batch time: 0.048 trainign loss: 6.9455 avg training loss: 6.7330
batch: [2260/10547] batch time: 0.380 trainign loss: 5.6042 avg training loss: 6.7328
batch: [2270/10547] batch time: 0.287 trainign loss: 5.6817 avg training loss: 6.7323
batch: [2280/10547] batch time: 0.752 trainign loss: 3.9746 avg training loss: 6.7320
batch: [2290/10547] batch time: 0.352 trainign loss: 6.7458 avg training loss: 6.7316
batch: [2300/10547] batch time: 0.966 trainign loss: 5.1780 avg training loss: 6.7313
batch: [2310/10547] batch time: 0.777 trainign loss: 5.9087 avg training loss: 6.7308
batch: [2320/10547] batch time: 0.047 trainign loss: 6.6992 avg training loss: 6.7304
batch: [2330/10547] batch time: 1.363 trainign loss: 5.6981 avg training loss: 6.7300
batch: [2340/10547] batch time: 0.956 trainign loss: 6.0766 avg training loss: 6.7298
batch: [2350/10547] batch time: 0.908 trainign loss: 5.0519 avg training loss: 6.7294
batch: [2360/10547] batch time: 0.318 trainign loss: 5.8131 avg training loss: 6.7286
batch: [2370/10547] batch time: 1.776 trainign loss: 5.7196 avg training loss: 6.7285
batch: [2380/10547] batch time: 0.700 trainign loss: 2.4811 avg training loss: 6.7279
batch: [2390/10547] batch time: 1.344 trainign loss: 11.0040 avg training loss: 6.7266
batch: [2400/10547] batch time: 1.718 trainign loss: 6.8332 avg training loss: 6.7267
batch: [2410/10547] batch time: 0.325 trainign loss: 4.7969 avg training loss: 6.7264
batch: [2420/10547] batch time: 1.542 trainign loss: 6.3674 avg training loss: 6.7260
batch: [2430/10547] batch time: 0.048 trainign loss: 5.5675 avg training loss: 6.7255
batch: [2440/10547] batch time: 1.334 trainign loss: 5.1720 avg training loss: 6.7251
batch: [2450/10547] batch time: 0.047 trainign loss: 5.4718 avg training loss: 6.7246
batch: [2460/10547] batch time: 0.317 trainign loss: 0.6888 avg training loss: 6.7237
batch: [2470/10547] batch time: 0.046 trainign loss: 9.1952 avg training loss: 6.7226
batch: [2480/10547] batch time: 0.042 trainign loss: 4.6942 avg training loss: 6.7223
batch: [2490/10547] batch time: 0.047 trainign loss: 6.7192 avg training loss: 6.7218
batch: [2500/10547] batch time: 0.114 trainign loss: 5.8229 avg training loss: 6.7215
batch: [2510/10547] batch time: 0.054 trainign loss: 0.5680 avg training loss: 6.7206
batch: [2520/10547] batch time: 0.046 trainign loss: 6.8171 avg training loss: 6.7202
batch: [2530/10547] batch time: 0.046 trainign loss: 6.1317 avg training loss: 6.7201
batch: [2540/10547] batch time: 0.047 trainign loss: 6.6925 avg training loss: 6.7201
batch: [2550/10547] batch time: 0.055 trainign loss: 5.6340 avg training loss: 6.7196
batch: [2560/10547] batch time: 0.042 trainign loss: 5.8855 avg training loss: 6.7193
batch: [2570/10547] batch time: 0.046 trainign loss: 5.3341 avg training loss: 6.7191
batch: [2580/10547] batch time: 0.047 trainign loss: 5.1952 avg training loss: 6.7188
batch: [2590/10547] batch time: 0.046 trainign loss: 6.1871 avg training loss: 6.7184
batch: [2600/10547] batch time: 0.042 trainign loss: 2.9902 avg training loss: 6.7181
batch: [2610/10547] batch time: 0.054 trainign loss: 6.4360 avg training loss: 6.7178
batch: [2620/10547] batch time: 0.046 trainign loss: 5.9965 avg training loss: 6.7176
batch: [2630/10547] batch time: 0.047 trainign loss: 6.2905 avg training loss: 6.7173
batch: [2640/10547] batch time: 0.042 trainign loss: 3.2148 avg training loss: 6.7169
batch: [2650/10547] batch time: 0.048 trainign loss: 5.3135 avg training loss: 6.7164
batch: [2660/10547] batch time: 0.046 trainign loss: 6.9464 avg training loss: 6.7160
batch: [2670/10547] batch time: 0.054 trainign loss: 5.6720 avg training loss: 6.7158
batch: [2680/10547] batch time: 0.046 trainign loss: 5.4517 avg training loss: 6.7155
batch: [2690/10547] batch time: 0.049 trainign loss: 4.6697 avg training loss: 6.7151
batch: [2700/10547] batch time: 0.046 trainign loss: 5.9448 avg training loss: 6.7145
batch: [2710/10547] batch time: 0.047 trainign loss: 5.7762 avg training loss: 6.7142
batch: [2720/10547] batch time: 0.045 trainign loss: 5.9180 avg training loss: 6.7140
batch: [2730/10547] batch time: 0.047 trainign loss: 6.5276 avg training loss: 6.7139
batch: [2740/10547] batch time: 0.047 trainign loss: 6.3755 avg training loss: 6.7132
batch: [2750/10547] batch time: 0.053 trainign loss: 5.7080 avg training loss: 6.7130
batch: [2760/10547] batch time: 0.044 trainign loss: 5.2297 avg training loss: 6.7127
batch: [2770/10547] batch time: 0.046 trainign loss: 0.2946 avg training loss: 6.7115
batch: [2780/10547] batch time: 0.047 trainign loss: 7.2568 avg training loss: 6.7117
batch: [2790/10547] batch time: 0.046 trainign loss: 6.1989 avg training loss: 6.7117
batch: [2800/10547] batch time: 0.046 trainign loss: 5.5171 avg training loss: 6.7114
batch: [2810/10547] batch time: 0.046 trainign loss: 4.9353 avg training loss: 6.7109
batch: [2820/10547] batch time: 0.047 trainign loss: 6.3897 avg training loss: 6.7106
batch: [2830/10547] batch time: 0.046 trainign loss: 5.3397 avg training loss: 6.7104
batch: [2840/10547] batch time: 0.046 trainign loss: 4.9072 avg training loss: 6.7098
batch: [2850/10547] batch time: 0.046 trainign loss: 6.6824 avg training loss: 6.7093
batch: [2860/10547] batch time: 0.053 trainign loss: 6.2746 avg training loss: 6.7092
batch: [2870/10547] batch time: 0.047 trainign loss: 6.2744 avg training loss: 6.7091
batch: [2880/10547] batch time: 0.047 trainign loss: 5.9086 avg training loss: 6.7088
batch: [2890/10547] batch time: 0.053 trainign loss: 5.5067 avg training loss: 6.7085
batch: [2900/10547] batch time: 0.047 trainign loss: 5.1972 avg training loss: 6.7081
batch: [2910/10547] batch time: 0.046 trainign loss: 5.7315 avg training loss: 6.7077
batch: [2920/10547] batch time: 0.046 trainign loss: 4.2140 avg training loss: 6.7074
batch: [2930/10547] batch time: 0.046 trainign loss: 3.7363 avg training loss: 6.7067
batch: [2940/10547] batch time: 0.046 trainign loss: 6.4839 avg training loss: 6.7062
batch: [2950/10547] batch time: 0.049 trainign loss: 4.1124 avg training loss: 6.7058
batch: [2960/10547] batch time: 0.055 trainign loss: 6.2253 avg training loss: 6.7053
batch: [2970/10547] batch time: 0.072 trainign loss: 5.9388 avg training loss: 6.7052
batch: [2980/10547] batch time: 1.016 trainign loss: 3.3433 avg training loss: 6.7046
batch: [2990/10547] batch time: 1.387 trainign loss: 6.3217 avg training loss: 6.7040
batch: [3000/10547] batch time: 0.278 trainign loss: 5.9745 avg training loss: 6.7038
batch: [3010/10547] batch time: 1.444 trainign loss: 4.5752 avg training loss: 6.7032
batch: [3020/10547] batch time: 0.205 trainign loss: 5.8967 avg training loss: 6.7026
batch: [3030/10547] batch time: 1.368 trainign loss: 6.6495 avg training loss: 6.7025
batch: [3040/10547] batch time: 1.149 trainign loss: 3.8532 avg training loss: 6.7021
batch: [3050/10547] batch time: 1.031 trainign loss: 5.3777 avg training loss: 6.7015
batch: [3060/10547] batch time: 0.320 trainign loss: 5.8512 avg training loss: 6.7012
batch: [3070/10547] batch time: 1.388 trainign loss: 6.8223 avg training loss: 6.7007
batch: [3080/10547] batch time: 0.045 trainign loss: 6.6800 avg training loss: 6.7006
batch: [3090/10547] batch time: 1.208 trainign loss: 5.1706 avg training loss: 6.7004
batch: [3100/10547] batch time: 0.046 trainign loss: 5.3436 avg training loss: 6.6999
batch: [3110/10547] batch time: 1.522 trainign loss: 5.3770 avg training loss: 6.6996
batch: [3120/10547] batch time: 0.046 trainign loss: 6.5425 avg training loss: 6.6992
batch: [3130/10547] batch time: 1.206 trainign loss: 5.4363 avg training loss: 6.6989
batch: [3140/10547] batch time: 0.049 trainign loss: 5.9443 avg training loss: 6.6985
batch: [3150/10547] batch time: 2.016 trainign loss: 5.9303 avg training loss: 6.6982
batch: [3160/10547] batch time: 0.352 trainign loss: 5.1620 avg training loss: 6.6977
batch: [3170/10547] batch time: 1.342 trainign loss: 4.9567 avg training loss: 6.6972
batch: [3180/10547] batch time: 1.020 trainign loss: 4.5154 avg training loss: 6.6966
batch: [3190/10547] batch time: 1.338 trainign loss: 2.9403 avg training loss: 6.6956
batch: [3200/10547] batch time: 2.193 trainign loss: 6.8491 avg training loss: 6.6956
batch: [3210/10547] batch time: 0.047 trainign loss: 5.7633 avg training loss: 6.6955
batch: [3220/10547] batch time: 2.301 trainign loss: 5.4110 avg training loss: 6.6951
batch: [3230/10547] batch time: 0.046 trainign loss: 5.5872 avg training loss: 6.6946
batch: [3240/10547] batch time: 2.130 trainign loss: 3.7409 avg training loss: 6.6942
batch: [3250/10547] batch time: 0.046 trainign loss: 5.7129 avg training loss: 6.6937
batch: [3260/10547] batch time: 2.502 trainign loss: 5.6929 avg training loss: 6.6934
batch: [3270/10547] batch time: 0.046 trainign loss: 4.6080 avg training loss: 6.6932
batch: [3280/10547] batch time: 2.545 trainign loss: 6.0254 avg training loss: 6.6929
batch: [3290/10547] batch time: 0.047 trainign loss: 4.9719 avg training loss: 6.6925
batch: [3300/10547] batch time: 2.465 trainign loss: 5.5597 avg training loss: 6.6921
batch: [3310/10547] batch time: 0.055 trainign loss: 4.5936 avg training loss: 6.6917
batch: [3320/10547] batch time: 1.812 trainign loss: 4.7247 avg training loss: 6.6912
batch: [3330/10547] batch time: 0.052 trainign loss: 6.3259 avg training loss: 6.6909
batch: [3340/10547] batch time: 2.056 trainign loss: 4.9454 avg training loss: 6.6906
batch: [3350/10547] batch time: 0.550 trainign loss: 5.6894 avg training loss: 6.6903
batch: [3360/10547] batch time: 2.087 trainign loss: 6.0404 avg training loss: 6.6900
batch: [3370/10547] batch time: 0.264 trainign loss: 1.0067 avg training loss: 6.6891
batch: [3380/10547] batch time: 2.296 trainign loss: 6.4865 avg training loss: 6.6888
batch: [3390/10547] batch time: 1.521 trainign loss: 5.9315 avg training loss: 6.6884
batch: [3400/10547] batch time: 0.505 trainign loss: 3.7383 avg training loss: 6.6879
batch: [3410/10547] batch time: 2.106 trainign loss: 6.5331 avg training loss: 6.6877
batch: [3420/10547] batch time: 0.463 trainign loss: 6.1514 avg training loss: 6.6876
batch: [3430/10547] batch time: 2.067 trainign loss: 3.0663 avg training loss: 6.6871
batch: [3440/10547] batch time: 0.929 trainign loss: 5.9933 avg training loss: 6.6867
batch: [3450/10547] batch time: 1.371 trainign loss: 3.7759 avg training loss: 6.6861
batch: [3460/10547] batch time: 1.175 trainign loss: 6.7709 avg training loss: 6.6859
batch: [3470/10547] batch time: 1.701 trainign loss: 5.3678 avg training loss: 6.6857
batch: [3480/10547] batch time: 0.708 trainign loss: 5.2838 avg training loss: 6.6855
batch: [3490/10547] batch time: 1.278 trainign loss: 5.8505 avg training loss: 6.6849
batch: [3500/10547] batch time: 1.146 trainign loss: 5.3343 avg training loss: 6.6846
batch: [3510/10547] batch time: 1.786 trainign loss: 3.7006 avg training loss: 6.6842
batch: [3520/10547] batch time: 0.047 trainign loss: 5.4522 avg training loss: 6.6834
batch: [3530/10547] batch time: 2.713 trainign loss: 6.5101 avg training loss: 6.6830
batch: [3540/10547] batch time: 0.046 trainign loss: 5.0496 avg training loss: 6.6829
batch: [3550/10547] batch time: 2.062 trainign loss: 6.1413 avg training loss: 6.6826
batch: [3560/10547] batch time: 0.046 trainign loss: 3.8429 avg training loss: 6.6823
batch: [3570/10547] batch time: 2.480 trainign loss: 5.1486 avg training loss: 6.6818
batch: [3580/10547] batch time: 0.046 trainign loss: 6.2689 avg training loss: 6.6813
batch: [3590/10547] batch time: 2.106 trainign loss: 4.7362 avg training loss: 6.6809
batch: [3600/10547] batch time: 0.048 trainign loss: 6.5185 avg training loss: 6.6806
batch: [3610/10547] batch time: 1.748 trainign loss: 5.7670 avg training loss: 6.6803
batch: [3620/10547] batch time: 0.221 trainign loss: 4.7970 avg training loss: 6.6799
batch: [3630/10547] batch time: 0.891 trainign loss: 5.7031 avg training loss: 6.6794
batch: [3640/10547] batch time: 0.053 trainign loss: 3.4825 avg training loss: 6.6790
batch: [3650/10547] batch time: 0.054 trainign loss: 5.8925 avg training loss: 6.6784
batch: [3660/10547] batch time: 0.056 trainign loss: 1.0646 avg training loss: 6.6777
batch: [3670/10547] batch time: 0.046 trainign loss: 6.4187 avg training loss: 6.6777
batch: [3680/10547] batch time: 0.046 trainign loss: 3.4147 avg training loss: 6.6773
batch: [3690/10547] batch time: 0.046 trainign loss: 1.7502 avg training loss: 6.6765
batch: [3700/10547] batch time: 0.047 trainign loss: 6.4935 avg training loss: 6.6763
batch: [3710/10547] batch time: 0.046 trainign loss: 6.2452 avg training loss: 6.6761
batch: [3720/10547] batch time: 0.046 trainign loss: 5.6627 avg training loss: 6.6757
batch: [3730/10547] batch time: 0.046 trainign loss: 5.1305 avg training loss: 6.6754
batch: [3740/10547] batch time: 0.047 trainign loss: 5.4081 avg training loss: 6.6750
batch: [3750/10547] batch time: 0.048 trainign loss: 5.3547 avg training loss: 6.6748
batch: [3760/10547] batch time: 0.949 trainign loss: 4.9069 avg training loss: 6.6743
batch: [3770/10547] batch time: 0.047 trainign loss: 3.5611 avg training loss: 6.6739
batch: [3780/10547] batch time: 0.487 trainign loss: 6.2575 avg training loss: 6.6735
batch: [3790/10547] batch time: 0.495 trainign loss: 4.7532 avg training loss: 6.6732
batch: [3800/10547] batch time: 1.890 trainign loss: 2.4039 avg training loss: 6.6726
batch: [3810/10547] batch time: 0.048 trainign loss: 3.3919 avg training loss: 6.6723
batch: [3820/10547] batch time: 1.695 trainign loss: 7.2583 avg training loss: 6.6715
batch: [3830/10547] batch time: 0.046 trainign loss: 7.2163 avg training loss: 6.6717
batch: [3840/10547] batch time: 2.020 trainign loss: 6.5514 avg training loss: 6.6717
batch: [3850/10547] batch time: 0.046 trainign loss: 5.5015 avg training loss: 6.6714
batch: [3860/10547] batch time: 0.881 trainign loss: 5.3112 avg training loss: 6.6710
batch: [3870/10547] batch time: 0.046 trainign loss: 6.1154 avg training loss: 6.6707
batch: [3880/10547] batch time: 0.600 trainign loss: 5.2873 avg training loss: 6.6704
batch: [3890/10547] batch time: 0.055 trainign loss: 4.7027 avg training loss: 6.6701
batch: [3900/10547] batch time: 0.043 trainign loss: 1.8378 avg training loss: 6.6693
batch: [3910/10547] batch time: 0.046 trainign loss: 5.6639 avg training loss: 6.6692
batch: [3920/10547] batch time: 0.047 trainign loss: 6.2035 avg training loss: 6.6691
batch: [3930/10547] batch time: 0.048 trainign loss: 5.8895 avg training loss: 6.6689
batch: [3940/10547] batch time: 0.046 trainign loss: 4.7036 avg training loss: 6.6685
batch: [3950/10547] batch time: 0.055 trainign loss: 4.8892 avg training loss: 6.6681
batch: [3960/10547] batch time: 0.048 trainign loss: 0.5800 avg training loss: 6.6672
batch: [3970/10547] batch time: 0.046 trainign loss: 5.5983 avg training loss: 6.6666
batch: [3980/10547] batch time: 0.044 trainign loss: 6.2602 avg training loss: 6.6665
batch: [3990/10547] batch time: 0.046 trainign loss: 5.1457 avg training loss: 6.6662
batch: [4000/10547] batch time: 0.046 trainign loss: 4.8610 avg training loss: 6.6659
batch: [4010/10547] batch time: 0.054 trainign loss: 6.2572 avg training loss: 6.6655
batch: [4020/10547] batch time: 0.046 trainign loss: 5.6636 avg training loss: 6.6652
batch: [4030/10547] batch time: 0.048 trainign loss: 4.2098 avg training loss: 6.6649
batch: [4040/10547] batch time: 0.044 trainign loss: 5.8683 avg training loss: 6.6645
batch: [4050/10547] batch time: 0.047 trainign loss: 4.4391 avg training loss: 6.6641
batch: [4060/10547] batch time: 0.046 trainign loss: 4.8996 avg training loss: 6.6639
batch: [4070/10547] batch time: 0.046 trainign loss: 5.0563 avg training loss: 6.6636
batch: [4080/10547] batch time: 0.047 trainign loss: 6.2443 avg training loss: 6.6634
batch: [4090/10547] batch time: 0.046 trainign loss: 5.8462 avg training loss: 6.6633
batch: [4100/10547] batch time: 0.054 trainign loss: 4.9100 avg training loss: 6.6629
batch: [4110/10547] batch time: 0.048 trainign loss: 4.6243 avg training loss: 6.6625
batch: [4120/10547] batch time: 0.046 trainign loss: 3.6266 avg training loss: 6.6620
batch: [4130/10547] batch time: 0.055 trainign loss: 0.1854 avg training loss: 6.6608
batch: [4140/10547] batch time: 0.055 trainign loss: 5.2279 avg training loss: 6.6605
batch: [4150/10547] batch time: 0.125 trainign loss: 5.8182 avg training loss: 6.6599
batch: [4160/10547] batch time: 0.046 trainign loss: 5.1292 avg training loss: 6.6596
batch: [4170/10547] batch time: 0.054 trainign loss: 5.3645 avg training loss: 6.6595
batch: [4180/10547] batch time: 0.054 trainign loss: 4.1499 avg training loss: 6.6591
batch: [4190/10547] batch time: 0.054 trainign loss: 6.8934 avg training loss: 6.6587
batch: [4200/10547] batch time: 0.046 trainign loss: 6.3852 avg training loss: 6.6585
batch: [4210/10547] batch time: 0.046 trainign loss: 5.1281 avg training loss: 6.6582
batch: [4220/10547] batch time: 0.046 trainign loss: 5.3996 avg training loss: 6.6580
batch: [4230/10547] batch time: 0.046 trainign loss: 6.0659 avg training loss: 6.6576
batch: [4240/10547] batch time: 0.046 trainign loss: 5.4557 avg training loss: 6.6571
batch: [4250/10547] batch time: 0.046 trainign loss: 5.9029 avg training loss: 6.6566
batch: [4260/10547] batch time: 0.044 trainign loss: 4.9206 avg training loss: 6.6562
batch: [4270/10547] batch time: 0.047 trainign loss: 4.3567 avg training loss: 6.6558
batch: [4280/10547] batch time: 0.047 trainign loss: 0.0888 avg training loss: 6.6547
batch: [4290/10547] batch time: 0.046 trainign loss: 0.0005 avg training loss: 6.6528
batch: [4300/10547] batch time: 0.050 trainign loss: 2.2232 avg training loss: 6.6510
batch: [4310/10547] batch time: 0.048 trainign loss: 6.9175 avg training loss: 6.6518
batch: [4320/10547] batch time: 0.047 trainign loss: 6.3696 avg training loss: 6.6519
batch: [4330/10547] batch time: 0.048 trainign loss: 4.2384 avg training loss: 6.6516
batch: [4340/10547] batch time: 0.046 trainign loss: 5.2066 avg training loss: 6.6511
batch: [4350/10547] batch time: 0.046 trainign loss: 5.5242 avg training loss: 6.6509
batch: [4360/10547] batch time: 0.054 trainign loss: 5.7324 avg training loss: 6.6505
batch: [4370/10547] batch time: 0.046 trainign loss: 5.6936 avg training loss: 6.6504
batch: [4380/10547] batch time: 0.046 trainign loss: 5.0059 avg training loss: 6.6500
batch: [4390/10547] batch time: 0.047 trainign loss: 5.5268 avg training loss: 6.6498
batch: [4400/10547] batch time: 0.054 trainign loss: 5.5276 avg training loss: 6.6493
batch: [4410/10547] batch time: 0.046 trainign loss: 5.3956 avg training loss: 6.6490
batch: [4420/10547] batch time: 0.046 trainign loss: 5.0993 avg training loss: 6.6488
batch: [4430/10547] batch time: 0.047 trainign loss: 6.1982 avg training loss: 6.6483
batch: [4440/10547] batch time: 0.046 trainign loss: 5.1215 avg training loss: 6.6475
batch: [4450/10547] batch time: 0.047 trainign loss: 6.6924 avg training loss: 6.6475
batch: [4460/10547] batch time: 0.046 trainign loss: 6.9749 avg training loss: 6.6473
batch: [4470/10547] batch time: 0.049 trainign loss: 6.0884 avg training loss: 6.6471
batch: [4480/10547] batch time: 0.841 trainign loss: 6.5407 avg training loss: 6.6467
batch: [4490/10547] batch time: 0.139 trainign loss: 6.7975 avg training loss: 6.6464
batch: [4500/10547] batch time: 0.231 trainign loss: 4.1094 avg training loss: 6.6462
batch: [4510/10547] batch time: 0.047 trainign loss: 6.3107 avg training loss: 6.6454
batch: [4520/10547] batch time: 0.498 trainign loss: 6.8659 avg training loss: 6.6451
batch: [4530/10547] batch time: 0.047 trainign loss: 4.8103 avg training loss: 6.6450
batch: [4540/10547] batch time: 0.526 trainign loss: 5.7912 avg training loss: 6.6446
batch: [4550/10547] batch time: 0.046 trainign loss: 5.7642 avg training loss: 6.6445
batch: [4560/10547] batch time: 0.585 trainign loss: 4.7052 avg training loss: 6.6440
batch: [4570/10547] batch time: 0.047 trainign loss: 5.3132 avg training loss: 6.6436
batch: [4580/10547] batch time: 0.729 trainign loss: 5.4229 avg training loss: 6.6433
batch: [4590/10547] batch time: 0.046 trainign loss: 4.9463 avg training loss: 6.6429
batch: [4600/10547] batch time: 1.008 trainign loss: 5.8119 avg training loss: 6.6424
batch: [4610/10547] batch time: 0.047 trainign loss: 5.7474 avg training loss: 6.6421
batch: [4620/10547] batch time: 1.071 trainign loss: 5.4732 avg training loss: 6.6417
batch: [4630/10547] batch time: 0.054 trainign loss: 4.4670 avg training loss: 6.6413
batch: [4640/10547] batch time: 0.849 trainign loss: 6.2217 avg training loss: 6.6410
batch: [4650/10547] batch time: 0.053 trainign loss: 4.4551 avg training loss: 6.6406
batch: [4660/10547] batch time: 0.255 trainign loss: 6.0481 avg training loss: 6.6401
batch: [4670/10547] batch time: 0.046 trainign loss: 4.5749 avg training loss: 6.6397
batch: [4680/10547] batch time: 0.390 trainign loss: 5.3413 avg training loss: 6.6392
batch: [4690/10547] batch time: 0.290 trainign loss: 5.7359 avg training loss: 6.6388
batch: [4700/10547] batch time: 0.290 trainign loss: 6.2906 avg training loss: 6.6384
batch: [4710/10547] batch time: 0.046 trainign loss: 5.9793 avg training loss: 6.6379
batch: [4720/10547] batch time: 1.081 trainign loss: 6.5454 avg training loss: 6.6374
batch: [4730/10547] batch time: 0.054 trainign loss: 6.4196 avg training loss: 6.6371
batch: [4740/10547] batch time: 1.613 trainign loss: 5.3943 avg training loss: 6.6368
batch: [4750/10547] batch time: 0.046 trainign loss: 5.9255 avg training loss: 6.6365
batch: [4760/10547] batch time: 1.685 trainign loss: 5.7977 avg training loss: 6.6362
batch: [4770/10547] batch time: 0.046 trainign loss: 7.1198 avg training loss: 6.6358
batch: [4780/10547] batch time: 1.273 trainign loss: 6.3592 avg training loss: 6.6358
batch: [4790/10547] batch time: 0.046 trainign loss: 5.2501 avg training loss: 6.6355
batch: [4800/10547] batch time: 1.313 trainign loss: 6.7371 avg training loss: 6.6353
batch: [4810/10547] batch time: 0.055 trainign loss: 6.2897 avg training loss: 6.6351
batch: [4820/10547] batch time: 1.485 trainign loss: 4.4284 avg training loss: 6.6346
batch: [4830/10547] batch time: 0.046 trainign loss: 6.4476 avg training loss: 6.6345
batch: [4840/10547] batch time: 2.108 trainign loss: 4.7498 avg training loss: 6.6340
batch: [4850/10547] batch time: 0.565 trainign loss: 4.0454 avg training loss: 6.6335
batch: [4860/10547] batch time: 1.311 trainign loss: 4.3156 avg training loss: 6.6329
batch: [4870/10547] batch time: 1.289 trainign loss: 6.4816 avg training loss: 6.6327
batch: [4880/10547] batch time: 1.177 trainign loss: 5.2362 avg training loss: 6.6326
batch: [4890/10547] batch time: 0.931 trainign loss: 6.1940 avg training loss: 6.6323
batch: [4900/10547] batch time: 1.015 trainign loss: 5.9571 avg training loss: 6.6321
batch: [4910/10547] batch time: 1.420 trainign loss: 6.6890 avg training loss: 6.6317
batch: [4920/10547] batch time: 1.495 trainign loss: 6.0329 avg training loss: 6.6316
batch: [4930/10547] batch time: 0.730 trainign loss: 4.4124 avg training loss: 6.6312
batch: [4940/10547] batch time: 1.496 trainign loss: 5.5183 avg training loss: 6.6307
batch: [4950/10547] batch time: 0.926 trainign loss: 5.7556 avg training loss: 6.6302
batch: [4960/10547] batch time: 0.706 trainign loss: 6.4938 avg training loss: 6.6300
batch: [4970/10547] batch time: 1.930 trainign loss: 6.8799 avg training loss: 6.6300
batch: [4980/10547] batch time: 0.311 trainign loss: 5.9274 avg training loss: 6.6300
batch: [4990/10547] batch time: 0.640 trainign loss: 5.1624 avg training loss: 6.6296
batch: [5000/10547] batch time: 0.127 trainign loss: 4.7930 avg training loss: 6.6294
batch: [5010/10547] batch time: 1.977 trainign loss: 4.4467 avg training loss: 6.6289
batch: [5020/10547] batch time: 0.054 trainign loss: 5.8566 avg training loss: 6.6283
batch: [5030/10547] batch time: 1.086 trainign loss: 6.1814 avg training loss: 6.6281
batch: [5040/10547] batch time: 0.055 trainign loss: 6.0587 avg training loss: 6.6277
batch: [5050/10547] batch time: 0.473 trainign loss: 4.8433 avg training loss: 6.6274
batch: [5060/10547] batch time: 0.992 trainign loss: 6.4110 avg training loss: 6.6266
batch: [5070/10547] batch time: 0.176 trainign loss: 5.7332 avg training loss: 6.6265
batch: [5080/10547] batch time: 0.509 trainign loss: 5.2916 avg training loss: 6.6260
batch: [5090/10547] batch time: 0.056 trainign loss: 5.5788 avg training loss: 6.6256
batch: [5100/10547] batch time: 0.615 trainign loss: 3.9638 avg training loss: 6.6254
batch: [5110/10547] batch time: 0.048 trainign loss: 5.6653 avg training loss: 6.6251
batch: [5120/10547] batch time: 1.154 trainign loss: 3.1339 avg training loss: 6.6247
batch: [5130/10547] batch time: 0.046 trainign loss: 4.0385 avg training loss: 6.6243
batch: [5140/10547] batch time: 2.087 trainign loss: 5.6466 avg training loss: 6.6239
batch: [5150/10547] batch time: 0.047 trainign loss: 6.0238 avg training loss: 6.6237
batch: [5160/10547] batch time: 0.746 trainign loss: 0.5522 avg training loss: 6.6229
batch: [5170/10547] batch time: 0.049 trainign loss: 6.5020 avg training loss: 6.6227
batch: [5180/10547] batch time: 0.047 trainign loss: 6.3181 avg training loss: 6.6226
batch: [5190/10547] batch time: 0.046 trainign loss: 6.4194 avg training loss: 6.6223
batch: [5200/10547] batch time: 0.046 trainign loss: 3.9592 avg training loss: 6.6220
batch: [5210/10547] batch time: 0.046 trainign loss: 5.3471 avg training loss: 6.6216
batch: [5220/10547] batch time: 0.045 trainign loss: 6.0369 avg training loss: 6.6212
batch: [5230/10547] batch time: 0.050 trainign loss: 4.2569 avg training loss: 6.6208
batch: [5240/10547] batch time: 0.046 trainign loss: 1.7432 avg training loss: 6.6198
batch: [5250/10547] batch time: 0.046 trainign loss: 6.8846 avg training loss: 6.6197
batch: [5260/10547] batch time: 0.044 trainign loss: 5.7539 avg training loss: 6.6196
batch: [5270/10547] batch time: 0.046 trainign loss: 4.6136 avg training loss: 6.6192
batch: [5280/10547] batch time: 0.046 trainign loss: 6.1043 avg training loss: 6.6187
batch: [5290/10547] batch time: 0.054 trainign loss: 5.6693 avg training loss: 6.6185
batch: [5300/10547] batch time: 0.043 trainign loss: 0.9567 avg training loss: 6.6177
batch: [5310/10547] batch time: 0.046 trainign loss: 6.5656 avg training loss: 6.6175
batch: [5320/10547] batch time: 0.046 trainign loss: 6.0283 avg training loss: 6.6174
batch: [5330/10547] batch time: 0.046 trainign loss: 4.8808 avg training loss: 6.6171
batch: [5340/10547] batch time: 0.044 trainign loss: 3.8830 avg training loss: 6.6166
batch: [5350/10547] batch time: 0.049 trainign loss: 5.4053 avg training loss: 6.6162
batch: [5360/10547] batch time: 0.369 trainign loss: 5.2933 avg training loss: 6.6157
batch: [5370/10547] batch time: 0.047 trainign loss: 5.5121 avg training loss: 6.6156
batch: [5380/10547] batch time: 0.502 trainign loss: 5.6123 avg training loss: 6.6152
batch: [5390/10547] batch time: 0.049 trainign loss: 3.4583 avg training loss: 6.6147
batch: [5400/10547] batch time: 0.055 trainign loss: 5.2274 avg training loss: 6.6143
batch: [5410/10547] batch time: 0.346 trainign loss: 6.5310 avg training loss: 6.6141
batch: [5420/10547] batch time: 0.047 trainign loss: 6.0456 avg training loss: 6.6139
batch: [5430/10547] batch time: 0.939 trainign loss: 6.0562 avg training loss: 6.6135
batch: [5440/10547] batch time: 0.047 trainign loss: 5.7349 avg training loss: 6.6134
batch: [5450/10547] batch time: 0.429 trainign loss: 5.2919 avg training loss: 6.6130
batch: [5460/10547] batch time: 0.051 trainign loss: 5.4552 avg training loss: 6.6125
batch: [5470/10547] batch time: 1.945 trainign loss: 2.3156 avg training loss: 6.6121
batch: [5480/10547] batch time: 0.046 trainign loss: 3.8713 avg training loss: 6.6115
batch: [5490/10547] batch time: 1.827 trainign loss: 7.0658 avg training loss: 6.6113
batch: [5500/10547] batch time: 0.046 trainign loss: 5.8492 avg training loss: 6.6113
batch: [5510/10547] batch time: 0.878 trainign loss: 5.2843 avg training loss: 6.6110
batch: [5520/10547] batch time: 0.045 trainign loss: 6.5244 avg training loss: 6.6102
batch: [5530/10547] batch time: 1.252 trainign loss: 4.7332 avg training loss: 6.6099
batch: [5540/10547] batch time: 0.046 trainign loss: 3.2972 avg training loss: 6.6096
batch: [5550/10547] batch time: 0.981 trainign loss: 5.7115 avg training loss: 6.6091
batch: [5560/10547] batch time: 0.047 trainign loss: 6.1139 avg training loss: 6.6090
batch: [5570/10547] batch time: 0.764 trainign loss: 6.3088 avg training loss: 6.6089
batch: [5580/10547] batch time: 0.047 trainign loss: 4.1158 avg training loss: 6.6084
batch: [5590/10547] batch time: 0.872 trainign loss: 9.4628 avg training loss: 6.6072
batch: [5600/10547] batch time: 0.054 trainign loss: 6.7685 avg training loss: 6.6074
batch: [5610/10547] batch time: 0.336 trainign loss: 7.1702 avg training loss: 6.6076
batch: [5620/10547] batch time: 0.054 trainign loss: 4.7095 avg training loss: 6.6074
batch: [5630/10547] batch time: 0.256 trainign loss: 4.1947 avg training loss: 6.6069
batch: [5640/10547] batch time: 0.048 trainign loss: 6.4791 avg training loss: 6.6064
batch: [5650/10547] batch time: 0.051 trainign loss: 6.5361 avg training loss: 6.6064
batch: [5660/10547] batch time: 0.046 trainign loss: 4.5390 avg training loss: 6.6061
batch: [5670/10547] batch time: 0.271 trainign loss: 3.4143 avg training loss: 6.6057
batch: [5680/10547] batch time: 0.051 trainign loss: 4.6834 avg training loss: 6.6051
batch: [5690/10547] batch time: 0.049 trainign loss: 6.2600 avg training loss: 6.6049
batch: [5700/10547] batch time: 0.046 trainign loss: 5.6606 avg training loss: 6.6047
batch: [5710/10547] batch time: 0.481 trainign loss: 4.6128 avg training loss: 6.6042
batch: [5720/10547] batch time: 0.048 trainign loss: 6.1660 avg training loss: 6.6040
batch: [5730/10547] batch time: 0.102 trainign loss: 5.9098 avg training loss: 6.6039
batch: [5740/10547] batch time: 0.050 trainign loss: 5.1001 avg training loss: 6.6036
batch: [5750/10547] batch time: 2.080 trainign loss: 4.0111 avg training loss: 6.6032
batch: [5760/10547] batch time: 0.046 trainign loss: 5.0180 avg training loss: 6.6026
batch: [5770/10547] batch time: 2.277 trainign loss: 6.0854 avg training loss: 6.6024
batch: [5780/10547] batch time: 0.046 trainign loss: 3.7381 avg training loss: 6.6020
batch: [5790/10547] batch time: 2.069 trainign loss: 0.0088 avg training loss: 6.6005
batch: [5800/10547] batch time: 0.046 trainign loss: 8.4555 avg training loss: 6.5996
batch: [5810/10547] batch time: 2.315 trainign loss: 6.4698 avg training loss: 6.5996
batch: [5820/10547] batch time: 0.056 trainign loss: 6.2961 avg training loss: 6.5992
batch: [5830/10547] batch time: 2.236 trainign loss: 4.6918 avg training loss: 6.5987
batch: [5840/10547] batch time: 0.047 trainign loss: 3.7864 avg training loss: 6.5981
batch: [5850/10547] batch time: 2.555 trainign loss: 6.3886 avg training loss: 6.5974
batch: [5860/10547] batch time: 0.046 trainign loss: 5.9375 avg training loss: 6.5974
batch: [5870/10547] batch time: 2.019 trainign loss: 4.1504 avg training loss: 6.5972
batch: [5880/10547] batch time: 0.046 trainign loss: 3.0460 avg training loss: 6.5966
batch: [5890/10547] batch time: 2.273 trainign loss: 5.7878 avg training loss: 6.5964
batch: [5900/10547] batch time: 0.055 trainign loss: 5.1867 avg training loss: 6.5962
batch: [5910/10547] batch time: 2.418 trainign loss: 6.3563 avg training loss: 6.5960
batch: [5920/10547] batch time: 0.054 trainign loss: 5.3787 avg training loss: 6.5958
batch: [5930/10547] batch time: 2.426 trainign loss: 2.4126 avg training loss: 6.5952
batch: [5940/10547] batch time: 0.046 trainign loss: 5.1586 avg training loss: 6.5943
batch: [5950/10547] batch time: 2.453 trainign loss: 4.6086 avg training loss: 6.5941
batch: [5960/10547] batch time: 0.046 trainign loss: 5.7225 avg training loss: 6.5941
batch: [5970/10547] batch time: 2.222 trainign loss: 6.9339 avg training loss: 6.5935
batch: [5980/10547] batch time: 0.440 trainign loss: 5.0243 avg training loss: 6.5929
batch: [5990/10547] batch time: 2.071 trainign loss: 7.4450 avg training loss: 6.5929
batch: [6000/10547] batch time: 0.046 trainign loss: 6.6018 avg training loss: 6.5929
batch: [6010/10547] batch time: 1.771 trainign loss: 5.7695 avg training loss: 6.5927
batch: [6020/10547] batch time: 0.051 trainign loss: 5.6953 avg training loss: 6.5925
batch: [6030/10547] batch time: 2.073 trainign loss: 4.0306 avg training loss: 6.5919
batch: [6040/10547] batch time: 0.049 trainign loss: 6.8970 avg training loss: 6.5917
batch: [6050/10547] batch time: 1.733 trainign loss: 5.8730 avg training loss: 6.5915
batch: [6060/10547] batch time: 0.046 trainign loss: 5.6562 avg training loss: 6.5913
batch: [6070/10547] batch time: 1.160 trainign loss: 5.1457 avg training loss: 6.5910
batch: [6080/10547] batch time: 0.046 trainign loss: 5.4466 avg training loss: 6.5907
batch: [6090/10547] batch time: 0.968 trainign loss: 6.3995 avg training loss: 6.5905
batch: [6100/10547] batch time: 0.051 trainign loss: 4.5511 avg training loss: 6.5901
batch: [6110/10547] batch time: 0.941 trainign loss: 6.2168 avg training loss: 6.5898
batch: [6120/10547] batch time: 0.049 trainign loss: 5.7136 avg training loss: 6.5896
batch: [6130/10547] batch time: 0.747 trainign loss: 3.5347 avg training loss: 6.5892
batch: [6140/10547] batch time: 0.047 trainign loss: 6.0282 avg training loss: 6.5888
batch: [6150/10547] batch time: 0.042 trainign loss: 5.3328 avg training loss: 6.5886
batch: [6160/10547] batch time: 0.048 trainign loss: 6.4511 avg training loss: 6.5882
batch: [6170/10547] batch time: 0.054 trainign loss: 0.1215 avg training loss: 6.5873
batch: [6180/10547] batch time: 0.056 trainign loss: 8.3017 avg training loss: 6.5868
batch: [6190/10547] batch time: 0.045 trainign loss: 5.8739 avg training loss: 6.5867
batch: [6200/10547] batch time: 0.046 trainign loss: 6.3682 avg training loss: 6.5864
batch: [6210/10547] batch time: 0.043 trainign loss: 6.2255 avg training loss: 6.5864
batch: [6220/10547] batch time: 0.053 trainign loss: 5.3243 avg training loss: 6.5862
batch: [6230/10547] batch time: 0.049 trainign loss: 3.9020 avg training loss: 6.5858
batch: [6240/10547] batch time: 0.048 trainign loss: 5.1779 avg training loss: 6.5853
batch: [6250/10547] batch time: 0.046 trainign loss: 4.5686 avg training loss: 6.5850
batch: [6260/10547] batch time: 0.056 trainign loss: 5.1040 avg training loss: 6.5845
batch: [6270/10547] batch time: 0.044 trainign loss: 2.3583 avg training loss: 6.5838
batch: [6280/10547] batch time: 0.046 trainign loss: 6.0919 avg training loss: 6.5835
batch: [6290/10547] batch time: 0.047 trainign loss: 5.4599 avg training loss: 6.5832
batch: [6300/10547] batch time: 0.046 trainign loss: 6.0758 avg training loss: 6.5830
batch: [6310/10547] batch time: 0.046 trainign loss: 2.7655 avg training loss: 6.5825
batch: [6320/10547] batch time: 0.046 trainign loss: 6.5159 avg training loss: 6.5823
batch: [6330/10547] batch time: 0.046 trainign loss: 5.6186 avg training loss: 6.5822
batch: [6340/10547] batch time: 0.046 trainign loss: 5.9567 avg training loss: 6.5819
batch: [6350/10547] batch time: 0.042 trainign loss: 6.0475 avg training loss: 6.5818
batch: [6360/10547] batch time: 0.048 trainign loss: 4.8805 avg training loss: 6.5815
batch: [6370/10547] batch time: 0.055 trainign loss: 5.8591 avg training loss: 6.5809
batch: [6380/10547] batch time: 0.047 trainign loss: 2.7382 avg training loss: 6.5805
batch: [6390/10547] batch time: 0.046 trainign loss: 6.0286 avg training loss: 6.5803
batch: [6400/10547] batch time: 0.047 trainign loss: 4.7030 avg training loss: 6.5799
batch: [6410/10547] batch time: 0.046 trainign loss: 5.0252 avg training loss: 6.5796
batch: [6420/10547] batch time: 0.046 trainign loss: 4.7953 avg training loss: 6.5789
batch: [6430/10547] batch time: 0.052 trainign loss: 6.7262 avg training loss: 6.5788
batch: [6440/10547] batch time: 0.049 trainign loss: 2.9319 avg training loss: 6.5783
batch: [6450/10547] batch time: 0.047 trainign loss: 7.4754 avg training loss: 6.5777
batch: [6460/10547] batch time: 0.054 trainign loss: 5.2314 avg training loss: 6.5777
batch: [6470/10547] batch time: 0.046 trainign loss: 6.6397 avg training loss: 6.5773
batch: [6480/10547] batch time: 0.047 trainign loss: 5.8044 avg training loss: 6.5770
batch: [6490/10547] batch time: 0.046 trainign loss: 4.2911 avg training loss: 6.5766
batch: [6500/10547] batch time: 0.054 trainign loss: 0.0226 avg training loss: 6.5752
batch: [6510/10547] batch time: 0.054 trainign loss: 0.0004 avg training loss: 6.5735
batch: [6520/10547] batch time: 0.046 trainign loss: 0.0008 avg training loss: 6.5718
batch: [6530/10547] batch time: 0.046 trainign loss: 8.6999 avg training loss: 6.5713
batch: [6540/10547] batch time: 0.046 trainign loss: 7.2048 avg training loss: 6.5715
batch: [6550/10547] batch time: 0.053 trainign loss: 5.2369 avg training loss: 6.5714
batch: [6560/10547] batch time: 0.046 trainign loss: 4.9488 avg training loss: 6.5712
batch: [6570/10547] batch time: 0.045 trainign loss: 2.2301 avg training loss: 6.5706
batch: [6580/10547] batch time: 0.049 trainign loss: 5.6958 avg training loss: 6.5702
batch: [6590/10547] batch time: 0.054 trainign loss: 5.6913 avg training loss: 6.5700
batch: [6600/10547] batch time: 0.444 trainign loss: 3.0423 avg training loss: 6.5695
batch: [6610/10547] batch time: 0.046 trainign loss: 2.8960 avg training loss: 6.5690
batch: [6620/10547] batch time: 0.046 trainign loss: 6.9207 avg training loss: 6.5690
batch: [6630/10547] batch time: 0.046 trainign loss: 5.2659 avg training loss: 6.5688
batch: [6640/10547] batch time: 0.566 trainign loss: 5.3752 avg training loss: 6.5683
batch: [6650/10547] batch time: 0.047 trainign loss: 3.0226 avg training loss: 6.5678
batch: [6660/10547] batch time: 0.056 trainign loss: 6.2182 avg training loss: 6.5674
batch: [6670/10547] batch time: 0.046 trainign loss: 6.8352 avg training loss: 6.5674
batch: [6680/10547] batch time: 0.095 trainign loss: 5.6381 avg training loss: 6.5671
batch: [6690/10547] batch time: 0.046 trainign loss: 5.7483 avg training loss: 6.5667
batch: [6700/10547] batch time: 0.976 trainign loss: 5.7124 avg training loss: 6.5665
batch: [6710/10547] batch time: 0.046 trainign loss: 5.9486 avg training loss: 6.5663
batch: [6720/10547] batch time: 1.908 trainign loss: 5.7725 avg training loss: 6.5660
batch: [6730/10547] batch time: 0.046 trainign loss: 5.9871 avg training loss: 6.5657
batch: [6740/10547] batch time: 1.330 trainign loss: 4.6534 avg training loss: 6.5655
batch: [6750/10547] batch time: 0.047 trainign loss: 4.9797 avg training loss: 6.5649
batch: [6760/10547] batch time: 1.578 trainign loss: 5.3888 avg training loss: 6.5647
batch: [6770/10547] batch time: 0.054 trainign loss: 5.1870 avg training loss: 6.5643
batch: [6780/10547] batch time: 1.366 trainign loss: 5.8067 avg training loss: 6.5640
batch: [6790/10547] batch time: 0.055 trainign loss: 6.0504 avg training loss: 6.5638
batch: [6800/10547] batch time: 1.001 trainign loss: 5.3942 avg training loss: 6.5636
batch: [6810/10547] batch time: 0.046 trainign loss: 5.2469 avg training loss: 6.5632
batch: [6820/10547] batch time: 1.121 trainign loss: 2.7397 avg training loss: 6.5623
batch: [6830/10547] batch time: 0.053 trainign loss: 4.4016 avg training loss: 6.5621
batch: [6840/10547] batch time: 0.047 trainign loss: 4.0128 avg training loss: 6.5619
batch: [6850/10547] batch time: 0.047 trainign loss: 2.6725 avg training loss: 6.5614
batch: [6860/10547] batch time: 0.053 trainign loss: 5.0954 avg training loss: 6.5610
batch: [6870/10547] batch time: 0.047 trainign loss: 5.3671 avg training loss: 6.5605
batch: [6880/10547] batch time: 0.047 trainign loss: 5.8207 avg training loss: 6.5603
batch: [6890/10547] batch time: 0.046 trainign loss: 5.5714 avg training loss: 6.5601
batch: [6900/10547] batch time: 0.050 trainign loss: 4.7337 avg training loss: 6.5597
batch: [6910/10547] batch time: 0.043 trainign loss: 5.0910 avg training loss: 6.5592
batch: [6920/10547] batch time: 0.047 trainign loss: 4.4947 avg training loss: 6.5587
batch: [6930/10547] batch time: 0.049 trainign loss: 6.7908 avg training loss: 6.5584
batch: [6940/10547] batch time: 0.049 trainign loss: 5.7636 avg training loss: 6.5584
batch: [6950/10547] batch time: 0.041 trainign loss: 4.6725 avg training loss: 6.5581
batch: [6960/10547] batch time: 0.055 trainign loss: 6.2488 avg training loss: 6.5575
batch: [6970/10547] batch time: 0.043 trainign loss: 5.9209 avg training loss: 6.5573
batch: [6980/10547] batch time: 0.046 trainign loss: 6.0011 avg training loss: 6.5572
batch: [6990/10547] batch time: 0.053 trainign loss: 5.7126 avg training loss: 6.5570
batch: [7000/10547] batch time: 0.049 trainign loss: 7.0248 avg training loss: 6.5567
batch: [7010/10547] batch time: 0.044 trainign loss: 4.2181 avg training loss: 6.5565
batch: [7020/10547] batch time: 0.048 trainign loss: 5.6228 avg training loss: 6.5561
batch: [7030/10547] batch time: 0.043 trainign loss: 6.2629 avg training loss: 6.5560
batch: [7040/10547] batch time: 0.046 trainign loss: 5.7570 avg training loss: 6.5557
batch: [7050/10547] batch time: 0.047 trainign loss: 5.7151 avg training loss: 6.5554
batch: [7060/10547] batch time: 0.047 trainign loss: 5.7249 avg training loss: 6.5552
batch: [7070/10547] batch time: 0.046 trainign loss: 7.5334 avg training loss: 6.5545
batch: [7080/10547] batch time: 0.047 trainign loss: 5.9980 avg training loss: 6.5543
batch: [7090/10547] batch time: 0.047 trainign loss: 6.5002 avg training loss: 6.5542
batch: [7100/10547] batch time: 0.046 trainign loss: 6.3298 avg training loss: 6.5539
batch: [7110/10547] batch time: 0.046 trainign loss: 4.9385 avg training loss: 6.5535
batch: [7120/10547] batch time: 0.046 trainign loss: 6.5492 avg training loss: 6.5532
batch: [7130/10547] batch time: 0.046 trainign loss: 4.7088 avg training loss: 6.5530
batch: [7140/10547] batch time: 0.047 trainign loss: 4.8113 avg training loss: 6.5525
batch: [7150/10547] batch time: 0.047 trainign loss: 0.1769 avg training loss: 6.5517
batch: [7160/10547] batch time: 0.046 trainign loss: 0.0005 avg training loss: 6.5500
batch: [7170/10547] batch time: 0.042 trainign loss: 0.0001 avg training loss: 6.5483
batch: [7180/10547] batch time: 0.047 trainign loss: 0.0001 avg training loss: 6.5466
batch: [7190/10547] batch time: 0.047 trainign loss: 0.0001 avg training loss: 6.5449
batch: [7200/10547] batch time: 0.048 trainign loss: 0.0001 avg training loss: 6.5432
batch: [7210/10547] batch time: 0.055 trainign loss: 0.0001 avg training loss: 6.5415
batch: [7220/10547] batch time: 0.047 trainign loss: 0.0001 avg training loss: 6.5399
batch: [7230/10547] batch time: 0.046 trainign loss: 0.0001 avg training loss: 6.5382
batch: [7240/10547] batch time: 0.047 trainign loss: 0.0001 avg training loss: 6.5365
batch: [7250/10547] batch time: 0.047 trainign loss: 7.0278 avg training loss: 6.5370
batch: [7260/10547] batch time: 0.046 trainign loss: 6.4389 avg training loss: 6.5371
batch: [7270/10547] batch time: 0.047 trainign loss: 5.2166 avg training loss: 6.5370
batch: [7280/10547] batch time: 0.050 trainign loss: 5.0855 avg training loss: 6.5368
batch: [7290/10547] batch time: 0.053 trainign loss: 6.3862 avg training loss: 6.5364
batch: [7300/10547] batch time: 0.046 trainign loss: 0.1318 avg training loss: 6.5354
batch: [7310/10547] batch time: 0.046 trainign loss: 7.9865 avg training loss: 6.5345
batch: [7320/10547] batch time: 0.045 trainign loss: 7.2356 avg training loss: 6.5345
batch: [7330/10547] batch time: 0.045 trainign loss: 6.0608 avg training loss: 6.5344
batch: [7340/10547] batch time: 0.054 trainign loss: 6.2037 avg training loss: 6.5344
batch: [7350/10547] batch time: 0.049 trainign loss: 6.1745 avg training loss: 6.5341
batch: [7360/10547] batch time: 0.046 trainign loss: 6.1084 avg training loss: 6.5339
batch: [7370/10547] batch time: 0.046 trainign loss: 5.3674 avg training loss: 6.5337
batch: [7380/10547] batch time: 0.051 trainign loss: 5.9546 avg training loss: 6.5335
batch: [7390/10547] batch time: 0.046 trainign loss: 5.4503 avg training loss: 6.5332
batch: [7400/10547] batch time: 0.046 trainign loss: 5.3910 avg training loss: 6.5327
batch: [7410/10547] batch time: 0.046 trainign loss: 5.5720 avg training loss: 6.5324
batch: [7420/10547] batch time: 0.046 trainign loss: 5.9755 avg training loss: 6.5322
batch: [7430/10547] batch time: 0.046 trainign loss: 4.8758 avg training loss: 6.5317
batch: [7440/10547] batch time: 0.309 trainign loss: 5.0559 avg training loss: 6.5313
batch: [7450/10547] batch time: 0.048 trainign loss: 6.0550 avg training loss: 6.5311
batch: [7460/10547] batch time: 0.049 trainign loss: 5.7061 avg training loss: 6.5309
batch: [7470/10547] batch time: 0.043 trainign loss: 6.0992 avg training loss: 6.5305
batch: [7480/10547] batch time: 0.055 trainign loss: 5.8354 avg training loss: 6.5302
batch: [7490/10547] batch time: 0.047 trainign loss: 0.6213 avg training loss: 6.5294
batch: [7500/10547] batch time: 0.046 trainign loss: 6.6308 avg training loss: 6.5293
batch: [7510/10547] batch time: 0.054 trainign loss: 6.1783 avg training loss: 6.5289
batch: [7520/10547] batch time: 0.152 trainign loss: 5.0827 avg training loss: 6.5285
batch: [7530/10547] batch time: 0.046 trainign loss: 6.8758 avg training loss: 6.5285
batch: [7540/10547] batch time: 0.409 trainign loss: 5.8993 avg training loss: 6.5282
batch: [7550/10547] batch time: 0.047 trainign loss: 2.0111 avg training loss: 6.5277
batch: [7560/10547] batch time: 0.971 trainign loss: 6.3279 avg training loss: 6.5275
batch: [7570/10547] batch time: 0.046 trainign loss: 5.5935 avg training loss: 6.5274
batch: [7580/10547] batch time: 0.227 trainign loss: 5.7214 avg training loss: 6.5270
batch: [7590/10547] batch time: 0.047 trainign loss: 6.2951 avg training loss: 6.5265
batch: [7600/10547] batch time: 0.947 trainign loss: 3.8273 avg training loss: 6.5262
batch: [7610/10547] batch time: 0.048 trainign loss: 6.9018 avg training loss: 6.5259
batch: [7620/10547] batch time: 0.545 trainign loss: 6.3644 avg training loss: 6.5259
batch: [7630/10547] batch time: 0.053 trainign loss: 3.9956 avg training loss: 6.5255
batch: [7640/10547] batch time: 0.934 trainign loss: 5.9564 avg training loss: 6.5250
batch: [7650/10547] batch time: 0.047 trainign loss: 6.7748 avg training loss: 6.5250
batch: [7660/10547] batch time: 0.598 trainign loss: 5.7637 avg training loss: 6.5248
batch: [7670/10547] batch time: 0.047 trainign loss: 4.6524 avg training loss: 6.5246
batch: [7680/10547] batch time: 0.730 trainign loss: 5.1329 avg training loss: 6.5242
batch: [7690/10547] batch time: 0.046 trainign loss: 5.5442 avg training loss: 6.5240
batch: [7700/10547] batch time: 0.450 trainign loss: 5.5524 avg training loss: 6.5236
batch: [7710/10547] batch time: 0.046 trainign loss: 7.5245 avg training loss: 6.5228
batch: [7720/10547] batch time: 0.048 trainign loss: 7.2168 avg training loss: 6.5227
batch: [7730/10547] batch time: 0.046 trainign loss: 4.9252 avg training loss: 6.5224
batch: [7740/10547] batch time: 0.229 trainign loss: 6.0891 avg training loss: 6.5223
batch: [7750/10547] batch time: 0.054 trainign loss: 5.6147 avg training loss: 6.5221
batch: [7760/10547] batch time: 0.047 trainign loss: 6.1521 avg training loss: 6.5219
batch: [7770/10547] batch time: 0.043 trainign loss: 5.7724 avg training loss: 6.5217
batch: [7780/10547] batch time: 0.140 trainign loss: 6.1312 avg training loss: 6.5215
batch: [7790/10547] batch time: 0.047 trainign loss: 5.3159 avg training loss: 6.5212
batch: [7800/10547] batch time: 0.048 trainign loss: 5.2664 avg training loss: 6.5209
batch: [7810/10547] batch time: 0.047 trainign loss: 0.1881 avg training loss: 6.5199
batch: [7820/10547] batch time: 0.439 trainign loss: 10.3747 avg training loss: 6.5188
batch: [7830/10547] batch time: 0.045 trainign loss: 7.3866 avg training loss: 6.5193
batch: [7840/10547] batch time: 0.864 trainign loss: 6.5709 avg training loss: 6.5193
batch: [7850/10547] batch time: 0.047 trainign loss: 3.5446 avg training loss: 6.5190
batch: [7860/10547] batch time: 0.671 trainign loss: 5.4964 avg training loss: 6.5187
batch: [7870/10547] batch time: 0.047 trainign loss: 6.1302 avg training loss: 6.5186
batch: [7880/10547] batch time: 0.909 trainign loss: 5.4583 avg training loss: 6.5183
batch: [7890/10547] batch time: 0.048 trainign loss: 5.8256 avg training loss: 6.5182
batch: [7900/10547] batch time: 1.947 trainign loss: 3.1105 avg training loss: 6.5177
batch: [7910/10547] batch time: 0.045 trainign loss: 5.9022 avg training loss: 6.5173
batch: [7920/10547] batch time: 2.111 trainign loss: 6.3648 avg training loss: 6.5171
batch: [7930/10547] batch time: 0.050 trainign loss: 6.0417 avg training loss: 6.5170
batch: [7940/10547] batch time: 2.848 trainign loss: 3.5442 avg training loss: 6.5167
batch: [7950/10547] batch time: 0.047 trainign loss: 7.5132 avg training loss: 6.5162
batch: [7960/10547] batch time: 2.134 trainign loss: 5.5231 avg training loss: 6.5161
batch: [7970/10547] batch time: 0.047 trainign loss: 6.4455 avg training loss: 6.5159
batch: [7980/10547] batch time: 2.361 trainign loss: 6.1280 avg training loss: 6.5157
batch: [7990/10547] batch time: 0.047 trainign loss: 5.8461 avg training loss: 6.5155
batch: [8000/10547] batch time: 2.236 trainign loss: 5.1514 avg training loss: 6.5151
batch: [8010/10547] batch time: 0.045 trainign loss: 5.1430 avg training loss: 6.5148
batch: [8020/10547] batch time: 2.535 trainign loss: 5.8844 avg training loss: 6.5144
batch: [8030/10547] batch time: 0.054 trainign loss: 6.2864 avg training loss: 6.5143
batch: [8040/10547] batch time: 1.896 trainign loss: 5.0617 avg training loss: 6.5141
batch: [8050/10547] batch time: 0.046 trainign loss: 5.4885 avg training loss: 6.5138
batch: [8060/10547] batch time: 2.102 trainign loss: 6.3502 avg training loss: 6.5137
batch: [8070/10547] batch time: 0.046 trainign loss: 5.2444 avg training loss: 6.5134
batch: [8080/10547] batch time: 1.749 trainign loss: 5.9834 avg training loss: 6.5133
batch: [8090/10547] batch time: 0.046 trainign loss: 5.3239 avg training loss: 6.5129
batch: [8100/10547] batch time: 2.246 trainign loss: 5.3767 avg training loss: 6.5123
batch: [8110/10547] batch time: 0.051 trainign loss: 5.2100 avg training loss: 6.5121
batch: [8120/10547] batch time: 1.433 trainign loss: 6.4073 avg training loss: 6.5120
batch: [8130/10547] batch time: 0.048 trainign loss: 5.3903 avg training loss: 6.5118
batch: [8140/10547] batch time: 1.617 trainign loss: 5.2968 avg training loss: 6.5117
batch: [8150/10547] batch time: 0.047 trainign loss: 5.5254 avg training loss: 6.5113
batch: [8160/10547] batch time: 1.251 trainign loss: 5.9471 avg training loss: 6.5111
batch: [8170/10547] batch time: 0.046 trainign loss: 3.2985 avg training loss: 6.5107
batch: [8180/10547] batch time: 0.047 trainign loss: 5.9113 avg training loss: 6.5103
batch: [8190/10547] batch time: 0.046 trainign loss: 5.6185 avg training loss: 6.5102
batch: [8200/10547] batch time: 0.048 trainign loss: 5.0262 avg training loss: 6.5099
batch: [8210/10547] batch time: 0.044 trainign loss: 1.2380 avg training loss: 6.5091
batch: [8220/10547] batch time: 0.046 trainign loss: 6.5035 avg training loss: 6.5087
batch: [8230/10547] batch time: 0.046 trainign loss: 6.2820 avg training loss: 6.5087
batch: [8240/10547] batch time: 0.046 trainign loss: 5.4595 avg training loss: 6.5084
batch: [8250/10547] batch time: 0.042 trainign loss: 5.8576 avg training loss: 6.5081
batch: [8260/10547] batch time: 0.048 trainign loss: 4.9519 avg training loss: 6.5079
batch: [8270/10547] batch time: 0.046 trainign loss: 6.1775 avg training loss: 6.5078
batch: [8280/10547] batch time: 0.046 trainign loss: 6.0414 avg training loss: 6.5077
batch: [8290/10547] batch time: 0.048 trainign loss: 4.2949 avg training loss: 6.5072
batch: [8300/10547] batch time: 0.046 trainign loss: 5.0214 avg training loss: 6.5068
batch: [8310/10547] batch time: 0.046 trainign loss: 5.8767 avg training loss: 6.5065
batch: [8320/10547] batch time: 0.046 trainign loss: 5.9410 avg training loss: 6.5063
batch: [8330/10547] batch time: 0.045 trainign loss: 6.2917 avg training loss: 6.5060
batch: [8340/10547] batch time: 0.046 trainign loss: 4.0938 avg training loss: 6.5057
batch: [8350/10547] batch time: 0.053 trainign loss: 0.0061 avg training loss: 6.5042
batch: [8360/10547] batch time: 0.046 trainign loss: 7.1204 avg training loss: 6.5045
batch: [8370/10547] batch time: 0.047 trainign loss: 6.9698 avg training loss: 6.5044
batch: [8380/10547] batch time: 0.049 trainign loss: 5.0998 avg training loss: 6.5041
batch: [8390/10547] batch time: 0.046 trainign loss: 2.8304 avg training loss: 6.5037
batch: [8400/10547] batch time: 0.046 trainign loss: 5.3719 avg training loss: 6.5031
batch: [8410/10547] batch time: 0.046 trainign loss: 5.2121 avg training loss: 6.5027
batch: [8420/10547] batch time: 0.362 trainign loss: 4.5658 avg training loss: 6.5016
batch: [8430/10547] batch time: 0.047 trainign loss: 5.4621 avg training loss: 6.5018
batch: [8440/10547] batch time: 0.432 trainign loss: 5.0661 avg training loss: 6.5017
batch: [8450/10547] batch time: 0.055 trainign loss: 6.5045 avg training loss: 6.5014
batch: [8460/10547] batch time: 0.056 trainign loss: 5.3965 avg training loss: 6.5010
batch: [8470/10547] batch time: 0.043 trainign loss: 6.9329 avg training loss: 6.5010
batch: [8480/10547] batch time: 0.268 trainign loss: 5.5098 avg training loss: 6.5010
batch: [8490/10547] batch time: 0.046 trainign loss: 5.5533 avg training loss: 6.5004
batch: [8500/10547] batch time: 0.231 trainign loss: 3.5065 avg training loss: 6.5000
batch: [8510/10547] batch time: 0.045 trainign loss: 5.6127 avg training loss: 6.4998
batch: [8520/10547] batch time: 0.728 trainign loss: 3.2857 avg training loss: 6.4993
batch: [8530/10547] batch time: 0.046 trainign loss: 2.4847 avg training loss: 6.4987
batch: [8540/10547] batch time: 0.046 trainign loss: 6.7423 avg training loss: 6.4988
batch: [8550/10547] batch time: 0.042 trainign loss: 5.8797 avg training loss: 6.4986
batch: [8560/10547] batch time: 0.046 trainign loss: 0.3785 avg training loss: 6.4977
batch: [8570/10547] batch time: 0.047 trainign loss: 8.1290 avg training loss: 6.4972
batch: [8580/10547] batch time: 0.056 trainign loss: 5.5892 avg training loss: 6.4971
batch: [8590/10547] batch time: 0.047 trainign loss: 7.0362 avg training loss: 6.4970
batch: [8600/10547] batch time: 0.054 trainign loss: 6.6799 avg training loss: 6.4970
batch: [8610/10547] batch time: 0.087 trainign loss: 6.2730 avg training loss: 6.4969
batch: [8620/10547] batch time: 0.448 trainign loss: 4.8802 avg training loss: 6.4966
batch: [8630/10547] batch time: 0.055 trainign loss: 3.0001 avg training loss: 6.4961
batch: [8640/10547] batch time: 0.048 trainign loss: 7.0467 avg training loss: 6.4960
batch: [8650/10547] batch time: 0.055 trainign loss: 5.4993 avg training loss: 6.4958
batch: [8660/10547] batch time: 0.306 trainign loss: 5.3431 avg training loss: 6.4955
batch: [8670/10547] batch time: 1.684 trainign loss: 5.4016 avg training loss: 6.4954
batch: [8680/10547] batch time: 0.117 trainign loss: 4.0215 avg training loss: 6.4949
batch: [8690/10547] batch time: 2.071 trainign loss: 0.0090 avg training loss: 6.4935
batch: [8700/10547] batch time: 0.051 trainign loss: 8.4885 avg training loss: 6.4933
batch: [8710/10547] batch time: 1.529 trainign loss: 6.3461 avg training loss: 6.4935
batch: [8720/10547] batch time: 0.385 trainign loss: 6.8262 avg training loss: 6.4935
batch: [8730/10547] batch time: 1.096 trainign loss: 5.8573 avg training loss: 6.4934
batch: [8740/10547] batch time: 0.391 trainign loss: 4.7913 avg training loss: 6.4931
batch: [8750/10547] batch time: 0.662 trainign loss: 5.3573 avg training loss: 6.4928
batch: [8760/10547] batch time: 1.125 trainign loss: 6.5199 avg training loss: 6.4924
batch: [8770/10547] batch time: 0.046 trainign loss: 6.3278 avg training loss: 6.4924
batch: [8780/10547] batch time: 1.715 trainign loss: 5.4661 avg training loss: 6.4921
batch: [8790/10547] batch time: 0.052 trainign loss: 6.1996 avg training loss: 6.4919
batch: [8800/10547] batch time: 1.963 trainign loss: 6.0434 avg training loss: 6.4918
batch: [8810/10547] batch time: 0.048 trainign loss: 5.6827 avg training loss: 6.4917
batch: [8820/10547] batch time: 2.966 trainign loss: 5.7550 avg training loss: 6.4915
batch: [8830/10547] batch time: 0.045 trainign loss: 5.2433 avg training loss: 6.4912
batch: [8840/10547] batch time: 2.448 trainign loss: 5.1108 avg training loss: 6.4908
batch: [8850/10547] batch time: 0.045 trainign loss: 5.6242 avg training loss: 6.4907
batch: [8860/10547] batch time: 2.209 trainign loss: 5.1649 avg training loss: 6.4905
batch: [8870/10547] batch time: 0.049 trainign loss: 5.5912 avg training loss: 6.4901
batch: [8880/10547] batch time: 2.419 trainign loss: 4.6865 avg training loss: 6.4899
batch: [8890/10547] batch time: 0.046 trainign loss: 4.9995 avg training loss: 6.4895
batch: [8900/10547] batch time: 2.053 trainign loss: 2.9668 avg training loss: 6.4891
batch: [8910/10547] batch time: 0.051 trainign loss: 5.2798 avg training loss: 6.4885
batch: [8920/10547] batch time: 2.452 trainign loss: 5.0785 avg training loss: 6.4884
batch: [8930/10547] batch time: 0.048 trainign loss: 6.4960 avg training loss: 6.4882
batch: [8940/10547] batch time: 2.131 trainign loss: 5.0914 avg training loss: 6.4880
batch: [8950/10547] batch time: 0.046 trainign loss: 5.0987 avg training loss: 6.4877
batch: [8960/10547] batch time: 2.194 trainign loss: 5.0694 avg training loss: 6.4875
batch: [8970/10547] batch time: 0.055 trainign loss: 3.7936 avg training loss: 6.4872
batch: [8980/10547] batch time: 2.605 trainign loss: 5.4541 avg training loss: 6.4867
batch: [8990/10547] batch time: 0.053 trainign loss: 5.1995 avg training loss: 6.4865
batch: [9000/10547] batch time: 2.153 trainign loss: 0.5439 avg training loss: 6.4858
batch: [9010/10547] batch time: 0.054 trainign loss: 7.1005 avg training loss: 6.4856
batch: [9020/10547] batch time: 1.962 trainign loss: 4.6375 avg training loss: 6.4854
batch: [9030/10547] batch time: 0.054 trainign loss: 4.7336 avg training loss: 6.4852
batch: [9040/10547] batch time: 2.537 trainign loss: 5.1365 avg training loss: 6.4851
batch: [9050/10547] batch time: 0.046 trainign loss: 4.8153 avg training loss: 6.4846
batch: [9060/10547] batch time: 2.536 trainign loss: 5.6545 avg training loss: 6.4843
batch: [9070/10547] batch time: 0.048 trainign loss: 4.0542 avg training loss: 6.4841
batch: [9080/10547] batch time: 2.548 trainign loss: 6.0881 avg training loss: 6.4838
batch: [9090/10547] batch time: 0.051 trainign loss: 4.8248 avg training loss: 6.4837
batch: [9100/10547] batch time: 2.317 trainign loss: 3.8162 avg training loss: 6.4834
batch: [9110/10547] batch time: 0.047 trainign loss: 5.8483 avg training loss: 6.4832
batch: [9120/10547] batch time: 2.431 trainign loss: 5.8169 avg training loss: 6.4829
batch: [9130/10547] batch time: 0.047 trainign loss: 0.0557 avg training loss: 6.4818
batch: [9140/10547] batch time: 2.783 trainign loss: 7.8894 avg training loss: 6.4817
batch: [9150/10547] batch time: 0.046 trainign loss: 6.9134 avg training loss: 6.4819
batch: [9160/10547] batch time: 2.546 trainign loss: 2.6529 avg training loss: 6.4815
batch: [9170/10547] batch time: 0.050 trainign loss: 0.0071 avg training loss: 6.4800
batch: [9180/10547] batch time: 2.023 trainign loss: 7.7178 avg training loss: 6.4805
batch: [9190/10547] batch time: 0.046 trainign loss: 5.9737 avg training loss: 6.4805
batch: [9200/10547] batch time: 2.233 trainign loss: 5.5138 avg training loss: 6.4801
batch: [9210/10547] batch time: 0.046 trainign loss: 5.5269 avg training loss: 6.4799
batch: [9220/10547] batch time: 2.335 trainign loss: 3.7760 avg training loss: 6.4796
batch: [9230/10547] batch time: 0.053 trainign loss: 6.2223 avg training loss: 6.4792
batch: [9240/10547] batch time: 2.331 trainign loss: 5.4845 avg training loss: 6.4791
batch: [9250/10547] batch time: 0.047 trainign loss: 0.0378 avg training loss: 6.4780
batch: [9260/10547] batch time: 2.089 trainign loss: 10.4570 avg training loss: 6.4770
batch: [9270/10547] batch time: 0.048 trainign loss: 7.2417 avg training loss: 6.4772
batch: [9280/10547] batch time: 2.697 trainign loss: 6.7438 avg training loss: 6.4773
batch: [9290/10547] batch time: 0.047 trainign loss: 6.3714 avg training loss: 6.4771
batch: [9300/10547] batch time: 2.623 trainign loss: 5.3590 avg training loss: 6.4768
batch: [9310/10547] batch time: 0.047 trainign loss: 4.9273 avg training loss: 6.4764
batch: [9320/10547] batch time: 2.417 trainign loss: 6.4095 avg training loss: 6.4761
batch: [9330/10547] batch time: 0.046 trainign loss: 6.2458 avg training loss: 6.4759
batch: [9340/10547] batch time: 2.149 trainign loss: 3.3604 avg training loss: 6.4756
batch: [9350/10547] batch time: 0.045 trainign loss: 6.3201 avg training loss: 6.4753
batch: [9360/10547] batch time: 2.013 trainign loss: 7.3230 avg training loss: 6.4750
batch: [9370/10547] batch time: 0.047 trainign loss: 6.4592 avg training loss: 6.4750
batch: [9380/10547] batch time: 2.238 trainign loss: 5.2299 avg training loss: 6.4749
batch: [9390/10547] batch time: 0.047 trainign loss: 6.5871 avg training loss: 6.4745
batch: [9400/10547] batch time: 2.490 trainign loss: 2.6208 avg training loss: 6.4742
batch: [9410/10547] batch time: 0.053 trainign loss: 5.0875 avg training loss: 6.4740
batch: [9420/10547] batch time: 2.155 trainign loss: 6.4370 avg training loss: 6.4740
batch: [9430/10547] batch time: 0.055 trainign loss: 5.5635 avg training loss: 6.4738
batch: [9440/10547] batch time: 2.110 trainign loss: 4.7931 avg training loss: 6.4736
batch: [9450/10547] batch time: 0.046 trainign loss: 3.9561 avg training loss: 6.4731
batch: [9460/10547] batch time: 2.145 trainign loss: 6.6583 avg training loss: 6.4731
batch: [9470/10547] batch time: 0.054 trainign loss: 3.6199 avg training loss: 6.4729
batch: [9480/10547] batch time: 1.891 trainign loss: 4.4725 avg training loss: 6.4727
batch: [9490/10547] batch time: 0.055 trainign loss: 6.6707 avg training loss: 6.4726
batch: [9500/10547] batch time: 2.463 trainign loss: 5.2601 avg training loss: 6.4725
batch: [9510/10547] batch time: 0.046 trainign loss: 5.1385 avg training loss: 6.4721
batch: [9520/10547] batch time: 1.780 trainign loss: 4.4273 avg training loss: 6.4718
batch: [9530/10547] batch time: 0.048 trainign loss: 6.3636 avg training loss: 6.4714
batch: [9540/10547] batch time: 1.941 trainign loss: 2.7947 avg training loss: 6.4711
batch: [9550/10547] batch time: 0.047 trainign loss: 5.3027 avg training loss: 6.4706
batch: [9560/10547] batch time: 2.186 trainign loss: 6.8834 avg training loss: 6.4705
batch: [9570/10547] batch time: 0.047 trainign loss: 6.2762 avg training loss: 6.4704
batch: [9580/10547] batch time: 1.789 trainign loss: 5.8164 avg training loss: 6.4702
batch: [9590/10547] batch time: 0.046 trainign loss: 3.1680 avg training loss: 6.4699
batch: [9600/10547] batch time: 2.408 trainign loss: 5.7351 avg training loss: 6.4697
batch: [9610/10547] batch time: 0.048 trainign loss: 5.3526 avg training loss: 6.4696
batch: [9620/10547] batch time: 1.865 trainign loss: 5.3513 avg training loss: 6.4693
batch: [9630/10547] batch time: 0.046 trainign loss: 6.2639 avg training loss: 6.4694
batch: [9640/10547] batch time: 2.187 trainign loss: 5.9587 avg training loss: 6.4692
batch: [9650/10547] batch time: 0.395 trainign loss: 5.8220 avg training loss: 6.4690
batch: [9660/10547] batch time: 2.119 trainign loss: 5.5732 avg training loss: 6.4688
batch: [9670/10547] batch time: 0.366 trainign loss: 5.6380 avg training loss: 6.4685
batch: [9680/10547] batch time: 1.705 trainign loss: 5.6354 avg training loss: 6.4683
batch: [9690/10547] batch time: 0.803 trainign loss: 5.2738 avg training loss: 6.4680
batch: [9700/10547] batch time: 2.234 trainign loss: 5.4421 avg training loss: 6.4679
batch: [9710/10547] batch time: 0.084 trainign loss: 5.5757 avg training loss: 6.4677
batch: [9720/10547] batch time: 2.534 trainign loss: 5.9112 avg training loss: 6.4675
batch: [9730/10547] batch time: 0.047 trainign loss: 0.4651 avg training loss: 6.4668
batch: [9740/10547] batch time: 2.201 trainign loss: 6.3790 avg training loss: 6.4665
batch: [9750/10547] batch time: 0.046 trainign loss: 7.0754 avg training loss: 6.4666
batch: [9760/10547] batch time: 2.411 trainign loss: 6.7590 avg training loss: 6.4666
batch: [9770/10547] batch time: 0.054 trainign loss: 5.5504 avg training loss: 6.4665
batch: [9780/10547] batch time: 2.166 trainign loss: 6.1674 avg training loss: 6.4664
batch: [9790/10547] batch time: 0.050 trainign loss: 5.4274 avg training loss: 6.4661
batch: [9800/10547] batch time: 2.598 trainign loss: 5.7085 avg training loss: 6.4656
batch: [9810/10547] batch time: 0.048 trainign loss: 6.0798 avg training loss: 6.4655
batch: [9820/10547] batch time: 2.189 trainign loss: 6.3967 avg training loss: 6.4653
batch: [9830/10547] batch time: 0.560 trainign loss: 5.0587 avg training loss: 6.4651
batch: [9840/10547] batch time: 2.111 trainign loss: 0.2492 avg training loss: 6.4642
batch: [9850/10547] batch time: 0.585 trainign loss: 0.0329 avg training loss: 6.4633
batch: [9860/10547] batch time: 1.943 trainign loss: 6.5362 avg training loss: 6.4634
batch: [9870/10547] batch time: 0.054 trainign loss: 4.6623 avg training loss: 6.4631
batch: [9880/10547] batch time: 2.278 trainign loss: 5.3277 avg training loss: 6.4628
batch: [9890/10547] batch time: 0.047 trainign loss: 6.4342 avg training loss: 6.4627
batch: [9900/10547] batch time: 2.325 trainign loss: 5.0948 avg training loss: 6.4624
batch: [9910/10547] batch time: 0.046 trainign loss: 1.6258 avg training loss: 6.4619
batch: [9920/10547] batch time: 2.529 trainign loss: 4.4721 avg training loss: 6.4614
batch: [9930/10547] batch time: 0.047 trainign loss: 6.2522 avg training loss: 6.4611
batch: [9940/10547] batch time: 2.223 trainign loss: 6.8806 avg training loss: 6.4606
batch: [9950/10547] batch time: 0.054 trainign loss: 6.7525 avg training loss: 6.4604
batch: [9960/10547] batch time: 2.498 trainign loss: 4.1979 avg training loss: 6.4603
batch: [9970/10547] batch time: 0.046 trainign loss: 8.1823 avg training loss: 6.4596
batch: [9980/10547] batch time: 2.615 trainign loss: 6.7985 avg training loss: 6.4596
batch: [9990/10547] batch time: 0.046 trainign loss: 5.5062 avg training loss: 6.4594
batch: [10000/10547] batch time: 2.048 trainign loss: 6.2442 avg training loss: 6.4592
batch: [10010/10547] batch time: 0.047 trainign loss: 5.3301 avg training loss: 6.4590
batch: [10020/10547] batch time: 2.734 trainign loss: 5.8637 avg training loss: 6.4586
batch: [10030/10547] batch time: 0.046 trainign loss: 6.1826 avg training loss: 6.4584
batch: [10040/10547] batch time: 2.378 trainign loss: 3.6745 avg training loss: 6.4581
batch: [10050/10547] batch time: 0.054 trainign loss: 4.5528 avg training loss: 6.4578
batch: [10060/10547] batch time: 2.390 trainign loss: 8.2967 avg training loss: 6.4572
batch: [10070/10547] batch time: 0.047 trainign loss: 6.1220 avg training loss: 6.4570
batch: [10080/10547] batch time: 2.298 trainign loss: 4.6150 avg training loss: 6.4568
batch: [10090/10547] batch time: 0.046 trainign loss: 6.6589 avg training loss: 6.4564
batch: [10100/10547] batch time: 2.678 trainign loss: 4.1753 avg training loss: 6.4562
batch: [10110/10547] batch time: 0.045 trainign loss: 5.5646 avg training loss: 6.4558
batch: [10120/10547] batch time: 2.507 trainign loss: 5.1576 avg training loss: 6.4557
batch: [10130/10547] batch time: 0.055 trainign loss: 4.7966 avg training loss: 6.4551
batch: [10140/10547] batch time: 2.490 trainign loss: 3.5762 avg training loss: 6.4547
batch: [10150/10547] batch time: 0.046 trainign loss: 4.8408 avg training loss: 6.4546
batch: [10160/10547] batch time: 1.871 trainign loss: 0.0112 avg training loss: 6.4533
batch: [10170/10547] batch time: 0.054 trainign loss: 8.2071 avg training loss: 6.4529
batch: [10180/10547] batch time: 2.302 trainign loss: 7.1333 avg training loss: 6.4532
batch: [10190/10547] batch time: 0.046 trainign loss: 2.0358 avg training loss: 6.4528
batch: [10200/10547] batch time: 2.419 trainign loss: 6.5239 avg training loss: 6.4529
batch: [10210/10547] batch time: 0.046 trainign loss: 6.1488 avg training loss: 6.4527
batch: [10220/10547] batch time: 2.371 trainign loss: 5.6281 avg training loss: 6.4524
batch: [10230/10547] batch time: 0.047 trainign loss: 6.1762 avg training loss: 6.4524
batch: [10240/10547] batch time: 2.300 trainign loss: 5.2297 avg training loss: 6.4522
batch: [10250/10547] batch time: 0.054 trainign loss: 5.2565 avg training loss: 6.4519
batch: [10260/10547] batch time: 2.152 trainign loss: 5.1501 avg training loss: 6.4515
batch: [10270/10547] batch time: 0.046 trainign loss: 6.3565 avg training loss: 6.4514
batch: [10280/10547] batch time: 2.320 trainign loss: 6.5157 avg training loss: 6.4514
batch: [10290/10547] batch time: 0.045 trainign loss: 5.1049 avg training loss: 6.4512
batch: [10300/10547] batch time: 2.319 trainign loss: 6.0551 avg training loss: 6.4510
batch: [10310/10547] batch time: 0.046 trainign loss: 5.9413 avg training loss: 6.4508
batch: [10320/10547] batch time: 1.546 trainign loss: 6.5757 avg training loss: 6.4507
batch: [10330/10547] batch time: 0.053 trainign loss: 4.6101 avg training loss: 6.4504
batch: [10340/10547] batch time: 1.012 trainign loss: 5.8745 avg training loss: 6.4502
batch: [10350/10547] batch time: 0.047 trainign loss: 6.0941 avg training loss: 6.4500
batch: [10360/10547] batch time: 1.748 trainign loss: 5.1980 avg training loss: 6.4496
batch: [10370/10547] batch time: 0.055 trainign loss: 1.0488 avg training loss: 6.4490
batch: [10380/10547] batch time: 2.464 trainign loss: 3.5490 avg training loss: 6.4484
batch: [10390/10547] batch time: 0.055 trainign loss: 6.4641 avg training loss: 6.4482
batch: [10400/10547] batch time: 2.218 trainign loss: 6.2655 avg training loss: 6.4481
batch: [10410/10547] batch time: 0.046 trainign loss: 6.4180 avg training loss: 6.4480
batch: [10420/10547] batch time: 2.210 trainign loss: 6.1435 avg training loss: 6.4479
batch: [10430/10547] batch time: 0.046 trainign loss: 5.5706 avg training loss: 6.4478
batch: [10440/10547] batch time: 2.135 trainign loss: 4.6874 avg training loss: 6.4474
batch: [10450/10547] batch time: 0.048 trainign loss: 4.4702 avg training loss: 6.4470
batch: [10460/10547] batch time: 1.585 trainign loss: 6.0652 avg training loss: 6.4467
batch: [10470/10547] batch time: 0.049 trainign loss: 5.5747 avg training loss: 6.4466
batch: [10480/10547] batch time: 1.619 trainign loss: 2.9679 avg training loss: 6.4463
batch: [10490/10547] batch time: 0.047 trainign loss: 6.6965 avg training loss: 6.4460
batch: [10500/10547] batch time: 1.483 trainign loss: 6.3670 avg training loss: 6.4460
batch: [10510/10547] batch time: 0.049 trainign loss: 6.3008 avg training loss: 6.4459
batch: [10520/10547] batch time: 2.043 trainign loss: 5.2191 avg training loss: 6.4458
batch: [10530/10547] batch time: 0.046 trainign loss: 5.8433 avg training loss: 6.4454
batch: [10540/10547] batch time: 2.198 trainign loss: 5.4033 avg training loss: 6.4451
Epoch: 5
----------------------------------------------------------------------
batch: [0/10547] batch time: 3.028 trainign loss: 6.0516 avg training loss: 6.4450
batch: [10/10547] batch time: 0.046 trainign loss: 6.9139 avg training loss: 6.4445
batch: [20/10547] batch time: 2.179 trainign loss: 6.6929 avg training loss: 6.4444
batch: [30/10547] batch time: 0.047 trainign loss: 5.8037 avg training loss: 6.4443
batch: [40/10547] batch time: 1.609 trainign loss: 6.4311 avg training loss: 6.4441
batch: [50/10547] batch time: 0.665 trainign loss: 6.3764 avg training loss: 6.4440
batch: [60/10547] batch time: 2.166 trainign loss: 5.0285 avg training loss: 6.4438
batch: [70/10547] batch time: 0.047 trainign loss: 5.6302 avg training loss: 6.4435
batch: [80/10547] batch time: 1.796 trainign loss: 6.1974 avg training loss: 6.4431
batch: [90/10547] batch time: 0.055 trainign loss: 5.4826 avg training loss: 6.4430
batch: [100/10547] batch time: 2.200 trainign loss: 2.3626 avg training loss: 6.4426
batch: [110/10547] batch time: 0.047 trainign loss: 0.0031 avg training loss: 6.4411
batch: [120/10547] batch time: 2.143 trainign loss: 7.6390 avg training loss: 6.4415
batch: [130/10547] batch time: 0.047 trainign loss: 6.9485 avg training loss: 6.4416
batch: [140/10547] batch time: 2.041 trainign loss: 5.8107 avg training loss: 6.4417
batch: [150/10547] batch time: 0.046 trainign loss: 3.8709 avg training loss: 6.4413
batch: [160/10547] batch time: 2.217 trainign loss: 5.6390 avg training loss: 6.4409
batch: [170/10547] batch time: 0.046 trainign loss: 5.9335 avg training loss: 6.4407
batch: [180/10547] batch time: 2.413 trainign loss: 6.0422 avg training loss: 6.4407
batch: [190/10547] batch time: 0.046 trainign loss: 5.8815 avg training loss: 6.4405
batch: [200/10547] batch time: 2.263 trainign loss: 5.2171 avg training loss: 6.4402
batch: [210/10547] batch time: 0.054 trainign loss: 5.5730 avg training loss: 6.4398
batch: [220/10547] batch time: 2.170 trainign loss: 6.9255 avg training loss: 6.4399
batch: [230/10547] batch time: 0.054 trainign loss: 5.6290 avg training loss: 6.4397
batch: [240/10547] batch time: 1.888 trainign loss: 6.1121 avg training loss: 6.4394
batch: [250/10547] batch time: 0.857 trainign loss: 6.4937 avg training loss: 6.4393
batch: [260/10547] batch time: 1.146 trainign loss: 6.1137 avg training loss: 6.4393
batch: [270/10547] batch time: 0.844 trainign loss: 4.9420 avg training loss: 6.4389
batch: [280/10547] batch time: 1.973 trainign loss: 5.6095 avg training loss: 6.4388
batch: [290/10547] batch time: 1.415 trainign loss: 4.6301 avg training loss: 6.4384
batch: [300/10547] batch time: 1.287 trainign loss: 5.9477 avg training loss: 6.4382
batch: [310/10547] batch time: 1.581 trainign loss: 5.5941 avg training loss: 6.4380
batch: [320/10547] batch time: 0.682 trainign loss: 6.5666 avg training loss: 6.4380
batch: [330/10547] batch time: 1.385 trainign loss: 5.4099 avg training loss: 6.4379
batch: [340/10547] batch time: 0.921 trainign loss: 5.3557 avg training loss: 6.4375
batch: [350/10547] batch time: 1.262 trainign loss: 6.2713 avg training loss: 6.4370
batch: [360/10547] batch time: 0.341 trainign loss: 6.7014 avg training loss: 6.4369
batch: [370/10547] batch time: 0.502 trainign loss: 6.3774 avg training loss: 6.4368
batch: [380/10547] batch time: 0.052 trainign loss: 5.9969 avg training loss: 6.4368
batch: [390/10547] batch time: 0.736 trainign loss: 6.1507 avg training loss: 6.4367
batch: [400/10547] batch time: 0.046 trainign loss: 5.7339 avg training loss: 6.4364
batch: [410/10547] batch time: 1.306 trainign loss: 4.9449 avg training loss: 6.4362
batch: [420/10547] batch time: 0.045 trainign loss: 5.8727 avg training loss: 6.4359
batch: [430/10547] batch time: 1.393 trainign loss: 5.8722 avg training loss: 6.4357
batch: [440/10547] batch time: 0.047 trainign loss: 3.3284 avg training loss: 6.4353
batch: [450/10547] batch time: 0.052 trainign loss: 5.8994 avg training loss: 6.4350
batch: [460/10547] batch time: 0.049 trainign loss: 5.4221 avg training loss: 6.4348
batch: [470/10547] batch time: 0.047 trainign loss: 0.8763 avg training loss: 6.4342
batch: [480/10547] batch time: 0.046 trainign loss: 10.1282 avg training loss: 6.4331
batch: [490/10547] batch time: 0.046 trainign loss: 7.2864 avg training loss: 6.4335
batch: [500/10547] batch time: 0.042 trainign loss: 6.5750 avg training loss: 6.4334
batch: [510/10547] batch time: 1.023 trainign loss: 5.5252 avg training loss: 6.4333
batch: [520/10547] batch time: 0.046 trainign loss: 5.6430 avg training loss: 6.4331
batch: [530/10547] batch time: 1.007 trainign loss: 0.3918 avg training loss: 6.4324
batch: [540/10547] batch time: 0.046 trainign loss: 6.4879 avg training loss: 6.4319
batch: [550/10547] batch time: 1.255 trainign loss: 6.8360 avg training loss: 6.4319
batch: [560/10547] batch time: 0.055 trainign loss: 6.5588 avg training loss: 6.4319
batch: [570/10547] batch time: 2.012 trainign loss: 5.6597 avg training loss: 6.4318
batch: [580/10547] batch time: 0.047 trainign loss: 5.5587 avg training loss: 6.4317
batch: [590/10547] batch time: 2.090 trainign loss: 6.1055 avg training loss: 6.4314
batch: [600/10547] batch time: 0.047 trainign loss: 4.7859 avg training loss: 6.4312
batch: [610/10547] batch time: 2.344 trainign loss: 5.8047 avg training loss: 6.4310
batch: [620/10547] batch time: 0.046 trainign loss: 5.4097 avg training loss: 6.4309
batch: [630/10547] batch time: 2.380 trainign loss: 5.5621 avg training loss: 6.4307
batch: [640/10547] batch time: 0.046 trainign loss: 3.0371 avg training loss: 6.4304
batch: [650/10547] batch time: 2.174 trainign loss: 4.9828 avg training loss: 6.4301
batch: [660/10547] batch time: 0.053 trainign loss: 5.3433 avg training loss: 6.4298
batch: [670/10547] batch time: 0.775 trainign loss: 5.1738 avg training loss: 6.4294
batch: [680/10547] batch time: 0.047 trainign loss: 6.1741 avg training loss: 6.4293
batch: [690/10547] batch time: 1.568 trainign loss: 5.6159 avg training loss: 6.4292
batch: [700/10547] batch time: 0.046 trainign loss: 4.9074 avg training loss: 6.4289
batch: [710/10547] batch time: 0.715 trainign loss: 5.8342 avg training loss: 6.4287
batch: [720/10547] batch time: 0.047 trainign loss: 3.2398 avg training loss: 6.4283
batch: [730/10547] batch time: 1.018 trainign loss: 8.8033 avg training loss: 6.4271
batch: [740/10547] batch time: 0.057 trainign loss: 4.9085 avg training loss: 6.4274
batch: [750/10547] batch time: 0.046 trainign loss: 4.6501 avg training loss: 6.4272
batch: [760/10547] batch time: 0.247 trainign loss: 1.4991 avg training loss: 6.4265
batch: [770/10547] batch time: 0.046 trainign loss: 6.8445 avg training loss: 6.4266
batch: [780/10547] batch time: 1.005 trainign loss: 6.4486 avg training loss: 6.4266
batch: [790/10547] batch time: 0.053 trainign loss: 5.5651 avg training loss: 6.4265
batch: [800/10547] batch time: 0.696 trainign loss: 5.7523 avg training loss: 6.4264
batch: [810/10547] batch time: 0.054 trainign loss: 4.4375 avg training loss: 6.4261
batch: [820/10547] batch time: 1.245 trainign loss: 5.6273 avg training loss: 6.4258
batch: [830/10547] batch time: 0.054 trainign loss: 6.3029 avg training loss: 6.4258
batch: [840/10547] batch time: 0.843 trainign loss: 5.1055 avg training loss: 6.4256
batch: [850/10547] batch time: 0.049 trainign loss: 5.3365 avg training loss: 6.4254
batch: [860/10547] batch time: 1.273 trainign loss: 5.2531 avg training loss: 6.4250
batch: [870/10547] batch time: 0.047 trainign loss: 6.3303 avg training loss: 6.4249
batch: [880/10547] batch time: 1.047 trainign loss: 4.6919 avg training loss: 6.4248
batch: [890/10547] batch time: 0.047 trainign loss: 4.0149 avg training loss: 6.4241
batch: [900/10547] batch time: 0.844 trainign loss: 6.0060 avg training loss: 6.4241
batch: [910/10547] batch time: 0.050 trainign loss: 5.3289 avg training loss: 6.4240
batch: [920/10547] batch time: 0.494 trainign loss: 5.8351 avg training loss: 6.4237
batch: [930/10547] batch time: 0.052 trainign loss: 4.3900 avg training loss: 6.4235
batch: [940/10547] batch time: 0.042 trainign loss: 5.3153 avg training loss: 6.4231
batch: [950/10547] batch time: 0.046 trainign loss: 5.1534 avg training loss: 6.4229
batch: [960/10547] batch time: 0.053 trainign loss: 0.7217 avg training loss: 6.4222
batch: [970/10547] batch time: 0.047 trainign loss: 6.4082 avg training loss: 6.4220
batch: [980/10547] batch time: 0.042 trainign loss: 4.4123 avg training loss: 6.4216
batch: [990/10547] batch time: 0.046 trainign loss: 0.6628 avg training loss: 6.4208
batch: [1000/10547] batch time: 0.055 trainign loss: 8.2755 avg training loss: 6.4196
batch: [1010/10547] batch time: 0.046 trainign loss: 5.8880 avg training loss: 6.4197
batch: [1020/10547] batch time: 0.047 trainign loss: 0.0404 avg training loss: 6.4186
batch: [1030/10547] batch time: 0.047 trainign loss: 0.0003 avg training loss: 6.4171
batch: [1040/10547] batch time: 0.047 trainign loss: 11.6155 avg training loss: 6.4165
batch: [1050/10547] batch time: 0.048 trainign loss: 7.6463 avg training loss: 6.4168
batch: [1060/10547] batch time: 0.046 trainign loss: 6.1249 avg training loss: 6.4169
batch: [1070/10547] batch time: 0.046 trainign loss: 6.2666 avg training loss: 6.4168
batch: [1080/10547] batch time: 0.046 trainign loss: 2.9153 avg training loss: 6.4164
batch: [1090/10547] batch time: 0.047 trainign loss: 5.6867 avg training loss: 6.4162
batch: [1100/10547] batch time: 0.046 trainign loss: 3.8622 avg training loss: 6.4158
batch: [1110/10547] batch time: 0.261 trainign loss: 5.8922 avg training loss: 6.4153
batch: [1120/10547] batch time: 0.046 trainign loss: 5.8880 avg training loss: 6.4148
batch: [1130/10547] batch time: 0.157 trainign loss: 6.2973 avg training loss: 6.4145
batch: [1140/10547] batch time: 0.049 trainign loss: 6.2719 avg training loss: 6.4144
batch: [1150/10547] batch time: 0.566 trainign loss: 5.8093 avg training loss: 6.4144
batch: [1160/10547] batch time: 0.046 trainign loss: 4.7069 avg training loss: 6.4141
batch: [1170/10547] batch time: 1.228 trainign loss: 6.0390 avg training loss: 6.4139
batch: [1180/10547] batch time: 0.046 trainign loss: 5.7910 avg training loss: 6.4138
batch: [1190/10547] batch time: 0.631 trainign loss: 5.6413 avg training loss: 6.4137
batch: [1200/10547] batch time: 0.047 trainign loss: 4.4068 avg training loss: 6.4134
batch: [1210/10547] batch time: 0.633 trainign loss: 5.2327 avg training loss: 6.4132
batch: [1220/10547] batch time: 0.044 trainign loss: 5.5458 avg training loss: 6.4130
batch: [1230/10547] batch time: 0.878 trainign loss: 5.6467 avg training loss: 6.4128
batch: [1240/10547] batch time: 0.056 trainign loss: 5.5203 avg training loss: 6.4126
batch: [1250/10547] batch time: 0.046 trainign loss: 5.2902 avg training loss: 6.4123
batch: [1260/10547] batch time: 0.046 trainign loss: 0.2201 avg training loss: 6.4114
batch: [1270/10547] batch time: 0.053 trainign loss: 6.4183 avg training loss: 6.4114
batch: [1280/10547] batch time: 0.054 trainign loss: 3.2334 avg training loss: 6.4112
batch: [1290/10547] batch time: 0.349 trainign loss: 6.5230 avg training loss: 6.4112
batch: [1300/10547] batch time: 0.047 trainign loss: 5.4387 avg training loss: 6.4110
batch: [1310/10547] batch time: 0.427 trainign loss: 4.5510 avg training loss: 6.4107
batch: [1320/10547] batch time: 0.164 trainign loss: 3.4234 avg training loss: 6.4102
batch: [1330/10547] batch time: 0.290 trainign loss: 5.8542 avg training loss: 6.4098
batch: [1340/10547] batch time: 0.054 trainign loss: 6.5565 avg training loss: 6.4098
batch: [1350/10547] batch time: 0.280 trainign loss: 5.6923 avg training loss: 6.4096
batch: [1360/10547] batch time: 0.046 trainign loss: 5.8551 avg training loss: 6.4095
batch: [1370/10547] batch time: 0.046 trainign loss: 4.6816 avg training loss: 6.4093
batch: [1380/10547] batch time: 0.048 trainign loss: 5.6668 avg training loss: 6.4090
batch: [1390/10547] batch time: 0.050 trainign loss: 7.1816 avg training loss: 6.4084
batch: [1400/10547] batch time: 0.047 trainign loss: 6.6346 avg training loss: 6.4084
batch: [1410/10547] batch time: 0.047 trainign loss: 5.8051 avg training loss: 6.4082
batch: [1420/10547] batch time: 0.048 trainign loss: 5.4220 avg training loss: 6.4080
batch: [1430/10547] batch time: 0.489 trainign loss: 5.7059 avg training loss: 6.4078
batch: [1440/10547] batch time: 0.046 trainign loss: 5.4547 avg training loss: 6.4077
batch: [1450/10547] batch time: 0.049 trainign loss: 1.9983 avg training loss: 6.4071
batch: [1460/10547] batch time: 0.047 trainign loss: 3.5000 avg training loss: 6.4066
batch: [1470/10547] batch time: 0.046 trainign loss: 6.6157 avg training loss: 6.4066
batch: [1480/10547] batch time: 0.046 trainign loss: 5.4723 avg training loss: 6.4063
batch: [1490/10547] batch time: 0.046 trainign loss: 5.4098 avg training loss: 6.4061
batch: [1500/10547] batch time: 0.047 trainign loss: 6.1194 avg training loss: 6.4061
batch: [1510/10547] batch time: 0.054 trainign loss: 5.6872 avg training loss: 6.4059
batch: [1520/10547] batch time: 0.046 trainign loss: 5.0421 avg training loss: 6.4056
batch: [1530/10547] batch time: 0.047 trainign loss: 5.4051 avg training loss: 6.4054
batch: [1540/10547] batch time: 0.047 trainign loss: 1.0309 avg training loss: 6.4048
batch: [1550/10547] batch time: 0.046 trainign loss: 4.9213 avg training loss: 6.4046
batch: [1560/10547] batch time: 0.048 trainign loss: 6.4303 avg training loss: 6.4044
batch: [1570/10547] batch time: 0.047 trainign loss: 5.6601 avg training loss: 6.4043
batch: [1580/10547] batch time: 0.046 trainign loss: 5.9718 avg training loss: 6.4041
batch: [1590/10547] batch time: 0.046 trainign loss: 2.7747 avg training loss: 6.4037
batch: [1600/10547] batch time: 0.046 trainign loss: 5.6284 avg training loss: 6.4035
batch: [1610/10547] batch time: 0.046 trainign loss: 2.0847 avg training loss: 6.4029
batch: [1620/10547] batch time: 0.046 trainign loss: 5.6356 avg training loss: 6.4027
batch: [1630/10547] batch time: 0.052 trainign loss: 1.6500 avg training loss: 6.4022
batch: [1640/10547] batch time: 0.047 trainign loss: 4.9950 avg training loss: 6.4019
batch: [1650/10547] batch time: 0.050 trainign loss: 5.6543 avg training loss: 6.4017
batch: [1660/10547] batch time: 0.048 trainign loss: 4.5572 avg training loss: 6.4014
batch: [1670/10547] batch time: 0.049 trainign loss: 3.1305 avg training loss: 6.4009
batch: [1680/10547] batch time: 0.044 trainign loss: 5.7275 avg training loss: 6.4006
batch: [1690/10547] batch time: 0.047 trainign loss: 6.0567 avg training loss: 6.4005
batch: [1700/10547] batch time: 0.053 trainign loss: 5.8452 avg training loss: 6.4002
batch: [1710/10547] batch time: 0.047 trainign loss: 5.7440 avg training loss: 6.4000
batch: [1720/10547] batch time: 0.046 trainign loss: 5.6785 avg training loss: 6.3998
batch: [1730/10547] batch time: 0.047 trainign loss: 6.0641 avg training loss: 6.3996
batch: [1740/10547] batch time: 0.044 trainign loss: 6.4528 avg training loss: 6.3995
batch: [1750/10547] batch time: 0.047 trainign loss: 5.1339 avg training loss: 6.3994
batch: [1760/10547] batch time: 0.042 trainign loss: 2.0101 avg training loss: 6.3987
batch: [1770/10547] batch time: 0.055 trainign loss: 6.5244 avg training loss: 6.3988
batch: [1780/10547] batch time: 0.054 trainign loss: 5.3957 avg training loss: 6.3985
batch: [1790/10547] batch time: 0.046 trainign loss: 4.4086 avg training loss: 6.3981
batch: [1800/10547] batch time: 0.046 trainign loss: 4.9474 avg training loss: 6.3978
batch: [1810/10547] batch time: 0.053 trainign loss: 6.1968 avg training loss: 6.3977
batch: [1820/10547] batch time: 0.053 trainign loss: 6.0597 avg training loss: 6.3975
batch: [1830/10547] batch time: 0.047 trainign loss: 4.8115 avg training loss: 6.3973
batch: [1840/10547] batch time: 0.042 trainign loss: 6.1067 avg training loss: 6.3971
batch: [1850/10547] batch time: 0.270 trainign loss: 5.0061 avg training loss: 6.3968
batch: [1860/10547] batch time: 0.053 trainign loss: 4.3139 avg training loss: 6.3965
batch: [1870/10547] batch time: 1.216 trainign loss: 6.5087 avg training loss: 6.3963
batch: [1880/10547] batch time: 0.049 trainign loss: 5.4019 avg training loss: 6.3963
batch: [1890/10547] batch time: 0.727 trainign loss: 5.7683 avg training loss: 6.3961
batch: [1900/10547] batch time: 0.046 trainign loss: 2.4903 avg training loss: 6.3957
batch: [1910/10547] batch time: 0.598 trainign loss: 1.2656 avg training loss: 6.3944
batch: [1920/10547] batch time: 0.044 trainign loss: 6.7882 avg training loss: 6.3946
batch: [1930/10547] batch time: 0.048 trainign loss: 6.4402 avg training loss: 6.3947
batch: [1940/10547] batch time: 0.048 trainign loss: 5.7667 avg training loss: 6.3947
batch: [1950/10547] batch time: 0.051 trainign loss: 5.7293 avg training loss: 6.3945
batch: [1960/10547] batch time: 0.054 trainign loss: 5.4319 avg training loss: 6.3940
batch: [1970/10547] batch time: 0.054 trainign loss: 5.7100 avg training loss: 6.3939
batch: [1980/10547] batch time: 0.046 trainign loss: 4.8353 avg training loss: 6.3937
batch: [1990/10547] batch time: 0.053 trainign loss: 0.1587 avg training loss: 6.3928
batch: [2000/10547] batch time: 0.046 trainign loss: 7.7342 avg training loss: 6.3923
batch: [2010/10547] batch time: 0.054 trainign loss: 6.4540 avg training loss: 6.3924
batch: [2020/10547] batch time: 0.046 trainign loss: 5.8735 avg training loss: 6.3922
batch: [2030/10547] batch time: 0.055 trainign loss: 4.6927 avg training loss: 6.3920
batch: [2040/10547] batch time: 0.047 trainign loss: 5.1078 avg training loss: 6.3915
batch: [2050/10547] batch time: 0.048 trainign loss: 5.8979 avg training loss: 6.3914
batch: [2060/10547] batch time: 0.046 trainign loss: 3.6249 avg training loss: 6.3912
batch: [2070/10547] batch time: 0.046 trainign loss: 3.8190 avg training loss: 6.3909
batch: [2080/10547] batch time: 0.046 trainign loss: 4.4595 avg training loss: 6.3904
batch: [2090/10547] batch time: 0.397 trainign loss: 5.1566 avg training loss: 6.3901
batch: [2100/10547] batch time: 0.046 trainign loss: 0.0571 avg training loss: 6.3891
batch: [2110/10547] batch time: 0.046 trainign loss: 6.7726 avg training loss: 6.3892
batch: [2120/10547] batch time: 0.043 trainign loss: 5.0147 avg training loss: 6.3892
batch: [2130/10547] batch time: 0.962 trainign loss: 5.7653 avg training loss: 6.3891
batch: [2140/10547] batch time: 0.045 trainign loss: 5.3133 avg training loss: 6.3889
batch: [2150/10547] batch time: 0.574 trainign loss: 3.6140 avg training loss: 6.3886
batch: [2160/10547] batch time: 0.048 trainign loss: 5.7914 avg training loss: 6.3880
batch: [2170/10547] batch time: 0.509 trainign loss: 6.0234 avg training loss: 6.3878
batch: [2180/10547] batch time: 0.046 trainign loss: 5.2995 avg training loss: 6.3877
batch: [2190/10547] batch time: 2.046 trainign loss: 0.1500 avg training loss: 6.3867
batch: [2200/10547] batch time: 0.047 trainign loss: 5.9025 avg training loss: 6.3865
batch: [2210/10547] batch time: 1.741 trainign loss: 3.8022 avg training loss: 6.3863
batch: [2220/10547] batch time: 0.052 trainign loss: 7.6986 avg training loss: 6.3859
batch: [2230/10547] batch time: 2.023 trainign loss: 4.2482 avg training loss: 6.3857
batch: [2240/10547] batch time: 0.046 trainign loss: 6.1770 avg training loss: 6.3854
batch: [2250/10547] batch time: 2.294 trainign loss: 6.6908 avg training loss: 6.3855
batch: [2260/10547] batch time: 0.046 trainign loss: 5.5542 avg training loss: 6.3854
batch: [2270/10547] batch time: 2.042 trainign loss: 5.4598 avg training loss: 6.3851
batch: [2280/10547] batch time: 0.047 trainign loss: 4.0095 avg training loss: 6.3849
batch: [2290/10547] batch time: 2.027 trainign loss: 6.4028 avg training loss: 6.3846
batch: [2300/10547] batch time: 0.046 trainign loss: 5.2645 avg training loss: 6.3845
batch: [2310/10547] batch time: 2.152 trainign loss: 5.6781 avg training loss: 6.3842
batch: [2320/10547] batch time: 0.046 trainign loss: 6.6387 avg training loss: 6.3839
batch: [2330/10547] batch time: 2.412 trainign loss: 5.5727 avg training loss: 6.3837
batch: [2340/10547] batch time: 0.046 trainign loss: 6.0488 avg training loss: 6.3836
batch: [2350/10547] batch time: 2.145 trainign loss: 4.9052 avg training loss: 6.3834
batch: [2360/10547] batch time: 0.052 trainign loss: 5.2716 avg training loss: 6.3828
batch: [2370/10547] batch time: 2.452 trainign loss: 5.7421 avg training loss: 6.3828
batch: [2380/10547] batch time: 0.046 trainign loss: 2.9369 avg training loss: 6.3825
batch: [2390/10547] batch time: 2.393 trainign loss: 10.1329 avg training loss: 6.3816
batch: [2400/10547] batch time: 0.047 trainign loss: 6.7835 avg training loss: 6.3818
batch: [2410/10547] batch time: 2.101 trainign loss: 4.8959 avg training loss: 6.3817
batch: [2420/10547] batch time: 0.046 trainign loss: 6.7043 avg training loss: 6.3814
batch: [2430/10547] batch time: 2.454 trainign loss: 5.1859 avg training loss: 6.3812
batch: [2440/10547] batch time: 0.055 trainign loss: 5.5643 avg training loss: 6.3810
batch: [2450/10547] batch time: 2.367 trainign loss: 5.3020 avg training loss: 6.3807
batch: [2460/10547] batch time: 0.047 trainign loss: 0.8167 avg training loss: 6.3802
batch: [2470/10547] batch time: 1.987 trainign loss: 9.2861 avg training loss: 6.3794
batch: [2480/10547] batch time: 0.048 trainign loss: 4.4729 avg training loss: 6.3793
batch: [2490/10547] batch time: 2.534 trainign loss: 6.7517 avg training loss: 6.3790
batch: [2500/10547] batch time: 0.047 trainign loss: 5.9014 avg training loss: 6.3789
batch: [2510/10547] batch time: 2.272 trainign loss: 0.6732 avg training loss: 6.3783
batch: [2520/10547] batch time: 0.054 trainign loss: 6.2273 avg training loss: 6.3779
batch: [2530/10547] batch time: 2.158 trainign loss: 5.9565 avg training loss: 6.3779
batch: [2540/10547] batch time: 0.054 trainign loss: 6.6788 avg training loss: 6.3779
batch: [2550/10547] batch time: 2.338 trainign loss: 5.7115 avg training loss: 6.3776
batch: [2560/10547] batch time: 0.046 trainign loss: 5.9683 avg training loss: 6.3775
batch: [2570/10547] batch time: 2.181 trainign loss: 5.3860 avg training loss: 6.3774
batch: [2580/10547] batch time: 0.046 trainign loss: 5.0798 avg training loss: 6.3772
batch: [2590/10547] batch time: 2.161 trainign loss: 5.9339 avg training loss: 6.3769
batch: [2600/10547] batch time: 0.046 trainign loss: 2.5337 avg training loss: 6.3766
batch: [2610/10547] batch time: 1.858 trainign loss: 6.5630 avg training loss: 6.3765
batch: [2620/10547] batch time: 0.459 trainign loss: 5.7729 avg training loss: 6.3764
batch: [2630/10547] batch time: 1.950 trainign loss: 5.8429 avg training loss: 6.3762
batch: [2640/10547] batch time: 0.394 trainign loss: 2.8347 avg training loss: 6.3758
batch: [2650/10547] batch time: 1.841 trainign loss: 5.1961 avg training loss: 6.3755
batch: [2660/10547] batch time: 0.251 trainign loss: 6.5791 avg training loss: 6.3752
batch: [2670/10547] batch time: 2.780 trainign loss: 5.5427 avg training loss: 6.3751
batch: [2680/10547] batch time: 0.542 trainign loss: 5.2497 avg training loss: 6.3749
batch: [2690/10547] batch time: 1.674 trainign loss: 4.8325 avg training loss: 6.3746
batch: [2700/10547] batch time: 0.047 trainign loss: 5.6004 avg training loss: 6.3743
batch: [2710/10547] batch time: 1.930 trainign loss: 5.6735 avg training loss: 6.3741
batch: [2720/10547] batch time: 0.602 trainign loss: 5.6745 avg training loss: 6.3739
batch: [2730/10547] batch time: 2.621 trainign loss: 6.3363 avg training loss: 6.3739
batch: [2740/10547] batch time: 0.045 trainign loss: 6.4327 avg training loss: 6.3734
batch: [2750/10547] batch time: 2.425 trainign loss: 5.4571 avg training loss: 6.3733
batch: [2760/10547] batch time: 0.108 trainign loss: 5.2200 avg training loss: 6.3730
batch: [2770/10547] batch time: 2.018 trainign loss: 0.3386 avg training loss: 6.3722
batch: [2780/10547] batch time: 0.046 trainign loss: 6.8554 avg training loss: 6.3723
batch: [2790/10547] batch time: 1.513 trainign loss: 6.4148 avg training loss: 6.3723
batch: [2800/10547] batch time: 0.048 trainign loss: 5.1028 avg training loss: 6.3721
batch: [2810/10547] batch time: 1.716 trainign loss: 4.2962 avg training loss: 6.3717
batch: [2820/10547] batch time: 0.046 trainign loss: 6.2403 avg training loss: 6.3715
batch: [2830/10547] batch time: 2.768 trainign loss: 5.3253 avg training loss: 6.3714
batch: [2840/10547] batch time: 0.053 trainign loss: 4.4486 avg training loss: 6.3709
batch: [2850/10547] batch time: 1.740 trainign loss: 6.6140 avg training loss: 6.3706
batch: [2860/10547] batch time: 0.644 trainign loss: 6.1877 avg training loss: 6.3706
batch: [2870/10547] batch time: 1.024 trainign loss: 5.6734 avg training loss: 6.3704
batch: [2880/10547] batch time: 0.586 trainign loss: 6.0389 avg training loss: 6.3702
batch: [2890/10547] batch time: 1.982 trainign loss: 5.3596 avg training loss: 6.3700
batch: [2900/10547] batch time: 0.573 trainign loss: 5.0262 avg training loss: 6.3698
batch: [2910/10547] batch time: 1.334 trainign loss: 5.5500 avg training loss: 6.3695
batch: [2920/10547] batch time: 0.160 trainign loss: 3.8375 avg training loss: 6.3693
batch: [2930/10547] batch time: 1.666 trainign loss: 3.7135 avg training loss: 6.3688
batch: [2940/10547] batch time: 0.046 trainign loss: 6.2773 avg training loss: 6.3684
batch: [2950/10547] batch time: 2.364 trainign loss: 4.4212 avg training loss: 6.3682
batch: [2960/10547] batch time: 0.093 trainign loss: 5.8352 avg training loss: 6.3679
batch: [2970/10547] batch time: 1.914 trainign loss: 6.0517 avg training loss: 6.3679
batch: [2980/10547] batch time: 0.226 trainign loss: 3.1371 avg training loss: 6.3675
batch: [2990/10547] batch time: 2.075 trainign loss: 6.2516 avg training loss: 6.3670
batch: [3000/10547] batch time: 0.054 trainign loss: 5.6485 avg training loss: 6.3669
batch: [3010/10547] batch time: 2.533 trainign loss: 4.7329 avg training loss: 6.3665
batch: [3020/10547] batch time: 0.149 trainign loss: 5.8782 avg training loss: 6.3662
batch: [3030/10547] batch time: 2.100 trainign loss: 6.3949 avg training loss: 6.3661
batch: [3040/10547] batch time: 0.324 trainign loss: 3.9311 avg training loss: 6.3659
batch: [3050/10547] batch time: 2.316 trainign loss: 5.2375 avg training loss: 6.3654
batch: [3060/10547] batch time: 0.047 trainign loss: 6.1751 avg training loss: 6.3654
batch: [3070/10547] batch time: 1.420 trainign loss: 6.8633 avg training loss: 6.3650
batch: [3080/10547] batch time: 0.047 trainign loss: 6.8200 avg training loss: 6.3651
batch: [3090/10547] batch time: 1.268 trainign loss: 5.2639 avg training loss: 6.3650
batch: [3100/10547] batch time: 0.046 trainign loss: 5.1159 avg training loss: 6.3647
batch: [3110/10547] batch time: 1.354 trainign loss: 4.9921 avg training loss: 6.3644
batch: [3120/10547] batch time: 0.047 trainign loss: 6.1635 avg training loss: 6.3641
batch: [3130/10547] batch time: 1.335 trainign loss: 5.4368 avg training loss: 6.3639
batch: [3140/10547] batch time: 0.055 trainign loss: 5.7275 avg training loss: 6.3637
batch: [3150/10547] batch time: 0.806 trainign loss: 5.7689 avg training loss: 6.3635
batch: [3160/10547] batch time: 0.046 trainign loss: 5.2209 avg training loss: 6.3631
batch: [3170/10547] batch time: 0.411 trainign loss: 4.3737 avg training loss: 6.3627
batch: [3180/10547] batch time: 0.046 trainign loss: 4.4036 avg training loss: 6.3623
batch: [3190/10547] batch time: 1.127 trainign loss: 2.6119 avg training loss: 6.3616
batch: [3200/10547] batch time: 0.048 trainign loss: 6.8705 avg training loss: 6.3616
batch: [3210/10547] batch time: 0.908 trainign loss: 6.0526 avg training loss: 6.3616
batch: [3220/10547] batch time: 0.048 trainign loss: 5.2107 avg training loss: 6.3613
batch: [3230/10547] batch time: 0.232 trainign loss: 5.3727 avg training loss: 6.3610
batch: [3240/10547] batch time: 0.528 trainign loss: 3.9442 avg training loss: 6.3608
batch: [3250/10547] batch time: 1.391 trainign loss: 5.6274 avg training loss: 6.3605
batch: [3260/10547] batch time: 0.048 trainign loss: 5.3522 avg training loss: 6.3603
batch: [3270/10547] batch time: 1.298 trainign loss: 4.3468 avg training loss: 6.3601
batch: [3280/10547] batch time: 0.215 trainign loss: 5.6961 avg training loss: 6.3598
batch: [3290/10547] batch time: 0.369 trainign loss: 5.0506 avg training loss: 6.3597
batch: [3300/10547] batch time: 0.507 trainign loss: 5.5079 avg training loss: 6.3594
batch: [3310/10547] batch time: 1.081 trainign loss: 4.6514 avg training loss: 6.3592
batch: [3320/10547] batch time: 0.409 trainign loss: 4.4954 avg training loss: 6.3588
batch: [3330/10547] batch time: 1.106 trainign loss: 6.2304 avg training loss: 6.3586
batch: [3340/10547] batch time: 1.010 trainign loss: 5.1577 avg training loss: 6.3585
batch: [3350/10547] batch time: 0.564 trainign loss: 5.2308 avg training loss: 6.3582
batch: [3360/10547] batch time: 0.462 trainign loss: 5.7674 avg training loss: 6.3580
batch: [3370/10547] batch time: 0.503 trainign loss: 0.8511 avg training loss: 6.3574
batch: [3380/10547] batch time: 0.166 trainign loss: 5.8548 avg training loss: 6.3571
batch: [3390/10547] batch time: 0.740 trainign loss: 5.8605 avg training loss: 6.3568
batch: [3400/10547] batch time: 0.055 trainign loss: 3.1868 avg training loss: 6.3564
batch: [3410/10547] batch time: 0.812 trainign loss: 6.3245 avg training loss: 6.3563
batch: [3420/10547] batch time: 0.685 trainign loss: 6.0378 avg training loss: 6.3562
batch: [3430/10547] batch time: 0.661 trainign loss: 3.2779 avg training loss: 6.3560
batch: [3440/10547] batch time: 0.053 trainign loss: 6.1126 avg training loss: 6.3557
batch: [3450/10547] batch time: 1.026 trainign loss: 3.5658 avg training loss: 6.3554
batch: [3460/10547] batch time: 0.046 trainign loss: 6.4373 avg training loss: 6.3553
batch: [3470/10547] batch time: 1.322 trainign loss: 5.2942 avg training loss: 6.3552
batch: [3480/10547] batch time: 0.046 trainign loss: 5.0398 avg training loss: 6.3550
batch: [3490/10547] batch time: 1.573 trainign loss: 6.0091 avg training loss: 6.3547
batch: [3500/10547] batch time: 0.046 trainign loss: 5.6012 avg training loss: 6.3546
batch: [3510/10547] batch time: 1.563 trainign loss: 3.5764 avg training loss: 6.3543
batch: [3520/10547] batch time: 0.054 trainign loss: 5.3886 avg training loss: 6.3537
batch: [3530/10547] batch time: 1.316 trainign loss: 6.1685 avg training loss: 6.3534
batch: [3540/10547] batch time: 0.047 trainign loss: 5.1482 avg training loss: 6.3534
batch: [3550/10547] batch time: 0.860 trainign loss: 5.9088 avg training loss: 6.3532
batch: [3560/10547] batch time: 0.049 trainign loss: 3.9419 avg training loss: 6.3530
batch: [3570/10547] batch time: 2.143 trainign loss: 5.1869 avg training loss: 6.3527
batch: [3580/10547] batch time: 0.046 trainign loss: 5.6244 avg training loss: 6.3524
batch: [3590/10547] batch time: 0.938 trainign loss: 4.5810 avg training loss: 6.3521
batch: [3600/10547] batch time: 0.047 trainign loss: 6.5905 avg training loss: 6.3518
batch: [3610/10547] batch time: 1.037 trainign loss: 5.9883 avg training loss: 6.3517
batch: [3620/10547] batch time: 0.046 trainign loss: 4.4611 avg training loss: 6.3514
batch: [3630/10547] batch time: 0.971 trainign loss: 5.9767 avg training loss: 6.3511
batch: [3640/10547] batch time: 0.047 trainign loss: 2.9985 avg training loss: 6.3509
batch: [3650/10547] batch time: 0.054 trainign loss: 5.6380 avg training loss: 6.3504
batch: [3660/10547] batch time: 0.047 trainign loss: 1.2886 avg training loss: 6.3499
batch: [3670/10547] batch time: 0.629 trainign loss: 6.3021 avg training loss: 6.3499
batch: [3680/10547] batch time: 0.055 trainign loss: 3.1684 avg training loss: 6.3496
batch: [3690/10547] batch time: 1.138 trainign loss: 1.7679 avg training loss: 6.3490
batch: [3700/10547] batch time: 0.047 trainign loss: 6.6884 avg training loss: 6.3490
batch: [3710/10547] batch time: 0.364 trainign loss: 6.1883 avg training loss: 6.3489
batch: [3720/10547] batch time: 0.047 trainign loss: 5.4929 avg training loss: 6.3487
batch: [3730/10547] batch time: 1.214 trainign loss: 4.9163 avg training loss: 6.3484
batch: [3740/10547] batch time: 0.046 trainign loss: 5.5352 avg training loss: 6.3482
batch: [3750/10547] batch time: 0.047 trainign loss: 4.9328 avg training loss: 6.3481
batch: [3760/10547] batch time: 0.046 trainign loss: 4.5132 avg training loss: 6.3477
batch: [3770/10547] batch time: 0.147 trainign loss: 3.5529 avg training loss: 6.3474
batch: [3780/10547] batch time: 0.047 trainign loss: 6.1786 avg training loss: 6.3472
batch: [3790/10547] batch time: 0.323 trainign loss: 4.4368 avg training loss: 6.3470
batch: [3800/10547] batch time: 0.048 trainign loss: 2.3412 avg training loss: 6.3465
batch: [3810/10547] batch time: 0.043 trainign loss: 3.1462 avg training loss: 6.3463
batch: [3820/10547] batch time: 0.054 trainign loss: 6.7814 avg training loss: 6.3458
batch: [3830/10547] batch time: 0.046 trainign loss: 7.1544 avg training loss: 6.3459
batch: [3840/10547] batch time: 0.049 trainign loss: 6.3646 avg training loss: 6.3459
batch: [3850/10547] batch time: 0.047 trainign loss: 5.8518 avg training loss: 6.3458
batch: [3860/10547] batch time: 0.046 trainign loss: 5.2280 avg training loss: 6.3455
batch: [3870/10547] batch time: 0.053 trainign loss: 6.1725 avg training loss: 6.3454
batch: [3880/10547] batch time: 0.048 trainign loss: 4.8366 avg training loss: 6.3451
batch: [3890/10547] batch time: 0.051 trainign loss: 4.3764 avg training loss: 6.3450
batch: [3900/10547] batch time: 0.046 trainign loss: 1.7836 avg training loss: 6.3443
batch: [3910/10547] batch time: 0.047 trainign loss: 5.9222 avg training loss: 6.3443
batch: [3920/10547] batch time: 0.049 trainign loss: 5.9524 avg training loss: 6.3443
batch: [3930/10547] batch time: 0.903 trainign loss: 5.6482 avg training loss: 6.3442
batch: [3940/10547] batch time: 0.046 trainign loss: 4.3148 avg training loss: 6.3439
batch: [3950/10547] batch time: 2.429 trainign loss: 4.6863 avg training loss: 6.3436
batch: [3960/10547] batch time: 0.047 trainign loss: 0.8173 avg training loss: 6.3429
batch: [3970/10547] batch time: 1.988 trainign loss: 4.8637 avg training loss: 6.3425
batch: [3980/10547] batch time: 0.046 trainign loss: 6.2770 avg training loss: 6.3424
batch: [3990/10547] batch time: 2.050 trainign loss: 5.4652 avg training loss: 6.3423
batch: [4000/10547] batch time: 0.048 trainign loss: 5.0903 avg training loss: 6.3421
batch: [4010/10547] batch time: 1.477 trainign loss: 5.5171 avg training loss: 6.3419
batch: [4020/10547] batch time: 0.046 trainign loss: 5.5840 avg training loss: 6.3416
batch: [4030/10547] batch time: 1.373 trainign loss: 4.0474 avg training loss: 6.3414
batch: [4040/10547] batch time: 0.049 trainign loss: 5.9764 avg training loss: 6.3412
batch: [4050/10547] batch time: 0.650 trainign loss: 4.2173 avg training loss: 6.3409
batch: [4060/10547] batch time: 0.048 trainign loss: 4.6478 avg training loss: 6.3408
batch: [4070/10547] batch time: 1.352 trainign loss: 4.9411 avg training loss: 6.3406
batch: [4080/10547] batch time: 0.047 trainign loss: 6.1753 avg training loss: 6.3405
batch: [4090/10547] batch time: 2.320 trainign loss: 5.8496 avg training loss: 6.3404
batch: [4100/10547] batch time: 0.046 trainign loss: 4.9316 avg training loss: 6.3402
batch: [4110/10547] batch time: 0.853 trainign loss: 4.4192 avg training loss: 6.3399
batch: [4120/10547] batch time: 0.049 trainign loss: 3.2519 avg training loss: 6.3395
batch: [4130/10547] batch time: 0.954 trainign loss: 0.1307 avg training loss: 6.3387
batch: [4140/10547] batch time: 0.051 trainign loss: 4.7331 avg training loss: 6.3385
batch: [4150/10547] batch time: 1.633 trainign loss: 5.8854 avg training loss: 6.3380
batch: [4160/10547] batch time: 0.046 trainign loss: 4.5767 avg training loss: 6.3378
batch: [4170/10547] batch time: 1.019 trainign loss: 5.2172 avg training loss: 6.3377
batch: [4180/10547] batch time: 0.054 trainign loss: 3.7293 avg training loss: 6.3374
batch: [4190/10547] batch time: 1.107 trainign loss: 6.1661 avg training loss: 6.3370
batch: [4200/10547] batch time: 0.477 trainign loss: 5.7517 avg training loss: 6.3369
batch: [4210/10547] batch time: 0.773 trainign loss: 4.8845 avg training loss: 6.3367
batch: [4220/10547] batch time: 0.858 trainign loss: 5.2059 avg training loss: 6.3365
batch: [4230/10547] batch time: 0.893 trainign loss: 6.0872 avg training loss: 6.3362
batch: [4240/10547] batch time: 0.809 trainign loss: 5.1982 avg training loss: 6.3359
batch: [4250/10547] batch time: 0.048 trainign loss: 5.8059 avg training loss: 6.3355
batch: [4260/10547] batch time: 0.046 trainign loss: 4.6196 avg training loss: 6.3353
batch: [4270/10547] batch time: 1.250 trainign loss: 4.1840 avg training loss: 6.3350
batch: [4280/10547] batch time: 0.056 trainign loss: 0.0951 avg training loss: 6.3341
batch: [4290/10547] batch time: 1.333 trainign loss: 0.0007 avg training loss: 6.3328
batch: [4300/10547] batch time: 0.046 trainign loss: 2.0602 avg training loss: 6.3315
batch: [4310/10547] batch time: 1.447 trainign loss: 6.2011 avg training loss: 6.3319
batch: [4320/10547] batch time: 0.046 trainign loss: 6.0305 avg training loss: 6.3320
batch: [4330/10547] batch time: 0.932 trainign loss: 3.8988 avg training loss: 6.3318
batch: [4340/10547] batch time: 0.047 trainign loss: 5.2368 avg training loss: 6.3315
batch: [4350/10547] batch time: 1.863 trainign loss: 5.5496 avg training loss: 6.3314
batch: [4360/10547] batch time: 0.055 trainign loss: 5.6504 avg training loss: 6.3311
batch: [4370/10547] batch time: 1.742 trainign loss: 5.5997 avg training loss: 6.3311
batch: [4380/10547] batch time: 0.047 trainign loss: 4.7675 avg training loss: 6.3308
batch: [4390/10547] batch time: 0.056 trainign loss: 4.9875 avg training loss: 6.3305
batch: [4400/10547] batch time: 0.047 trainign loss: 5.6589 avg training loss: 6.3302
batch: [4410/10547] batch time: 0.049 trainign loss: 5.6593 avg training loss: 6.3300
batch: [4420/10547] batch time: 0.047 trainign loss: 5.1873 avg training loss: 6.3299
batch: [4430/10547] batch time: 0.046 trainign loss: 5.6719 avg training loss: 6.3296
batch: [4440/10547] batch time: 0.041 trainign loss: 4.9138 avg training loss: 6.3290
batch: [4450/10547] batch time: 0.048 trainign loss: 5.6585 avg training loss: 6.3289
batch: [4460/10547] batch time: 0.046 trainign loss: 6.1435 avg training loss: 6.3286
batch: [4470/10547] batch time: 0.245 trainign loss: 5.6412 avg training loss: 6.3284
batch: [4480/10547] batch time: 0.042 trainign loss: 6.3826 avg training loss: 6.3281
batch: [4490/10547] batch time: 0.055 trainign loss: 6.4192 avg training loss: 6.3278
batch: [4500/10547] batch time: 0.047 trainign loss: 3.4617 avg training loss: 6.3276
batch: [4510/10547] batch time: 0.047 trainign loss: 6.2549 avg training loss: 6.3271
batch: [4520/10547] batch time: 0.046 trainign loss: 6.2702 avg training loss: 6.3268
batch: [4530/10547] batch time: 0.046 trainign loss: 4.5763 avg training loss: 6.3267
batch: [4540/10547] batch time: 0.046 trainign loss: 5.4995 avg training loss: 6.3265
batch: [4550/10547] batch time: 0.046 trainign loss: 5.5815 avg training loss: 6.3264
batch: [4560/10547] batch time: 0.047 trainign loss: 4.7020 avg training loss: 6.3261
batch: [4570/10547] batch time: 0.046 trainign loss: 5.1177 avg training loss: 6.3258
batch: [4580/10547] batch time: 0.046 trainign loss: 5.4848 avg training loss: 6.3257
batch: [4590/10547] batch time: 0.046 trainign loss: 4.7585 avg training loss: 6.3254
batch: [4600/10547] batch time: 0.046 trainign loss: 5.9188 avg training loss: 6.3251
batch: [4610/10547] batch time: 0.048 trainign loss: 5.5850 avg training loss: 6.3248
batch: [4620/10547] batch time: 0.046 trainign loss: 5.3977 avg training loss: 6.3246
batch: [4630/10547] batch time: 0.046 trainign loss: 4.8190 avg training loss: 6.3244
batch: [4640/10547] batch time: 0.045 trainign loss: 6.3677 avg training loss: 6.3243
batch: [4650/10547] batch time: 0.068 trainign loss: 4.8085 avg training loss: 6.3241
batch: [4660/10547] batch time: 0.053 trainign loss: 6.0927 avg training loss: 6.3238
batch: [4670/10547] batch time: 0.244 trainign loss: 4.4782 avg training loss: 6.3235
batch: [4680/10547] batch time: 0.049 trainign loss: 5.1428 avg training loss: 6.3232
batch: [4690/10547] batch time: 0.048 trainign loss: 5.4911 avg training loss: 6.3229
batch: [4700/10547] batch time: 0.053 trainign loss: 5.9620 avg training loss: 6.3226
batch: [4710/10547] batch time: 0.046 trainign loss: 5.4989 avg training loss: 6.3223
batch: [4720/10547] batch time: 0.047 trainign loss: 6.3981 avg training loss: 6.3219
batch: [4730/10547] batch time: 0.046 trainign loss: 6.5077 avg training loss: 6.3217
batch: [4740/10547] batch time: 0.046 trainign loss: 5.6089 avg training loss: 6.3216
batch: [4750/10547] batch time: 0.053 trainign loss: 5.7302 avg training loss: 6.3214
batch: [4760/10547] batch time: 0.046 trainign loss: 5.7613 avg training loss: 6.3213
batch: [4770/10547] batch time: 0.049 trainign loss: 6.5752 avg training loss: 6.3210
batch: [4780/10547] batch time: 0.054 trainign loss: 6.4667 avg training loss: 6.3211
batch: [4790/10547] batch time: 0.054 trainign loss: 5.3773 avg training loss: 6.3209
batch: [4800/10547] batch time: 0.044 trainign loss: 6.0628 avg training loss: 6.3207
batch: [4810/10547] batch time: 0.046 trainign loss: 5.6637 avg training loss: 6.3205
batch: [4820/10547] batch time: 0.046 trainign loss: 4.3977 avg training loss: 6.3202
batch: [4830/10547] batch time: 0.055 trainign loss: 6.4489 avg training loss: 6.3201
batch: [4840/10547] batch time: 0.045 trainign loss: 4.6591 avg training loss: 6.3199
batch: [4850/10547] batch time: 0.047 trainign loss: 3.8434 avg training loss: 6.3194
batch: [4860/10547] batch time: 0.050 trainign loss: 3.8259 avg training loss: 6.3190
batch: [4870/10547] batch time: 0.046 trainign loss: 6.3388 avg training loss: 6.3188
batch: [4880/10547] batch time: 0.042 trainign loss: 5.1822 avg training loss: 6.3188
batch: [4890/10547] batch time: 0.046 trainign loss: 5.9672 avg training loss: 6.3186
batch: [4900/10547] batch time: 0.053 trainign loss: 5.8642 avg training loss: 6.3185
batch: [4910/10547] batch time: 0.046 trainign loss: 6.2258 avg training loss: 6.3182
batch: [4920/10547] batch time: 0.046 trainign loss: 6.1288 avg training loss: 6.3182
batch: [4930/10547] batch time: 0.055 trainign loss: 4.2330 avg training loss: 6.3179
batch: [4940/10547] batch time: 0.047 trainign loss: 5.4476 avg training loss: 6.3176
batch: [4950/10547] batch time: 0.047 trainign loss: 5.1659 avg training loss: 6.3172
batch: [4960/10547] batch time: 0.047 trainign loss: 6.3804 avg training loss: 6.3170
batch: [4970/10547] batch time: 0.048 trainign loss: 6.9534 avg training loss: 6.3171
batch: [4980/10547] batch time: 0.047 trainign loss: 5.9448 avg training loss: 6.3171
batch: [4990/10547] batch time: 0.047 trainign loss: 4.8437 avg training loss: 6.3169
batch: [5000/10547] batch time: 0.043 trainign loss: 4.5683 avg training loss: 6.3167
batch: [5010/10547] batch time: 0.047 trainign loss: 4.5339 avg training loss: 6.3163
batch: [5020/10547] batch time: 0.046 trainign loss: 5.8277 avg training loss: 6.3159
batch: [5030/10547] batch time: 0.046 trainign loss: 6.0945 avg training loss: 6.3158
batch: [5040/10547] batch time: 0.047 trainign loss: 6.1184 avg training loss: 6.3156
batch: [5050/10547] batch time: 0.046 trainign loss: 4.8038 avg training loss: 6.3154
batch: [5060/10547] batch time: 0.042 trainign loss: 6.3355 avg training loss: 6.3148
batch: [5070/10547] batch time: 0.047 trainign loss: 5.8687 avg training loss: 6.3148
batch: [5080/10547] batch time: 0.046 trainign loss: 4.7720 avg training loss: 6.3145
batch: [5090/10547] batch time: 0.048 trainign loss: 5.5190 avg training loss: 6.3142
batch: [5100/10547] batch time: 0.045 trainign loss: 3.9542 avg training loss: 6.3141
batch: [5110/10547] batch time: 0.046 trainign loss: 5.6787 avg training loss: 6.3139
batch: [5120/10547] batch time: 0.046 trainign loss: 2.8182 avg training loss: 6.3136
batch: [5130/10547] batch time: 0.046 trainign loss: 4.1283 avg training loss: 6.3134
batch: [5140/10547] batch time: 0.042 trainign loss: 5.8243 avg training loss: 6.3131
batch: [5150/10547] batch time: 0.054 trainign loss: 6.0099 avg training loss: 6.3131
batch: [5160/10547] batch time: 0.046 trainign loss: 0.5589 avg training loss: 6.3125
batch: [5170/10547] batch time: 0.046 trainign loss: 6.6753 avg training loss: 6.3124
batch: [5180/10547] batch time: 0.045 trainign loss: 6.0484 avg training loss: 6.3124
batch: [5190/10547] batch time: 0.055 trainign loss: 6.1513 avg training loss: 6.3122
batch: [5200/10547] batch time: 0.046 trainign loss: 2.9881 avg training loss: 6.3119
batch: [5210/10547] batch time: 0.048 trainign loss: 4.9923 avg training loss: 6.3116
batch: [5220/10547] batch time: 0.042 trainign loss: 5.7344 avg training loss: 6.3113
batch: [5230/10547] batch time: 0.047 trainign loss: 3.8956 avg training loss: 6.3109
batch: [5240/10547] batch time: 0.053 trainign loss: 1.8736 avg training loss: 6.3102
batch: [5250/10547] batch time: 0.047 trainign loss: 6.5544 avg training loss: 6.3101
batch: [5260/10547] batch time: 0.062 trainign loss: 5.4730 avg training loss: 6.3101
batch: [5270/10547] batch time: 0.046 trainign loss: 4.6110 avg training loss: 6.3098
batch: [5280/10547] batch time: 0.046 trainign loss: 6.1139 avg training loss: 6.3095
batch: [5290/10547] batch time: 0.046 trainign loss: 5.5989 avg training loss: 6.3094
batch: [5300/10547] batch time: 0.046 trainign loss: 1.1972 avg training loss: 6.3089
batch: [5310/10547] batch time: 0.047 trainign loss: 6.5237 avg training loss: 6.3087
batch: [5320/10547] batch time: 0.043 trainign loss: 5.7771 avg training loss: 6.3087
batch: [5330/10547] batch time: 0.051 trainign loss: 4.8326 avg training loss: 6.3084
batch: [5340/10547] batch time: 0.047 trainign loss: 3.8809 avg training loss: 6.3081
batch: [5350/10547] batch time: 0.047 trainign loss: 5.3049 avg training loss: 6.3078
batch: [5360/10547] batch time: 0.050 trainign loss: 5.0749 avg training loss: 6.3075
batch: [5370/10547] batch time: 0.046 trainign loss: 5.4337 avg training loss: 6.3075
batch: [5380/10547] batch time: 0.046 trainign loss: 5.6691 avg training loss: 6.3072
batch: [5390/10547] batch time: 0.047 trainign loss: 3.6669 avg training loss: 6.3069
batch: [5400/10547] batch time: 0.047 trainign loss: 5.2321 avg training loss: 6.3067
batch: [5410/10547] batch time: 0.046 trainign loss: 6.1778 avg training loss: 6.3065
batch: [5420/10547] batch time: 0.469 trainign loss: 5.9713 avg training loss: 6.3064
batch: [5430/10547] batch time: 0.047 trainign loss: 5.9013 avg training loss: 6.3062
batch: [5440/10547] batch time: 1.203 trainign loss: 5.7552 avg training loss: 6.3061
batch: [5450/10547] batch time: 0.046 trainign loss: 5.3392 avg training loss: 6.3059
batch: [5460/10547] batch time: 0.474 trainign loss: 5.2022 avg training loss: 6.3055
batch: [5470/10547] batch time: 0.047 trainign loss: 2.7608 avg training loss: 6.3053
batch: [5480/10547] batch time: 0.289 trainign loss: 3.9042 avg training loss: 6.3050
batch: [5490/10547] batch time: 0.047 trainign loss: 6.5905 avg training loss: 6.3047
batch: [5500/10547] batch time: 0.197 trainign loss: 5.4443 avg training loss: 6.3047
batch: [5510/10547] batch time: 0.048 trainign loss: 5.3639 avg training loss: 6.3045
batch: [5520/10547] batch time: 0.042 trainign loss: 6.0883 avg training loss: 6.3040
batch: [5530/10547] batch time: 0.053 trainign loss: 4.9908 avg training loss: 6.3038
batch: [5540/10547] batch time: 0.516 trainign loss: 2.9926 avg training loss: 6.3036
batch: [5550/10547] batch time: 0.046 trainign loss: 5.0808 avg training loss: 6.3032
batch: [5560/10547] batch time: 0.046 trainign loss: 5.7608 avg training loss: 6.3030
batch: [5570/10547] batch time: 0.046 trainign loss: 6.1498 avg training loss: 6.3029
batch: [5580/10547] batch time: 0.056 trainign loss: 3.9554 avg training loss: 6.3026
batch: [5590/10547] batch time: 0.047 trainign loss: 8.4380 avg training loss: 6.3017
batch: [5600/10547] batch time: 1.632 trainign loss: 6.3157 avg training loss: 6.3018
batch: [5610/10547] batch time: 0.046 trainign loss: 6.5998 avg training loss: 6.3018
batch: [5620/10547] batch time: 1.578 trainign loss: 4.6160 avg training loss: 6.3017
batch: [5630/10547] batch time: 0.053 trainign loss: 4.7852 avg training loss: 6.3014
batch: [5640/10547] batch time: 2.084 trainign loss: 5.7618 avg training loss: 6.3010
batch: [5650/10547] batch time: 0.054 trainign loss: 6.5103 avg training loss: 6.3010
batch: [5660/10547] batch time: 1.514 trainign loss: 4.3834 avg training loss: 6.3008
batch: [5670/10547] batch time: 0.047 trainign loss: 3.1743 avg training loss: 6.3005
batch: [5680/10547] batch time: 1.290 trainign loss: 4.1744 avg training loss: 6.3001
batch: [5690/10547] batch time: 0.046 trainign loss: 6.1492 avg training loss: 6.3000
batch: [5700/10547] batch time: 0.893 trainign loss: 5.6583 avg training loss: 6.2998
batch: [5710/10547] batch time: 0.054 trainign loss: 4.4439 avg training loss: 6.2995
batch: [5720/10547] batch time: 1.102 trainign loss: 6.0118 avg training loss: 6.2993
batch: [5730/10547] batch time: 0.052 trainign loss: 5.7360 avg training loss: 6.2993
batch: [5740/10547] batch time: 0.047 trainign loss: 4.8329 avg training loss: 6.2991
batch: [5750/10547] batch time: 0.045 trainign loss: 3.7906 avg training loss: 6.2988
batch: [5760/10547] batch time: 1.207 trainign loss: 4.7450 avg training loss: 6.2983
batch: [5770/10547] batch time: 0.048 trainign loss: 6.1002 avg training loss: 6.2982
batch: [5780/10547] batch time: 1.277 trainign loss: 3.6742 avg training loss: 6.2979
batch: [5790/10547] batch time: 0.054 trainign loss: 0.0118 avg training loss: 6.2968
batch: [5800/10547] batch time: 0.781 trainign loss: 8.7905 avg training loss: 6.2962
batch: [5810/10547] batch time: 0.046 trainign loss: 6.4037 avg training loss: 6.2962
batch: [5820/10547] batch time: 0.833 trainign loss: 6.1259 avg training loss: 6.2960
batch: [5830/10547] batch time: 0.047 trainign loss: 4.7861 avg training loss: 6.2957
batch: [5840/10547] batch time: 0.327 trainign loss: 3.4272 avg training loss: 6.2952
batch: [5850/10547] batch time: 0.046 trainign loss: 6.6735 avg training loss: 6.2948
batch: [5860/10547] batch time: 0.910 trainign loss: 5.5951 avg training loss: 6.2948
batch: [5870/10547] batch time: 0.052 trainign loss: 4.0190 avg training loss: 6.2946
batch: [5880/10547] batch time: 1.177 trainign loss: 2.6843 avg training loss: 6.2942
batch: [5890/10547] batch time: 0.046 trainign loss: 5.8958 avg training loss: 6.2940
batch: [5900/10547] batch time: 2.038 trainign loss: 5.1311 avg training loss: 6.2940
batch: [5910/10547] batch time: 0.047 trainign loss: 6.3078 avg training loss: 6.2939
batch: [5920/10547] batch time: 2.180 trainign loss: 5.3841 avg training loss: 6.2938
batch: [5930/10547] batch time: 0.700 trainign loss: 2.2928 avg training loss: 6.2934
batch: [5940/10547] batch time: 2.036 trainign loss: 4.6575 avg training loss: 6.2926
batch: [5950/10547] batch time: 0.054 trainign loss: 4.5029 avg training loss: 6.2924
batch: [5960/10547] batch time: 2.329 trainign loss: 5.3946 avg training loss: 6.2924
batch: [5970/10547] batch time: 0.046 trainign loss: 6.2094 avg training loss: 6.2919
batch: [5980/10547] batch time: 2.266 trainign loss: 4.8701 avg training loss: 6.2914
batch: [5990/10547] batch time: 0.046 trainign loss: 7.0375 avg training loss: 6.2915
batch: [6000/10547] batch time: 1.894 trainign loss: 6.2648 avg training loss: 6.2914
batch: [6010/10547] batch time: 0.051 trainign loss: 5.5378 avg training loss: 6.2913
batch: [6020/10547] batch time: 2.220 trainign loss: 5.6073 avg training loss: 6.2912
batch: [6030/10547] batch time: 1.222 trainign loss: 4.1812 avg training loss: 6.2908
batch: [6040/10547] batch time: 0.790 trainign loss: 6.8408 avg training loss: 6.2906
batch: [6050/10547] batch time: 1.690 trainign loss: 5.5682 avg training loss: 6.2906
batch: [6060/10547] batch time: 0.047 trainign loss: 5.4764 avg training loss: 6.2904
batch: [6070/10547] batch time: 1.836 trainign loss: 5.0560 avg training loss: 6.2901
batch: [6080/10547] batch time: 0.611 trainign loss: 4.9625 avg training loss: 6.2899
batch: [6090/10547] batch time: 1.404 trainign loss: 5.9010 avg training loss: 6.2897
batch: [6100/10547] batch time: 0.047 trainign loss: 4.6552 avg training loss: 6.2895
batch: [6110/10547] batch time: 1.531 trainign loss: 6.0081 avg training loss: 6.2893
batch: [6120/10547] batch time: 0.046 trainign loss: 5.3976 avg training loss: 6.2891
batch: [6130/10547] batch time: 1.149 trainign loss: 3.0475 avg training loss: 6.2888
batch: [6140/10547] batch time: 0.989 trainign loss: 6.4152 avg training loss: 6.2886
batch: [6150/10547] batch time: 0.755 trainign loss: 5.5712 avg training loss: 6.2885
batch: [6160/10547] batch time: 0.776 trainign loss: 6.7051 avg training loss: 6.2883
batch: [6170/10547] batch time: 1.609 trainign loss: 0.3755 avg training loss: 6.2878
batch: [6180/10547] batch time: 0.048 trainign loss: 7.3461 avg training loss: 6.2874
batch: [6190/10547] batch time: 1.431 trainign loss: 5.9758 avg training loss: 6.2873
batch: [6200/10547] batch time: 0.047 trainign loss: 5.7241 avg training loss: 6.2871
batch: [6210/10547] batch time: 1.873 trainign loss: 6.0340 avg training loss: 6.2870
batch: [6220/10547] batch time: 0.048 trainign loss: 4.9433 avg training loss: 6.2869
batch: [6230/10547] batch time: 2.035 trainign loss: 3.6146 avg training loss: 6.2865
batch: [6240/10547] batch time: 0.383 trainign loss: 4.7268 avg training loss: 6.2862
batch: [6250/10547] batch time: 1.394 trainign loss: 4.9978 avg training loss: 6.2860
batch: [6260/10547] batch time: 0.914 trainign loss: 4.9915 avg training loss: 6.2856
batch: [6270/10547] batch time: 1.143 trainign loss: 2.5011 avg training loss: 6.2852
batch: [6280/10547] batch time: 1.480 trainign loss: 6.1415 avg training loss: 6.2851
batch: [6290/10547] batch time: 1.150 trainign loss: 5.3811 avg training loss: 6.2849
batch: [6300/10547] batch time: 0.787 trainign loss: 6.0879 avg training loss: 6.2847
batch: [6310/10547] batch time: 1.686 trainign loss: 2.8189 avg training loss: 6.2843
batch: [6320/10547] batch time: 1.243 trainign loss: 6.7731 avg training loss: 6.2842
batch: [6330/10547] batch time: 0.726 trainign loss: 5.6548 avg training loss: 6.2843
batch: [6340/10547] batch time: 1.095 trainign loss: 5.9885 avg training loss: 6.2841
batch: [6350/10547] batch time: 1.751 trainign loss: 5.7542 avg training loss: 6.2840
batch: [6360/10547] batch time: 0.452 trainign loss: 5.0008 avg training loss: 6.2838
batch: [6370/10547] batch time: 1.654 trainign loss: 6.6206 avg training loss: 6.2835
batch: [6380/10547] batch time: 1.016 trainign loss: 3.3498 avg training loss: 6.2833
batch: [6390/10547] batch time: 0.870 trainign loss: 5.6838 avg training loss: 6.2831
batch: [6400/10547] batch time: 1.409 trainign loss: 4.8325 avg training loss: 6.2829
batch: [6410/10547] batch time: 1.023 trainign loss: 4.7923 avg training loss: 6.2827
batch: [6420/10547] batch time: 1.790 trainign loss: 5.2514 avg training loss: 6.2822
batch: [6430/10547] batch time: 0.055 trainign loss: 6.4373 avg training loss: 6.2822
batch: [6440/10547] batch time: 2.250 trainign loss: 2.8469 avg training loss: 6.2817
batch: [6450/10547] batch time: 0.055 trainign loss: 6.9094 avg training loss: 6.2814
batch: [6460/10547] batch time: 2.830 trainign loss: 5.2778 avg training loss: 6.2815
batch: [6470/10547] batch time: 0.051 trainign loss: 6.6274 avg training loss: 6.2812
batch: [6480/10547] batch time: 2.555 trainign loss: 5.8562 avg training loss: 6.2811
batch: [6490/10547] batch time: 0.046 trainign loss: 3.9459 avg training loss: 6.2808
batch: [6500/10547] batch time: 2.047 trainign loss: 0.0382 avg training loss: 6.2798
batch: [6510/10547] batch time: 0.052 trainign loss: 0.0004 avg training loss: 6.2785
batch: [6520/10547] batch time: 2.029 trainign loss: 0.0001 avg training loss: 6.2772
batch: [6530/10547] batch time: 0.046 trainign loss: 9.4521 avg training loss: 6.2769
batch: [6540/10547] batch time: 2.409 trainign loss: 7.2370 avg training loss: 6.2772
batch: [6550/10547] batch time: 0.046 trainign loss: 4.9728 avg training loss: 6.2771
batch: [6560/10547] batch time: 2.487 trainign loss: 5.1195 avg training loss: 6.2771
batch: [6570/10547] batch time: 0.046 trainign loss: 2.2955 avg training loss: 6.2767
batch: [6580/10547] batch time: 2.273 trainign loss: 5.7661 avg training loss: 6.2764
batch: [6590/10547] batch time: 0.046 trainign loss: 6.0013 avg training loss: 6.2764
batch: [6600/10547] batch time: 2.233 trainign loss: 3.4999 avg training loss: 6.2761
batch: [6610/10547] batch time: 0.046 trainign loss: 2.7372 avg training loss: 6.2757
batch: [6620/10547] batch time: 2.612 trainign loss: 6.8352 avg training loss: 6.2757
batch: [6630/10547] batch time: 0.046 trainign loss: 5.4166 avg training loss: 6.2757
batch: [6640/10547] batch time: 2.117 trainign loss: 5.1519 avg training loss: 6.2753
batch: [6650/10547] batch time: 0.054 trainign loss: 3.1587 avg training loss: 6.2749
batch: [6660/10547] batch time: 1.993 trainign loss: 6.1649 avg training loss: 6.2747
batch: [6670/10547] batch time: 0.047 trainign loss: 6.4697 avg training loss: 6.2747
batch: [6680/10547] batch time: 2.029 trainign loss: 5.9155 avg training loss: 6.2744
batch: [6690/10547] batch time: 0.054 trainign loss: 5.0192 avg training loss: 6.2742
batch: [6700/10547] batch time: 2.635 trainign loss: 5.8609 avg training loss: 6.2741
batch: [6710/10547] batch time: 0.046 trainign loss: 5.5976 avg training loss: 6.2740
batch: [6720/10547] batch time: 2.121 trainign loss: 6.0961 avg training loss: 6.2738
batch: [6730/10547] batch time: 0.045 trainign loss: 6.0881 avg training loss: 6.2737
batch: [6740/10547] batch time: 1.682 trainign loss: 4.9367 avg training loss: 6.2736
batch: [6750/10547] batch time: 0.312 trainign loss: 4.5289 avg training loss: 6.2733
batch: [6760/10547] batch time: 1.716 trainign loss: 5.6267 avg training loss: 6.2731
batch: [6770/10547] batch time: 0.603 trainign loss: 5.1828 avg training loss: 6.2729
batch: [6780/10547] batch time: 0.715 trainign loss: 5.9426 avg training loss: 6.2726
batch: [6790/10547] batch time: 1.311 trainign loss: 5.8209 avg training loss: 6.2726
batch: [6800/10547] batch time: 0.055 trainign loss: 5.4615 avg training loss: 6.2725
batch: [6810/10547] batch time: 1.690 trainign loss: 5.3244 avg training loss: 6.2723
batch: [6820/10547] batch time: 0.047 trainign loss: 2.3827 avg training loss: 6.2717
batch: [6830/10547] batch time: 2.398 trainign loss: 4.8952 avg training loss: 6.2715
batch: [6840/10547] batch time: 0.051 trainign loss: 3.9472 avg training loss: 6.2713
batch: [6850/10547] batch time: 2.215 trainign loss: 2.9176 avg training loss: 6.2711
batch: [6860/10547] batch time: 0.047 trainign loss: 5.0269 avg training loss: 6.2708
batch: [6870/10547] batch time: 2.668 trainign loss: 5.2555 avg training loss: 6.2704
batch: [6880/10547] batch time: 0.053 trainign loss: 5.7115 avg training loss: 6.2703
batch: [6890/10547] batch time: 1.772 trainign loss: 5.4847 avg training loss: 6.2702
batch: [6900/10547] batch time: 0.997 trainign loss: 4.4879 avg training loss: 6.2698
batch: [6910/10547] batch time: 1.893 trainign loss: 4.6502 avg training loss: 6.2694
batch: [6920/10547] batch time: 1.126 trainign loss: 3.9687 avg training loss: 6.2691
batch: [6930/10547] batch time: 0.948 trainign loss: 6.5311 avg training loss: 6.2688
batch: [6940/10547] batch time: 1.019 trainign loss: 5.4598 avg training loss: 6.2688
batch: [6950/10547] batch time: 1.700 trainign loss: 4.0660 avg training loss: 6.2685
batch: [6960/10547] batch time: 0.246 trainign loss: 5.5583 avg training loss: 6.2680
batch: [6970/10547] batch time: 1.963 trainign loss: 5.8026 avg training loss: 6.2678
batch: [6980/10547] batch time: 0.470 trainign loss: 5.6477 avg training loss: 6.2677
batch: [6990/10547] batch time: 1.675 trainign loss: 4.7539 avg training loss: 6.2675
batch: [7000/10547] batch time: 0.674 trainign loss: 6.3571 avg training loss: 6.2671
batch: [7010/10547] batch time: 0.631 trainign loss: 3.0318 avg training loss: 6.2669
batch: [7020/10547] batch time: 1.002 trainign loss: 5.1826 avg training loss: 6.2665
batch: [7030/10547] batch time: 0.046 trainign loss: 6.0234 avg training loss: 6.2665
batch: [7040/10547] batch time: 1.571 trainign loss: 5.6239 avg training loss: 6.2662
batch: [7050/10547] batch time: 0.053 trainign loss: 5.8873 avg training loss: 6.2661
batch: [7060/10547] batch time: 1.628 trainign loss: 5.7580 avg training loss: 6.2659
batch: [7070/10547] batch time: 0.046 trainign loss: 6.9594 avg training loss: 6.2655
batch: [7080/10547] batch time: 1.742 trainign loss: 5.6454 avg training loss: 6.2653
batch: [7090/10547] batch time: 0.047 trainign loss: 6.1013 avg training loss: 6.2652
batch: [7100/10547] batch time: 2.196 trainign loss: 5.7306 avg training loss: 6.2650
batch: [7110/10547] batch time: 0.047 trainign loss: 4.8627 avg training loss: 6.2646
batch: [7120/10547] batch time: 2.016 trainign loss: 6.3343 avg training loss: 6.2645
batch: [7130/10547] batch time: 0.054 trainign loss: 4.4830 avg training loss: 6.2643
batch: [7140/10547] batch time: 1.656 trainign loss: 4.5252 avg training loss: 6.2638
batch: [7150/10547] batch time: 0.047 trainign loss: 0.2571 avg training loss: 6.2632
batch: [7160/10547] batch time: 0.901 trainign loss: 0.0008 avg training loss: 6.2620
batch: [7170/10547] batch time: 0.054 trainign loss: 0.0002 avg training loss: 6.2607
batch: [7180/10547] batch time: 0.342 trainign loss: 0.0001 avg training loss: 6.2594
batch: [7190/10547] batch time: 0.052 trainign loss: 0.0001 avg training loss: 6.2582
batch: [7200/10547] batch time: 0.129 trainign loss: 0.0001 avg training loss: 6.2569
batch: [7210/10547] batch time: 0.054 trainign loss: 0.0001 avg training loss: 6.2556
batch: [7220/10547] batch time: 0.656 trainign loss: 0.0001 avg training loss: 6.2544
batch: [7230/10547] batch time: 0.255 trainign loss: 0.0001 avg training loss: 6.2531
batch: [7240/10547] batch time: 1.465 trainign loss: 0.0001 avg training loss: 6.2518
batch: [7250/10547] batch time: 0.656 trainign loss: 7.1363 avg training loss: 6.2522
batch: [7260/10547] batch time: 1.309 trainign loss: 6.5628 avg training loss: 6.2523
batch: [7270/10547] batch time: 0.378 trainign loss: 5.2222 avg training loss: 6.2523
batch: [7280/10547] batch time: 1.080 trainign loss: 4.7985 avg training loss: 6.2522
batch: [7290/10547] batch time: 0.622 trainign loss: 6.2143 avg training loss: 6.2518
batch: [7300/10547] batch time: 1.553 trainign loss: 0.1129 avg training loss: 6.2511
batch: [7310/10547] batch time: 0.054 trainign loss: 8.0646 avg training loss: 6.2504
batch: [7320/10547] batch time: 2.026 trainign loss: 6.4639 avg training loss: 6.2504
batch: [7330/10547] batch time: 0.046 trainign loss: 5.8723 avg training loss: 6.2503
batch: [7340/10547] batch time: 2.015 trainign loss: 6.1213 avg training loss: 6.2503
batch: [7350/10547] batch time: 0.055 trainign loss: 5.9637 avg training loss: 6.2501
batch: [7360/10547] batch time: 2.082 trainign loss: 6.3038 avg training loss: 6.2500
batch: [7370/10547] batch time: 0.285 trainign loss: 5.5268 avg training loss: 6.2500
batch: [7380/10547] batch time: 1.297 trainign loss: 5.8752 avg training loss: 6.2499
batch: [7390/10547] batch time: 0.866 trainign loss: 5.5290 avg training loss: 6.2497
batch: [7400/10547] batch time: 0.284 trainign loss: 5.5993 avg training loss: 6.2494
batch: [7410/10547] batch time: 1.206 trainign loss: 5.6826 avg training loss: 6.2492
batch: [7420/10547] batch time: 0.054 trainign loss: 5.7048 avg training loss: 6.2491
batch: [7430/10547] batch time: 1.234 trainign loss: 4.5788 avg training loss: 6.2487
batch: [7440/10547] batch time: 0.047 trainign loss: 4.2642 avg training loss: 6.2484
batch: [7450/10547] batch time: 0.435 trainign loss: 6.0949 avg training loss: 6.2482
batch: [7460/10547] batch time: 0.047 trainign loss: 5.7451 avg training loss: 6.2481
batch: [7470/10547] batch time: 0.565 trainign loss: 5.7079 avg training loss: 6.2478
batch: [7480/10547] batch time: 0.047 trainign loss: 6.1190 avg training loss: 6.2477
batch: [7490/10547] batch time: 0.049 trainign loss: 0.6674 avg training loss: 6.2472
batch: [7500/10547] batch time: 0.046 trainign loss: 6.5662 avg training loss: 6.2471
batch: [7510/10547] batch time: 0.048 trainign loss: 6.1518 avg training loss: 6.2468
batch: [7520/10547] batch time: 0.054 trainign loss: 4.7330 avg training loss: 6.2465
batch: [7530/10547] batch time: 0.047 trainign loss: 6.6700 avg training loss: 6.2464
batch: [7540/10547] batch time: 0.048 trainign loss: 5.5482 avg training loss: 6.2462
batch: [7550/10547] batch time: 0.046 trainign loss: 1.8769 avg training loss: 6.2458
batch: [7560/10547] batch time: 0.048 trainign loss: 6.1814 avg training loss: 6.2457
batch: [7570/10547] batch time: 0.049 trainign loss: 5.5872 avg training loss: 6.2457
batch: [7580/10547] batch time: 0.053 trainign loss: 5.5580 avg training loss: 6.2454
batch: [7590/10547] batch time: 0.046 trainign loss: 6.2079 avg training loss: 6.2451
batch: [7600/10547] batch time: 0.047 trainign loss: 3.8232 avg training loss: 6.2449
batch: [7610/10547] batch time: 0.046 trainign loss: 6.5590 avg training loss: 6.2446
batch: [7620/10547] batch time: 0.045 trainign loss: 6.3425 avg training loss: 6.2446
batch: [7630/10547] batch time: 0.050 trainign loss: 3.7081 avg training loss: 6.2443
batch: [7640/10547] batch time: 0.046 trainign loss: 6.2520 avg training loss: 6.2441
batch: [7650/10547] batch time: 0.450 trainign loss: 6.5675 avg training loss: 6.2441
batch: [7660/10547] batch time: 0.050 trainign loss: 5.4464 avg training loss: 6.2440
batch: [7670/10547] batch time: 1.005 trainign loss: 4.3467 avg training loss: 6.2438
batch: [7680/10547] batch time: 0.047 trainign loss: 4.6327 avg training loss: 6.2436
batch: [7690/10547] batch time: 0.481 trainign loss: 5.9380 avg training loss: 6.2434
batch: [7700/10547] batch time: 0.046 trainign loss: 5.1888 avg training loss: 6.2432
batch: [7710/10547] batch time: 0.849 trainign loss: 6.9379 avg training loss: 6.2426
batch: [7720/10547] batch time: 0.046 trainign loss: 6.8166 avg training loss: 6.2425
batch: [7730/10547] batch time: 0.556 trainign loss: 4.4721 avg training loss: 6.2423
batch: [7740/10547] batch time: 0.051 trainign loss: 5.6768 avg training loss: 6.2422
batch: [7750/10547] batch time: 0.046 trainign loss: 5.2966 avg training loss: 6.2421
batch: [7760/10547] batch time: 0.046 trainign loss: 6.4829 avg training loss: 6.2420
batch: [7770/10547] batch time: 0.054 trainign loss: 5.5861 avg training loss: 6.2419
batch: [7780/10547] batch time: 0.047 trainign loss: 5.6289 avg training loss: 6.2417
batch: [7790/10547] batch time: 0.052 trainign loss: 5.3428 avg training loss: 6.2415
batch: [7800/10547] batch time: 0.047 trainign loss: 5.0402 avg training loss: 6.2412
batch: [7810/10547] batch time: 0.050 trainign loss: 0.2799 avg training loss: 6.2405
batch: [7820/10547] batch time: 0.046 trainign loss: 9.1644 avg training loss: 6.2397
batch: [7830/10547] batch time: 0.054 trainign loss: 7.1574 avg training loss: 6.2399
batch: [7840/10547] batch time: 0.042 trainign loss: 6.1847 avg training loss: 6.2400
batch: [7850/10547] batch time: 0.047 trainign loss: 3.2001 avg training loss: 6.2397
batch: [7860/10547] batch time: 0.050 trainign loss: 5.6313 avg training loss: 6.2395
batch: [7870/10547] batch time: 0.046 trainign loss: 5.7533 avg training loss: 6.2394
batch: [7880/10547] batch time: 0.043 trainign loss: 5.0660 avg training loss: 6.2392
batch: [7890/10547] batch time: 0.046 trainign loss: 6.0278 avg training loss: 6.2391
batch: [7900/10547] batch time: 0.046 trainign loss: 3.1830 avg training loss: 6.2389
batch: [7910/10547] batch time: 0.046 trainign loss: 5.5547 avg training loss: 6.2385
batch: [7920/10547] batch time: 0.046 trainign loss: 5.7764 avg training loss: 6.2383
batch: [7930/10547] batch time: 0.047 trainign loss: 5.5488 avg training loss: 6.2382
batch: [7940/10547] batch time: 0.042 trainign loss: 3.3251 avg training loss: 6.2379
batch: [7950/10547] batch time: 0.047 trainign loss: 7.1226 avg training loss: 6.2376
batch: [7960/10547] batch time: 0.047 trainign loss: 5.2925 avg training loss: 6.2375
batch: [7970/10547] batch time: 0.138 trainign loss: 6.2359 avg training loss: 6.2374
batch: [7980/10547] batch time: 0.047 trainign loss: 5.9655 avg training loss: 6.2373
batch: [7990/10547] batch time: 0.647 trainign loss: 5.8193 avg training loss: 6.2371
batch: [8000/10547] batch time: 0.047 trainign loss: 4.8599 avg training loss: 6.2369
batch: [8010/10547] batch time: 1.514 trainign loss: 5.1645 avg training loss: 6.2366
batch: [8020/10547] batch time: 0.046 trainign loss: 5.7186 avg training loss: 6.2364
batch: [8030/10547] batch time: 2.841 trainign loss: 6.3108 avg training loss: 6.2363
batch: [8040/10547] batch time: 0.054 trainign loss: 5.0254 avg training loss: 6.2362
batch: [8050/10547] batch time: 2.068 trainign loss: 5.2939 avg training loss: 6.2361
batch: [8060/10547] batch time: 0.047 trainign loss: 6.1447 avg training loss: 6.2360
batch: [8070/10547] batch time: 2.109 trainign loss: 5.3551 avg training loss: 6.2358
batch: [8080/10547] batch time: 0.047 trainign loss: 5.7639 avg training loss: 6.2357
batch: [8090/10547] batch time: 2.738 trainign loss: 5.1655 avg training loss: 6.2354
batch: [8100/10547] batch time: 0.046 trainign loss: 5.7046 avg training loss: 6.2350
batch: [8110/10547] batch time: 2.115 trainign loss: 5.4192 avg training loss: 6.2350
batch: [8120/10547] batch time: 0.054 trainign loss: 6.3575 avg training loss: 6.2349
batch: [8130/10547] batch time: 2.711 trainign loss: 5.1223 avg training loss: 6.2348
batch: [8140/10547] batch time: 0.055 trainign loss: 4.9135 avg training loss: 6.2347
batch: [8150/10547] batch time: 2.429 trainign loss: 5.4905 avg training loss: 6.2345
batch: [8160/10547] batch time: 0.046 trainign loss: 6.2300 avg training loss: 6.2343
batch: [8170/10547] batch time: 2.439 trainign loss: 3.5130 avg training loss: 6.2341
batch: [8180/10547] batch time: 0.046 trainign loss: 5.7112 avg training loss: 6.2338
batch: [8190/10547] batch time: 2.123 trainign loss: 5.4412 avg training loss: 6.2338
batch: [8200/10547] batch time: 0.046 trainign loss: 4.4714 avg training loss: 6.2335
batch: [8210/10547] batch time: 2.663 trainign loss: 1.2650 avg training loss: 6.2329
batch: [8220/10547] batch time: 0.054 trainign loss: 6.2841 avg training loss: 6.2326
batch: [8230/10547] batch time: 2.443 trainign loss: 6.1246 avg training loss: 6.2326
batch: [8240/10547] batch time: 0.047 trainign loss: 5.6118 avg training loss: 6.2324
batch: [8250/10547] batch time: 2.126 trainign loss: 5.7192 avg training loss: 6.2323
batch: [8260/10547] batch time: 0.054 trainign loss: 4.7054 avg training loss: 6.2321
batch: [8270/10547] batch time: 2.265 trainign loss: 5.8032 avg training loss: 6.2320
batch: [8280/10547] batch time: 0.046 trainign loss: 5.7218 avg training loss: 6.2319
batch: [8290/10547] batch time: 2.134 trainign loss: 3.7563 avg training loss: 6.2315
batch: [8300/10547] batch time: 0.046 trainign loss: 5.2539 avg training loss: 6.2312
batch: [8310/10547] batch time: 2.430 trainign loss: 5.6135 avg training loss: 6.2310
batch: [8320/10547] batch time: 0.046 trainign loss: 5.9452 avg training loss: 6.2309
batch: [8330/10547] batch time: 2.174 trainign loss: 5.9132 avg training loss: 6.2307
batch: [8340/10547] batch time: 0.054 trainign loss: 3.7635 avg training loss: 6.2304
batch: [8350/10547] batch time: 2.608 trainign loss: 0.0100 avg training loss: 6.2294
batch: [8360/10547] batch time: 0.046 trainign loss: 7.1265 avg training loss: 6.2296
batch: [8370/10547] batch time: 2.308 trainign loss: 6.6785 avg training loss: 6.2295
batch: [8380/10547] batch time: 0.046 trainign loss: 5.0517 avg training loss: 6.2293
batch: [8390/10547] batch time: 2.344 trainign loss: 2.8672 avg training loss: 6.2290
batch: [8400/10547] batch time: 0.047 trainign loss: 4.9821 avg training loss: 6.2286
batch: [8410/10547] batch time: 2.348 trainign loss: 4.9919 avg training loss: 6.2283
batch: [8420/10547] batch time: 0.054 trainign loss: 4.3268 avg training loss: 6.2274
batch: [8430/10547] batch time: 2.711 trainign loss: 5.0165 avg training loss: 6.2275
batch: [8440/10547] batch time: 0.047 trainign loss: 4.8294 avg training loss: 6.2274
batch: [8450/10547] batch time: 2.208 trainign loss: 5.8353 avg training loss: 6.2271
batch: [8460/10547] batch time: 0.047 trainign loss: 5.1948 avg training loss: 6.2269
batch: [8470/10547] batch time: 2.164 trainign loss: 6.2016 avg training loss: 6.2268
batch: [8480/10547] batch time: 0.046 trainign loss: 5.0953 avg training loss: 6.2267
batch: [8490/10547] batch time: 0.848 trainign loss: 5.1516 avg training loss: 6.2263
batch: [8500/10547] batch time: 0.050 trainign loss: 3.2923 avg training loss: 6.2260
batch: [8510/10547] batch time: 0.307 trainign loss: 5.0420 avg training loss: 6.2258
batch: [8520/10547] batch time: 0.047 trainign loss: 3.2849 avg training loss: 6.2254
batch: [8530/10547] batch time: 1.399 trainign loss: 2.4312 avg training loss: 6.2249
batch: [8540/10547] batch time: 0.046 trainign loss: 6.6328 avg training loss: 6.2250
batch: [8550/10547] batch time: 0.545 trainign loss: 5.1283 avg training loss: 6.2249
batch: [8560/10547] batch time: 0.047 trainign loss: 0.5520 avg training loss: 6.2242
batch: [8570/10547] batch time: 1.281 trainign loss: 6.5097 avg training loss: 6.2238
batch: [8580/10547] batch time: 0.046 trainign loss: 5.8241 avg training loss: 6.2236
batch: [8590/10547] batch time: 0.052 trainign loss: 6.5739 avg training loss: 6.2236
batch: [8600/10547] batch time: 0.043 trainign loss: 6.2656 avg training loss: 6.2236
batch: [8610/10547] batch time: 0.113 trainign loss: 6.1560 avg training loss: 6.2235
batch: [8620/10547] batch time: 0.047 trainign loss: 4.8232 avg training loss: 6.2233
batch: [8630/10547] batch time: 0.999 trainign loss: 2.9570 avg training loss: 6.2229
batch: [8640/10547] batch time: 0.055 trainign loss: 6.6141 avg training loss: 6.2228
batch: [8650/10547] batch time: 0.587 trainign loss: 5.2822 avg training loss: 6.2226
batch: [8660/10547] batch time: 0.047 trainign loss: 5.2597 avg training loss: 6.2224
batch: [8670/10547] batch time: 1.819 trainign loss: 5.2768 avg training loss: 6.2223
batch: [8680/10547] batch time: 0.047 trainign loss: 3.8106 avg training loss: 6.2220
batch: [8690/10547] batch time: 2.595 trainign loss: 0.0095 avg training loss: 6.2209
batch: [8700/10547] batch time: 0.046 trainign loss: 7.0958 avg training loss: 6.2206
batch: [8710/10547] batch time: 2.423 trainign loss: 5.6067 avg training loss: 6.2207
batch: [8720/10547] batch time: 0.047 trainign loss: 6.8008 avg training loss: 6.2208
batch: [8730/10547] batch time: 2.361 trainign loss: 5.8528 avg training loss: 6.2207
batch: [8740/10547] batch time: 0.047 trainign loss: 4.3983 avg training loss: 6.2205
batch: [8750/10547] batch time: 2.695 trainign loss: 5.4334 avg training loss: 6.2203
batch: [8760/10547] batch time: 0.053 trainign loss: 6.3994 avg training loss: 6.2200
batch: [8770/10547] batch time: 2.368 trainign loss: 6.1704 avg training loss: 6.2200
batch: [8780/10547] batch time: 0.046 trainign loss: 5.3536 avg training loss: 6.2198
batch: [8790/10547] batch time: 1.954 trainign loss: 5.8425 avg training loss: 6.2196
batch: [8800/10547] batch time: 0.047 trainign loss: 5.5981 avg training loss: 6.2195
batch: [8810/10547] batch time: 2.155 trainign loss: 5.4471 avg training loss: 6.2194
batch: [8820/10547] batch time: 0.045 trainign loss: 5.7820 avg training loss: 6.2193
batch: [8830/10547] batch time: 2.023 trainign loss: 5.2309 avg training loss: 6.2192
batch: [8840/10547] batch time: 0.050 trainign loss: 4.6855 avg training loss: 6.2188
batch: [8850/10547] batch time: 2.566 trainign loss: 5.3420 avg training loss: 6.2187
batch: [8860/10547] batch time: 0.053 trainign loss: 4.5948 avg training loss: 6.2186
batch: [8870/10547] batch time: 2.374 trainign loss: 5.5039 avg training loss: 6.2183
batch: [8880/10547] batch time: 0.046 trainign loss: 4.3132 avg training loss: 6.2181
batch: [8890/10547] batch time: 2.350 trainign loss: 5.0666 avg training loss: 6.2179
batch: [8900/10547] batch time: 0.046 trainign loss: 2.6140 avg training loss: 6.2176
batch: [8910/10547] batch time: 2.455 trainign loss: 5.4668 avg training loss: 6.2172
batch: [8920/10547] batch time: 0.046 trainign loss: 5.3799 avg training loss: 6.2171
batch: [8930/10547] batch time: 2.157 trainign loss: 6.7167 avg training loss: 6.2170
batch: [8940/10547] batch time: 0.046 trainign loss: 5.3158 avg training loss: 6.2170
batch: [8950/10547] batch time: 2.066 trainign loss: 5.0148 avg training loss: 6.2168
batch: [8960/10547] batch time: 0.056 trainign loss: 4.9619 avg training loss: 6.2167
batch: [8970/10547] batch time: 2.088 trainign loss: 3.5463 avg training loss: 6.2165
batch: [8980/10547] batch time: 0.054 trainign loss: 4.8453 avg training loss: 6.2161
batch: [8990/10547] batch time: 2.110 trainign loss: 5.5696 avg training loss: 6.2160
batch: [9000/10547] batch time: 0.047 trainign loss: 0.6874 avg training loss: 6.2155
batch: [9010/10547] batch time: 2.383 trainign loss: 6.2838 avg training loss: 6.2153
batch: [9020/10547] batch time: 0.049 trainign loss: 4.4252 avg training loss: 6.2152
batch: [9030/10547] batch time: 2.160 trainign loss: 4.6098 avg training loss: 6.2151
batch: [9040/10547] batch time: 0.047 trainign loss: 5.0804 avg training loss: 6.2150
batch: [9050/10547] batch time: 2.247 trainign loss: 4.6597 avg training loss: 6.2146
batch: [9060/10547] batch time: 0.047 trainign loss: 5.2474 avg training loss: 6.2144
batch: [9070/10547] batch time: 1.234 trainign loss: 3.4058 avg training loss: 6.2142
batch: [9080/10547] batch time: 0.046 trainign loss: 5.9094 avg training loss: 6.2140
batch: [9090/10547] batch time: 0.078 trainign loss: 4.6425 avg training loss: 6.2140
batch: [9100/10547] batch time: 0.046 trainign loss: 3.5367 avg training loss: 6.2137
batch: [9110/10547] batch time: 0.048 trainign loss: 5.6943 avg training loss: 6.2136
batch: [9120/10547] batch time: 0.047 trainign loss: 5.4555 avg training loss: 6.2134
batch: [9130/10547] batch time: 0.242 trainign loss: 0.0741 avg training loss: 6.2126
batch: [9140/10547] batch time: 0.049 trainign loss: 7.3730 avg training loss: 6.2124
batch: [9150/10547] batch time: 1.164 trainign loss: 6.4205 avg training loss: 6.2125
batch: [9160/10547] batch time: 0.048 trainign loss: 1.7518 avg training loss: 6.2121
batch: [9170/10547] batch time: 0.631 trainign loss: 0.0032 avg training loss: 6.2110
batch: [9180/10547] batch time: 0.046 trainign loss: 7.6077 avg training loss: 6.2113
batch: [9190/10547] batch time: 0.965 trainign loss: 5.6427 avg training loss: 6.2113
batch: [9200/10547] batch time: 0.048 trainign loss: 5.8599 avg training loss: 6.2111
batch: [9210/10547] batch time: 0.397 trainign loss: 5.5479 avg training loss: 6.2109
batch: [9220/10547] batch time: 0.054 trainign loss: 4.2922 avg training loss: 6.2108
batch: [9230/10547] batch time: 0.046 trainign loss: 5.7212 avg training loss: 6.2105
batch: [9240/10547] batch time: 0.050 trainign loss: 5.0288 avg training loss: 6.2103
batch: [9250/10547] batch time: 0.047 trainign loss: 0.0528 avg training loss: 6.2095
batch: [9260/10547] batch time: 0.047 trainign loss: 11.1258 avg training loss: 6.2087
batch: [9270/10547] batch time: 0.046 trainign loss: 6.8127 avg training loss: 6.2090
batch: [9280/10547] batch time: 0.052 trainign loss: 6.3817 avg training loss: 6.2090
batch: [9290/10547] batch time: 0.043 trainign loss: 6.2188 avg training loss: 6.2089
batch: [9300/10547] batch time: 0.054 trainign loss: 5.3953 avg training loss: 6.2087
batch: [9310/10547] batch time: 0.047 trainign loss: 5.4332 avg training loss: 6.2084
batch: [9320/10547] batch time: 0.046 trainign loss: 6.4219 avg training loss: 6.2083
batch: [9330/10547] batch time: 0.046 trainign loss: 6.1255 avg training loss: 6.2082
batch: [9340/10547] batch time: 0.046 trainign loss: 3.1498 avg training loss: 6.2079
batch: [9350/10547] batch time: 0.044 trainign loss: 5.9741 avg training loss: 6.2077
batch: [9360/10547] batch time: 0.054 trainign loss: 7.3440 avg training loss: 6.2075
batch: [9370/10547] batch time: 0.049 trainign loss: 6.5775 avg training loss: 6.2076
batch: [9380/10547] batch time: 0.047 trainign loss: 4.9908 avg training loss: 6.2075
batch: [9390/10547] batch time: 0.053 trainign loss: 6.1843 avg training loss: 6.2072
batch: [9400/10547] batch time: 0.047 trainign loss: 3.0823 avg training loss: 6.2071
batch: [9410/10547] batch time: 0.048 trainign loss: 5.2631 avg training loss: 6.2070
batch: [9420/10547] batch time: 0.047 trainign loss: 6.5086 avg training loss: 6.2070
batch: [9430/10547] batch time: 0.046 trainign loss: 5.5349 avg training loss: 6.2070
batch: [9440/10547] batch time: 0.046 trainign loss: 4.9035 avg training loss: 6.2068
batch: [9450/10547] batch time: 0.046 trainign loss: 3.4197 avg training loss: 6.2064
batch: [9460/10547] batch time: 0.046 trainign loss: 6.3961 avg training loss: 6.2064
batch: [9470/10547] batch time: 0.046 trainign loss: 3.9203 avg training loss: 6.2063
batch: [9480/10547] batch time: 0.047 trainign loss: 3.9614 avg training loss: 6.2060
batch: [9490/10547] batch time: 0.045 trainign loss: 6.2635 avg training loss: 6.2059
batch: [9500/10547] batch time: 0.048 trainign loss: 4.9552 avg training loss: 6.2058
batch: [9510/10547] batch time: 0.046 trainign loss: 4.9677 avg training loss: 6.2055
batch: [9520/10547] batch time: 0.168 trainign loss: 4.3160 avg training loss: 6.2053
batch: [9530/10547] batch time: 0.046 trainign loss: 6.4268 avg training loss: 6.2050
batch: [9540/10547] batch time: 1.064 trainign loss: 2.5635 avg training loss: 6.2048
batch: [9550/10547] batch time: 0.054 trainign loss: 5.4821 avg training loss: 6.2045
batch: [9560/10547] batch time: 2.141 trainign loss: 6.9421 avg training loss: 6.2044
batch: [9570/10547] batch time: 0.051 trainign loss: 6.2834 avg training loss: 6.2044
batch: [9580/10547] batch time: 2.834 trainign loss: 5.9884 avg training loss: 6.2043
batch: [9590/10547] batch time: 0.046 trainign loss: 3.8843 avg training loss: 6.2042
batch: [9600/10547] batch time: 2.185 trainign loss: 4.9825 avg training loss: 6.2039
batch: [9610/10547] batch time: 0.046 trainign loss: 5.3624 avg training loss: 6.2039
batch: [9620/10547] batch time: 2.185 trainign loss: 4.6612 avg training loss: 6.2037
batch: [9630/10547] batch time: 0.046 trainign loss: 6.0020 avg training loss: 6.2036
batch: [9640/10547] batch time: 2.208 trainign loss: 5.7421 avg training loss: 6.2035
batch: [9650/10547] batch time: 0.054 trainign loss: 5.5422 avg training loss: 6.2034
batch: [9660/10547] batch time: 2.319 trainign loss: 5.6022 avg training loss: 6.2032
batch: [9670/10547] batch time: 0.047 trainign loss: 5.5024 avg training loss: 6.2030
batch: [9680/10547] batch time: 2.143 trainign loss: 5.4747 avg training loss: 6.2030
batch: [9690/10547] batch time: 0.046 trainign loss: 5.0611 avg training loss: 6.2027
batch: [9700/10547] batch time: 2.266 trainign loss: 5.3330 avg training loss: 6.2026
batch: [9710/10547] batch time: 0.050 trainign loss: 5.7168 avg training loss: 6.2026
batch: [9720/10547] batch time: 2.685 trainign loss: 5.2216 avg training loss: 6.2024
batch: [9730/10547] batch time: 0.046 trainign loss: 0.4298 avg training loss: 6.2018
batch: [9740/10547] batch time: 2.385 trainign loss: 5.4447 avg training loss: 6.2016
batch: [9750/10547] batch time: 0.053 trainign loss: 6.6276 avg training loss: 6.2017
batch: [9760/10547] batch time: 2.463 trainign loss: 6.4349 avg training loss: 6.2016
batch: [9770/10547] batch time: 0.047 trainign loss: 5.2388 avg training loss: 6.2015
batch: [9780/10547] batch time: 2.288 trainign loss: 6.1090 avg training loss: 6.2015
batch: [9790/10547] batch time: 0.047 trainign loss: 5.3769 avg training loss: 6.2012
batch: [9800/10547] batch time: 2.282 trainign loss: 5.4725 avg training loss: 6.2009
batch: [9810/10547] batch time: 0.054 trainign loss: 6.1479 avg training loss: 6.2009
batch: [9820/10547] batch time: 2.106 trainign loss: 5.9847 avg training loss: 6.2007
batch: [9830/10547] batch time: 0.188 trainign loss: 4.7807 avg training loss: 6.2005
batch: [9840/10547] batch time: 2.506 trainign loss: 0.3348 avg training loss: 6.1998
batch: [9850/10547] batch time: 0.046 trainign loss: 0.0214 avg training loss: 6.1991
batch: [9860/10547] batch time: 2.438 trainign loss: 6.3169 avg training loss: 6.1992
batch: [9870/10547] batch time: 0.051 trainign loss: 4.7400 avg training loss: 6.1991
batch: [9880/10547] batch time: 2.438 trainign loss: 5.2579 avg training loss: 6.1988
batch: [9890/10547] batch time: 0.054 trainign loss: 6.7310 avg training loss: 6.1988
batch: [9900/10547] batch time: 2.495 trainign loss: 5.1622 avg training loss: 6.1987
batch: [9910/10547] batch time: 0.047 trainign loss: 1.4179 avg training loss: 6.1983
batch: [9920/10547] batch time: 2.640 trainign loss: 4.0782 avg training loss: 6.1979
batch: [9930/10547] batch time: 0.046 trainign loss: 6.3952 avg training loss: 6.1977
batch: [9940/10547] batch time: 2.308 trainign loss: 6.5782 avg training loss: 6.1974
batch: [9950/10547] batch time: 0.048 trainign loss: 6.3383 avg training loss: 6.1972
batch: [9960/10547] batch time: 2.636 trainign loss: 3.9206 avg training loss: 6.1971
batch: [9970/10547] batch time: 0.047 trainign loss: 8.1096 avg training loss: 6.1966
batch: [9980/10547] batch time: 2.429 trainign loss: 7.0403 avg training loss: 6.1967
batch: [9990/10547] batch time: 0.045 trainign loss: 5.6709 avg training loss: 6.1966
batch: [10000/10547] batch time: 2.474 trainign loss: 6.0187 avg training loss: 6.1965
batch: [10010/10547] batch time: 0.054 trainign loss: 4.7101 avg training loss: 6.1962
batch: [10020/10547] batch time: 2.239 trainign loss: 5.9510 avg training loss: 6.1960
batch: [10030/10547] batch time: 0.046 trainign loss: 6.3698 avg training loss: 6.1958
batch: [10040/10547] batch time: 2.499 trainign loss: 3.5975 avg training loss: 6.1957
batch: [10050/10547] batch time: 0.045 trainign loss: 4.2117 avg training loss: 6.1954
batch: [10060/10547] batch time: 2.408 trainign loss: 7.8008 avg training loss: 6.1949
batch: [10070/10547] batch time: 0.046 trainign loss: 6.4896 avg training loss: 6.1949
batch: [10080/10547] batch time: 2.196 trainign loss: 4.4577 avg training loss: 6.1947
batch: [10090/10547] batch time: 0.047 trainign loss: 7.3494 avg training loss: 6.1945
batch: [10100/10547] batch time: 2.450 trainign loss: 4.8651 avg training loss: 6.1946
batch: [10110/10547] batch time: 0.047 trainign loss: 5.2623 avg training loss: 6.1944
batch: [10120/10547] batch time: 2.511 trainign loss: 4.9694 avg training loss: 6.1943
batch: [10130/10547] batch time: 0.054 trainign loss: 4.5279 avg training loss: 6.1939
batch: [10140/10547] batch time: 2.674 trainign loss: 3.4700 avg training loss: 6.1935
batch: [10150/10547] batch time: 0.046 trainign loss: 4.3018 avg training loss: 6.1935
batch: [10160/10547] batch time: 2.317 trainign loss: 0.0109 avg training loss: 6.1925
batch: [10170/10547] batch time: 0.046 trainign loss: 8.9394 avg training loss: 6.1922
batch: [10180/10547] batch time: 2.299 trainign loss: 6.7219 avg training loss: 6.1924
batch: [10190/10547] batch time: 0.046 trainign loss: 2.0125 avg training loss: 6.1921
batch: [10200/10547] batch time: 2.387 trainign loss: 6.5600 avg training loss: 6.1922
batch: [10210/10547] batch time: 0.049 trainign loss: 5.6852 avg training loss: 6.1921
batch: [10220/10547] batch time: 2.614 trainign loss: 4.9322 avg training loss: 6.1919
batch: [10230/10547] batch time: 0.047 trainign loss: 6.2922 avg training loss: 6.1919
batch: [10240/10547] batch time: 2.175 trainign loss: 5.3957 avg training loss: 6.1917
batch: [10250/10547] batch time: 0.049 trainign loss: 5.1282 avg training loss: 6.1916
batch: [10260/10547] batch time: 1.746 trainign loss: 4.9385 avg training loss: 6.1913
batch: [10270/10547] batch time: 0.958 trainign loss: 6.0893 avg training loss: 6.1913
batch: [10280/10547] batch time: 1.626 trainign loss: 6.3869 avg training loss: 6.1913
batch: [10290/10547] batch time: 0.528 trainign loss: 5.3062 avg training loss: 6.1912
batch: [10300/10547] batch time: 1.379 trainign loss: 5.9374 avg training loss: 6.1911
batch: [10310/10547] batch time: 0.277 trainign loss: 5.5836 avg training loss: 6.1909
batch: [10320/10547] batch time: 1.560 trainign loss: 6.6303 avg training loss: 6.1909
batch: [10330/10547] batch time: 0.046 trainign loss: 4.5913 avg training loss: 6.1907
batch: [10340/10547] batch time: 0.211 trainign loss: 6.0668 avg training loss: 6.1906
batch: [10350/10547] batch time: 0.046 trainign loss: 5.7169 avg training loss: 6.1905
batch: [10360/10547] batch time: 0.046 trainign loss: 4.9824 avg training loss: 6.1901
batch: [10370/10547] batch time: 0.046 trainign loss: 0.7180 avg training loss: 6.1897
batch: [10380/10547] batch time: 0.047 trainign loss: 2.8833 avg training loss: 6.1891
batch: [10390/10547] batch time: 0.426 trainign loss: 7.0593 avg training loss: 6.1890
batch: [10400/10547] batch time: 0.050 trainign loss: 6.2595 avg training loss: 6.1891
batch: [10410/10547] batch time: 0.811 trainign loss: 6.0919 avg training loss: 6.1890
batch: [10420/10547] batch time: 0.047 trainign loss: 6.0816 avg training loss: 6.1889
batch: [10430/10547] batch time: 1.181 trainign loss: 5.0921 avg training loss: 6.1888
batch: [10440/10547] batch time: 0.046 trainign loss: 4.4613 avg training loss: 6.1885
batch: [10450/10547] batch time: 1.944 trainign loss: 4.1601 avg training loss: 6.1882
batch: [10460/10547] batch time: 0.048 trainign loss: 6.5230 avg training loss: 6.1880
batch: [10470/10547] batch time: 0.481 trainign loss: 5.6030 avg training loss: 6.1880
batch: [10480/10547] batch time: 0.047 trainign loss: 3.0198 avg training loss: 6.1878
batch: [10490/10547] batch time: 0.047 trainign loss: 6.1152 avg training loss: 6.1876
batch: [10500/10547] batch time: 0.047 trainign loss: 6.3175 avg training loss: 6.1876
batch: [10510/10547] batch time: 0.046 trainign loss: 5.8410 avg training loss: 6.1875
batch: [10520/10547] batch time: 0.053 trainign loss: 4.6477 avg training loss: 6.1873
batch: [10530/10547] batch time: 0.049 trainign loss: 5.3972 avg training loss: 6.1869
batch: [10540/10547] batch time: 0.046 trainign loss: 4.7021 avg training loss: 6.1867
Epoch: 6
----------------------------------------------------------------------
batch: [0/10547] batch time: 2.769 trainign loss: 5.8087 avg training loss: 6.1866
batch: [10/10547] batch time: 0.046 trainign loss: 6.9752 avg training loss: 6.1862
batch: [20/10547] batch time: 2.010 trainign loss: 6.6098 avg training loss: 6.1862
batch: [30/10547] batch time: 0.053 trainign loss: 5.4593 avg training loss: 6.1861
batch: [40/10547] batch time: 2.042 trainign loss: 6.2153 avg training loss: 6.1860
batch: [50/10547] batch time: 0.046 trainign loss: 6.2473 avg training loss: 6.1859
batch: [60/10547] batch time: 2.441 trainign loss: 4.7209 avg training loss: 6.1857
batch: [70/10547] batch time: 0.046 trainign loss: 5.9863 avg training loss: 6.1856
batch: [80/10547] batch time: 1.983 trainign loss: 5.9747 avg training loss: 6.1853
batch: [90/10547] batch time: 0.480 trainign loss: 5.6589 avg training loss: 6.1853
batch: [100/10547] batch time: 2.132 trainign loss: 2.3895 avg training loss: 6.1850
batch: [110/10547] batch time: 0.046 trainign loss: 0.0039 avg training loss: 6.1839
batch: [120/10547] batch time: 2.272 trainign loss: 7.1197 avg training loss: 6.1842
batch: [130/10547] batch time: 0.049 trainign loss: 6.7948 avg training loss: 6.1843
batch: [140/10547] batch time: 2.284 trainign loss: 5.8268 avg training loss: 6.1844
batch: [150/10547] batch time: 0.051 trainign loss: 3.8075 avg training loss: 6.1841
batch: [160/10547] batch time: 2.257 trainign loss: 5.3721 avg training loss: 6.1838
batch: [170/10547] batch time: 0.047 trainign loss: 5.7114 avg training loss: 6.1837
batch: [180/10547] batch time: 2.305 trainign loss: 6.0356 avg training loss: 6.1837
batch: [190/10547] batch time: 0.047 trainign loss: 5.6875 avg training loss: 6.1835
batch: [200/10547] batch time: 2.392 trainign loss: 5.0466 avg training loss: 6.1834
batch: [210/10547] batch time: 0.055 trainign loss: 5.4747 avg training loss: 6.1830
batch: [220/10547] batch time: 1.687 trainign loss: 6.9437 avg training loss: 6.1832
batch: [230/10547] batch time: 0.046 trainign loss: 5.7295 avg training loss: 6.1831
batch: [240/10547] batch time: 2.407 trainign loss: 5.6995 avg training loss: 6.1829
batch: [250/10547] batch time: 0.527 trainign loss: 6.1854 avg training loss: 6.1828
batch: [260/10547] batch time: 1.336 trainign loss: 5.9838 avg training loss: 6.1828
batch: [270/10547] batch time: 0.984 trainign loss: 4.7815 avg training loss: 6.1825
batch: [280/10547] batch time: 1.200 trainign loss: 4.7809 avg training loss: 6.1823
batch: [290/10547] batch time: 1.393 trainign loss: 4.7274 avg training loss: 6.1821
batch: [300/10547] batch time: 0.207 trainign loss: 5.6001 avg training loss: 6.1819
batch: [310/10547] batch time: 2.104 trainign loss: 5.2992 avg training loss: 6.1817
batch: [320/10547] batch time: 1.027 trainign loss: 6.1252 avg training loss: 6.1816
batch: [330/10547] batch time: 1.036 trainign loss: 5.3963 avg training loss: 6.1816
batch: [340/10547] batch time: 0.620 trainign loss: 5.1862 avg training loss: 6.1813
batch: [350/10547] batch time: 2.294 trainign loss: 6.2587 avg training loss: 6.1809
batch: [360/10547] batch time: 0.046 trainign loss: 6.4934 avg training loss: 6.1809
batch: [370/10547] batch time: 2.578 trainign loss: 6.2616 avg training loss: 6.1808
batch: [380/10547] batch time: 0.052 trainign loss: 5.5311 avg training loss: 6.1807
batch: [390/10547] batch time: 2.026 trainign loss: 5.6595 avg training loss: 6.1806
batch: [400/10547] batch time: 0.081 trainign loss: 5.5147 avg training loss: 6.1804
batch: [410/10547] batch time: 2.323 trainign loss: 4.3108 avg training loss: 6.1802
batch: [420/10547] batch time: 0.234 trainign loss: 5.5381 avg training loss: 6.1799
batch: [430/10547] batch time: 2.223 trainign loss: 5.8160 avg training loss: 6.1798
batch: [440/10547] batch time: 0.553 trainign loss: 2.8952 avg training loss: 6.1794
batch: [450/10547] batch time: 1.682 trainign loss: 5.6842 avg training loss: 6.1792
batch: [460/10547] batch time: 0.338 trainign loss: 5.3486 avg training loss: 6.1791
batch: [470/10547] batch time: 1.053 trainign loss: 1.2435 avg training loss: 6.1786
batch: [480/10547] batch time: 0.645 trainign loss: 9.3918 avg training loss: 6.1778
batch: [490/10547] batch time: 2.496 trainign loss: 6.7336 avg training loss: 6.1781
batch: [500/10547] batch time: 0.403 trainign loss: 6.4859 avg training loss: 6.1780
batch: [510/10547] batch time: 1.967 trainign loss: 5.0253 avg training loss: 6.1779
batch: [520/10547] batch time: 1.487 trainign loss: 5.5088 avg training loss: 6.1778
batch: [530/10547] batch time: 0.616 trainign loss: 0.3991 avg training loss: 6.1772
batch: [540/10547] batch time: 2.064 trainign loss: 6.5736 avg training loss: 6.1769
batch: [550/10547] batch time: 0.063 trainign loss: 6.4915 avg training loss: 6.1768
batch: [560/10547] batch time: 2.232 trainign loss: 6.6570 avg training loss: 6.1769
batch: [570/10547] batch time: 0.196 trainign loss: 5.7190 avg training loss: 6.1768
batch: [580/10547] batch time: 1.918 trainign loss: 5.4765 avg training loss: 6.1768
batch: [590/10547] batch time: 1.022 trainign loss: 6.1663 avg training loss: 6.1766
batch: [600/10547] batch time: 1.895 trainign loss: 4.7547 avg training loss: 6.1764
batch: [610/10547] batch time: 0.724 trainign loss: 5.5571 avg training loss: 6.1763
batch: [620/10547] batch time: 1.253 trainign loss: 5.6374 avg training loss: 6.1763
batch: [630/10547] batch time: 1.745 trainign loss: 5.3494 avg training loss: 6.1762
batch: [640/10547] batch time: 0.548 trainign loss: 2.8977 avg training loss: 6.1759
batch: [650/10547] batch time: 2.024 trainign loss: 4.9384 avg training loss: 6.1757
batch: [660/10547] batch time: 0.826 trainign loss: 5.2772 avg training loss: 6.1755
batch: [670/10547] batch time: 2.249 trainign loss: 4.4832 avg training loss: 6.1752
batch: [680/10547] batch time: 0.370 trainign loss: 6.0872 avg training loss: 6.1751
batch: [690/10547] batch time: 2.283 trainign loss: 5.4010 avg training loss: 6.1750
batch: [700/10547] batch time: 0.050 trainign loss: 4.8586 avg training loss: 6.1748
batch: [710/10547] batch time: 2.396 trainign loss: 5.7088 avg training loss: 6.1746
batch: [720/10547] batch time: 0.045 trainign loss: 3.0244 avg training loss: 6.1744
batch: [730/10547] batch time: 2.572 trainign loss: 8.4217 avg training loss: 6.1735
batch: [740/10547] batch time: 0.046 trainign loss: 4.8254 avg training loss: 6.1737
batch: [750/10547] batch time: 2.089 trainign loss: 4.7308 avg training loss: 6.1736
batch: [760/10547] batch time: 0.048 trainign loss: 1.7150 avg training loss: 6.1731
batch: [770/10547] batch time: 2.534 trainign loss: 6.3563 avg training loss: 6.1732
batch: [780/10547] batch time: 0.046 trainign loss: 6.4862 avg training loss: 6.1732
batch: [790/10547] batch time: 2.143 trainign loss: 5.7936 avg training loss: 6.1732
batch: [800/10547] batch time: 0.047 trainign loss: 6.0294 avg training loss: 6.1731
batch: [810/10547] batch time: 2.149 trainign loss: 4.5179 avg training loss: 6.1730
batch: [820/10547] batch time: 0.046 trainign loss: 5.4646 avg training loss: 6.1728
batch: [830/10547] batch time: 2.016 trainign loss: 6.5848 avg training loss: 6.1728
batch: [840/10547] batch time: 0.046 trainign loss: 5.1244 avg training loss: 6.1728
batch: [850/10547] batch time: 2.146 trainign loss: 5.4150 avg training loss: 6.1726
batch: [860/10547] batch time: 0.046 trainign loss: 5.2631 avg training loss: 6.1724
batch: [870/10547] batch time: 2.059 trainign loss: 5.9922 avg training loss: 6.1723
batch: [880/10547] batch time: 0.045 trainign loss: 4.8264 avg training loss: 6.1722
batch: [890/10547] batch time: 2.353 trainign loss: 4.1455 avg training loss: 6.1717
batch: [900/10547] batch time: 0.047 trainign loss: 5.9700 avg training loss: 6.1717
batch: [910/10547] batch time: 2.355 trainign loss: 5.1184 avg training loss: 6.1716
batch: [920/10547] batch time: 0.046 trainign loss: 5.9389 avg training loss: 6.1714
batch: [930/10547] batch time: 2.192 trainign loss: 4.5151 avg training loss: 6.1713
batch: [940/10547] batch time: 0.046 trainign loss: 5.6124 avg training loss: 6.1711
batch: [950/10547] batch time: 2.259 trainign loss: 4.8919 avg training loss: 6.1710
batch: [960/10547] batch time: 0.048 trainign loss: 0.6240 avg training loss: 6.1704
batch: [970/10547] batch time: 2.748 trainign loss: 6.2713 avg training loss: 6.1704
batch: [980/10547] batch time: 0.048 trainign loss: 4.5243 avg training loss: 6.1701
batch: [990/10547] batch time: 2.239 trainign loss: 0.5218 avg training loss: 6.1694
batch: [1000/10547] batch time: 0.046 trainign loss: 8.1731 avg training loss: 6.1685
batch: [1010/10547] batch time: 1.389 trainign loss: 5.8122 avg training loss: 6.1686
batch: [1020/10547] batch time: 0.047 trainign loss: 0.0209 avg training loss: 6.1678
batch: [1030/10547] batch time: 2.003 trainign loss: 0.0005 avg training loss: 6.1666
batch: [1040/10547] batch time: 0.055 trainign loss: 10.1137 avg training loss: 6.1662
batch: [1050/10547] batch time: 2.385 trainign loss: 7.3259 avg training loss: 6.1664
batch: [1060/10547] batch time: 0.054 trainign loss: 6.0518 avg training loss: 6.1664
batch: [1070/10547] batch time: 2.136 trainign loss: 6.2017 avg training loss: 6.1664
batch: [1080/10547] batch time: 0.047 trainign loss: 2.8384 avg training loss: 6.1661
batch: [1090/10547] batch time: 2.072 trainign loss: 5.4543 avg training loss: 6.1659
batch: [1100/10547] batch time: 1.059 trainign loss: 3.9329 avg training loss: 6.1656
batch: [1110/10547] batch time: 1.055 trainign loss: 5.2380 avg training loss: 6.1652
batch: [1120/10547] batch time: 0.932 trainign loss: 5.1172 avg training loss: 6.1648
batch: [1130/10547] batch time: 1.143 trainign loss: 5.8144 avg training loss: 6.1646
batch: [1140/10547] batch time: 1.394 trainign loss: 6.3090 avg training loss: 6.1646
batch: [1150/10547] batch time: 1.904 trainign loss: 5.3934 avg training loss: 6.1645
batch: [1160/10547] batch time: 0.501 trainign loss: 4.7548 avg training loss: 6.1643
batch: [1170/10547] batch time: 2.510 trainign loss: 5.6957 avg training loss: 6.1642
batch: [1180/10547] batch time: 0.047 trainign loss: 5.5800 avg training loss: 6.1642
batch: [1190/10547] batch time: 2.418 trainign loss: 5.7311 avg training loss: 6.1641
batch: [1200/10547] batch time: 0.410 trainign loss: 4.0650 avg training loss: 6.1639
batch: [1210/10547] batch time: 2.494 trainign loss: 4.6062 avg training loss: 6.1637
batch: [1220/10547] batch time: 0.046 trainign loss: 5.6543 avg training loss: 6.1635
batch: [1230/10547] batch time: 2.051 trainign loss: 5.2049 avg training loss: 6.1633
batch: [1240/10547] batch time: 0.047 trainign loss: 5.2571 avg training loss: 6.1632
batch: [1250/10547] batch time: 2.487 trainign loss: 4.9716 avg training loss: 6.1629
batch: [1260/10547] batch time: 0.055 trainign loss: 0.1435 avg training loss: 6.1622
batch: [1270/10547] batch time: 2.769 trainign loss: 6.5054 avg training loss: 6.1622
batch: [1280/10547] batch time: 0.053 trainign loss: 3.3980 avg training loss: 6.1621
batch: [1290/10547] batch time: 1.954 trainign loss: 6.3054 avg training loss: 6.1621
batch: [1300/10547] batch time: 0.054 trainign loss: 5.3972 avg training loss: 6.1620
batch: [1310/10547] batch time: 2.550 trainign loss: 4.3443 avg training loss: 6.1618
batch: [1320/10547] batch time: 0.046 trainign loss: 3.2648 avg training loss: 6.1614
batch: [1330/10547] batch time: 2.194 trainign loss: 6.4572 avg training loss: 6.1612
batch: [1340/10547] batch time: 0.054 trainign loss: 6.3400 avg training loss: 6.1612
batch: [1350/10547] batch time: 2.373 trainign loss: 5.5802 avg training loss: 6.1611
batch: [1360/10547] batch time: 0.046 trainign loss: 5.3145 avg training loss: 6.1610
batch: [1370/10547] batch time: 1.798 trainign loss: 4.8504 avg training loss: 6.1609
batch: [1380/10547] batch time: 0.047 trainign loss: 5.3775 avg training loss: 6.1607
batch: [1390/10547] batch time: 2.102 trainign loss: 7.0528 avg training loss: 6.1602
batch: [1400/10547] batch time: 0.046 trainign loss: 6.5164 avg training loss: 6.1602
batch: [1410/10547] batch time: 1.978 trainign loss: 5.6045 avg training loss: 6.1600
batch: [1420/10547] batch time: 0.047 trainign loss: 5.1092 avg training loss: 6.1599
batch: [1430/10547] batch time: 2.014 trainign loss: 5.5907 avg training loss: 6.1597
batch: [1440/10547] batch time: 0.054 trainign loss: 5.1252 avg training loss: 6.1596
batch: [1450/10547] batch time: 2.289 trainign loss: 1.9470 avg training loss: 6.1592
batch: [1460/10547] batch time: 0.047 trainign loss: 3.3140 avg training loss: 6.1588
batch: [1470/10547] batch time: 2.609 trainign loss: 5.9974 avg training loss: 6.1588
batch: [1480/10547] batch time: 0.046 trainign loss: 5.9051 avg training loss: 6.1585
batch: [1490/10547] batch time: 2.427 trainign loss: 5.2955 avg training loss: 6.1584
batch: [1500/10547] batch time: 0.046 trainign loss: 6.1891 avg training loss: 6.1584
batch: [1510/10547] batch time: 1.976 trainign loss: 5.5370 avg training loss: 6.1583
batch: [1520/10547] batch time: 0.046 trainign loss: 4.9929 avg training loss: 6.1581
batch: [1530/10547] batch time: 2.392 trainign loss: 5.2836 avg training loss: 6.1580
batch: [1540/10547] batch time: 0.046 trainign loss: 1.0344 avg training loss: 6.1575
batch: [1550/10547] batch time: 2.259 trainign loss: 4.6592 avg training loss: 6.1574
batch: [1560/10547] batch time: 0.048 trainign loss: 6.5416 avg training loss: 6.1572
batch: [1570/10547] batch time: 2.552 trainign loss: 5.5305 avg training loss: 6.1572
batch: [1580/10547] batch time: 0.047 trainign loss: 5.9212 avg training loss: 6.1570
batch: [1590/10547] batch time: 2.459 trainign loss: 2.9468 avg training loss: 6.1567
batch: [1600/10547] batch time: 0.047 trainign loss: 5.8749 avg training loss: 6.1566
batch: [1610/10547] batch time: 2.160 trainign loss: 2.0761 avg training loss: 6.1562
batch: [1620/10547] batch time: 0.047 trainign loss: 5.3266 avg training loss: 6.1561
batch: [1630/10547] batch time: 1.958 trainign loss: 1.5589 avg training loss: 6.1557
batch: [1640/10547] batch time: 0.046 trainign loss: 5.6957 avg training loss: 6.1556
batch: [1650/10547] batch time: 2.325 trainign loss: 5.5040 avg training loss: 6.1555
batch: [1660/10547] batch time: 0.049 trainign loss: 4.5048 avg training loss: 6.1552
batch: [1670/10547] batch time: 2.719 trainign loss: 2.8760 avg training loss: 6.1548
batch: [1680/10547] batch time: 0.048 trainign loss: 5.1839 avg training loss: 6.1545
batch: [1690/10547] batch time: 2.040 trainign loss: 5.7768 avg training loss: 6.1545
batch: [1700/10547] batch time: 0.047 trainign loss: 5.4557 avg training loss: 6.1542
batch: [1710/10547] batch time: 2.359 trainign loss: 5.4467 avg training loss: 6.1541
batch: [1720/10547] batch time: 0.046 trainign loss: 5.7792 avg training loss: 6.1539
batch: [1730/10547] batch time: 2.649 trainign loss: 5.7848 avg training loss: 6.1538
batch: [1740/10547] batch time: 0.048 trainign loss: 6.2482 avg training loss: 6.1538
batch: [1750/10547] batch time: 2.470 trainign loss: 4.8714 avg training loss: 6.1536
batch: [1760/10547] batch time: 0.046 trainign loss: 2.1786 avg training loss: 6.1531
batch: [1770/10547] batch time: 2.060 trainign loss: 6.4173 avg training loss: 6.1531
batch: [1780/10547] batch time: 0.046 trainign loss: 5.2378 avg training loss: 6.1529
batch: [1790/10547] batch time: 2.110 trainign loss: 4.6359 avg training loss: 6.1527
batch: [1800/10547] batch time: 0.053 trainign loss: 4.7782 avg training loss: 6.1524
batch: [1810/10547] batch time: 2.109 trainign loss: 5.8832 avg training loss: 6.1524
batch: [1820/10547] batch time: 0.048 trainign loss: 5.8518 avg training loss: 6.1523
batch: [1830/10547] batch time: 2.448 trainign loss: 4.5393 avg training loss: 6.1521
batch: [1840/10547] batch time: 0.046 trainign loss: 6.1363 avg training loss: 6.1520
batch: [1850/10547] batch time: 1.788 trainign loss: 4.4244 avg training loss: 6.1517
batch: [1860/10547] batch time: 0.049 trainign loss: 4.2112 avg training loss: 6.1514
batch: [1870/10547] batch time: 1.518 trainign loss: 6.2438 avg training loss: 6.1513
batch: [1880/10547] batch time: 0.050 trainign loss: 4.9472 avg training loss: 6.1513
batch: [1890/10547] batch time: 2.177 trainign loss: 5.5169 avg training loss: 6.1511
batch: [1900/10547] batch time: 0.466 trainign loss: 2.3712 avg training loss: 6.1508
batch: [1910/10547] batch time: 1.792 trainign loss: 1.3113 avg training loss: 6.1498
batch: [1920/10547] batch time: 0.048 trainign loss: 6.7973 avg training loss: 6.1500
batch: [1930/10547] batch time: 1.364 trainign loss: 5.9312 avg training loss: 6.1500
batch: [1940/10547] batch time: 0.046 trainign loss: 6.1519 avg training loss: 6.1500
batch: [1950/10547] batch time: 0.561 trainign loss: 5.5811 avg training loss: 6.1499
batch: [1960/10547] batch time: 0.046 trainign loss: 4.9777 avg training loss: 6.1496
batch: [1970/10547] batch time: 0.126 trainign loss: 5.2604 avg training loss: 6.1495
batch: [1980/10547] batch time: 0.054 trainign loss: 5.2730 avg training loss: 6.1493
batch: [1990/10547] batch time: 0.055 trainign loss: 0.2073 avg training loss: 6.1487
batch: [2000/10547] batch time: 0.049 trainign loss: 7.4096 avg training loss: 6.1482
batch: [2010/10547] batch time: 0.046 trainign loss: 6.3570 avg training loss: 6.1483
batch: [2020/10547] batch time: 0.054 trainign loss: 5.7636 avg training loss: 6.1482
batch: [2030/10547] batch time: 0.055 trainign loss: 4.2801 avg training loss: 6.1480
batch: [2040/10547] batch time: 0.046 trainign loss: 4.9928 avg training loss: 6.1476
batch: [2050/10547] batch time: 0.046 trainign loss: 5.8045 avg training loss: 6.1476
batch: [2060/10547] batch time: 0.046 trainign loss: 3.8109 avg training loss: 6.1475
batch: [2070/10547] batch time: 0.046 trainign loss: 3.5985 avg training loss: 6.1473
batch: [2080/10547] batch time: 0.047 trainign loss: 4.3336 avg training loss: 6.1469
batch: [2090/10547] batch time: 0.048 trainign loss: 5.0751 avg training loss: 6.1467
batch: [2100/10547] batch time: 0.047 trainign loss: 0.0689 avg training loss: 6.1459
batch: [2110/10547] batch time: 0.056 trainign loss: 6.5354 avg training loss: 6.1460
batch: [2120/10547] batch time: 0.046 trainign loss: 4.5287 avg training loss: 6.1460
batch: [2130/10547] batch time: 0.046 trainign loss: 5.6373 avg training loss: 6.1459
batch: [2140/10547] batch time: 0.054 trainign loss: 5.2600 avg training loss: 6.1458
batch: [2150/10547] batch time: 0.048 trainign loss: 3.7809 avg training loss: 6.1456
batch: [2160/10547] batch time: 0.046 trainign loss: 6.0568 avg training loss: 6.1452
batch: [2170/10547] batch time: 0.046 trainign loss: 6.3433 avg training loss: 6.1452
batch: [2180/10547] batch time: 0.054 trainign loss: 5.5585 avg training loss: 6.1451
batch: [2190/10547] batch time: 0.046 trainign loss: 0.2027 avg training loss: 6.1445
batch: [2200/10547] batch time: 0.054 trainign loss: 5.7751 avg training loss: 6.1443
batch: [2210/10547] batch time: 0.116 trainign loss: 4.1000 avg training loss: 6.1442
batch: [2220/10547] batch time: 0.046 trainign loss: 6.5841 avg training loss: 6.1439
batch: [2230/10547] batch time: 0.190 trainign loss: 4.5924 avg training loss: 6.1438
batch: [2240/10547] batch time: 0.054 trainign loss: 5.9040 avg training loss: 6.1435
batch: [2250/10547] batch time: 0.054 trainign loss: 6.1128 avg training loss: 6.1436
batch: [2260/10547] batch time: 0.053 trainign loss: 5.6292 avg training loss: 6.1435
batch: [2270/10547] batch time: 0.046 trainign loss: 5.4742 avg training loss: 6.1433
batch: [2280/10547] batch time: 0.047 trainign loss: 3.4355 avg training loss: 6.1431
batch: [2290/10547] batch time: 0.558 trainign loss: 5.7223 avg training loss: 6.1429
batch: [2300/10547] batch time: 0.046 trainign loss: 4.3753 avg training loss: 6.1427
batch: [2310/10547] batch time: 1.930 trainign loss: 5.5542 avg training loss: 6.1425
batch: [2320/10547] batch time: 0.049 trainign loss: 6.2999 avg training loss: 6.1423
batch: [2330/10547] batch time: 1.425 trainign loss: 5.5300 avg training loss: 6.1421
batch: [2340/10547] batch time: 0.046 trainign loss: 5.2589 avg training loss: 6.1420
batch: [2350/10547] batch time: 0.949 trainign loss: 4.8881 avg training loss: 6.1418
batch: [2360/10547] batch time: 0.048 trainign loss: 4.5981 avg training loss: 6.1413
batch: [2370/10547] batch time: 1.475 trainign loss: 4.7543 avg training loss: 6.1412
batch: [2380/10547] batch time: 0.046 trainign loss: 2.4804 avg training loss: 6.1408
batch: [2390/10547] batch time: 0.593 trainign loss: 9.9684 avg training loss: 6.1401
batch: [2400/10547] batch time: 0.047 trainign loss: 6.6910 avg training loss: 6.1402
batch: [2410/10547] batch time: 1.760 trainign loss: 4.3504 avg training loss: 6.1401
batch: [2420/10547] batch time: 0.046 trainign loss: 6.1047 avg training loss: 6.1399
batch: [2430/10547] batch time: 1.361 trainign loss: 5.2350 avg training loss: 6.1397
batch: [2440/10547] batch time: 0.050 trainign loss: 4.9094 avg training loss: 6.1395
batch: [2450/10547] batch time: 1.848 trainign loss: 4.9508 avg training loss: 6.1392
batch: [2460/10547] batch time: 0.047 trainign loss: 0.6991 avg training loss: 6.1387
batch: [2470/10547] batch time: 1.190 trainign loss: 8.7964 avg training loss: 6.1381
batch: [2480/10547] batch time: 0.060 trainign loss: 4.0910 avg training loss: 6.1380
batch: [2490/10547] batch time: 2.197 trainign loss: 6.0087 avg training loss: 6.1377
batch: [2500/10547] batch time: 0.554 trainign loss: 5.7910 avg training loss: 6.1375
batch: [2510/10547] batch time: 0.845 trainign loss: 0.5049 avg training loss: 6.1370
batch: [2520/10547] batch time: 0.606 trainign loss: 5.8986 avg training loss: 6.1368
batch: [2530/10547] batch time: 1.869 trainign loss: 5.8426 avg training loss: 6.1367
batch: [2540/10547] batch time: 0.046 trainign loss: 6.0811 avg training loss: 6.1367
batch: [2550/10547] batch time: 2.171 trainign loss: 5.5500 avg training loss: 6.1364
batch: [2560/10547] batch time: 0.208 trainign loss: 5.5900 avg training loss: 6.1363
batch: [2570/10547] batch time: 2.290 trainign loss: 5.3904 avg training loss: 6.1362
batch: [2580/10547] batch time: 0.446 trainign loss: 4.9606 avg training loss: 6.1361
batch: [2590/10547] batch time: 2.119 trainign loss: 5.5746 avg training loss: 6.1359
batch: [2600/10547] batch time: 0.330 trainign loss: 2.6246 avg training loss: 6.1357
batch: [2610/10547] batch time: 1.688 trainign loss: 6.6404 avg training loss: 6.1356
batch: [2620/10547] batch time: 0.865 trainign loss: 5.7276 avg training loss: 6.1356
batch: [2630/10547] batch time: 1.151 trainign loss: 5.5840 avg training loss: 6.1354
batch: [2640/10547] batch time: 1.306 trainign loss: 2.4771 avg training loss: 6.1351
batch: [2650/10547] batch time: 1.494 trainign loss: 5.0211 avg training loss: 6.1348
batch: [2660/10547] batch time: 0.582 trainign loss: 6.8950 avg training loss: 6.1346
batch: [2670/10547] batch time: 2.328 trainign loss: 5.2535 avg training loss: 6.1346
batch: [2680/10547] batch time: 0.048 trainign loss: 5.2096 avg training loss: 6.1344
batch: [2690/10547] batch time: 1.590 trainign loss: 4.2278 avg training loss: 6.1342
batch: [2700/10547] batch time: 0.046 trainign loss: 5.4223 avg training loss: 6.1339
batch: [2710/10547] batch time: 2.329 trainign loss: 5.3528 avg training loss: 6.1337
batch: [2720/10547] batch time: 0.047 trainign loss: 5.2224 avg training loss: 6.1336
batch: [2730/10547] batch time: 2.693 trainign loss: 5.8030 avg training loss: 6.1335
batch: [2740/10547] batch time: 0.055 trainign loss: 6.6679 avg training loss: 6.1330
batch: [2750/10547] batch time: 2.204 trainign loss: 5.0335 avg training loss: 6.1330
batch: [2760/10547] batch time: 0.046 trainign loss: 4.8705 avg training loss: 6.1327
batch: [2770/10547] batch time: 2.524 trainign loss: 0.2443 avg training loss: 6.1321
batch: [2780/10547] batch time: 0.045 trainign loss: 6.3535 avg training loss: 6.1321
batch: [2790/10547] batch time: 2.217 trainign loss: 5.5662 avg training loss: 6.1321
batch: [2800/10547] batch time: 0.047 trainign loss: 5.1082 avg training loss: 6.1319
batch: [2810/10547] batch time: 2.521 trainign loss: 4.2744 avg training loss: 6.1316
batch: [2820/10547] batch time: 0.047 trainign loss: 6.0384 avg training loss: 6.1314
batch: [2830/10547] batch time: 2.550 trainign loss: 5.0903 avg training loss: 6.1313
batch: [2840/10547] batch time: 0.046 trainign loss: 4.6894 avg training loss: 6.1310
batch: [2850/10547] batch time: 1.969 trainign loss: 6.7423 avg training loss: 6.1308
batch: [2860/10547] batch time: 0.046 trainign loss: 5.8264 avg training loss: 6.1308
batch: [2870/10547] batch time: 2.754 trainign loss: 5.5737 avg training loss: 6.1307
batch: [2880/10547] batch time: 0.045 trainign loss: 5.4739 avg training loss: 6.1305
batch: [2890/10547] batch time: 2.207 trainign loss: 5.3943 avg training loss: 6.1304
batch: [2900/10547] batch time: 0.046 trainign loss: 5.2258 avg training loss: 6.1303
batch: [2910/10547] batch time: 2.380 trainign loss: 5.4532 avg training loss: 6.1301
batch: [2920/10547] batch time: 0.047 trainign loss: 3.8206 avg training loss: 6.1300
batch: [2930/10547] batch time: 2.319 trainign loss: 3.7764 avg training loss: 6.1296
batch: [2940/10547] batch time: 0.046 trainign loss: 6.2067 avg training loss: 6.1293
batch: [2950/10547] batch time: 2.137 trainign loss: 4.1912 avg training loss: 6.1291
batch: [2960/10547] batch time: 0.046 trainign loss: 5.5756 avg training loss: 6.1289
batch: [2970/10547] batch time: 2.107 trainign loss: 5.4648 avg training loss: 6.1289
batch: [2980/10547] batch time: 0.054 trainign loss: 2.9066 avg training loss: 6.1285
batch: [2990/10547] batch time: 2.375 trainign loss: 6.0674 avg training loss: 6.1282
batch: [3000/10547] batch time: 0.046 trainign loss: 5.3954 avg training loss: 6.1281
batch: [3010/10547] batch time: 2.508 trainign loss: 4.2888 avg training loss: 6.1277
batch: [3020/10547] batch time: 0.046 trainign loss: 5.5455 avg training loss: 6.1275
batch: [3030/10547] batch time: 2.345 trainign loss: 6.1974 avg training loss: 6.1273
batch: [3040/10547] batch time: 0.047 trainign loss: 4.0443 avg training loss: 6.1271
batch: [3050/10547] batch time: 2.482 trainign loss: 5.2014 avg training loss: 6.1269
batch: [3060/10547] batch time: 0.046 trainign loss: 5.4653 avg training loss: 6.1267
batch: [3070/10547] batch time: 2.435 trainign loss: 6.4418 avg training loss: 6.1264
batch: [3080/10547] batch time: 0.046 trainign loss: 6.4924 avg training loss: 6.1265
batch: [3090/10547] batch time: 2.936 trainign loss: 5.2775 avg training loss: 6.1264
batch: [3100/10547] batch time: 0.046 trainign loss: 5.0610 avg training loss: 6.1262
batch: [3110/10547] batch time: 2.132 trainign loss: 5.0240 avg training loss: 6.1260
batch: [3120/10547] batch time: 0.046 trainign loss: 6.3859 avg training loss: 6.1258
batch: [3130/10547] batch time: 2.221 trainign loss: 4.9589 avg training loss: 6.1257
batch: [3140/10547] batch time: 0.056 trainign loss: 5.5371 avg training loss: 6.1255
batch: [3150/10547] batch time: 2.185 trainign loss: 5.3278 avg training loss: 6.1252
batch: [3160/10547] batch time: 0.047 trainign loss: 5.2800 avg training loss: 6.1249
batch: [3170/10547] batch time: 1.835 trainign loss: 4.0446 avg training loss: 6.1247
batch: [3180/10547] batch time: 0.047 trainign loss: 4.1241 avg training loss: 6.1244
batch: [3190/10547] batch time: 2.382 trainign loss: 2.5542 avg training loss: 6.1238
batch: [3200/10547] batch time: 0.046 trainign loss: 6.3999 avg training loss: 6.1238
batch: [3210/10547] batch time: 2.122 trainign loss: 5.5528 avg training loss: 6.1238
batch: [3220/10547] batch time: 0.046 trainign loss: 4.9702 avg training loss: 6.1236
batch: [3230/10547] batch time: 1.466 trainign loss: 5.0903 avg training loss: 6.1233
batch: [3240/10547] batch time: 0.537 trainign loss: 3.3252 avg training loss: 6.1231
batch: [3250/10547] batch time: 2.605 trainign loss: 5.2887 avg training loss: 6.1228
batch: [3260/10547] batch time: 0.046 trainign loss: 4.7788 avg training loss: 6.1226
batch: [3270/10547] batch time: 2.158 trainign loss: 4.5865 avg training loss: 6.1225
batch: [3280/10547] batch time: 0.046 trainign loss: 5.1276 avg training loss: 6.1223
batch: [3290/10547] batch time: 2.093 trainign loss: 4.7836 avg training loss: 6.1221
batch: [3300/10547] batch time: 0.396 trainign loss: 5.3445 avg training loss: 6.1219
batch: [3310/10547] batch time: 1.265 trainign loss: 4.3386 avg training loss: 6.1217
batch: [3320/10547] batch time: 0.537 trainign loss: 4.5724 avg training loss: 6.1214
batch: [3330/10547] batch time: 1.123 trainign loss: 6.3550 avg training loss: 6.1213
batch: [3340/10547] batch time: 0.046 trainign loss: 4.7961 avg training loss: 6.1212
batch: [3350/10547] batch time: 0.472 trainign loss: 4.9733 avg training loss: 6.1210
batch: [3360/10547] batch time: 0.046 trainign loss: 5.8391 avg training loss: 6.1209
batch: [3370/10547] batch time: 0.735 trainign loss: 1.0372 avg training loss: 6.1204
batch: [3380/10547] batch time: 0.047 trainign loss: 5.5780 avg training loss: 6.1202
batch: [3390/10547] batch time: 0.046 trainign loss: 5.8752 avg training loss: 6.1200
batch: [3400/10547] batch time: 0.047 trainign loss: 2.9506 avg training loss: 6.1197
batch: [3410/10547] batch time: 0.046 trainign loss: 6.3156 avg training loss: 6.1197
batch: [3420/10547] batch time: 0.049 trainign loss: 5.7047 avg training loss: 6.1197
batch: [3430/10547] batch time: 0.047 trainign loss: 3.1008 avg training loss: 6.1194
batch: [3440/10547] batch time: 0.386 trainign loss: 6.0747 avg training loss: 6.1193
batch: [3450/10547] batch time: 0.049 trainign loss: 4.0722 avg training loss: 6.1191
batch: [3460/10547] batch time: 0.457 trainign loss: 6.2172 avg training loss: 6.1190
batch: [3470/10547] batch time: 0.046 trainign loss: 4.6359 avg training loss: 6.1189
batch: [3480/10547] batch time: 0.046 trainign loss: 4.8355 avg training loss: 6.1187
batch: [3490/10547] batch time: 0.047 trainign loss: 5.6137 avg training loss: 6.1184
batch: [3500/10547] batch time: 0.046 trainign loss: 4.8638 avg training loss: 6.1183
batch: [3510/10547] batch time: 0.273 trainign loss: 3.1869 avg training loss: 6.1180
batch: [3520/10547] batch time: 0.047 trainign loss: 5.2400 avg training loss: 6.1176
batch: [3530/10547] batch time: 0.401 trainign loss: 5.6701 avg training loss: 6.1174
batch: [3540/10547] batch time: 0.047 trainign loss: 4.6587 avg training loss: 6.1173
batch: [3550/10547] batch time: 1.178 trainign loss: 5.7766 avg training loss: 6.1171
batch: [3560/10547] batch time: 0.047 trainign loss: 3.9881 avg training loss: 6.1171
batch: [3570/10547] batch time: 1.724 trainign loss: 5.1627 avg training loss: 6.1169
batch: [3580/10547] batch time: 0.048 trainign loss: 5.7913 avg training loss: 6.1166
batch: [3590/10547] batch time: 1.806 trainign loss: 4.5818 avg training loss: 6.1164
batch: [3600/10547] batch time: 0.050 trainign loss: 6.3468 avg training loss: 6.1162
batch: [3610/10547] batch time: 0.557 trainign loss: 5.6806 avg training loss: 6.1161
batch: [3620/10547] batch time: 0.047 trainign loss: 4.6355 avg training loss: 6.1159
batch: [3630/10547] batch time: 0.408 trainign loss: 5.9739 avg training loss: 6.1157
batch: [3640/10547] batch time: 0.048 trainign loss: 2.9154 avg training loss: 6.1155
batch: [3650/10547] batch time: 0.046 trainign loss: 5.7643 avg training loss: 6.1152
batch: [3660/10547] batch time: 0.046 trainign loss: 1.0938 avg training loss: 6.1148
batch: [3670/10547] batch time: 0.047 trainign loss: 6.5449 avg training loss: 6.1148
batch: [3680/10547] batch time: 0.047 trainign loss: 3.2080 avg training loss: 6.1146
batch: [3690/10547] batch time: 0.046 trainign loss: 1.5720 avg training loss: 6.1142
batch: [3700/10547] batch time: 0.047 trainign loss: 6.4554 avg training loss: 6.1141
batch: [3710/10547] batch time: 0.054 trainign loss: 5.7948 avg training loss: 6.1140
batch: [3720/10547] batch time: 0.047 trainign loss: 5.0244 avg training loss: 6.1138
batch: [3730/10547] batch time: 0.047 trainign loss: 4.8045 avg training loss: 6.1136
batch: [3740/10547] batch time: 0.049 trainign loss: 5.2130 avg training loss: 6.1134
batch: [3750/10547] batch time: 0.047 trainign loss: 4.7291 avg training loss: 6.1133
batch: [3760/10547] batch time: 0.047 trainign loss: 4.5300 avg training loss: 6.1130
batch: [3770/10547] batch time: 0.054 trainign loss: 2.9917 avg training loss: 6.1128
batch: [3780/10547] batch time: 0.053 trainign loss: 6.0888 avg training loss: 6.1126
batch: [3790/10547] batch time: 0.041 trainign loss: 4.3991 avg training loss: 6.1125
batch: [3800/10547] batch time: 0.046 trainign loss: 2.3525 avg training loss: 6.1122
batch: [3810/10547] batch time: 0.045 trainign loss: 2.9971 avg training loss: 6.1120
batch: [3820/10547] batch time: 0.046 trainign loss: 5.8873 avg training loss: 6.1115
batch: [3830/10547] batch time: 0.051 trainign loss: 7.0646 avg training loss: 6.1116
batch: [3840/10547] batch time: 0.049 trainign loss: 5.8161 avg training loss: 6.1116
batch: [3850/10547] batch time: 0.047 trainign loss: 5.4687 avg training loss: 6.1115
batch: [3860/10547] batch time: 0.047 trainign loss: 5.2770 avg training loss: 6.1113
batch: [3870/10547] batch time: 0.053 trainign loss: 5.9320 avg training loss: 6.1112
batch: [3880/10547] batch time: 0.046 trainign loss: 4.7001 avg training loss: 6.1110
batch: [3890/10547] batch time: 0.048 trainign loss: 3.7671 avg training loss: 6.1108
batch: [3900/10547] batch time: 0.049 trainign loss: 1.5422 avg training loss: 6.1102
batch: [3910/10547] batch time: 0.046 trainign loss: 5.9050 avg training loss: 6.1103
batch: [3920/10547] batch time: 0.047 trainign loss: 5.6855 avg training loss: 6.1103
batch: [3930/10547] batch time: 0.047 trainign loss: 5.5399 avg training loss: 6.1102
batch: [3940/10547] batch time: 0.054 trainign loss: 4.7651 avg training loss: 6.1100
batch: [3950/10547] batch time: 0.050 trainign loss: 4.4143 avg training loss: 6.1098
batch: [3960/10547] batch time: 0.053 trainign loss: 0.5449 avg training loss: 6.1093
batch: [3970/10547] batch time: 0.053 trainign loss: 5.0489 avg training loss: 6.1089
batch: [3980/10547] batch time: 0.054 trainign loss: 6.0528 avg training loss: 6.1089
batch: [3990/10547] batch time: 0.044 trainign loss: 4.9152 avg training loss: 6.1088
batch: [4000/10547] batch time: 0.046 trainign loss: 4.8697 avg training loss: 6.1087
batch: [4010/10547] batch time: 0.054 trainign loss: 5.9093 avg training loss: 6.1085
batch: [4020/10547] batch time: 0.054 trainign loss: 5.4613 avg training loss: 6.1084
batch: [4030/10547] batch time: 0.048 trainign loss: 3.8532 avg training loss: 6.1082
batch: [4040/10547] batch time: 0.047 trainign loss: 5.6464 avg training loss: 6.1081
batch: [4050/10547] batch time: 0.047 trainign loss: 4.0428 avg training loss: 6.1079
batch: [4060/10547] batch time: 0.046 trainign loss: 4.3266 avg training loss: 6.1078
batch: [4070/10547] batch time: 0.048 trainign loss: 5.1322 avg training loss: 6.1076
batch: [4080/10547] batch time: 0.047 trainign loss: 6.1060 avg training loss: 6.1075
batch: [4090/10547] batch time: 0.046 trainign loss: 5.5894 avg training loss: 6.1075
batch: [4100/10547] batch time: 0.047 trainign loss: 5.0228 avg training loss: 6.1074
batch: [4110/10547] batch time: 0.047 trainign loss: 4.2926 avg training loss: 6.1072
batch: [4120/10547] batch time: 0.046 trainign loss: 3.1890 avg training loss: 6.1069
batch: [4130/10547] batch time: 0.046 trainign loss: 0.1293 avg training loss: 6.1062
batch: [4140/10547] batch time: 0.046 trainign loss: 4.1321 avg training loss: 6.1061
batch: [4150/10547] batch time: 0.046 trainign loss: 5.7404 avg training loss: 6.1057
batch: [4160/10547] batch time: 0.047 trainign loss: 4.2434 avg training loss: 6.1056
batch: [4170/10547] batch time: 0.046 trainign loss: 4.7676 avg training loss: 6.1055
batch: [4180/10547] batch time: 0.051 trainign loss: 3.9179 avg training loss: 6.1052
batch: [4190/10547] batch time: 0.047 trainign loss: 6.2361 avg training loss: 6.1050
batch: [4200/10547] batch time: 0.054 trainign loss: 5.9562 avg training loss: 6.1049
batch: [4210/10547] batch time: 0.046 trainign loss: 4.6012 avg training loss: 6.1047
batch: [4220/10547] batch time: 0.047 trainign loss: 5.4190 avg training loss: 6.1046
batch: [4230/10547] batch time: 0.055 trainign loss: 6.1974 avg training loss: 6.1044
batch: [4240/10547] batch time: 0.050 trainign loss: 5.2708 avg training loss: 6.1042
batch: [4250/10547] batch time: 0.048 trainign loss: 5.7658 avg training loss: 6.1040
batch: [4260/10547] batch time: 0.046 trainign loss: 4.9868 avg training loss: 6.1039
batch: [4270/10547] batch time: 0.053 trainign loss: 4.0053 avg training loss: 6.1037
batch: [4280/10547] batch time: 0.049 trainign loss: 0.1082 avg training loss: 6.1030
batch: [4290/10547] batch time: 0.056 trainign loss: 0.0011 avg training loss: 6.1020
batch: [4300/10547] batch time: 0.049 trainign loss: 1.9759 avg training loss: 6.1009
batch: [4310/10547] batch time: 0.048 trainign loss: 5.9883 avg training loss: 6.1013
batch: [4320/10547] batch time: 0.047 trainign loss: 5.4091 avg training loss: 6.1013
batch: [4330/10547] batch time: 0.046 trainign loss: 4.0246 avg training loss: 6.1011
batch: [4340/10547] batch time: 0.047 trainign loss: 4.9262 avg training loss: 6.1009
batch: [4350/10547] batch time: 0.077 trainign loss: 5.6226 avg training loss: 6.1009
batch: [4360/10547] batch time: 0.046 trainign loss: 5.6592 avg training loss: 6.1007
batch: [4370/10547] batch time: 0.365 trainign loss: 5.7897 avg training loss: 6.1008
batch: [4380/10547] batch time: 0.055 trainign loss: 4.7726 avg training loss: 6.1006
batch: [4390/10547] batch time: 0.268 trainign loss: 5.1644 avg training loss: 6.1004
batch: [4400/10547] batch time: 0.047 trainign loss: 5.4223 avg training loss: 6.1002
batch: [4410/10547] batch time: 0.054 trainign loss: 5.3149 avg training loss: 6.1001
batch: [4420/10547] batch time: 0.045 trainign loss: 5.0249 avg training loss: 6.1000
batch: [4430/10547] batch time: 0.046 trainign loss: 5.5876 avg training loss: 6.0998
batch: [4440/10547] batch time: 0.046 trainign loss: 5.1028 avg training loss: 6.0993
batch: [4450/10547] batch time: 0.054 trainign loss: 5.5842 avg training loss: 6.0993
batch: [4460/10547] batch time: 0.046 trainign loss: 6.3407 avg training loss: 6.0991
batch: [4470/10547] batch time: 0.047 trainign loss: 5.5331 avg training loss: 6.0990
batch: [4480/10547] batch time: 0.046 trainign loss: 5.7867 avg training loss: 6.0987
batch: [4490/10547] batch time: 0.046 trainign loss: 6.6701 avg training loss: 6.0985
batch: [4500/10547] batch time: 0.046 trainign loss: 3.9437 avg training loss: 6.0985
batch: [4510/10547] batch time: 0.048 trainign loss: 5.7628 avg training loss: 6.0981
batch: [4520/10547] batch time: 0.047 trainign loss: 6.6012 avg training loss: 6.0979
batch: [4530/10547] batch time: 0.054 trainign loss: 4.6263 avg training loss: 6.0979
batch: [4540/10547] batch time: 0.043 trainign loss: 5.3970 avg training loss: 6.0977
batch: [4550/10547] batch time: 0.046 trainign loss: 5.4514 avg training loss: 6.0977
batch: [4560/10547] batch time: 0.046 trainign loss: 4.5280 avg training loss: 6.0975
batch: [4570/10547] batch time: 0.046 trainign loss: 4.9034 avg training loss: 6.0972
batch: [4580/10547] batch time: 0.046 trainign loss: 4.9242 avg training loss: 6.0971
batch: [4590/10547] batch time: 0.080 trainign loss: 4.8715 avg training loss: 6.0969
batch: [4600/10547] batch time: 0.046 trainign loss: 5.4787 avg training loss: 6.0966
batch: [4610/10547] batch time: 0.739 trainign loss: 5.3907 avg training loss: 6.0965
batch: [4620/10547] batch time: 0.054 trainign loss: 5.5140 avg training loss: 6.0963
batch: [4630/10547] batch time: 0.954 trainign loss: 4.7123 avg training loss: 6.0961
batch: [4640/10547] batch time: 0.046 trainign loss: 6.0929 avg training loss: 6.0960
batch: [4650/10547] batch time: 1.709 trainign loss: 4.3724 avg training loss: 6.0958
batch: [4660/10547] batch time: 0.054 trainign loss: 5.8796 avg training loss: 6.0956
batch: [4670/10547] batch time: 1.687 trainign loss: 4.6262 avg training loss: 6.0953
batch: [4680/10547] batch time: 0.851 trainign loss: 4.7889 avg training loss: 6.0951
batch: [4690/10547] batch time: 0.738 trainign loss: 4.7742 avg training loss: 6.0948
batch: [4700/10547] batch time: 1.515 trainign loss: 5.8611 avg training loss: 6.0946
batch: [4710/10547] batch time: 0.989 trainign loss: 5.4591 avg training loss: 6.0944
batch: [4720/10547] batch time: 1.506 trainign loss: 6.1550 avg training loss: 6.0941
batch: [4730/10547] batch time: 1.451 trainign loss: 6.0568 avg training loss: 6.0939
batch: [4740/10547] batch time: 0.428 trainign loss: 5.1732 avg training loss: 6.0938
batch: [4750/10547] batch time: 1.801 trainign loss: 5.5663 avg training loss: 6.0936
batch: [4760/10547] batch time: 0.045 trainign loss: 5.9682 avg training loss: 6.0936
batch: [4770/10547] batch time: 2.365 trainign loss: 6.3296 avg training loss: 6.0934
batch: [4780/10547] batch time: 0.054 trainign loss: 6.3991 avg training loss: 6.0934
batch: [4790/10547] batch time: 2.406 trainign loss: 4.9277 avg training loss: 6.0933
batch: [4800/10547] batch time: 0.476 trainign loss: 5.9032 avg training loss: 6.0931
batch: [4810/10547] batch time: 1.563 trainign loss: 6.0850 avg training loss: 6.0930
batch: [4820/10547] batch time: 0.414 trainign loss: 4.2512 avg training loss: 6.0928
batch: [4830/10547] batch time: 1.924 trainign loss: 6.1151 avg training loss: 6.0927
batch: [4840/10547] batch time: 0.739 trainign loss: 4.7982 avg training loss: 6.0925
batch: [4850/10547] batch time: 0.588 trainign loss: 3.6030 avg training loss: 6.0921
batch: [4860/10547] batch time: 0.053 trainign loss: 3.6412 avg training loss: 6.0919
batch: [4870/10547] batch time: 1.455 trainign loss: 6.1440 avg training loss: 6.0918
batch: [4880/10547] batch time: 0.046 trainign loss: 5.0383 avg training loss: 6.0917
batch: [4890/10547] batch time: 1.574 trainign loss: 5.7866 avg training loss: 6.0916
batch: [4900/10547] batch time: 0.046 trainign loss: 5.6746 avg training loss: 6.0915
batch: [4910/10547] batch time: 1.969 trainign loss: 6.1149 avg training loss: 6.0913
batch: [4920/10547] batch time: 0.045 trainign loss: 6.1490 avg training loss: 6.0913
batch: [4930/10547] batch time: 0.978 trainign loss: 4.5866 avg training loss: 6.0911
batch: [4940/10547] batch time: 0.307 trainign loss: 5.2281 avg training loss: 6.0909
batch: [4950/10547] batch time: 0.684 trainign loss: 5.3628 avg training loss: 6.0907
batch: [4960/10547] batch time: 0.047 trainign loss: 6.1998 avg training loss: 6.0905
batch: [4970/10547] batch time: 1.171 trainign loss: 6.5364 avg training loss: 6.0906
batch: [4980/10547] batch time: 0.046 trainign loss: 5.5992 avg training loss: 6.0906
batch: [4990/10547] batch time: 2.309 trainign loss: 4.5386 avg training loss: 6.0903
batch: [5000/10547] batch time: 0.046 trainign loss: 4.5880 avg training loss: 6.0902
batch: [5010/10547] batch time: 1.429 trainign loss: 4.5100 avg training loss: 6.0900
batch: [5020/10547] batch time: 0.046 trainign loss: 5.3840 avg training loss: 6.0897
batch: [5030/10547] batch time: 1.719 trainign loss: 6.1140 avg training loss: 6.0896
batch: [5040/10547] batch time: 0.046 trainign loss: 5.2992 avg training loss: 6.0894
batch: [5050/10547] batch time: 0.214 trainign loss: 4.6264 avg training loss: 6.0892
batch: [5060/10547] batch time: 0.051 trainign loss: 5.4668 avg training loss: 6.0888
batch: [5070/10547] batch time: 0.053 trainign loss: 5.5716 avg training loss: 6.0888
batch: [5080/10547] batch time: 0.047 trainign loss: 5.0390 avg training loss: 6.0885
batch: [5090/10547] batch time: 0.047 trainign loss: 5.1233 avg training loss: 6.0883
batch: [5100/10547] batch time: 0.046 trainign loss: 3.8400 avg training loss: 6.0881
batch: [5110/10547] batch time: 0.048 trainign loss: 5.4924 avg training loss: 6.0880
batch: [5120/10547] batch time: 0.042 trainign loss: 2.4833 avg training loss: 6.0878
batch: [5130/10547] batch time: 0.048 trainign loss: 4.0593 avg training loss: 6.0876
batch: [5140/10547] batch time: 0.048 trainign loss: 5.7757 avg training loss: 6.0874
batch: [5150/10547] batch time: 0.048 trainign loss: 5.7579 avg training loss: 6.0874
batch: [5160/10547] batch time: 0.047 trainign loss: 0.5113 avg training loss: 6.0869
batch: [5170/10547] batch time: 0.047 trainign loss: 6.0731 avg training loss: 6.0868
batch: [5180/10547] batch time: 0.043 trainign loss: 5.8046 avg training loss: 6.0868
batch: [5190/10547] batch time: 0.046 trainign loss: 5.9941 avg training loss: 6.0866
batch: [5200/10547] batch time: 0.041 trainign loss: 2.8797 avg training loss: 6.0863
batch: [5210/10547] batch time: 0.048 trainign loss: 5.1383 avg training loss: 6.0861
batch: [5220/10547] batch time: 0.043 trainign loss: 5.8200 avg training loss: 6.0859
batch: [5230/10547] batch time: 0.046 trainign loss: 4.1524 avg training loss: 6.0857
batch: [5240/10547] batch time: 0.046 trainign loss: 1.5478 avg training loss: 6.0852
batch: [5250/10547] batch time: 0.047 trainign loss: 6.6366 avg training loss: 6.0851
batch: [5260/10547] batch time: 0.046 trainign loss: 5.5180 avg training loss: 6.0851
batch: [5270/10547] batch time: 0.046 trainign loss: 4.4185 avg training loss: 6.0848
batch: [5280/10547] batch time: 0.046 trainign loss: 5.7127 avg training loss: 6.0846
batch: [5290/10547] batch time: 0.047 trainign loss: 5.3634 avg training loss: 6.0845
batch: [5300/10547] batch time: 0.042 trainign loss: 1.0098 avg training loss: 6.0840
batch: [5310/10547] batch time: 0.051 trainign loss: 6.7152 avg training loss: 6.0840
batch: [5320/10547] batch time: 0.047 trainign loss: 5.9793 avg training loss: 6.0840
batch: [5330/10547] batch time: 0.046 trainign loss: 4.7909 avg training loss: 6.0839
batch: [5340/10547] batch time: 0.046 trainign loss: 3.5247 avg training loss: 6.0836
batch: [5350/10547] batch time: 0.050 trainign loss: 5.2356 avg training loss: 6.0834
batch: [5360/10547] batch time: 0.052 trainign loss: 4.7612 avg training loss: 6.0831
batch: [5370/10547] batch time: 0.047 trainign loss: 5.2448 avg training loss: 6.0831
batch: [5380/10547] batch time: 0.046 trainign loss: 5.3991 avg training loss: 6.0829
batch: [5390/10547] batch time: 0.047 trainign loss: 3.2614 avg training loss: 6.0827
batch: [5400/10547] batch time: 0.047 trainign loss: 4.9397 avg training loss: 6.0824
batch: [5410/10547] batch time: 0.046 trainign loss: 6.0841 avg training loss: 6.0823
batch: [5420/10547] batch time: 0.046 trainign loss: 6.0439 avg training loss: 6.0823
batch: [5430/10547] batch time: 0.047 trainign loss: 5.8212 avg training loss: 6.0821
batch: [5440/10547] batch time: 0.045 trainign loss: 5.5420 avg training loss: 6.0820
batch: [5450/10547] batch time: 0.047 trainign loss: 4.9720 avg training loss: 6.0819
batch: [5460/10547] batch time: 0.044 trainign loss: 4.6300 avg training loss: 6.0815
batch: [5470/10547] batch time: 0.046 trainign loss: 2.1889 avg training loss: 6.0813
batch: [5480/10547] batch time: 0.042 trainign loss: 3.0823 avg training loss: 6.0810
batch: [5490/10547] batch time: 0.046 trainign loss: 6.3773 avg training loss: 6.0807
batch: [5500/10547] batch time: 0.046 trainign loss: 5.7920 avg training loss: 6.0808
batch: [5510/10547] batch time: 0.046 trainign loss: 5.0484 avg training loss: 6.0807
batch: [5520/10547] batch time: 0.046 trainign loss: 6.3941 avg training loss: 6.0802
batch: [5530/10547] batch time: 0.047 trainign loss: 4.3665 avg training loss: 6.0801
batch: [5540/10547] batch time: 0.046 trainign loss: 2.8142 avg training loss: 6.0799
batch: [5550/10547] batch time: 0.046 trainign loss: 5.0339 avg training loss: 6.0796
batch: [5560/10547] batch time: 0.053 trainign loss: 5.3180 avg training loss: 6.0794
batch: [5570/10547] batch time: 0.053 trainign loss: 6.1502 avg training loss: 6.0794
batch: [5580/10547] batch time: 0.165 trainign loss: 3.7528 avg training loss: 6.0792
batch: [5590/10547] batch time: 0.046 trainign loss: 8.6425 avg training loss: 6.0785
batch: [5600/10547] batch time: 0.054 trainign loss: 6.3306 avg training loss: 6.0787
batch: [5610/10547] batch time: 0.047 trainign loss: 6.4795 avg training loss: 6.0787
batch: [5620/10547] batch time: 0.047 trainign loss: 4.7038 avg training loss: 6.0786
batch: [5630/10547] batch time: 0.046 trainign loss: 3.7506 avg training loss: 6.0783
batch: [5640/10547] batch time: 1.160 trainign loss: 5.6454 avg training loss: 6.0781
batch: [5650/10547] batch time: 0.054 trainign loss: 6.2933 avg training loss: 6.0780
batch: [5660/10547] batch time: 0.469 trainign loss: 4.4693 avg training loss: 6.0779
batch: [5670/10547] batch time: 0.053 trainign loss: 3.1968 avg training loss: 6.0777
batch: [5680/10547] batch time: 0.312 trainign loss: 3.7694 avg training loss: 6.0773
batch: [5690/10547] batch time: 0.046 trainign loss: 5.8594 avg training loss: 6.0773
batch: [5700/10547] batch time: 0.043 trainign loss: 5.5162 avg training loss: 6.0771
batch: [5710/10547] batch time: 0.051 trainign loss: 4.4014 avg training loss: 6.0768
batch: [5720/10547] batch time: 0.046 trainign loss: 5.9190 avg training loss: 6.0768
batch: [5730/10547] batch time: 0.046 trainign loss: 5.8095 avg training loss: 6.0768
batch: [5740/10547] batch time: 0.488 trainign loss: 4.6751 avg training loss: 6.0766
batch: [5750/10547] batch time: 0.905 trainign loss: 3.7008 avg training loss: 6.0764
batch: [5760/10547] batch time: 0.090 trainign loss: 4.5845 avg training loss: 6.0761
batch: [5770/10547] batch time: 1.288 trainign loss: 6.1220 avg training loss: 6.0760
batch: [5780/10547] batch time: 0.054 trainign loss: 4.3503 avg training loss: 6.0758
batch: [5790/10547] batch time: 0.047 trainign loss: 0.0156 avg training loss: 6.0750
batch: [5800/10547] batch time: 0.044 trainign loss: 7.4724 avg training loss: 6.0745
batch: [5810/10547] batch time: 0.055 trainign loss: 6.4541 avg training loss: 6.0745
batch: [5820/10547] batch time: 0.046 trainign loss: 5.7113 avg training loss: 6.0744
batch: [5830/10547] batch time: 0.046 trainign loss: 4.9294 avg training loss: 6.0741
batch: [5840/10547] batch time: 0.054 trainign loss: 3.2369 avg training loss: 6.0737
batch: [5850/10547] batch time: 0.300 trainign loss: 6.3212 avg training loss: 6.0734
batch: [5860/10547] batch time: 0.047 trainign loss: 5.5771 avg training loss: 6.0734
batch: [5870/10547] batch time: 1.280 trainign loss: 3.9264 avg training loss: 6.0733
batch: [5880/10547] batch time: 0.046 trainign loss: 2.5375 avg training loss: 6.0730
batch: [5890/10547] batch time: 1.035 trainign loss: 5.1479 avg training loss: 6.0728
batch: [5900/10547] batch time: 0.043 trainign loss: 4.6466 avg training loss: 6.0727
batch: [5910/10547] batch time: 0.304 trainign loss: 6.2241 avg training loss: 6.0726
batch: [5920/10547] batch time: 0.101 trainign loss: 5.0446 avg training loss: 6.0725
batch: [5930/10547] batch time: 0.206 trainign loss: 2.2874 avg training loss: 6.0722
batch: [5940/10547] batch time: 0.048 trainign loss: 4.4769 avg training loss: 6.0716
batch: [5950/10547] batch time: 0.049 trainign loss: 4.5970 avg training loss: 6.0716
batch: [5960/10547] batch time: 0.046 trainign loss: 5.0764 avg training loss: 6.0716
batch: [5970/10547] batch time: 0.046 trainign loss: 6.2913 avg training loss: 6.0712
batch: [5980/10547] batch time: 1.744 trainign loss: 4.4982 avg training loss: 6.0708
batch: [5990/10547] batch time: 0.046 trainign loss: 6.9521 avg training loss: 6.0708
batch: [6000/10547] batch time: 0.440 trainign loss: 6.2304 avg training loss: 6.0708
batch: [6010/10547] batch time: 0.258 trainign loss: 5.4616 avg training loss: 6.0707
batch: [6020/10547] batch time: 0.054 trainign loss: 5.4967 avg training loss: 6.0706
batch: [6030/10547] batch time: 0.758 trainign loss: 3.6547 avg training loss: 6.0703
batch: [6040/10547] batch time: 0.051 trainign loss: 6.7401 avg training loss: 6.0701
batch: [6050/10547] batch time: 0.165 trainign loss: 5.7902 avg training loss: 6.0701
batch: [6060/10547] batch time: 0.045 trainign loss: 5.1407 avg training loss: 6.0700
batch: [6070/10547] batch time: 0.052 trainign loss: 5.2616 avg training loss: 6.0698
batch: [6080/10547] batch time: 0.048 trainign loss: 5.1741 avg training loss: 6.0697
batch: [6090/10547] batch time: 0.048 trainign loss: 6.0949 avg training loss: 6.0696
batch: [6100/10547] batch time: 0.046 trainign loss: 4.3761 avg training loss: 6.0695
batch: [6110/10547] batch time: 0.054 trainign loss: 5.5445 avg training loss: 6.0692
batch: [6120/10547] batch time: 0.047 trainign loss: 5.4902 avg training loss: 6.0692
batch: [6130/10547] batch time: 0.046 trainign loss: 3.5213 avg training loss: 6.0690
batch: [6140/10547] batch time: 0.047 trainign loss: 6.1129 avg training loss: 6.0688
batch: [6150/10547] batch time: 0.046 trainign loss: 5.2203 avg training loss: 6.0687
batch: [6160/10547] batch time: 0.428 trainign loss: 6.0167 avg training loss: 6.0685
batch: [6170/10547] batch time: 0.046 trainign loss: 0.2020 avg training loss: 6.0680
batch: [6180/10547] batch time: 0.046 trainign loss: 8.1576 avg training loss: 6.0677
batch: [6190/10547] batch time: 0.046 trainign loss: 4.9298 avg training loss: 6.0676
batch: [6200/10547] batch time: 0.047 trainign loss: 5.9149 avg training loss: 6.0674
batch: [6210/10547] batch time: 0.047 trainign loss: 5.9776 avg training loss: 6.0674
batch: [6220/10547] batch time: 0.053 trainign loss: 5.0962 avg training loss: 6.0673
batch: [6230/10547] batch time: 0.055 trainign loss: 3.7126 avg training loss: 6.0672
batch: [6240/10547] batch time: 0.054 trainign loss: 4.3991 avg training loss: 6.0668
batch: [6250/10547] batch time: 0.046 trainign loss: 5.3107 avg training loss: 6.0667
batch: [6260/10547] batch time: 0.757 trainign loss: 4.8370 avg training loss: 6.0665
batch: [6270/10547] batch time: 0.047 trainign loss: 2.5086 avg training loss: 6.0662
batch: [6280/10547] batch time: 0.545 trainign loss: 5.8370 avg training loss: 6.0660
batch: [6290/10547] batch time: 0.054 trainign loss: 5.2776 avg training loss: 6.0659
batch: [6300/10547] batch time: 0.372 trainign loss: 5.8798 avg training loss: 6.0657
batch: [6310/10547] batch time: 0.054 trainign loss: 3.0152 avg training loss: 6.0655
batch: [6320/10547] batch time: 0.047 trainign loss: 6.4393 avg training loss: 6.0655
batch: [6330/10547] batch time: 0.048 trainign loss: 5.1998 avg training loss: 6.0655
batch: [6340/10547] batch time: 1.089 trainign loss: 5.5339 avg training loss: 6.0653
batch: [6350/10547] batch time: 0.054 trainign loss: 5.6990 avg training loss: 6.0653
batch: [6360/10547] batch time: 0.278 trainign loss: 4.9055 avg training loss: 6.0651
batch: [6370/10547] batch time: 0.049 trainign loss: 5.6018 avg training loss: 6.0648
batch: [6380/10547] batch time: 0.595 trainign loss: 3.3656 avg training loss: 6.0646
batch: [6390/10547] batch time: 0.047 trainign loss: 5.1362 avg training loss: 6.0645
batch: [6400/10547] batch time: 0.682 trainign loss: 4.6625 avg training loss: 6.0643
batch: [6410/10547] batch time: 0.046 trainign loss: 4.8965 avg training loss: 6.0641
batch: [6420/10547] batch time: 1.111 trainign loss: 4.8020 avg training loss: 6.0638
batch: [6430/10547] batch time: 0.054 trainign loss: 6.2503 avg training loss: 6.0637
batch: [6440/10547] batch time: 0.863 trainign loss: 2.5196 avg training loss: 6.0634
batch: [6450/10547] batch time: 0.047 trainign loss: 6.8486 avg training loss: 6.0631
batch: [6460/10547] batch time: 0.641 trainign loss: 4.8496 avg training loss: 6.0632
batch: [6470/10547] batch time: 0.046 trainign loss: 5.9440 avg training loss: 6.0629
batch: [6480/10547] batch time: 0.046 trainign loss: 5.7234 avg training loss: 6.0628
batch: [6490/10547] batch time: 0.047 trainign loss: 3.9413 avg training loss: 6.0625
batch: [6500/10547] batch time: 0.054 trainign loss: 0.0431 avg training loss: 6.0618
batch: [6510/10547] batch time: 0.046 trainign loss: 0.0006 avg training loss: 6.0607
batch: [6520/10547] batch time: 0.047 trainign loss: 0.0002 avg training loss: 6.0597
batch: [6530/10547] batch time: 0.046 trainign loss: 7.7715 avg training loss: 6.0594
batch: [6540/10547] batch time: 0.043 trainign loss: 6.8733 avg training loss: 6.0595
batch: [6550/10547] batch time: 0.054 trainign loss: 5.0326 avg training loss: 6.0595
batch: [6560/10547] batch time: 0.046 trainign loss: 4.8298 avg training loss: 6.0595
batch: [6570/10547] batch time: 0.047 trainign loss: 1.8837 avg training loss: 6.0591
batch: [6580/10547] batch time: 0.048 trainign loss: 5.8229 avg training loss: 6.0589
batch: [6590/10547] batch time: 0.046 trainign loss: 5.8028 avg training loss: 6.0589
batch: [6600/10547] batch time: 0.053 trainign loss: 3.0278 avg training loss: 6.0587
batch: [6610/10547] batch time: 0.048 trainign loss: 2.5575 avg training loss: 6.0583
batch: [6620/10547] batch time: 0.046 trainign loss: 6.7053 avg training loss: 6.0584
batch: [6630/10547] batch time: 0.046 trainign loss: 4.9951 avg training loss: 6.0583
batch: [6640/10547] batch time: 0.047 trainign loss: 4.9192 avg training loss: 6.0580
batch: [6650/10547] batch time: 0.052 trainign loss: 2.9031 avg training loss: 6.0577
batch: [6660/10547] batch time: 0.047 trainign loss: 5.9465 avg training loss: 6.0575
batch: [6670/10547] batch time: 0.047 trainign loss: 6.5459 avg training loss: 6.0575
batch: [6680/10547] batch time: 0.046 trainign loss: 5.4221 avg training loss: 6.0574
batch: [6690/10547] batch time: 0.643 trainign loss: 5.1435 avg training loss: 6.0572
batch: [6700/10547] batch time: 0.054 trainign loss: 5.3600 avg training loss: 6.0571
batch: [6710/10547] batch time: 0.865 trainign loss: 5.6010 avg training loss: 6.0570
batch: [6720/10547] batch time: 0.055 trainign loss: 5.5847 avg training loss: 6.0568
batch: [6730/10547] batch time: 1.431 trainign loss: 5.7202 avg training loss: 6.0568
batch: [6740/10547] batch time: 0.047 trainign loss: 4.6510 avg training loss: 6.0567
batch: [6750/10547] batch time: 1.370 trainign loss: 4.9383 avg training loss: 6.0564
batch: [6760/10547] batch time: 0.052 trainign loss: 5.4103 avg training loss: 6.0564
batch: [6770/10547] batch time: 0.052 trainign loss: 5.3013 avg training loss: 6.0562
batch: [6780/10547] batch time: 0.046 trainign loss: 5.7515 avg training loss: 6.0560
batch: [6790/10547] batch time: 0.046 trainign loss: 5.6661 avg training loss: 6.0560
batch: [6800/10547] batch time: 0.047 trainign loss: 5.1755 avg training loss: 6.0559
batch: [6810/10547] batch time: 0.047 trainign loss: 5.3404 avg training loss: 6.0557
batch: [6820/10547] batch time: 0.048 trainign loss: 2.5527 avg training loss: 6.0552
batch: [6830/10547] batch time: 0.053 trainign loss: 4.3199 avg training loss: 6.0551
batch: [6840/10547] batch time: 0.046 trainign loss: 4.3081 avg training loss: 6.0551
batch: [6850/10547] batch time: 0.054 trainign loss: 2.0726 avg training loss: 6.0548
batch: [6860/10547] batch time: 0.044 trainign loss: 4.9408 avg training loss: 6.0546
batch: [6870/10547] batch time: 0.055 trainign loss: 5.1093 avg training loss: 6.0543
batch: [6880/10547] batch time: 0.047 trainign loss: 5.4688 avg training loss: 6.0542
batch: [6890/10547] batch time: 0.047 trainign loss: 5.6033 avg training loss: 6.0542
batch: [6900/10547] batch time: 0.054 trainign loss: 4.5516 avg training loss: 6.0539
batch: [6910/10547] batch time: 0.213 trainign loss: 5.0720 avg training loss: 6.0537
batch: [6920/10547] batch time: 0.047 trainign loss: 4.3705 avg training loss: 6.0534
batch: [6930/10547] batch time: 0.433 trainign loss: 6.5205 avg training loss: 6.0532
batch: [6940/10547] batch time: 0.046 trainign loss: 5.4602 avg training loss: 6.0532
batch: [6950/10547] batch time: 0.051 trainign loss: 4.4370 avg training loss: 6.0531
batch: [6960/10547] batch time: 0.048 trainign loss: 5.6422 avg training loss: 6.0528
batch: [6970/10547] batch time: 0.736 trainign loss: 5.4527 avg training loss: 6.0526
batch: [6980/10547] batch time: 0.052 trainign loss: 5.5163 avg training loss: 6.0525
batch: [6990/10547] batch time: 1.995 trainign loss: 4.4352 avg training loss: 6.0523
batch: [7000/10547] batch time: 0.051 trainign loss: 6.5050 avg training loss: 6.0520
batch: [7010/10547] batch time: 2.263 trainign loss: 3.1046 avg training loss: 6.0519
batch: [7020/10547] batch time: 0.046 trainign loss: 5.2392 avg training loss: 6.0516
batch: [7030/10547] batch time: 1.376 trainign loss: 5.9676 avg training loss: 6.0516
batch: [7040/10547] batch time: 0.052 trainign loss: 5.6490 avg training loss: 6.0514
batch: [7050/10547] batch time: 1.595 trainign loss: 5.7348 avg training loss: 6.0513
batch: [7060/10547] batch time: 0.046 trainign loss: 5.5369 avg training loss: 6.0512
batch: [7070/10547] batch time: 1.286 trainign loss: 7.1623 avg training loss: 6.0509
batch: [7080/10547] batch time: 0.054 trainign loss: 5.2503 avg training loss: 6.0508
batch: [7090/10547] batch time: 1.047 trainign loss: 6.1807 avg training loss: 6.0507
batch: [7100/10547] batch time: 0.049 trainign loss: 6.0600 avg training loss: 6.0506
batch: [7110/10547] batch time: 2.155 trainign loss: 4.6875 avg training loss: 6.0503
batch: [7120/10547] batch time: 0.046 trainign loss: 6.4320 avg training loss: 6.0502
batch: [7130/10547] batch time: 2.382 trainign loss: 4.3663 avg training loss: 6.0501
batch: [7140/10547] batch time: 0.046 trainign loss: 4.5432 avg training loss: 6.0497
batch: [7150/10547] batch time: 1.695 trainign loss: 0.3058 avg training loss: 6.0493
batch: [7160/10547] batch time: 0.054 trainign loss: 0.0015 avg training loss: 6.0483
batch: [7170/10547] batch time: 1.877 trainign loss: 0.0004 avg training loss: 6.0473
batch: [7180/10547] batch time: 0.054 trainign loss: 0.0003 avg training loss: 6.0463
batch: [7190/10547] batch time: 1.911 trainign loss: 0.0002 avg training loss: 6.0453
batch: [7200/10547] batch time: 0.054 trainign loss: 0.0002 avg training loss: 6.0443
batch: [7210/10547] batch time: 1.810 trainign loss: 0.0002 avg training loss: 6.0433
batch: [7220/10547] batch time: 0.055 trainign loss: 0.0002 avg training loss: 6.0423
batch: [7230/10547] batch time: 2.099 trainign loss: 0.0001 avg training loss: 6.0413
batch: [7240/10547] batch time: 0.046 trainign loss: 0.0002 avg training loss: 6.0403
batch: [7250/10547] batch time: 2.339 trainign loss: 7.0790 avg training loss: 6.0405
batch: [7260/10547] batch time: 0.054 trainign loss: 6.3881 avg training loss: 6.0406
batch: [7270/10547] batch time: 2.145 trainign loss: 5.1789 avg training loss: 6.0406
batch: [7280/10547] batch time: 0.047 trainign loss: 4.7339 avg training loss: 6.0405
batch: [7290/10547] batch time: 2.501 trainign loss: 6.1821 avg training loss: 6.0402
batch: [7300/10547] batch time: 0.045 trainign loss: 0.1866 avg training loss: 6.0397
batch: [7310/10547] batch time: 2.353 trainign loss: 7.5939 avg training loss: 6.0392
batch: [7320/10547] batch time: 0.047 trainign loss: 6.6164 avg training loss: 6.0392
batch: [7330/10547] batch time: 2.053 trainign loss: 5.6181 avg training loss: 6.0391
batch: [7340/10547] batch time: 0.049 trainign loss: 6.6694 avg training loss: 6.0392
batch: [7350/10547] batch time: 2.222 trainign loss: 5.9359 avg training loss: 6.0391
batch: [7360/10547] batch time: 0.046 trainign loss: 6.0405 avg training loss: 6.0390
batch: [7370/10547] batch time: 0.689 trainign loss: 5.2632 avg training loss: 6.0390
batch: [7380/10547] batch time: 0.046 trainign loss: 5.7715 avg training loss: 6.0389
batch: [7390/10547] batch time: 0.285 trainign loss: 5.5081 avg training loss: 6.0388
batch: [7400/10547] batch time: 0.048 trainign loss: 5.3208 avg training loss: 6.0385
batch: [7410/10547] batch time: 0.736 trainign loss: 5.5879 avg training loss: 6.0384
batch: [7420/10547] batch time: 0.046 trainign loss: 5.6181 avg training loss: 6.0382
batch: [7430/10547] batch time: 1.093 trainign loss: 4.2211 avg training loss: 6.0379
batch: [7440/10547] batch time: 0.046 trainign loss: 4.0292 avg training loss: 6.0377
batch: [7450/10547] batch time: 0.597 trainign loss: 5.6854 avg training loss: 6.0375
batch: [7460/10547] batch time: 0.049 trainign loss: 5.9780 avg training loss: 6.0374
batch: [7470/10547] batch time: 0.050 trainign loss: 5.1900 avg training loss: 6.0372
batch: [7480/10547] batch time: 0.054 trainign loss: 5.7165 avg training loss: 6.0370
batch: [7490/10547] batch time: 0.046 trainign loss: 0.4744 avg training loss: 6.0366
batch: [7500/10547] batch time: 0.048 trainign loss: 6.2802 avg training loss: 6.0365
batch: [7510/10547] batch time: 0.042 trainign loss: 6.0992 avg training loss: 6.0363
batch: [7520/10547] batch time: 0.046 trainign loss: 4.8301 avg training loss: 6.0361
batch: [7530/10547] batch time: 0.046 trainign loss: 6.4371 avg training loss: 6.0360
batch: [7540/10547] batch time: 0.046 trainign loss: 5.6274 avg training loss: 6.0359
batch: [7550/10547] batch time: 0.047 trainign loss: 1.8422 avg training loss: 6.0356
batch: [7560/10547] batch time: 0.055 trainign loss: 5.9646 avg training loss: 6.0355
batch: [7570/10547] batch time: 0.051 trainign loss: 5.3788 avg training loss: 6.0355
batch: [7580/10547] batch time: 0.054 trainign loss: 5.3953 avg training loss: 6.0352
batch: [7590/10547] batch time: 0.046 trainign loss: 6.0087 avg training loss: 6.0349
batch: [7600/10547] batch time: 0.049 trainign loss: 4.1042 avg training loss: 6.0348
batch: [7610/10547] batch time: 0.050 trainign loss: 6.2747 avg training loss: 6.0347
batch: [7620/10547] batch time: 0.055 trainign loss: 6.0717 avg training loss: 6.0347
batch: [7630/10547] batch time: 0.047 trainign loss: 3.6270 avg training loss: 6.0344
batch: [7640/10547] batch time: 0.046 trainign loss: 5.7601 avg training loss: 6.0342
batch: [7650/10547] batch time: 0.046 trainign loss: 6.5396 avg training loss: 6.0343
batch: [7660/10547] batch time: 0.054 trainign loss: 5.0492 avg training loss: 6.0341
batch: [7670/10547] batch time: 0.046 trainign loss: 3.7493 avg training loss: 6.0340
batch: [7680/10547] batch time: 0.047 trainign loss: 4.7546 avg training loss: 6.0338
batch: [7690/10547] batch time: 0.046 trainign loss: 5.4458 avg training loss: 6.0337
batch: [7700/10547] batch time: 0.047 trainign loss: 4.9989 avg training loss: 6.0334
batch: [7710/10547] batch time: 0.044 trainign loss: 6.6788 avg training loss: 6.0330
batch: [7720/10547] batch time: 0.046 trainign loss: 6.7454 avg training loss: 6.0329
batch: [7730/10547] batch time: 0.045 trainign loss: 4.1597 avg training loss: 6.0327
batch: [7740/10547] batch time: 0.048 trainign loss: 5.7240 avg training loss: 6.0326
batch: [7750/10547] batch time: 0.048 trainign loss: 5.3929 avg training loss: 6.0325
batch: [7760/10547] batch time: 0.054 trainign loss: 6.0049 avg training loss: 6.0325
batch: [7770/10547] batch time: 0.046 trainign loss: 5.5737 avg training loss: 6.0324
batch: [7780/10547] batch time: 0.049 trainign loss: 5.9851 avg training loss: 6.0323
batch: [7790/10547] batch time: 0.046 trainign loss: 5.3186 avg training loss: 6.0322
batch: [7800/10547] batch time: 0.046 trainign loss: 4.9250 avg training loss: 6.0320
batch: [7810/10547] batch time: 0.047 trainign loss: 0.2028 avg training loss: 6.0314
batch: [7820/10547] batch time: 0.053 trainign loss: 9.2781 avg training loss: 6.0307
batch: [7830/10547] batch time: 0.047 trainign loss: 7.0188 avg training loss: 6.0310
batch: [7840/10547] batch time: 0.053 trainign loss: 6.2505 avg training loss: 6.0310
batch: [7850/10547] batch time: 0.045 trainign loss: 2.8389 avg training loss: 6.0309
batch: [7860/10547] batch time: 0.047 trainign loss: 6.2903 avg training loss: 6.0308
batch: [7870/10547] batch time: 0.046 trainign loss: 5.7518 avg training loss: 6.0308
batch: [7880/10547] batch time: 0.048 trainign loss: 5.3327 avg training loss: 6.0306
batch: [7890/10547] batch time: 0.044 trainign loss: 5.9716 avg training loss: 6.0306
batch: [7900/10547] batch time: 0.054 trainign loss: 3.0486 avg training loss: 6.0304
batch: [7910/10547] batch time: 0.047 trainign loss: 5.6290 avg training loss: 6.0301
batch: [7920/10547] batch time: 0.052 trainign loss: 5.8136 avg training loss: 6.0301
batch: [7930/10547] batch time: 0.046 trainign loss: 5.4338 avg training loss: 6.0300
batch: [7940/10547] batch time: 0.046 trainign loss: 3.2315 avg training loss: 6.0298
batch: [7950/10547] batch time: 0.048 trainign loss: 7.5095 avg training loss: 6.0295
batch: [7960/10547] batch time: 0.046 trainign loss: 5.1499 avg training loss: 6.0295
batch: [7970/10547] batch time: 0.046 trainign loss: 6.1257 avg training loss: 6.0294
batch: [7980/10547] batch time: 0.049 trainign loss: 5.9786 avg training loss: 6.0293
batch: [7990/10547] batch time: 0.047 trainign loss: 5.8409 avg training loss: 6.0292
batch: [8000/10547] batch time: 0.048 trainign loss: 5.0976 avg training loss: 6.0291
batch: [8010/10547] batch time: 0.046 trainign loss: 5.3668 avg training loss: 6.0289
batch: [8020/10547] batch time: 0.054 trainign loss: 5.6961 avg training loss: 6.0288
batch: [8030/10547] batch time: 0.046 trainign loss: 5.8939 avg training loss: 6.0288
batch: [8040/10547] batch time: 0.046 trainign loss: 5.1047 avg training loss: 6.0287
batch: [8050/10547] batch time: 0.054 trainign loss: 4.9617 avg training loss: 6.0285
batch: [8060/10547] batch time: 0.049 trainign loss: 5.7338 avg training loss: 6.0284
batch: [8070/10547] batch time: 0.299 trainign loss: 5.1368 avg training loss: 6.0283
batch: [8080/10547] batch time: 0.047 trainign loss: 5.3820 avg training loss: 6.0282
batch: [8090/10547] batch time: 0.048 trainign loss: 4.9281 avg training loss: 6.0279
batch: [8100/10547] batch time: 0.048 trainign loss: 4.8875 avg training loss: 6.0275
batch: [8110/10547] batch time: 0.047 trainign loss: 4.9895 avg training loss: 6.0274
batch: [8120/10547] batch time: 0.055 trainign loss: 6.0970 avg training loss: 6.0273
batch: [8130/10547] batch time: 0.055 trainign loss: 5.3303 avg training loss: 6.0273
batch: [8140/10547] batch time: 0.046 trainign loss: 4.9712 avg training loss: 6.0272
batch: [8150/10547] batch time: 0.257 trainign loss: 5.4112 avg training loss: 6.0271
batch: [8160/10547] batch time: 0.046 trainign loss: 5.9757 avg training loss: 6.0270
batch: [8170/10547] batch time: 0.050 trainign loss: 3.2519 avg training loss: 6.0268
batch: [8180/10547] batch time: 0.054 trainign loss: 5.7011 avg training loss: 6.0266
batch: [8190/10547] batch time: 0.054 trainign loss: 5.3701 avg training loss: 6.0265
batch: [8200/10547] batch time: 0.046 trainign loss: 4.2895 avg training loss: 6.0263
batch: [8210/10547] batch time: 0.048 trainign loss: 1.2883 avg training loss: 6.0258
batch: [8220/10547] batch time: 0.046 trainign loss: 6.3430 avg training loss: 6.0256
batch: [8230/10547] batch time: 0.047 trainign loss: 6.0630 avg training loss: 6.0257
batch: [8240/10547] batch time: 0.047 trainign loss: 5.6106 avg training loss: 6.0256
batch: [8250/10547] batch time: 0.047 trainign loss: 5.6943 avg training loss: 6.0255
batch: [8260/10547] batch time: 0.047 trainign loss: 4.5867 avg training loss: 6.0254
batch: [8270/10547] batch time: 0.046 trainign loss: 5.5278 avg training loss: 6.0253
batch: [8280/10547] batch time: 0.046 trainign loss: 5.8112 avg training loss: 6.0252
batch: [8290/10547] batch time: 0.046 trainign loss: 3.8673 avg training loss: 6.0249
batch: [8300/10547] batch time: 0.050 trainign loss: 5.5547 avg training loss: 6.0248
batch: [8310/10547] batch time: 0.046 trainign loss: 5.2685 avg training loss: 6.0246
batch: [8320/10547] batch time: 0.044 trainign loss: 5.6815 avg training loss: 6.0245
batch: [8330/10547] batch time: 0.050 trainign loss: 5.9620 avg training loss: 6.0243
batch: [8340/10547] batch time: 0.054 trainign loss: 3.3493 avg training loss: 6.0242
batch: [8350/10547] batch time: 0.048 trainign loss: 0.0132 avg training loss: 6.0233
batch: [8360/10547] batch time: 0.047 trainign loss: 6.5000 avg training loss: 6.0234
batch: [8370/10547] batch time: 0.047 trainign loss: 6.5491 avg training loss: 6.0233
batch: [8380/10547] batch time: 0.041 trainign loss: 4.9654 avg training loss: 6.0232
batch: [8390/10547] batch time: 0.046 trainign loss: 2.4756 avg training loss: 6.0229
batch: [8400/10547] batch time: 0.047 trainign loss: 5.2590 avg training loss: 6.0227
batch: [8410/10547] batch time: 0.047 trainign loss: 5.0618 avg training loss: 6.0225
batch: [8420/10547] batch time: 0.044 trainign loss: 4.2258 avg training loss: 6.0218
batch: [8430/10547] batch time: 0.046 trainign loss: 4.3909 avg training loss: 6.0219
batch: [8440/10547] batch time: 0.043 trainign loss: 4.6198 avg training loss: 6.0218
batch: [8450/10547] batch time: 0.054 trainign loss: 6.0481 avg training loss: 6.0216
batch: [8460/10547] batch time: 0.044 trainign loss: 4.8411 avg training loss: 6.0214
batch: [8470/10547] batch time: 0.046 trainign loss: 6.3851 avg training loss: 6.0213
batch: [8480/10547] batch time: 0.046 trainign loss: 5.3579 avg training loss: 6.0214
batch: [8490/10547] batch time: 0.046 trainign loss: 5.0569 avg training loss: 6.0210
batch: [8500/10547] batch time: 0.045 trainign loss: 3.2854 avg training loss: 6.0208
batch: [8510/10547] batch time: 0.047 trainign loss: 5.1471 avg training loss: 6.0207
batch: [8520/10547] batch time: 0.055 trainign loss: 3.0687 avg training loss: 6.0203
batch: [8530/10547] batch time: 0.046 trainign loss: 2.4725 avg training loss: 6.0201
batch: [8540/10547] batch time: 0.054 trainign loss: 6.9548 avg training loss: 6.0202
batch: [8550/10547] batch time: 0.054 trainign loss: 5.3665 avg training loss: 6.0201
batch: [8560/10547] batch time: 0.046 trainign loss: 0.4398 avg training loss: 6.0195
batch: [8570/10547] batch time: 0.046 trainign loss: 6.3168 avg training loss: 6.0192
batch: [8580/10547] batch time: 0.047 trainign loss: 5.1683 avg training loss: 6.0191
batch: [8590/10547] batch time: 0.046 trainign loss: 6.5162 avg training loss: 6.0190
batch: [8600/10547] batch time: 0.050 trainign loss: 6.1887 avg training loss: 6.0190
batch: [8610/10547] batch time: 0.047 trainign loss: 6.0490 avg training loss: 6.0189
batch: [8620/10547] batch time: 0.046 trainign loss: 4.8311 avg training loss: 6.0188
batch: [8630/10547] batch time: 0.046 trainign loss: 3.1766 avg training loss: 6.0186
batch: [8640/10547] batch time: 0.046 trainign loss: 6.5924 avg training loss: 6.0185
batch: [8650/10547] batch time: 0.046 trainign loss: 5.2127 avg training loss: 6.0184
batch: [8660/10547] batch time: 0.047 trainign loss: 5.1226 avg training loss: 6.0183
batch: [8670/10547] batch time: 0.047 trainign loss: 5.3115 avg training loss: 6.0182
batch: [8680/10547] batch time: 0.049 trainign loss: 3.4159 avg training loss: 6.0179
batch: [8690/10547] batch time: 0.052 trainign loss: 0.0292 avg training loss: 6.0171
batch: [8700/10547] batch time: 0.047 trainign loss: 7.8018 avg training loss: 6.0169
batch: [8710/10547] batch time: 0.048 trainign loss: 5.5862 avg training loss: 6.0170
batch: [8720/10547] batch time: 0.046 trainign loss: 6.5126 avg training loss: 6.0170
batch: [8730/10547] batch time: 0.054 trainign loss: 5.9761 avg training loss: 6.0170
batch: [8740/10547] batch time: 0.047 trainign loss: 4.7085 avg training loss: 6.0169
batch: [8750/10547] batch time: 0.048 trainign loss: 5.0002 avg training loss: 6.0167
batch: [8760/10547] batch time: 0.053 trainign loss: 6.6925 avg training loss: 6.0165
batch: [8770/10547] batch time: 0.046 trainign loss: 6.2662 avg training loss: 6.0165
batch: [8780/10547] batch time: 0.046 trainign loss: 5.1744 avg training loss: 6.0164
batch: [8790/10547] batch time: 0.046 trainign loss: 5.7244 avg training loss: 6.0163
batch: [8800/10547] batch time: 0.043 trainign loss: 5.6170 avg training loss: 6.0162
batch: [8810/10547] batch time: 0.047 trainign loss: 5.6064 avg training loss: 6.0161
batch: [8820/10547] batch time: 0.045 trainign loss: 5.4407 avg training loss: 6.0161
batch: [8830/10547] batch time: 0.046 trainign loss: 5.2625 avg training loss: 6.0160
batch: [8840/10547] batch time: 0.046 trainign loss: 4.6237 avg training loss: 6.0157
batch: [8850/10547] batch time: 0.049 trainign loss: 5.4141 avg training loss: 6.0157
batch: [8860/10547] batch time: 0.044 trainign loss: 4.9104 avg training loss: 6.0156
batch: [8870/10547] batch time: 0.046 trainign loss: 5.0394 avg training loss: 6.0153
batch: [8880/10547] batch time: 0.050 trainign loss: 4.5263 avg training loss: 6.0152
batch: [8890/10547] batch time: 0.047 trainign loss: 5.1806 avg training loss: 6.0150
batch: [8900/10547] batch time: 0.043 trainign loss: 2.4880 avg training loss: 6.0148
batch: [8910/10547] batch time: 0.048 trainign loss: 5.4631 avg training loss: 6.0145
batch: [8920/10547] batch time: 0.052 trainign loss: 4.8704 avg training loss: 6.0144
batch: [8930/10547] batch time: 0.046 trainign loss: 6.7386 avg training loss: 6.0144
batch: [8940/10547] batch time: 0.047 trainign loss: 5.4615 avg training loss: 6.0144
batch: [8950/10547] batch time: 0.047 trainign loss: 4.7982 avg training loss: 6.0143
batch: [8960/10547] batch time: 0.046 trainign loss: 4.9152 avg training loss: 6.0142
batch: [8970/10547] batch time: 0.054 trainign loss: 3.3412 avg training loss: 6.0140
batch: [8980/10547] batch time: 0.046 trainign loss: 4.7219 avg training loss: 6.0136
batch: [8990/10547] batch time: 0.047 trainign loss: 5.4309 avg training loss: 6.0136
batch: [9000/10547] batch time: 0.047 trainign loss: 0.5314 avg training loss: 6.0132
batch: [9010/10547] batch time: 0.046 trainign loss: 6.0050 avg training loss: 6.0130
batch: [9020/10547] batch time: 0.046 trainign loss: 3.9998 avg training loss: 6.0128
batch: [9030/10547] batch time: 0.046 trainign loss: 4.3301 avg training loss: 6.0127
batch: [9040/10547] batch time: 0.047 trainign loss: 4.5703 avg training loss: 6.0127
batch: [9050/10547] batch time: 0.046 trainign loss: 4.1680 avg training loss: 6.0123
batch: [9060/10547] batch time: 0.048 trainign loss: 5.2798 avg training loss: 6.0121
batch: [9070/10547] batch time: 0.046 trainign loss: 3.3202 avg training loss: 6.0120
batch: [9080/10547] batch time: 0.046 trainign loss: 6.0099 avg training loss: 6.0119
batch: [9090/10547] batch time: 0.046 trainign loss: 4.6998 avg training loss: 6.0119
batch: [9100/10547] batch time: 0.048 trainign loss: 3.2016 avg training loss: 6.0117
batch: [9110/10547] batch time: 0.051 trainign loss: 5.5851 avg training loss: 6.0116
batch: [9120/10547] batch time: 0.049 trainign loss: 5.4729 avg training loss: 6.0115
batch: [9130/10547] batch time: 0.055 trainign loss: 0.0702 avg training loss: 6.0108
batch: [9140/10547] batch time: 0.054 trainign loss: 6.2890 avg training loss: 6.0107
batch: [9150/10547] batch time: 0.055 trainign loss: 6.5167 avg training loss: 6.0107
batch: [9160/10547] batch time: 0.047 trainign loss: 1.6680 avg training loss: 6.0105
batch: [9170/10547] batch time: 0.046 trainign loss: 0.0026 avg training loss: 6.0095
batch: [9180/10547] batch time: 0.052 trainign loss: 7.4704 avg training loss: 6.0098
batch: [9190/10547] batch time: 0.444 trainign loss: 5.3444 avg training loss: 6.0097
batch: [9200/10547] batch time: 0.047 trainign loss: 5.3415 avg training loss: 6.0095
batch: [9210/10547] batch time: 1.306 trainign loss: 5.0748 avg training loss: 6.0094
batch: [9220/10547] batch time: 0.047 trainign loss: 3.4122 avg training loss: 6.0092
batch: [9230/10547] batch time: 1.272 trainign loss: 5.4286 avg training loss: 6.0089
batch: [9240/10547] batch time: 0.046 trainign loss: 5.0737 avg training loss: 6.0089
batch: [9250/10547] batch time: 1.747 trainign loss: 0.0516 avg training loss: 6.0082
batch: [9260/10547] batch time: 0.046 trainign loss: 10.1047 avg training loss: 6.0076
batch: [9270/10547] batch time: 2.638 trainign loss: 6.5170 avg training loss: 6.0078
batch: [9280/10547] batch time: 0.046 trainign loss: 6.4794 avg training loss: 6.0078
batch: [9290/10547] batch time: 2.928 trainign loss: 6.0899 avg training loss: 6.0078
batch: [9300/10547] batch time: 0.054 trainign loss: 5.1306 avg training loss: 6.0076
batch: [9310/10547] batch time: 2.172 trainign loss: 4.8650 avg training loss: 6.0074
batch: [9320/10547] batch time: 0.055 trainign loss: 6.0733 avg training loss: 6.0072
batch: [9330/10547] batch time: 1.996 trainign loss: 5.6176 avg training loss: 6.0071
batch: [9340/10547] batch time: 0.055 trainign loss: 2.6771 avg training loss: 6.0068
batch: [9350/10547] batch time: 1.373 trainign loss: 5.7133 avg training loss: 6.0066
batch: [9360/10547] batch time: 0.444 trainign loss: 6.5930 avg training loss: 6.0064
batch: [9370/10547] batch time: 0.048 trainign loss: 6.3489 avg training loss: 6.0064
batch: [9380/10547] batch time: 1.560 trainign loss: 4.7792 avg training loss: 6.0064
batch: [9390/10547] batch time: 0.048 trainign loss: 5.5660 avg training loss: 6.0061
batch: [9400/10547] batch time: 1.441 trainign loss: 2.4417 avg training loss: 6.0059
batch: [9410/10547] batch time: 0.046 trainign loss: 5.1106 avg training loss: 6.0059
batch: [9420/10547] batch time: 1.067 trainign loss: 6.0264 avg training loss: 6.0059
batch: [9430/10547] batch time: 0.047 trainign loss: 5.1984 avg training loss: 6.0058
batch: [9440/10547] batch time: 1.004 trainign loss: 4.5692 avg training loss: 6.0057
batch: [9450/10547] batch time: 0.054 trainign loss: 3.6261 avg training loss: 6.0054
batch: [9460/10547] batch time: 1.373 trainign loss: 6.2645 avg training loss: 6.0053
batch: [9470/10547] batch time: 0.046 trainign loss: 3.3459 avg training loss: 6.0052
batch: [9480/10547] batch time: 1.817 trainign loss: 3.9008 avg training loss: 6.0051
batch: [9490/10547] batch time: 0.046 trainign loss: 6.3426 avg training loss: 6.0050
batch: [9500/10547] batch time: 0.977 trainign loss: 4.5797 avg training loss: 6.0050
batch: [9510/10547] batch time: 0.047 trainign loss: 4.5761 avg training loss: 6.0047
batch: [9520/10547] batch time: 1.142 trainign loss: 4.8098 avg training loss: 6.0045
batch: [9530/10547] batch time: 0.046 trainign loss: 6.1870 avg training loss: 6.0043
batch: [9540/10547] batch time: 1.401 trainign loss: 2.5624 avg training loss: 6.0042
batch: [9550/10547] batch time: 0.047 trainign loss: 5.4989 avg training loss: 6.0039
batch: [9560/10547] batch time: 0.342 trainign loss: 6.7731 avg training loss: 6.0039
batch: [9570/10547] batch time: 0.046 trainign loss: 6.2984 avg training loss: 6.0040
batch: [9580/10547] batch time: 0.839 trainign loss: 5.6188 avg training loss: 6.0039
batch: [9590/10547] batch time: 0.046 trainign loss: 3.3894 avg training loss: 6.0038
batch: [9600/10547] batch time: 0.651 trainign loss: 4.6523 avg training loss: 6.0036
batch: [9610/10547] batch time: 0.048 trainign loss: 5.1244 avg training loss: 6.0036
batch: [9620/10547] batch time: 1.397 trainign loss: 4.2475 avg training loss: 6.0033
batch: [9630/10547] batch time: 0.047 trainign loss: 5.5098 avg training loss: 6.0033
batch: [9640/10547] batch time: 1.769 trainign loss: 5.8822 avg training loss: 6.0032
batch: [9650/10547] batch time: 0.047 trainign loss: 5.4945 avg training loss: 6.0031
batch: [9660/10547] batch time: 0.755 trainign loss: 5.2914 avg training loss: 6.0029
batch: [9670/10547] batch time: 0.046 trainign loss: 5.2954 avg training loss: 6.0028
batch: [9680/10547] batch time: 1.604 trainign loss: 5.1208 avg training loss: 6.0027
batch: [9690/10547] batch time: 0.048 trainign loss: 4.9460 avg training loss: 6.0025
batch: [9700/10547] batch time: 1.567 trainign loss: 4.9394 avg training loss: 6.0024
batch: [9710/10547] batch time: 0.048 trainign loss: 5.9542 avg training loss: 6.0024
batch: [9720/10547] batch time: 2.432 trainign loss: 5.2630 avg training loss: 6.0022
batch: [9730/10547] batch time: 0.056 trainign loss: 0.2644 avg training loss: 6.0017
batch: [9740/10547] batch time: 2.420 trainign loss: 5.7610 avg training loss: 6.0016
batch: [9750/10547] batch time: 0.049 trainign loss: 6.3064 avg training loss: 6.0017
batch: [9760/10547] batch time: 2.217 trainign loss: 6.4959 avg training loss: 6.0016
batch: [9770/10547] batch time: 0.047 trainign loss: 5.1605 avg training loss: 6.0015
batch: [9780/10547] batch time: 2.530 trainign loss: 6.0677 avg training loss: 6.0015
batch: [9790/10547] batch time: 0.046 trainign loss: 5.3333 avg training loss: 6.0014
batch: [9800/10547] batch time: 2.445 trainign loss: 5.2137 avg training loss: 6.0011
batch: [9810/10547] batch time: 0.052 trainign loss: 6.3807 avg training loss: 6.0011
batch: [9820/10547] batch time: 1.067 trainign loss: 5.9696 avg training loss: 6.0010
batch: [9830/10547] batch time: 0.046 trainign loss: 4.8172 avg training loss: 6.0009
batch: [9840/10547] batch time: 1.273 trainign loss: 0.3063 avg training loss: 6.0004
batch: [9850/10547] batch time: 0.046 trainign loss: 0.0322 avg training loss: 5.9998
batch: [9860/10547] batch time: 0.811 trainign loss: 6.3710 avg training loss: 5.9999
batch: [9870/10547] batch time: 0.048 trainign loss: 4.1526 avg training loss: 5.9997
batch: [9880/10547] batch time: 0.636 trainign loss: 5.3517 avg training loss: 5.9996
batch: [9890/10547] batch time: 0.048 trainign loss: 6.5568 avg training loss: 5.9996
batch: [9900/10547] batch time: 1.887 trainign loss: 5.2827 avg training loss: 5.9995
batch: [9910/10547] batch time: 0.046 trainign loss: 1.2605 avg training loss: 5.9992
batch: [9920/10547] batch time: 2.656 trainign loss: 4.0821 avg training loss: 5.9989
batch: [9930/10547] batch time: 0.049 trainign loss: 5.9262 avg training loss: 5.9986
batch: [9940/10547] batch time: 2.083 trainign loss: 6.3110 avg training loss: 5.9984
batch: [9950/10547] batch time: 0.047 trainign loss: 6.4362 avg training loss: 5.9982
batch: [9960/10547] batch time: 1.808 trainign loss: 3.9426 avg training loss: 5.9982
batch: [9970/10547] batch time: 0.046 trainign loss: 7.0227 avg training loss: 5.9977
batch: [9980/10547] batch time: 1.766 trainign loss: 6.6512 avg training loss: 5.9978
batch: [9990/10547] batch time: 0.046 trainign loss: 5.6435 avg training loss: 5.9977
batch: [10000/10547] batch time: 0.665 trainign loss: 5.9731 avg training loss: 5.9976
batch: [10010/10547] batch time: 0.053 trainign loss: 4.6791 avg training loss: 5.9975
batch: [10020/10547] batch time: 0.778 trainign loss: 5.8216 avg training loss: 5.9973
batch: [10030/10547] batch time: 0.046 trainign loss: 6.1540 avg training loss: 5.9972
batch: [10040/10547] batch time: 0.182 trainign loss: 3.8582 avg training loss: 5.9971
batch: [10050/10547] batch time: 0.046 trainign loss: 3.9304 avg training loss: 5.9969
batch: [10060/10547] batch time: 1.357 trainign loss: 7.8462 avg training loss: 5.9965
batch: [10070/10547] batch time: 0.046 trainign loss: 6.1523 avg training loss: 5.9965
batch: [10080/10547] batch time: 0.909 trainign loss: 4.5094 avg training loss: 5.9964
batch: [10090/10547] batch time: 0.050 trainign loss: 6.5410 avg training loss: 5.9962
batch: [10100/10547] batch time: 1.052 trainign loss: 4.6082 avg training loss: 5.9962
batch: [10110/10547] batch time: 0.046 trainign loss: 5.0502 avg training loss: 5.9960
batch: [10120/10547] batch time: 1.960 trainign loss: 4.7116 avg training loss: 5.9959
batch: [10130/10547] batch time: 0.051 trainign loss: 4.3716 avg training loss: 5.9956
batch: [10140/10547] batch time: 2.876 trainign loss: 3.4500 avg training loss: 5.9954
batch: [10150/10547] batch time: 0.055 trainign loss: 4.4763 avg training loss: 5.9954
batch: [10160/10547] batch time: 1.930 trainign loss: 0.0154 avg training loss: 5.9946
batch: [10170/10547] batch time: 0.045 trainign loss: 6.5515 avg training loss: 5.9943
batch: [10180/10547] batch time: 2.296 trainign loss: 5.9505 avg training loss: 5.9944
batch: [10190/10547] batch time: 0.046 trainign loss: 1.4409 avg training loss: 5.9940
batch: [10200/10547] batch time: 2.390 trainign loss: 6.2152 avg training loss: 5.9941
batch: [10210/10547] batch time: 0.046 trainign loss: 6.0109 avg training loss: 5.9941
batch: [10220/10547] batch time: 2.279 trainign loss: 4.8781 avg training loss: 5.9939
batch: [10230/10547] batch time: 0.054 trainign loss: 6.0226 avg training loss: 5.9939
batch: [10240/10547] batch time: 2.086 trainign loss: 5.3068 avg training loss: 5.9938
batch: [10250/10547] batch time: 0.046 trainign loss: 4.9777 avg training loss: 5.9937
batch: [10260/10547] batch time: 2.119 trainign loss: 4.8223 avg training loss: 5.9935
batch: [10270/10547] batch time: 0.051 trainign loss: 5.7375 avg training loss: 5.9934
batch: [10280/10547] batch time: 2.243 trainign loss: 6.3992 avg training loss: 5.9934
batch: [10290/10547] batch time: 0.046 trainign loss: 5.2670 avg training loss: 5.9934
batch: [10300/10547] batch time: 1.841 trainign loss: 5.8644 avg training loss: 5.9933
batch: [10310/10547] batch time: 0.054 trainign loss: 5.4939 avg training loss: 5.9932
batch: [10320/10547] batch time: 1.211 trainign loss: 6.1353 avg training loss: 5.9931
batch: [10330/10547] batch time: 0.047 trainign loss: 4.2685 avg training loss: 5.9929
batch: [10340/10547] batch time: 0.828 trainign loss: 5.6430 avg training loss: 5.9928
batch: [10350/10547] batch time: 0.046 trainign loss: 5.5885 avg training loss: 5.9927
batch: [10360/10547] batch time: 0.543 trainign loss: 4.6436 avg training loss: 5.9924
batch: [10370/10547] batch time: 0.046 trainign loss: 0.5558 avg training loss: 5.9920
batch: [10380/10547] batch time: 0.473 trainign loss: 2.8795 avg training loss: 5.9915
batch: [10390/10547] batch time: 0.046 trainign loss: 6.2999 avg training loss: 5.9914
batch: [10400/10547] batch time: 1.320 trainign loss: 5.7524 avg training loss: 5.9914
batch: [10410/10547] batch time: 0.048 trainign loss: 5.4882 avg training loss: 5.9913
batch: [10420/10547] batch time: 0.992 trainign loss: 5.4909 avg training loss: 5.9911
batch: [10430/10547] batch time: 0.050 trainign loss: 5.3920 avg training loss: 5.9911
batch: [10440/10547] batch time: 0.310 trainign loss: 4.6653 avg training loss: 5.9909
batch: [10450/10547] batch time: 0.046 trainign loss: 4.4025 avg training loss: 5.9906
batch: [10460/10547] batch time: 0.102 trainign loss: 5.7555 avg training loss: 5.9904
batch: [10470/10547] batch time: 0.048 trainign loss: 5.3895 avg training loss: 5.9904
batch: [10480/10547] batch time: 0.042 trainign loss: 2.7320 avg training loss: 5.9902
batch: [10490/10547] batch time: 0.046 trainign loss: 6.0874 avg training loss: 5.9901
batch: [10500/10547] batch time: 0.054 trainign loss: 5.9265 avg training loss: 5.9900
batch: [10510/10547] batch time: 0.046 trainign loss: 5.6829 avg training loss: 5.9899
batch: [10520/10547] batch time: 0.042 trainign loss: 4.4680 avg training loss: 5.9898
batch: [10530/10547] batch time: 0.054 trainign loss: 5.3030 avg training loss: 5.9895
batch: [10540/10547] batch time: 0.046 trainign loss: 4.3271 avg training loss: 5.9893
Epoch: 7
----------------------------------------------------------------------
batch: [0/10547] batch time: 2.648 trainign loss: 5.6196 avg training loss: 5.9893
batch: [10/10547] batch time: 0.046 trainign loss: 6.7259 avg training loss: 5.9889
batch: [20/10547] batch time: 1.846 trainign loss: 6.0566 avg training loss: 5.9889
batch: [30/10547] batch time: 0.047 trainign loss: 5.4196 avg training loss: 5.9888
batch: [40/10547] batch time: 1.774 trainign loss: 6.2210 avg training loss: 5.9888
batch: [50/10547] batch time: 0.324 trainign loss: 6.0389 avg training loss: 5.9887
batch: [60/10547] batch time: 1.824 trainign loss: 4.7533 avg training loss: 5.9886
batch: [70/10547] batch time: 0.228 trainign loss: 5.5242 avg training loss: 5.9884
batch: [80/10547] batch time: 1.901 trainign loss: 5.8045 avg training loss: 5.9883
batch: [90/10547] batch time: 0.048 trainign loss: 5.3280 avg training loss: 5.9882
batch: [100/10547] batch time: 0.394 trainign loss: 2.3065 avg training loss: 5.9880
batch: [110/10547] batch time: 0.046 trainign loss: 0.0042 avg training loss: 5.9871
batch: [120/10547] batch time: 0.046 trainign loss: 7.0075 avg training loss: 5.9873
batch: [130/10547] batch time: 0.047 trainign loss: 6.4325 avg training loss: 5.9874
batch: [140/10547] batch time: 0.055 trainign loss: 5.5203 avg training loss: 5.9874
batch: [150/10547] batch time: 0.048 trainign loss: 3.8245 avg training loss: 5.9872
batch: [160/10547] batch time: 0.047 trainign loss: 5.3952 avg training loss: 5.9869
batch: [170/10547] batch time: 0.046 trainign loss: 5.1855 avg training loss: 5.9869
batch: [180/10547] batch time: 0.047 trainign loss: 5.9162 avg training loss: 5.9868
batch: [190/10547] batch time: 0.046 trainign loss: 5.5660 avg training loss: 5.9867
batch: [200/10547] batch time: 0.189 trainign loss: 4.9378 avg training loss: 5.9866
batch: [210/10547] batch time: 0.047 trainign loss: 4.5914 avg training loss: 5.9863
batch: [220/10547] batch time: 0.211 trainign loss: 6.7322 avg training loss: 5.9864
batch: [230/10547] batch time: 0.047 trainign loss: 5.2944 avg training loss: 5.9863
batch: [240/10547] batch time: 0.047 trainign loss: 5.7694 avg training loss: 5.9861
batch: [250/10547] batch time: 0.047 trainign loss: 6.0618 avg training loss: 5.9860
batch: [260/10547] batch time: 0.369 trainign loss: 5.8612 avg training loss: 5.9860
batch: [270/10547] batch time: 0.584 trainign loss: 4.6586 avg training loss: 5.9858
batch: [280/10547] batch time: 0.795 trainign loss: 4.9314 avg training loss: 5.9857
batch: [290/10547] batch time: 0.788 trainign loss: 4.7152 avg training loss: 5.9855
batch: [300/10547] batch time: 0.857 trainign loss: 4.8547 avg training loss: 5.9853
batch: [310/10547] batch time: 0.593 trainign loss: 5.0140 avg training loss: 5.9852
batch: [320/10547] batch time: 1.332 trainign loss: 5.9892 avg training loss: 5.9851
batch: [330/10547] batch time: 0.046 trainign loss: 5.2037 avg training loss: 5.9851
batch: [340/10547] batch time: 0.936 trainign loss: 5.2125 avg training loss: 5.9849
batch: [350/10547] batch time: 0.346 trainign loss: 5.8122 avg training loss: 5.9845
batch: [360/10547] batch time: 1.093 trainign loss: 6.3414 avg training loss: 5.9845
batch: [370/10547] batch time: 0.054 trainign loss: 6.1969 avg training loss: 5.9844
batch: [380/10547] batch time: 0.898 trainign loss: 5.5593 avg training loss: 5.9844
batch: [390/10547] batch time: 0.598 trainign loss: 5.4927 avg training loss: 5.9843
batch: [400/10547] batch time: 0.973 trainign loss: 5.5712 avg training loss: 5.9841
batch: [410/10547] batch time: 1.946 trainign loss: 4.4185 avg training loss: 5.9840
batch: [420/10547] batch time: 1.411 trainign loss: 5.6330 avg training loss: 5.9838
batch: [430/10547] batch time: 1.313 trainign loss: 5.8308 avg training loss: 5.9837
batch: [440/10547] batch time: 1.200 trainign loss: 3.1750 avg training loss: 5.9835
batch: [450/10547] batch time: 1.255 trainign loss: 5.8591 avg training loss: 5.9833
batch: [460/10547] batch time: 0.738 trainign loss: 4.9227 avg training loss: 5.9832
batch: [470/10547] batch time: 1.767 trainign loss: 0.8592 avg training loss: 5.9829
batch: [480/10547] batch time: 0.054 trainign loss: 8.8663 avg training loss: 5.9822
batch: [490/10547] batch time: 2.649 trainign loss: 6.6484 avg training loss: 5.9824
batch: [500/10547] batch time: 0.046 trainign loss: 6.2522 avg training loss: 5.9824
batch: [510/10547] batch time: 2.026 trainign loss: 5.1689 avg training loss: 5.9823
batch: [520/10547] batch time: 0.047 trainign loss: 5.3197 avg training loss: 5.9822
batch: [530/10547] batch time: 0.410 trainign loss: 0.4630 avg training loss: 5.9817
batch: [540/10547] batch time: 0.047 trainign loss: 5.4999 avg training loss: 5.9814
batch: [550/10547] batch time: 0.047 trainign loss: 6.7089 avg training loss: 5.9814
batch: [560/10547] batch time: 0.047 trainign loss: 6.2621 avg training loss: 5.9814
batch: [570/10547] batch time: 0.042 trainign loss: 5.5360 avg training loss: 5.9814
batch: [580/10547] batch time: 0.047 trainign loss: 5.4538 avg training loss: 5.9814
batch: [590/10547] batch time: 0.043 trainign loss: 5.9745 avg training loss: 5.9812
batch: [600/10547] batch time: 0.047 trainign loss: 4.5894 avg training loss: 5.9811
batch: [610/10547] batch time: 0.235 trainign loss: 5.2138 avg training loss: 5.9810
batch: [620/10547] batch time: 0.046 trainign loss: 5.1556 avg training loss: 5.9810
batch: [630/10547] batch time: 0.722 trainign loss: 5.1012 avg training loss: 5.9809
batch: [640/10547] batch time: 0.046 trainign loss: 2.7896 avg training loss: 5.9807
batch: [650/10547] batch time: 1.509 trainign loss: 4.5952 avg training loss: 5.9804
batch: [660/10547] batch time: 0.047 trainign loss: 5.0803 avg training loss: 5.9803
batch: [670/10547] batch time: 1.595 trainign loss: 4.6348 avg training loss: 5.9800
batch: [680/10547] batch time: 0.909 trainign loss: 5.7623 avg training loss: 5.9800
batch: [690/10547] batch time: 1.294 trainign loss: 5.1597 avg training loss: 5.9799
batch: [700/10547] batch time: 0.877 trainign loss: 4.9532 avg training loss: 5.9798
batch: [710/10547] batch time: 1.240 trainign loss: 5.8354 avg training loss: 5.9797
batch: [720/10547] batch time: 0.403 trainign loss: 2.9222 avg training loss: 5.9794
batch: [730/10547] batch time: 1.004 trainign loss: 8.0776 avg training loss: 5.9787
batch: [740/10547] batch time: 0.273 trainign loss: 4.9918 avg training loss: 5.9789
batch: [750/10547] batch time: 0.332 trainign loss: 4.7900 avg training loss: 5.9789
batch: [760/10547] batch time: 0.961 trainign loss: 1.5102 avg training loss: 5.9785
batch: [770/10547] batch time: 0.047 trainign loss: 6.5563 avg training loss: 5.9786
batch: [780/10547] batch time: 1.390 trainign loss: 6.1473 avg training loss: 5.9786
batch: [790/10547] batch time: 0.056 trainign loss: 5.7003 avg training loss: 5.9786
batch: [800/10547] batch time: 2.115 trainign loss: 5.5701 avg training loss: 5.9786
batch: [810/10547] batch time: 0.045 trainign loss: 4.0290 avg training loss: 5.9784
batch: [820/10547] batch time: 2.388 trainign loss: 5.4406 avg training loss: 5.9783
batch: [830/10547] batch time: 0.047 trainign loss: 6.3279 avg training loss: 5.9782
batch: [840/10547] batch time: 2.210 trainign loss: 4.9225 avg training loss: 5.9782
batch: [850/10547] batch time: 0.047 trainign loss: 5.0554 avg training loss: 5.9780
batch: [860/10547] batch time: 1.579 trainign loss: 4.7647 avg training loss: 5.9778
batch: [870/10547] batch time: 0.046 trainign loss: 5.8975 avg training loss: 5.9777
batch: [880/10547] batch time: 2.489 trainign loss: 4.4499 avg training loss: 5.9777
batch: [890/10547] batch time: 0.046 trainign loss: 3.9439 avg training loss: 5.9772
batch: [900/10547] batch time: 2.247 trainign loss: 5.4732 avg training loss: 5.9772
batch: [910/10547] batch time: 0.054 trainign loss: 4.6498 avg training loss: 5.9771
batch: [920/10547] batch time: 2.003 trainign loss: 5.4905 avg training loss: 5.9769
batch: [930/10547] batch time: 0.050 trainign loss: 4.8451 avg training loss: 5.9769
batch: [940/10547] batch time: 2.061 trainign loss: 5.2262 avg training loss: 5.9767
batch: [950/10547] batch time: 0.046 trainign loss: 4.6708 avg training loss: 5.9765
batch: [960/10547] batch time: 2.146 trainign loss: 0.5567 avg training loss: 5.9761
batch: [970/10547] batch time: 0.047 trainign loss: 6.1171 avg training loss: 5.9760
batch: [980/10547] batch time: 1.863 trainign loss: 4.3711 avg training loss: 5.9757
batch: [990/10547] batch time: 0.047 trainign loss: 0.3350 avg training loss: 5.9751
batch: [1000/10547] batch time: 1.909 trainign loss: 8.0421 avg training loss: 5.9744
batch: [1010/10547] batch time: 0.049 trainign loss: 5.1613 avg training loss: 5.9745
batch: [1020/10547] batch time: 2.345 trainign loss: 0.0288 avg training loss: 5.9738
batch: [1030/10547] batch time: 0.045 trainign loss: 0.0004 avg training loss: 5.9729
batch: [1040/10547] batch time: 2.046 trainign loss: 10.5179 avg training loss: 5.9725
batch: [1050/10547] batch time: 0.046 trainign loss: 7.3922 avg training loss: 5.9727
batch: [1060/10547] batch time: 2.393 trainign loss: 5.7382 avg training loss: 5.9728
batch: [1070/10547] batch time: 0.046 trainign loss: 6.0028 avg training loss: 5.9727
batch: [1080/10547] batch time: 1.866 trainign loss: 2.6795 avg training loss: 5.9725
batch: [1090/10547] batch time: 0.051 trainign loss: 5.7389 avg training loss: 5.9724
batch: [1100/10547] batch time: 2.412 trainign loss: 3.6015 avg training loss: 5.9722
batch: [1110/10547] batch time: 0.047 trainign loss: 5.6516 avg training loss: 5.9719
batch: [1120/10547] batch time: 2.332 trainign loss: 5.2250 avg training loss: 5.9716
batch: [1130/10547] batch time: 0.054 trainign loss: 5.8578 avg training loss: 5.9713
batch: [1140/10547] batch time: 2.432 trainign loss: 5.9628 avg training loss: 5.9713
batch: [1150/10547] batch time: 0.046 trainign loss: 5.3548 avg training loss: 5.9713
batch: [1160/10547] batch time: 1.724 trainign loss: 4.5498 avg training loss: 5.9711
batch: [1170/10547] batch time: 0.050 trainign loss: 5.4364 avg training loss: 5.9710
batch: [1180/10547] batch time: 2.167 trainign loss: 5.9198 avg training loss: 5.9710
batch: [1190/10547] batch time: 0.055 trainign loss: 5.5159 avg training loss: 5.9710
batch: [1200/10547] batch time: 2.314 trainign loss: 4.1173 avg training loss: 5.9708
batch: [1210/10547] batch time: 0.055 trainign loss: 4.4874 avg training loss: 5.9707
batch: [1220/10547] batch time: 2.436 trainign loss: 5.5959 avg training loss: 5.9705
batch: [1230/10547] batch time: 0.046 trainign loss: 5.2513 avg training loss: 5.9704
batch: [1240/10547] batch time: 2.202 trainign loss: 5.3362 avg training loss: 5.9704
batch: [1250/10547] batch time: 0.047 trainign loss: 5.8606 avg training loss: 5.9702
batch: [1260/10547] batch time: 2.397 trainign loss: 0.1865 avg training loss: 5.9697
batch: [1270/10547] batch time: 0.047 trainign loss: 6.4975 avg training loss: 5.9697
batch: [1280/10547] batch time: 2.487 trainign loss: 3.3275 avg training loss: 5.9696
batch: [1290/10547] batch time: 0.047 trainign loss: 5.9479 avg training loss: 5.9696
batch: [1300/10547] batch time: 2.397 trainign loss: 5.2728 avg training loss: 5.9695
batch: [1310/10547] batch time: 0.047 trainign loss: 4.4185 avg training loss: 5.9693
batch: [1320/10547] batch time: 2.098 trainign loss: 3.3519 avg training loss: 5.9691
batch: [1330/10547] batch time: 0.046 trainign loss: 6.1776 avg training loss: 5.9688
batch: [1340/10547] batch time: 2.068 trainign loss: 6.3711 avg training loss: 5.9689
batch: [1350/10547] batch time: 0.055 trainign loss: 5.3155 avg training loss: 5.9688
batch: [1360/10547] batch time: 2.124 trainign loss: 5.5075 avg training loss: 5.9687
batch: [1370/10547] batch time: 0.046 trainign loss: 4.6791 avg training loss: 5.9686
batch: [1380/10547] batch time: 2.214 trainign loss: 4.9131 avg training loss: 5.9685
batch: [1390/10547] batch time: 0.046 trainign loss: 6.7541 avg training loss: 5.9681
batch: [1400/10547] batch time: 2.504 trainign loss: 6.0643 avg training loss: 5.9680
batch: [1410/10547] batch time: 0.047 trainign loss: 5.8093 avg training loss: 5.9679
batch: [1420/10547] batch time: 1.799 trainign loss: 5.1250 avg training loss: 5.9678
batch: [1430/10547] batch time: 0.046 trainign loss: 5.4592 avg training loss: 5.9677
batch: [1440/10547] batch time: 2.353 trainign loss: 5.1611 avg training loss: 5.9676
batch: [1450/10547] batch time: 0.047 trainign loss: 1.5074 avg training loss: 5.9673
batch: [1460/10547] batch time: 2.476 trainign loss: 3.1047 avg training loss: 5.9669
batch: [1470/10547] batch time: 0.046 trainign loss: 5.6339 avg training loss: 5.9670
batch: [1480/10547] batch time: 2.026 trainign loss: 5.2411 avg training loss: 5.9667
batch: [1490/10547] batch time: 0.054 trainign loss: 5.2123 avg training loss: 5.9666
batch: [1500/10547] batch time: 1.806 trainign loss: 6.0155 avg training loss: 5.9666
batch: [1510/10547] batch time: 0.046 trainign loss: 5.3669 avg training loss: 5.9665
batch: [1520/10547] batch time: 1.934 trainign loss: 4.9359 avg training loss: 5.9664
batch: [1530/10547] batch time: 0.054 trainign loss: 5.1752 avg training loss: 5.9663
batch: [1540/10547] batch time: 2.959 trainign loss: 0.8263 avg training loss: 5.9659
batch: [1550/10547] batch time: 0.046 trainign loss: 5.1406 avg training loss: 5.9658
batch: [1560/10547] batch time: 2.245 trainign loss: 6.0697 avg training loss: 5.9656
batch: [1570/10547] batch time: 0.048 trainign loss: 5.4743 avg training loss: 5.9656
batch: [1580/10547] batch time: 2.342 trainign loss: 5.7335 avg training loss: 5.9655
batch: [1590/10547] batch time: 0.046 trainign loss: 2.7890 avg training loss: 5.9653
batch: [1600/10547] batch time: 2.189 trainign loss: 5.1646 avg training loss: 5.9651
batch: [1610/10547] batch time: 0.046 trainign loss: 1.8937 avg training loss: 5.9647
batch: [1620/10547] batch time: 1.290 trainign loss: 4.6777 avg training loss: 5.9646
batch: [1630/10547] batch time: 0.048 trainign loss: 1.1919 avg training loss: 5.9642
batch: [1640/10547] batch time: 0.831 trainign loss: 5.2205 avg training loss: 5.9641
batch: [1650/10547] batch time: 0.046 trainign loss: 5.1548 avg training loss: 5.9639
batch: [1660/10547] batch time: 0.723 trainign loss: 4.4721 avg training loss: 5.9638
batch: [1670/10547] batch time: 0.046 trainign loss: 2.5046 avg training loss: 5.9634
batch: [1680/10547] batch time: 0.808 trainign loss: 4.8944 avg training loss: 5.9632
batch: [1690/10547] batch time: 0.047 trainign loss: 5.5640 avg training loss: 5.9631
batch: [1700/10547] batch time: 1.721 trainign loss: 5.5404 avg training loss: 5.9629
batch: [1710/10547] batch time: 0.048 trainign loss: 5.5690 avg training loss: 5.9628
batch: [1720/10547] batch time: 2.022 trainign loss: 5.3421 avg training loss: 5.9627
batch: [1730/10547] batch time: 0.055 trainign loss: 5.6340 avg training loss: 5.9626
batch: [1740/10547] batch time: 1.841 trainign loss: 6.0215 avg training loss: 5.9626
batch: [1750/10547] batch time: 0.048 trainign loss: 4.6822 avg training loss: 5.9625
batch: [1760/10547] batch time: 2.160 trainign loss: 1.6374 avg training loss: 5.9621
batch: [1770/10547] batch time: 0.254 trainign loss: 6.2234 avg training loss: 5.9621
batch: [1780/10547] batch time: 1.495 trainign loss: 5.0741 avg training loss: 5.9619
batch: [1790/10547] batch time: 1.606 trainign loss: 4.7100 avg training loss: 5.9617
batch: [1800/10547] batch time: 1.319 trainign loss: 4.4690 avg training loss: 5.9615
batch: [1810/10547] batch time: 1.553 trainign loss: 5.5977 avg training loss: 5.9615
batch: [1820/10547] batch time: 0.046 trainign loss: 5.5101 avg training loss: 5.9613
batch: [1830/10547] batch time: 2.065 trainign loss: 4.5487 avg training loss: 5.9612
batch: [1840/10547] batch time: 0.054 trainign loss: 5.6581 avg training loss: 5.9611
batch: [1850/10547] batch time: 1.997 trainign loss: 4.1973 avg training loss: 5.9608
batch: [1860/10547] batch time: 0.046 trainign loss: 4.0961 avg training loss: 5.9606
batch: [1870/10547] batch time: 1.859 trainign loss: 6.0377 avg training loss: 5.9605
batch: [1880/10547] batch time: 0.046 trainign loss: 4.8953 avg training loss: 5.9605
batch: [1890/10547] batch time: 2.214 trainign loss: 5.1903 avg training loss: 5.9603
batch: [1900/10547] batch time: 0.054 trainign loss: 2.4471 avg training loss: 5.9601
batch: [1910/10547] batch time: 1.645 trainign loss: 1.2273 avg training loss: 5.9593
batch: [1920/10547] batch time: 0.049 trainign loss: 6.1095 avg training loss: 5.9594
batch: [1930/10547] batch time: 2.036 trainign loss: 5.4195 avg training loss: 5.9594
batch: [1940/10547] batch time: 0.047 trainign loss: 5.7815 avg training loss: 5.9594
batch: [1950/10547] batch time: 1.619 trainign loss: 5.4510 avg training loss: 5.9593
batch: [1960/10547] batch time: 0.046 trainign loss: 5.3832 avg training loss: 5.9590
batch: [1970/10547] batch time: 2.133 trainign loss: 4.8615 avg training loss: 5.9589
batch: [1980/10547] batch time: 0.046 trainign loss: 5.0342 avg training loss: 5.9588
batch: [1990/10547] batch time: 2.671 trainign loss: 0.1706 avg training loss: 5.9582
batch: [2000/10547] batch time: 0.052 trainign loss: 7.0714 avg training loss: 5.9579
batch: [2010/10547] batch time: 2.036 trainign loss: 5.9016 avg training loss: 5.9580
batch: [2020/10547] batch time: 0.054 trainign loss: 5.4576 avg training loss: 5.9579
batch: [2030/10547] batch time: 2.246 trainign loss: 3.9828 avg training loss: 5.9577
batch: [2040/10547] batch time: 0.046 trainign loss: 4.7112 avg training loss: 5.9574
batch: [2050/10547] batch time: 1.383 trainign loss: 5.6637 avg training loss: 5.9574
batch: [2060/10547] batch time: 0.046 trainign loss: 3.5559 avg training loss: 5.9573
batch: [2070/10547] batch time: 1.160 trainign loss: 3.5909 avg training loss: 5.9571
batch: [2080/10547] batch time: 0.048 trainign loss: 4.5397 avg training loss: 5.9568
batch: [2090/10547] batch time: 0.958 trainign loss: 4.7664 avg training loss: 5.9566
batch: [2100/10547] batch time: 0.053 trainign loss: 0.0812 avg training loss: 5.9560
batch: [2110/10547] batch time: 1.417 trainign loss: 6.3565 avg training loss: 5.9560
batch: [2120/10547] batch time: 0.048 trainign loss: 4.8282 avg training loss: 5.9560
batch: [2130/10547] batch time: 1.656 trainign loss: 5.2629 avg training loss: 5.9560
batch: [2140/10547] batch time: 0.049 trainign loss: 5.2603 avg training loss: 5.9558
batch: [2150/10547] batch time: 1.473 trainign loss: 3.8793 avg training loss: 5.9557
batch: [2160/10547] batch time: 0.047 trainign loss: 5.2254 avg training loss: 5.9554
batch: [2170/10547] batch time: 1.857 trainign loss: 5.9086 avg training loss: 5.9553
batch: [2180/10547] batch time: 0.048 trainign loss: 4.9678 avg training loss: 5.9552
batch: [2190/10547] batch time: 1.018 trainign loss: 0.1169 avg training loss: 5.9546
batch: [2200/10547] batch time: 0.049 trainign loss: 5.0266 avg training loss: 5.9544
batch: [2210/10547] batch time: 0.052 trainign loss: 3.5677 avg training loss: 5.9543
batch: [2220/10547] batch time: 0.379 trainign loss: 6.3460 avg training loss: 5.9540
batch: [2230/10547] batch time: 0.048 trainign loss: 3.5181 avg training loss: 5.9539
batch: [2240/10547] batch time: 1.176 trainign loss: 5.9142 avg training loss: 5.9537
batch: [2250/10547] batch time: 0.888 trainign loss: 6.1221 avg training loss: 5.9537
batch: [2260/10547] batch time: 0.592 trainign loss: 5.6779 avg training loss: 5.9537
batch: [2270/10547] batch time: 1.020 trainign loss: 5.5877 avg training loss: 5.9536
batch: [2280/10547] batch time: 0.326 trainign loss: 3.6623 avg training loss: 5.9535
batch: [2290/10547] batch time: 0.912 trainign loss: 6.3608 avg training loss: 5.9534
batch: [2300/10547] batch time: 0.047 trainign loss: 4.4497 avg training loss: 5.9533
batch: [2310/10547] batch time: 1.765 trainign loss: 5.4821 avg training loss: 5.9531
batch: [2320/10547] batch time: 1.595 trainign loss: 5.9705 avg training loss: 5.9530
batch: [2330/10547] batch time: 0.715 trainign loss: 5.5315 avg training loss: 5.9528
batch: [2340/10547] batch time: 1.409 trainign loss: 4.9250 avg training loss: 5.9527
batch: [2350/10547] batch time: 0.054 trainign loss: 4.9493 avg training loss: 5.9525
batch: [2360/10547] batch time: 1.273 trainign loss: 4.8293 avg training loss: 5.9522
batch: [2370/10547] batch time: 0.468 trainign loss: 4.9782 avg training loss: 5.9522
batch: [2380/10547] batch time: 1.295 trainign loss: 2.8085 avg training loss: 5.9520
batch: [2390/10547] batch time: 0.519 trainign loss: 9.4249 avg training loss: 5.9514
batch: [2400/10547] batch time: 0.818 trainign loss: 6.3079 avg training loss: 5.9514
batch: [2410/10547] batch time: 0.786 trainign loss: 4.3581 avg training loss: 5.9513
batch: [2420/10547] batch time: 0.835 trainign loss: 6.2996 avg training loss: 5.9512
batch: [2430/10547] batch time: 0.360 trainign loss: 5.6572 avg training loss: 5.9511
batch: [2440/10547] batch time: 1.783 trainign loss: 4.5743 avg training loss: 5.9509
batch: [2450/10547] batch time: 0.046 trainign loss: 4.8755 avg training loss: 5.9507
batch: [2460/10547] batch time: 0.492 trainign loss: 0.6699 avg training loss: 5.9503
batch: [2470/10547] batch time: 0.047 trainign loss: 8.3077 avg training loss: 5.9498
batch: [2480/10547] batch time: 0.057 trainign loss: 4.0298 avg training loss: 5.9497
batch: [2490/10547] batch time: 0.046 trainign loss: 6.0459 avg training loss: 5.9495
batch: [2500/10547] batch time: 0.314 trainign loss: 5.3216 avg training loss: 5.9494
batch: [2510/10547] batch time: 0.046 trainign loss: 0.5396 avg training loss: 5.9490
batch: [2520/10547] batch time: 0.046 trainign loss: 5.8263 avg training loss: 5.9488
batch: [2530/10547] batch time: 0.046 trainign loss: 5.9287 avg training loss: 5.9488
batch: [2540/10547] batch time: 0.046 trainign loss: 5.8553 avg training loss: 5.9487
batch: [2550/10547] batch time: 0.046 trainign loss: 5.0801 avg training loss: 5.9485
batch: [2560/10547] batch time: 0.050 trainign loss: 5.7360 avg training loss: 5.9484
batch: [2570/10547] batch time: 0.047 trainign loss: 5.0399 avg training loss: 5.9484
batch: [2580/10547] batch time: 0.077 trainign loss: 4.8243 avg training loss: 5.9483
batch: [2590/10547] batch time: 0.046 trainign loss: 5.6650 avg training loss: 5.9482
batch: [2600/10547] batch time: 0.089 trainign loss: 2.5271 avg training loss: 5.9480
batch: [2610/10547] batch time: 0.046 trainign loss: 6.3423 avg training loss: 5.9479
batch: [2620/10547] batch time: 1.078 trainign loss: 5.4377 avg training loss: 5.9479
batch: [2630/10547] batch time: 0.046 trainign loss: 5.4271 avg training loss: 5.9478
batch: [2640/10547] batch time: 1.896 trainign loss: 2.5642 avg training loss: 5.9475
batch: [2650/10547] batch time: 0.050 trainign loss: 4.9445 avg training loss: 5.9473
batch: [2660/10547] batch time: 1.553 trainign loss: 6.3215 avg training loss: 5.9472
batch: [2670/10547] batch time: 0.053 trainign loss: 5.5001 avg training loss: 5.9472
batch: [2680/10547] batch time: 1.112 trainign loss: 4.9208 avg training loss: 5.9470
batch: [2690/10547] batch time: 0.047 trainign loss: 4.3949 avg training loss: 5.9469
batch: [2700/10547] batch time: 1.240 trainign loss: 5.4389 avg training loss: 5.9466
batch: [2710/10547] batch time: 0.692 trainign loss: 5.0627 avg training loss: 5.9465
batch: [2720/10547] batch time: 1.009 trainign loss: 5.4953 avg training loss: 5.9464
batch: [2730/10547] batch time: 0.046 trainign loss: 5.7425 avg training loss: 5.9464
batch: [2740/10547] batch time: 0.049 trainign loss: 6.0653 avg training loss: 5.9461
batch: [2750/10547] batch time: 0.049 trainign loss: 4.9687 avg training loss: 5.9460
batch: [2760/10547] batch time: 0.995 trainign loss: 4.9561 avg training loss: 5.9458
batch: [2770/10547] batch time: 0.046 trainign loss: 0.2269 avg training loss: 5.9453
batch: [2780/10547] batch time: 1.064 trainign loss: 6.3096 avg training loss: 5.9453
batch: [2790/10547] batch time: 0.052 trainign loss: 5.6248 avg training loss: 5.9453
batch: [2800/10547] batch time: 0.557 trainign loss: 5.1459 avg training loss: 5.9452
batch: [2810/10547] batch time: 0.046 trainign loss: 4.3445 avg training loss: 5.9450
batch: [2820/10547] batch time: 0.317 trainign loss: 5.7448 avg training loss: 5.9449
batch: [2830/10547] batch time: 0.046 trainign loss: 5.2321 avg training loss: 5.9448
batch: [2840/10547] batch time: 1.215 trainign loss: 4.6115 avg training loss: 5.9445
batch: [2850/10547] batch time: 0.049 trainign loss: 6.5059 avg training loss: 5.9444
batch: [2860/10547] batch time: 1.276 trainign loss: 5.9639 avg training loss: 5.9444
batch: [2870/10547] batch time: 0.046 trainign loss: 5.4704 avg training loss: 5.9444
batch: [2880/10547] batch time: 1.238 trainign loss: 5.3699 avg training loss: 5.9442
batch: [2890/10547] batch time: 0.046 trainign loss: 5.1329 avg training loss: 5.9441
batch: [2900/10547] batch time: 0.918 trainign loss: 5.0019 avg training loss: 5.9440
batch: [2910/10547] batch time: 0.047 trainign loss: 5.3490 avg training loss: 5.9439
batch: [2920/10547] batch time: 0.048 trainign loss: 3.8720 avg training loss: 5.9438
batch: [2930/10547] batch time: 0.048 trainign loss: 3.5935 avg training loss: 5.9435
batch: [2940/10547] batch time: 0.432 trainign loss: 6.0992 avg training loss: 5.9432
batch: [2950/10547] batch time: 0.048 trainign loss: 4.2111 avg training loss: 5.9431
batch: [2960/10547] batch time: 0.301 trainign loss: 5.4982 avg training loss: 5.9429
batch: [2970/10547] batch time: 0.046 trainign loss: 5.5732 avg training loss: 5.9429
batch: [2980/10547] batch time: 0.046 trainign loss: 2.9552 avg training loss: 5.9427
batch: [2990/10547] batch time: 0.190 trainign loss: 6.7029 avg training loss: 5.9424
batch: [3000/10547] batch time: 0.045 trainign loss: 5.5510 avg training loss: 5.9424
batch: [3010/10547] batch time: 0.046 trainign loss: 4.5415 avg training loss: 5.9422
batch: [3020/10547] batch time: 0.050 trainign loss: 5.5769 avg training loss: 5.9420
batch: [3030/10547] batch time: 0.569 trainign loss: 6.1308 avg training loss: 5.9420
batch: [3040/10547] batch time: 0.046 trainign loss: 3.9362 avg training loss: 5.9419
batch: [3050/10547] batch time: 0.666 trainign loss: 5.7745 avg training loss: 5.9417
batch: [3060/10547] batch time: 0.047 trainign loss: 5.4031 avg training loss: 5.9417
batch: [3070/10547] batch time: 2.135 trainign loss: 6.3293 avg training loss: 5.9414
batch: [3080/10547] batch time: 0.055 trainign loss: 6.7043 avg training loss: 5.9415
batch: [3090/10547] batch time: 2.502 trainign loss: 5.2168 avg training loss: 5.9415
batch: [3100/10547] batch time: 0.046 trainign loss: 5.0931 avg training loss: 5.9413
batch: [3110/10547] batch time: 2.697 trainign loss: 4.8557 avg training loss: 5.9412
batch: [3120/10547] batch time: 0.046 trainign loss: 6.0764 avg training loss: 5.9411
batch: [3130/10547] batch time: 2.308 trainign loss: 4.9928 avg training loss: 5.9409
batch: [3140/10547] batch time: 0.045 trainign loss: 5.6039 avg training loss: 5.9408
batch: [3150/10547] batch time: 2.090 trainign loss: 5.2079 avg training loss: 5.9406
batch: [3160/10547] batch time: 0.046 trainign loss: 5.0787 avg training loss: 5.9404
batch: [3170/10547] batch time: 2.289 trainign loss: 3.9504 avg training loss: 5.9402
batch: [3180/10547] batch time: 0.047 trainign loss: 4.6700 avg training loss: 5.9400
batch: [3190/10547] batch time: 2.553 trainign loss: 2.6127 avg training loss: 5.9396
batch: [3200/10547] batch time: 0.048 trainign loss: 6.9588 avg training loss: 5.9397
batch: [3210/10547] batch time: 2.051 trainign loss: 5.7609 avg training loss: 5.9397
batch: [3220/10547] batch time: 0.046 trainign loss: 4.9480 avg training loss: 5.9396
batch: [3230/10547] batch time: 2.149 trainign loss: 5.0414 avg training loss: 5.9394
batch: [3240/10547] batch time: 0.047 trainign loss: 3.5480 avg training loss: 5.9393
batch: [3250/10547] batch time: 2.787 trainign loss: 5.5051 avg training loss: 5.9391
batch: [3260/10547] batch time: 0.045 trainign loss: 4.8675 avg training loss: 5.9390
batch: [3270/10547] batch time: 2.324 trainign loss: 4.3204 avg training loss: 5.9389
batch: [3280/10547] batch time: 0.055 trainign loss: 5.1194 avg training loss: 5.9388
batch: [3290/10547] batch time: 2.144 trainign loss: 4.9853 avg training loss: 5.9387
batch: [3300/10547] batch time: 0.046 trainign loss: 5.2142 avg training loss: 5.9385
batch: [3310/10547] batch time: 1.110 trainign loss: 4.4092 avg training loss: 5.9384
batch: [3320/10547] batch time: 0.046 trainign loss: 4.3066 avg training loss: 5.9382
batch: [3330/10547] batch time: 1.362 trainign loss: 6.7034 avg training loss: 5.9381
batch: [3340/10547] batch time: 0.046 trainign loss: 5.0465 avg training loss: 5.9381
batch: [3350/10547] batch time: 2.201 trainign loss: 5.2755 avg training loss: 5.9380
batch: [3360/10547] batch time: 0.399 trainign loss: 5.7990 avg training loss: 5.9379
batch: [3370/10547] batch time: 0.986 trainign loss: 1.2518 avg training loss: 5.9375
batch: [3380/10547] batch time: 0.825 trainign loss: 5.9545 avg training loss: 5.9374
batch: [3390/10547] batch time: 0.739 trainign loss: 5.7066 avg training loss: 5.9372
batch: [3400/10547] batch time: 1.672 trainign loss: 3.1771 avg training loss: 5.9370
batch: [3410/10547] batch time: 0.047 trainign loss: 6.0618 avg training loss: 5.9370
batch: [3420/10547] batch time: 1.371 trainign loss: 5.6916 avg training loss: 5.9369
batch: [3430/10547] batch time: 0.054 trainign loss: 2.9738 avg training loss: 5.9368
batch: [3440/10547] batch time: 1.548 trainign loss: 5.7898 avg training loss: 5.9366
batch: [3450/10547] batch time: 0.063 trainign loss: 3.7362 avg training loss: 5.9364
batch: [3460/10547] batch time: 2.244 trainign loss: 6.0393 avg training loss: 5.9363
batch: [3470/10547] batch time: 0.047 trainign loss: 4.9349 avg training loss: 5.9362
batch: [3480/10547] batch time: 1.939 trainign loss: 5.0252 avg training loss: 5.9362
batch: [3490/10547] batch time: 0.055 trainign loss: 5.5844 avg training loss: 5.9360
batch: [3500/10547] batch time: 2.255 trainign loss: 4.9224 avg training loss: 5.9359
batch: [3510/10547] batch time: 0.048 trainign loss: 3.5158 avg training loss: 5.9357
batch: [3520/10547] batch time: 1.631 trainign loss: 5.1028 avg training loss: 5.9354
batch: [3530/10547] batch time: 0.048 trainign loss: 5.7761 avg training loss: 5.9352
batch: [3540/10547] batch time: 1.217 trainign loss: 4.4346 avg training loss: 5.9352
batch: [3550/10547] batch time: 0.054 trainign loss: 5.4088 avg training loss: 5.9350
batch: [3560/10547] batch time: 0.047 trainign loss: 3.4457 avg training loss: 5.9349
batch: [3570/10547] batch time: 0.046 trainign loss: 5.1758 avg training loss: 5.9348
batch: [3580/10547] batch time: 0.046 trainign loss: 5.6185 avg training loss: 5.9346
batch: [3590/10547] batch time: 0.046 trainign loss: 4.1995 avg training loss: 5.9344
batch: [3600/10547] batch time: 0.047 trainign loss: 6.1420 avg training loss: 5.9342
batch: [3610/10547] batch time: 0.047 trainign loss: 5.6015 avg training loss: 5.9341
batch: [3620/10547] batch time: 0.046 trainign loss: 4.7335 avg training loss: 5.9340
batch: [3630/10547] batch time: 0.053 trainign loss: 5.4219 avg training loss: 5.9338
batch: [3640/10547] batch time: 0.046 trainign loss: 2.8215 avg training loss: 5.9336
batch: [3650/10547] batch time: 0.048 trainign loss: 5.4364 avg training loss: 5.9333
batch: [3660/10547] batch time: 0.054 trainign loss: 1.0366 avg training loss: 5.9330
batch: [3670/10547] batch time: 0.048 trainign loss: 6.2736 avg training loss: 5.9331
batch: [3680/10547] batch time: 0.046 trainign loss: 3.0903 avg training loss: 5.9329
batch: [3690/10547] batch time: 0.051 trainign loss: 1.4356 avg training loss: 5.9325
batch: [3700/10547] batch time: 0.046 trainign loss: 6.0488 avg training loss: 5.9325
batch: [3710/10547] batch time: 0.049 trainign loss: 5.7096 avg training loss: 5.9324
batch: [3720/10547] batch time: 0.047 trainign loss: 5.2580 avg training loss: 5.9323
batch: [3730/10547] batch time: 0.046 trainign loss: 4.6613 avg training loss: 5.9321
batch: [3740/10547] batch time: 0.045 trainign loss: 5.1393 avg training loss: 5.9320
batch: [3750/10547] batch time: 0.050 trainign loss: 4.9329 avg training loss: 5.9319
batch: [3760/10547] batch time: 0.047 trainign loss: 4.7234 avg training loss: 5.9318
batch: [3770/10547] batch time: 0.046 trainign loss: 3.2259 avg training loss: 5.9316
batch: [3780/10547] batch time: 0.046 trainign loss: 5.6837 avg training loss: 5.9314
batch: [3790/10547] batch time: 0.046 trainign loss: 4.2579 avg training loss: 5.9313
batch: [3800/10547] batch time: 0.046 trainign loss: 2.1442 avg training loss: 5.9310
batch: [3810/10547] batch time: 0.048 trainign loss: 3.0895 avg training loss: 5.9309
batch: [3820/10547] batch time: 0.046 trainign loss: 6.1574 avg training loss: 5.9305
batch: [3830/10547] batch time: 0.046 trainign loss: 6.7112 avg training loss: 5.9306
batch: [3840/10547] batch time: 0.051 trainign loss: 5.7719 avg training loss: 5.9306
batch: [3850/10547] batch time: 0.047 trainign loss: 5.1418 avg training loss: 5.9305
batch: [3860/10547] batch time: 0.055 trainign loss: 4.9659 avg training loss: 5.9303
batch: [3870/10547] batch time: 0.046 trainign loss: 5.8331 avg training loss: 5.9302
batch: [3880/10547] batch time: 0.046 trainign loss: 4.8614 avg training loss: 5.9301
batch: [3890/10547] batch time: 0.047 trainign loss: 4.8036 avg training loss: 5.9301
batch: [3900/10547] batch time: 0.042 trainign loss: 1.5156 avg training loss: 5.9297
batch: [3910/10547] batch time: 0.046 trainign loss: 6.1129 avg training loss: 5.9297
batch: [3920/10547] batch time: 0.054 trainign loss: 5.7161 avg training loss: 5.9298
batch: [3930/10547] batch time: 0.047 trainign loss: 5.4510 avg training loss: 5.9298
batch: [3940/10547] batch time: 0.042 trainign loss: 4.4836 avg training loss: 5.9296
batch: [3950/10547] batch time: 0.054 trainign loss: 4.5810 avg training loss: 5.9294
batch: [3960/10547] batch time: 0.292 trainign loss: 0.7602 avg training loss: 5.9290
batch: [3970/10547] batch time: 0.053 trainign loss: 4.7681 avg training loss: 5.9287
batch: [3980/10547] batch time: 0.345 trainign loss: 5.9167 avg training loss: 5.9287
batch: [3990/10547] batch time: 0.047 trainign loss: 4.9061 avg training loss: 5.9286
batch: [4000/10547] batch time: 0.051 trainign loss: 4.7781 avg training loss: 5.9285
batch: [4010/10547] batch time: 0.049 trainign loss: 5.1463 avg training loss: 5.9284
batch: [4020/10547] batch time: 0.179 trainign loss: 5.6563 avg training loss: 5.9282
batch: [4030/10547] batch time: 0.047 trainign loss: 3.8058 avg training loss: 5.9281
batch: [4040/10547] batch time: 0.048 trainign loss: 5.6983 avg training loss: 5.9280
batch: [4050/10547] batch time: 0.414 trainign loss: 4.3059 avg training loss: 5.9279
batch: [4060/10547] batch time: 0.046 trainign loss: 4.3459 avg training loss: 5.9278
batch: [4070/10547] batch time: 1.330 trainign loss: 4.7635 avg training loss: 5.9277
batch: [4080/10547] batch time: 0.054 trainign loss: 5.5988 avg training loss: 5.9276
batch: [4090/10547] batch time: 0.900 trainign loss: 5.1887 avg training loss: 5.9275
batch: [4100/10547] batch time: 0.047 trainign loss: 4.7995 avg training loss: 5.9274
batch: [4110/10547] batch time: 0.530 trainign loss: 4.6874 avg training loss: 5.9273
batch: [4120/10547] batch time: 0.046 trainign loss: 3.0099 avg training loss: 5.9270
batch: [4130/10547] batch time: 0.587 trainign loss: 0.1450 avg training loss: 5.9265
batch: [4140/10547] batch time: 0.046 trainign loss: 4.7081 avg training loss: 5.9264
batch: [4150/10547] batch time: 0.587 trainign loss: 5.3232 avg training loss: 5.9261
batch: [4160/10547] batch time: 0.046 trainign loss: 3.7497 avg training loss: 5.9259
batch: [4170/10547] batch time: 0.047 trainign loss: 4.6994 avg training loss: 5.9258
batch: [4180/10547] batch time: 0.046 trainign loss: 4.0492 avg training loss: 5.9256
batch: [4190/10547] batch time: 0.048 trainign loss: 5.9911 avg training loss: 5.9254
batch: [4200/10547] batch time: 0.046 trainign loss: 6.0129 avg training loss: 5.9254
batch: [4210/10547] batch time: 0.047 trainign loss: 4.6853 avg training loss: 5.9253
batch: [4220/10547] batch time: 0.046 trainign loss: 4.7542 avg training loss: 5.9252
batch: [4230/10547] batch time: 0.046 trainign loss: 5.4521 avg training loss: 5.9250
batch: [4240/10547] batch time: 0.047 trainign loss: 5.0083 avg training loss: 5.9248
batch: [4250/10547] batch time: 0.046 trainign loss: 5.4898 avg training loss: 5.9246
batch: [4260/10547] batch time: 0.053 trainign loss: 4.2311 avg training loss: 5.9244
batch: [4270/10547] batch time: 0.048 trainign loss: 3.7282 avg training loss: 5.9243
batch: [4280/10547] batch time: 0.048 trainign loss: 0.1223 avg training loss: 5.9237
batch: [4290/10547] batch time: 0.047 trainign loss: 0.0027 avg training loss: 5.9229
batch: [4300/10547] batch time: 0.047 trainign loss: 1.8623 avg training loss: 5.9220
batch: [4310/10547] batch time: 0.046 trainign loss: 5.4051 avg training loss: 5.9222
batch: [4320/10547] batch time: 0.047 trainign loss: 5.2568 avg training loss: 5.9223
batch: [4330/10547] batch time: 0.047 trainign loss: 3.6933 avg training loss: 5.9221
batch: [4340/10547] batch time: 0.047 trainign loss: 4.6592 avg training loss: 5.9219
batch: [4350/10547] batch time: 0.183 trainign loss: 5.4487 avg training loss: 5.9219
batch: [4360/10547] batch time: 0.046 trainign loss: 5.3663 avg training loss: 5.9218
batch: [4370/10547] batch time: 0.122 trainign loss: 5.3534 avg training loss: 5.9217
batch: [4380/10547] batch time: 0.050 trainign loss: 4.6039 avg training loss: 5.9216
batch: [4390/10547] batch time: 0.129 trainign loss: 4.6624 avg training loss: 5.9215
batch: [4400/10547] batch time: 0.046 trainign loss: 5.6186 avg training loss: 5.9212
batch: [4410/10547] batch time: 0.933 trainign loss: 5.0596 avg training loss: 5.9212
batch: [4420/10547] batch time: 0.046 trainign loss: 4.6070 avg training loss: 5.9211
batch: [4430/10547] batch time: 1.230 trainign loss: 5.6930 avg training loss: 5.9209
batch: [4440/10547] batch time: 0.046 trainign loss: 5.1124 avg training loss: 5.9205
batch: [4450/10547] batch time: 0.663 trainign loss: 5.6670 avg training loss: 5.9205
batch: [4460/10547] batch time: 0.055 trainign loss: 6.4909 avg training loss: 5.9204
batch: [4470/10547] batch time: 1.172 trainign loss: 5.4439 avg training loss: 5.9203
batch: [4480/10547] batch time: 0.046 trainign loss: 5.8835 avg training loss: 5.9201
batch: [4490/10547] batch time: 0.997 trainign loss: 6.3783 avg training loss: 5.9200
batch: [4500/10547] batch time: 0.054 trainign loss: 3.3406 avg training loss: 5.9199
batch: [4510/10547] batch time: 0.053 trainign loss: 5.4009 avg training loss: 5.9196
batch: [4520/10547] batch time: 0.046 trainign loss: 6.3331 avg training loss: 5.9194
batch: [4530/10547] batch time: 0.054 trainign loss: 4.0673 avg training loss: 5.9194
batch: [4540/10547] batch time: 0.047 trainign loss: 5.5842 avg training loss: 5.9192
batch: [4550/10547] batch time: 0.052 trainign loss: 5.5390 avg training loss: 5.9192
batch: [4560/10547] batch time: 0.044 trainign loss: 4.5731 avg training loss: 5.9191
batch: [4570/10547] batch time: 0.047 trainign loss: 4.8080 avg training loss: 5.9189
batch: [4580/10547] batch time: 0.046 trainign loss: 4.8781 avg training loss: 5.9188
batch: [4590/10547] batch time: 0.046 trainign loss: 4.6434 avg training loss: 5.9186
batch: [4600/10547] batch time: 0.042 trainign loss: 5.4821 avg training loss: 5.9184
batch: [4610/10547] batch time: 0.054 trainign loss: 5.6427 avg training loss: 5.9183
batch: [4620/10547] batch time: 0.046 trainign loss: 5.4387 avg training loss: 5.9182
batch: [4630/10547] batch time: 0.046 trainign loss: 4.7380 avg training loss: 5.9181
batch: [4640/10547] batch time: 0.046 trainign loss: 6.2627 avg training loss: 5.9181
batch: [4650/10547] batch time: 0.046 trainign loss: 4.3249 avg training loss: 5.9180
batch: [4660/10547] batch time: 0.048 trainign loss: 6.1397 avg training loss: 5.9178
batch: [4670/10547] batch time: 0.046 trainign loss: 4.3634 avg training loss: 5.9176
batch: [4680/10547] batch time: 0.052 trainign loss: 5.2152 avg training loss: 5.9175
batch: [4690/10547] batch time: 0.047 trainign loss: 5.0304 avg training loss: 5.9173
batch: [4700/10547] batch time: 0.047 trainign loss: 5.8940 avg training loss: 5.9171
batch: [4710/10547] batch time: 0.046 trainign loss: 5.4352 avg training loss: 5.9170
batch: [4720/10547] batch time: 0.047 trainign loss: 5.7172 avg training loss: 5.9167
batch: [4730/10547] batch time: 0.046 trainign loss: 6.3646 avg training loss: 5.9166
batch: [4740/10547] batch time: 0.046 trainign loss: 5.6109 avg training loss: 5.9165
batch: [4750/10547] batch time: 0.046 trainign loss: 5.1662 avg training loss: 5.9164
batch: [4760/10547] batch time: 0.041 trainign loss: 5.6396 avg training loss: 5.9164
batch: [4770/10547] batch time: 0.047 trainign loss: 6.4207 avg training loss: 5.9162
batch: [4780/10547] batch time: 0.046 trainign loss: 6.1736 avg training loss: 5.9162
batch: [4790/10547] batch time: 0.048 trainign loss: 4.9168 avg training loss: 5.9161
batch: [4800/10547] batch time: 0.047 trainign loss: 5.9780 avg training loss: 5.9160
batch: [4810/10547] batch time: 0.047 trainign loss: 5.7617 avg training loss: 5.9160
batch: [4820/10547] batch time: 0.041 trainign loss: 4.1612 avg training loss: 5.9157
batch: [4830/10547] batch time: 0.047 trainign loss: 5.8917 avg training loss: 5.9156
batch: [4840/10547] batch time: 0.046 trainign loss: 4.7075 avg training loss: 5.9155
batch: [4850/10547] batch time: 0.055 trainign loss: 3.5827 avg training loss: 5.9152
batch: [4860/10547] batch time: 0.044 trainign loss: 4.0139 avg training loss: 5.9150
batch: [4870/10547] batch time: 0.048 trainign loss: 6.1689 avg training loss: 5.9149
batch: [4880/10547] batch time: 0.047 trainign loss: 4.9799 avg training loss: 5.9149
batch: [4890/10547] batch time: 0.046 trainign loss: 5.4910 avg training loss: 5.9148
batch: [4900/10547] batch time: 0.046 trainign loss: 5.6831 avg training loss: 5.9147
batch: [4910/10547] batch time: 1.198 trainign loss: 5.8546 avg training loss: 5.9146
batch: [4920/10547] batch time: 0.047 trainign loss: 6.1443 avg training loss: 5.9146
batch: [4930/10547] batch time: 0.046 trainign loss: 4.4551 avg training loss: 5.9145
batch: [4940/10547] batch time: 0.046 trainign loss: 5.1841 avg training loss: 5.9143
batch: [4950/10547] batch time: 0.046 trainign loss: 5.2414 avg training loss: 5.9141
batch: [4960/10547] batch time: 0.054 trainign loss: 6.3512 avg training loss: 5.9140
batch: [4970/10547] batch time: 0.047 trainign loss: 6.3393 avg training loss: 5.9141
batch: [4980/10547] batch time: 0.046 trainign loss: 5.6728 avg training loss: 5.9141
batch: [4990/10547] batch time: 0.048 trainign loss: 4.6282 avg training loss: 5.9139
batch: [5000/10547] batch time: 0.074 trainign loss: 4.1612 avg training loss: 5.9139
batch: [5010/10547] batch time: 0.957 trainign loss: 4.5331 avg training loss: 5.9137
batch: [5020/10547] batch time: 0.046 trainign loss: 5.4901 avg training loss: 5.9135
batch: [5030/10547] batch time: 0.046 trainign loss: 6.2349 avg training loss: 5.9134
batch: [5040/10547] batch time: 0.047 trainign loss: 5.4727 avg training loss: 5.9133
batch: [5050/10547] batch time: 0.054 trainign loss: 4.6433 avg training loss: 5.9132
batch: [5060/10547] batch time: 0.047 trainign loss: 5.6310 avg training loss: 5.9128
batch: [5070/10547] batch time: 0.055 trainign loss: 6.0811 avg training loss: 5.9129
batch: [5080/10547] batch time: 0.048 trainign loss: 4.6898 avg training loss: 5.9127
batch: [5090/10547] batch time: 0.047 trainign loss: 5.4916 avg training loss: 5.9125
batch: [5100/10547] batch time: 0.054 trainign loss: 3.7864 avg training loss: 5.9125
batch: [5110/10547] batch time: 0.046 trainign loss: 5.5707 avg training loss: 5.9124
batch: [5120/10547] batch time: 0.048 trainign loss: 2.6223 avg training loss: 5.9122
batch: [5130/10547] batch time: 0.047 trainign loss: 4.3686 avg training loss: 5.9122
batch: [5140/10547] batch time: 0.047 trainign loss: 5.4118 avg training loss: 5.9120
batch: [5150/10547] batch time: 0.046 trainign loss: 5.7235 avg training loss: 5.9120
batch: [5160/10547] batch time: 0.187 trainign loss: 0.5371 avg training loss: 5.9116
batch: [5170/10547] batch time: 0.046 trainign loss: 5.8595 avg training loss: 5.9115
batch: [5180/10547] batch time: 0.042 trainign loss: 5.7430 avg training loss: 5.9115
batch: [5190/10547] batch time: 0.054 trainign loss: 5.9592 avg training loss: 5.9113
batch: [5200/10547] batch time: 0.050 trainign loss: 2.8256 avg training loss: 5.9111
batch: [5210/10547] batch time: 0.048 trainign loss: 5.0611 avg training loss: 5.9109
batch: [5220/10547] batch time: 0.051 trainign loss: 6.1233 avg training loss: 5.9108
batch: [5230/10547] batch time: 0.353 trainign loss: 3.7028 avg training loss: 5.9106
batch: [5240/10547] batch time: 0.046 trainign loss: 1.7546 avg training loss: 5.9102
batch: [5250/10547] batch time: 0.055 trainign loss: 6.2936 avg training loss: 5.9101
batch: [5260/10547] batch time: 0.045 trainign loss: 4.9019 avg training loss: 5.9101
batch: [5270/10547] batch time: 0.054 trainign loss: 4.5804 avg training loss: 5.9099
batch: [5280/10547] batch time: 0.054 trainign loss: 5.4591 avg training loss: 5.9097
batch: [5290/10547] batch time: 0.049 trainign loss: 5.3356 avg training loss: 5.9097
batch: [5300/10547] batch time: 0.046 trainign loss: 0.7814 avg training loss: 5.9093
batch: [5310/10547] batch time: 0.046 trainign loss: 6.2575 avg training loss: 5.9092
batch: [5320/10547] batch time: 0.042 trainign loss: 5.7762 avg training loss: 5.9092
batch: [5330/10547] batch time: 0.046 trainign loss: 4.6074 avg training loss: 5.9091
batch: [5340/10547] batch time: 0.047 trainign loss: 3.4024 avg training loss: 5.9088
batch: [5350/10547] batch time: 0.051 trainign loss: 5.3510 avg training loss: 5.9087
batch: [5360/10547] batch time: 0.047 trainign loss: 4.9660 avg training loss: 5.9085
batch: [5370/10547] batch time: 0.050 trainign loss: 4.9298 avg training loss: 5.9085
batch: [5380/10547] batch time: 0.051 trainign loss: 5.7201 avg training loss: 5.9084
batch: [5390/10547] batch time: 0.047 trainign loss: 2.7916 avg training loss: 5.9082
batch: [5400/10547] batch time: 0.046 trainign loss: 4.7065 avg training loss: 5.9080
batch: [5410/10547] batch time: 0.049 trainign loss: 6.0197 avg training loss: 5.9079
batch: [5420/10547] batch time: 0.047 trainign loss: 5.4575 avg training loss: 5.9079
batch: [5430/10547] batch time: 0.046 trainign loss: 5.8017 avg training loss: 5.9077
batch: [5440/10547] batch time: 0.046 trainign loss: 5.4330 avg training loss: 5.9076
batch: [5450/10547] batch time: 0.046 trainign loss: 4.9540 avg training loss: 5.9074
batch: [5460/10547] batch time: 0.044 trainign loss: 4.4248 avg training loss: 5.9072
batch: [5470/10547] batch time: 0.053 trainign loss: 1.7923 avg training loss: 5.9070
batch: [5480/10547] batch time: 0.043 trainign loss: 3.2553 avg training loss: 5.9067
batch: [5490/10547] batch time: 0.046 trainign loss: 6.4093 avg training loss: 5.9065
batch: [5500/10547] batch time: 0.046 trainign loss: 5.0892 avg training loss: 5.9066
batch: [5510/10547] batch time: 0.047 trainign loss: 4.8266 avg training loss: 5.9064
batch: [5520/10547] batch time: 0.042 trainign loss: 5.7527 avg training loss: 5.9060
batch: [5530/10547] batch time: 0.047 trainign loss: 4.0171 avg training loss: 5.9059
batch: [5540/10547] batch time: 0.046 trainign loss: 2.1826 avg training loss: 5.9057
batch: [5550/10547] batch time: 0.046 trainign loss: 4.8484 avg training loss: 5.9054
batch: [5560/10547] batch time: 0.046 trainign loss: 5.3596 avg training loss: 5.9053
batch: [5570/10547] batch time: 0.046 trainign loss: 5.8226 avg training loss: 5.9053
batch: [5580/10547] batch time: 0.789 trainign loss: 3.7505 avg training loss: 5.9051
batch: [5590/10547] batch time: 0.046 trainign loss: 7.7511 avg training loss: 5.9045
batch: [5600/10547] batch time: 0.903 trainign loss: 6.0870 avg training loss: 5.9046
batch: [5610/10547] batch time: 0.054 trainign loss: 6.1317 avg training loss: 5.9046
batch: [5620/10547] batch time: 1.397 trainign loss: 4.3304 avg training loss: 5.9045
batch: [5630/10547] batch time: 0.054 trainign loss: 3.9837 avg training loss: 5.9043
batch: [5640/10547] batch time: 1.581 trainign loss: 5.5748 avg training loss: 5.9041
batch: [5650/10547] batch time: 0.053 trainign loss: 5.8213 avg training loss: 5.9040
batch: [5660/10547] batch time: 2.224 trainign loss: 4.0071 avg training loss: 5.9039
batch: [5670/10547] batch time: 0.046 trainign loss: 2.7440 avg training loss: 5.9037
batch: [5680/10547] batch time: 1.992 trainign loss: 4.1149 avg training loss: 5.9035
batch: [5690/10547] batch time: 0.046 trainign loss: 5.8145 avg training loss: 5.9034
batch: [5700/10547] batch time: 1.696 trainign loss: 5.0860 avg training loss: 5.9032
batch: [5710/10547] batch time: 0.124 trainign loss: 4.4030 avg training loss: 5.9030
batch: [5720/10547] batch time: 1.713 trainign loss: 5.6203 avg training loss: 5.9030
batch: [5730/10547] batch time: 1.354 trainign loss: 5.1418 avg training loss: 5.9029
batch: [5740/10547] batch time: 0.047 trainign loss: 4.5961 avg training loss: 5.9028
batch: [5750/10547] batch time: 2.428 trainign loss: 3.1926 avg training loss: 5.9025
batch: [5760/10547] batch time: 0.054 trainign loss: 4.2496 avg training loss: 5.9022
batch: [5770/10547] batch time: 1.123 trainign loss: 5.6431 avg training loss: 5.9022
batch: [5780/10547] batch time: 0.047 trainign loss: 3.3101 avg training loss: 5.9020
batch: [5790/10547] batch time: 0.048 trainign loss: 0.0177 avg training loss: 5.9012
batch: [5800/10547] batch time: 0.050 trainign loss: 8.1724 avg training loss: 5.9008
batch: [5810/10547] batch time: 0.047 trainign loss: 6.1902 avg training loss: 5.9009
batch: [5820/10547] batch time: 0.044 trainign loss: 5.5742 avg training loss: 5.9007
batch: [5830/10547] batch time: 0.046 trainign loss: 4.7706 avg training loss: 5.9005
batch: [5840/10547] batch time: 0.047 trainign loss: 2.6989 avg training loss: 5.9002
batch: [5850/10547] batch time: 0.047 trainign loss: 6.1162 avg training loss: 5.8999
batch: [5860/10547] batch time: 0.046 trainign loss: 4.9903 avg training loss: 5.8999
batch: [5870/10547] batch time: 0.048 trainign loss: 3.5800 avg training loss: 5.8997
batch: [5880/10547] batch time: 0.047 trainign loss: 2.5882 avg training loss: 5.8995
batch: [5890/10547] batch time: 0.046 trainign loss: 5.4469 avg training loss: 5.8994
batch: [5900/10547] batch time: 0.325 trainign loss: 4.9610 avg training loss: 5.8994
batch: [5910/10547] batch time: 0.331 trainign loss: 6.1397 avg training loss: 5.8993
batch: [5920/10547] batch time: 0.054 trainign loss: 5.0247 avg training loss: 5.8993
batch: [5930/10547] batch time: 2.404 trainign loss: 2.1038 avg training loss: 5.8990
batch: [5940/10547] batch time: 0.046 trainign loss: 4.4814 avg training loss: 5.8985
batch: [5950/10547] batch time: 2.461 trainign loss: 4.1578 avg training loss: 5.8984
batch: [5960/10547] batch time: 0.045 trainign loss: 5.0797 avg training loss: 5.8985
batch: [5970/10547] batch time: 2.253 trainign loss: 6.1724 avg training loss: 5.8982
batch: [5980/10547] batch time: 0.047 trainign loss: 4.8722 avg training loss: 5.8979
batch: [5990/10547] batch time: 2.445 trainign loss: 6.7652 avg training loss: 5.8979
batch: [6000/10547] batch time: 0.054 trainign loss: 5.8689 avg training loss: 5.8979
batch: [6010/10547] batch time: 2.240 trainign loss: 5.3914 avg training loss: 5.8978
batch: [6020/10547] batch time: 0.054 trainign loss: 5.6238 avg training loss: 5.8977
batch: [6030/10547] batch time: 2.202 trainign loss: 3.8739 avg training loss: 5.8975
batch: [6040/10547] batch time: 0.047 trainign loss: 6.6774 avg training loss: 5.8974
batch: [6050/10547] batch time: 2.536 trainign loss: 5.8424 avg training loss: 5.8974
batch: [6060/10547] batch time: 0.047 trainign loss: 5.1053 avg training loss: 5.8973
batch: [6070/10547] batch time: 2.296 trainign loss: 5.1926 avg training loss: 5.8972
batch: [6080/10547] batch time: 0.046 trainign loss: 5.0363 avg training loss: 5.8971
batch: [6090/10547] batch time: 2.537 trainign loss: 5.5331 avg training loss: 5.8970
batch: [6100/10547] batch time: 0.054 trainign loss: 4.2549 avg training loss: 5.8968
batch: [6110/10547] batch time: 2.009 trainign loss: 5.4917 avg training loss: 5.8967
batch: [6120/10547] batch time: 0.049 trainign loss: 5.2026 avg training loss: 5.8966
batch: [6130/10547] batch time: 1.718 trainign loss: 3.2998 avg training loss: 5.8964
batch: [6140/10547] batch time: 0.049 trainign loss: 6.0639 avg training loss: 5.8963
batch: [6150/10547] batch time: 1.882 trainign loss: 5.2346 avg training loss: 5.8963
batch: [6160/10547] batch time: 0.047 trainign loss: 5.5452 avg training loss: 5.8961
batch: [6170/10547] batch time: 1.209 trainign loss: 0.2697 avg training loss: 5.8957
batch: [6180/10547] batch time: 0.048 trainign loss: 6.7701 avg training loss: 5.8954
batch: [6190/10547] batch time: 0.926 trainign loss: 4.7948 avg training loss: 5.8953
batch: [6200/10547] batch time: 0.046 trainign loss: 5.6609 avg training loss: 5.8951
batch: [6210/10547] batch time: 0.047 trainign loss: 5.6758 avg training loss: 5.8951
batch: [6220/10547] batch time: 0.046 trainign loss: 4.7337 avg training loss: 5.8951
batch: [6230/10547] batch time: 0.111 trainign loss: 3.2559 avg training loss: 5.8949
batch: [6240/10547] batch time: 0.046 trainign loss: 4.5219 avg training loss: 5.8946
batch: [6250/10547] batch time: 0.690 trainign loss: 4.7792 avg training loss: 5.8945
batch: [6260/10547] batch time: 0.831 trainign loss: 4.6866 avg training loss: 5.8943
batch: [6270/10547] batch time: 0.047 trainign loss: 2.2873 avg training loss: 5.8940
batch: [6280/10547] batch time: 1.431 trainign loss: 5.7131 avg training loss: 5.8939
batch: [6290/10547] batch time: 0.048 trainign loss: 5.0364 avg training loss: 5.8938
batch: [6300/10547] batch time: 0.446 trainign loss: 5.6101 avg training loss: 5.8936
batch: [6310/10547] batch time: 0.787 trainign loss: 2.7524 avg training loss: 5.8934
batch: [6320/10547] batch time: 0.046 trainign loss: 5.7162 avg training loss: 5.8933
batch: [6330/10547] batch time: 0.771 trainign loss: 5.0477 avg training loss: 5.8933
batch: [6340/10547] batch time: 0.047 trainign loss: 5.3998 avg training loss: 5.8931
batch: [6350/10547] batch time: 1.005 trainign loss: 5.6810 avg training loss: 5.8931
batch: [6360/10547] batch time: 0.052 trainign loss: 4.5327 avg training loss: 5.8929
batch: [6370/10547] batch time: 1.523 trainign loss: 5.5982 avg training loss: 5.8927
batch: [6380/10547] batch time: 0.054 trainign loss: 2.7552 avg training loss: 5.8925
batch: [6390/10547] batch time: 1.474 trainign loss: 4.7369 avg training loss: 5.8924
batch: [6400/10547] batch time: 0.049 trainign loss: 4.2828 avg training loss: 5.8922
batch: [6410/10547] batch time: 1.660 trainign loss: 4.9194 avg training loss: 5.8920
batch: [6420/10547] batch time: 0.047 trainign loss: 4.6944 avg training loss: 5.8917
batch: [6430/10547] batch time: 0.672 trainign loss: 5.5138 avg training loss: 5.8917
batch: [6440/10547] batch time: 0.049 trainign loss: 2.5609 avg training loss: 5.8913
batch: [6450/10547] batch time: 0.349 trainign loss: 6.5223 avg training loss: 5.8911
batch: [6460/10547] batch time: 0.047 trainign loss: 4.2804 avg training loss: 5.8911
batch: [6470/10547] batch time: 0.978 trainign loss: 5.7106 avg training loss: 5.8909
batch: [6480/10547] batch time: 0.049 trainign loss: 5.1953 avg training loss: 5.8908
batch: [6490/10547] batch time: 1.012 trainign loss: 3.4763 avg training loss: 5.8905
batch: [6500/10547] batch time: 0.048 trainign loss: 0.0348 avg training loss: 5.8899
batch: [6510/10547] batch time: 0.047 trainign loss: 0.0009 avg training loss: 5.8890
batch: [6520/10547] batch time: 0.049 trainign loss: 0.0003 avg training loss: 5.8882
batch: [6530/10547] batch time: 0.662 trainign loss: 6.5825 avg training loss: 5.8879
batch: [6540/10547] batch time: 0.046 trainign loss: 6.8010 avg training loss: 5.8880
batch: [6550/10547] batch time: 1.141 trainign loss: 4.9075 avg training loss: 5.8880
batch: [6560/10547] batch time: 0.046 trainign loss: 4.4874 avg training loss: 5.8879
batch: [6570/10547] batch time: 2.097 trainign loss: 1.7195 avg training loss: 5.8876
batch: [6580/10547] batch time: 0.048 trainign loss: 5.7746 avg training loss: 5.8875
batch: [6590/10547] batch time: 1.669 trainign loss: 5.8686 avg training loss: 5.8874
batch: [6600/10547] batch time: 0.046 trainign loss: 3.0941 avg training loss: 5.8873
batch: [6610/10547] batch time: 1.301 trainign loss: 2.7101 avg training loss: 5.8871
batch: [6620/10547] batch time: 0.046 trainign loss: 6.6136 avg training loss: 5.8871
batch: [6630/10547] batch time: 0.047 trainign loss: 4.9493 avg training loss: 5.8870
batch: [6640/10547] batch time: 0.048 trainign loss: 4.8198 avg training loss: 5.8868
batch: [6650/10547] batch time: 0.047 trainign loss: 2.5138 avg training loss: 5.8865
batch: [6660/10547] batch time: 0.046 trainign loss: 5.6283 avg training loss: 5.8864
batch: [6670/10547] batch time: 0.046 trainign loss: 6.3306 avg training loss: 5.8863
batch: [6680/10547] batch time: 0.046 trainign loss: 5.2558 avg training loss: 5.8862
batch: [6690/10547] batch time: 0.046 trainign loss: 5.1668 avg training loss: 5.8861
batch: [6700/10547] batch time: 0.048 trainign loss: 5.2350 avg training loss: 5.8860
batch: [6710/10547] batch time: 0.046 trainign loss: 5.2963 avg training loss: 5.8859
batch: [6720/10547] batch time: 0.047 trainign loss: 5.3230 avg training loss: 5.8857
batch: [6730/10547] batch time: 0.047 trainign loss: 5.7762 avg training loss: 5.8857
batch: [6740/10547] batch time: 0.046 trainign loss: 4.2501 avg training loss: 5.8856
batch: [6750/10547] batch time: 0.046 trainign loss: 4.7643 avg training loss: 5.8853
batch: [6760/10547] batch time: 0.047 trainign loss: 4.7824 avg training loss: 5.8852
batch: [6770/10547] batch time: 0.048 trainign loss: 5.2837 avg training loss: 5.8851
batch: [6780/10547] batch time: 0.046 trainign loss: 5.7574 avg training loss: 5.8850
batch: [6790/10547] batch time: 0.051 trainign loss: 5.7337 avg training loss: 5.8850
batch: [6800/10547] batch time: 0.047 trainign loss: 5.1442 avg training loss: 5.8849
batch: [6810/10547] batch time: 0.047 trainign loss: 4.9595 avg training loss: 5.8847
batch: [6820/10547] batch time: 0.231 trainign loss: 2.5046 avg training loss: 5.8844
batch: [6830/10547] batch time: 0.046 trainign loss: 4.3016 avg training loss: 5.8843
batch: [6840/10547] batch time: 0.099 trainign loss: 3.8453 avg training loss: 5.8842
batch: [6850/10547] batch time: 0.047 trainign loss: 2.0292 avg training loss: 5.8840
batch: [6860/10547] batch time: 1.222 trainign loss: 4.3933 avg training loss: 5.8838
batch: [6870/10547] batch time: 0.047 trainign loss: 5.0095 avg training loss: 5.8835
batch: [6880/10547] batch time: 1.646 trainign loss: 5.2569 avg training loss: 5.8834
batch: [6890/10547] batch time: 0.046 trainign loss: 5.6034 avg training loss: 5.8834
batch: [6900/10547] batch time: 1.472 trainign loss: 4.5137 avg training loss: 5.8832
batch: [6910/10547] batch time: 0.046 trainign loss: 4.7231 avg training loss: 5.8830
batch: [6920/10547] batch time: 0.669 trainign loss: 3.9582 avg training loss: 5.8828
batch: [6930/10547] batch time: 0.046 trainign loss: 6.3520 avg training loss: 5.8826
batch: [6940/10547] batch time: 1.021 trainign loss: 5.0769 avg training loss: 5.8826
batch: [6950/10547] batch time: 0.047 trainign loss: 4.0692 avg training loss: 5.8824
batch: [6960/10547] batch time: 1.177 trainign loss: 5.2187 avg training loss: 5.8821
batch: [6970/10547] batch time: 0.046 trainign loss: 5.5518 avg training loss: 5.8820
batch: [6980/10547] batch time: 1.344 trainign loss: 5.2331 avg training loss: 5.8819
batch: [6990/10547] batch time: 0.791 trainign loss: 4.4227 avg training loss: 5.8818
batch: [7000/10547] batch time: 0.998 trainign loss: 6.0352 avg training loss: 5.8816
batch: [7010/10547] batch time: 0.287 trainign loss: 2.3847 avg training loss: 5.8814
batch: [7020/10547] batch time: 1.466 trainign loss: 5.2807 avg training loss: 5.8811
batch: [7030/10547] batch time: 0.048 trainign loss: 5.8200 avg training loss: 5.8812
batch: [7040/10547] batch time: 0.651 trainign loss: 5.3097 avg training loss: 5.8810
batch: [7050/10547] batch time: 0.047 trainign loss: 5.4686 avg training loss: 5.8809
batch: [7060/10547] batch time: 0.196 trainign loss: 5.4416 avg training loss: 5.8808
batch: [7070/10547] batch time: 0.046 trainign loss: 7.1163 avg training loss: 5.8805
batch: [7080/10547] batch time: 1.138 trainign loss: 5.3019 avg training loss: 5.8804
batch: [7090/10547] batch time: 0.048 trainign loss: 6.2930 avg training loss: 5.8803
batch: [7100/10547] batch time: 1.228 trainign loss: 5.6588 avg training loss: 5.8802
batch: [7110/10547] batch time: 0.047 trainign loss: 5.0339 avg training loss: 5.8800
batch: [7120/10547] batch time: 1.084 trainign loss: 6.1357 avg training loss: 5.8800
batch: [7130/10547] batch time: 0.053 trainign loss: 4.4770 avg training loss: 5.8799
batch: [7140/10547] batch time: 0.622 trainign loss: 4.2369 avg training loss: 5.8796
batch: [7150/10547] batch time: 0.054 trainign loss: 0.2127 avg training loss: 5.8792
batch: [7160/10547] batch time: 0.423 trainign loss: 0.0012 avg training loss: 5.8784
batch: [7170/10547] batch time: 0.056 trainign loss: 0.0003 avg training loss: 5.8776
batch: [7180/10547] batch time: 0.495 trainign loss: 0.0002 avg training loss: 5.8767
batch: [7190/10547] batch time: 0.046 trainign loss: 0.0002 avg training loss: 5.8759
batch: [7200/10547] batch time: 0.194 trainign loss: 0.0002 avg training loss: 5.8751
batch: [7210/10547] batch time: 0.050 trainign loss: 0.0004 avg training loss: 5.8742
batch: [7220/10547] batch time: 0.248 trainign loss: 0.0002 avg training loss: 5.8734
batch: [7230/10547] batch time: 0.050 trainign loss: 0.0002 avg training loss: 5.8726
batch: [7240/10547] batch time: 0.602 trainign loss: 0.0002 avg training loss: 5.8717
batch: [7250/10547] batch time: 0.046 trainign loss: 6.5906 avg training loss: 5.8719
batch: [7260/10547] batch time: 0.053 trainign loss: 5.7201 avg training loss: 5.8719
batch: [7270/10547] batch time: 0.055 trainign loss: 4.5960 avg training loss: 5.8719
batch: [7280/10547] batch time: 0.048 trainign loss: 4.1752 avg training loss: 5.8718
batch: [7290/10547] batch time: 0.049 trainign loss: 5.9571 avg training loss: 5.8715
batch: [7300/10547] batch time: 0.048 trainign loss: 0.1415 avg training loss: 5.8710
batch: [7310/10547] batch time: 0.048 trainign loss: 7.5064 avg training loss: 5.8706
batch: [7320/10547] batch time: 0.046 trainign loss: 5.9628 avg training loss: 5.8706
batch: [7330/10547] batch time: 0.047 trainign loss: 5.5109 avg training loss: 5.8705
batch: [7340/10547] batch time: 0.054 trainign loss: 6.1701 avg training loss: 5.8705
batch: [7350/10547] batch time: 0.051 trainign loss: 5.5132 avg training loss: 5.8704
batch: [7360/10547] batch time: 0.046 trainign loss: 5.6441 avg training loss: 5.8703
batch: [7370/10547] batch time: 0.046 trainign loss: 5.0601 avg training loss: 5.8703
batch: [7380/10547] batch time: 0.050 trainign loss: 5.7876 avg training loss: 5.8702
batch: [7390/10547] batch time: 0.047 trainign loss: 5.3032 avg training loss: 5.8701
batch: [7400/10547] batch time: 0.048 trainign loss: 4.9997 avg training loss: 5.8699
batch: [7410/10547] batch time: 0.048 trainign loss: 5.5546 avg training loss: 5.8697
batch: [7420/10547] batch time: 0.047 trainign loss: 5.0937 avg training loss: 5.8696
batch: [7430/10547] batch time: 0.047 trainign loss: 4.4826 avg training loss: 5.8694
batch: [7440/10547] batch time: 0.046 trainign loss: 3.9755 avg training loss: 5.8692
batch: [7450/10547] batch time: 0.046 trainign loss: 5.7455 avg training loss: 5.8690
batch: [7460/10547] batch time: 0.047 trainign loss: 5.5825 avg training loss: 5.8690
batch: [7470/10547] batch time: 0.047 trainign loss: 5.4978 avg training loss: 5.8688
batch: [7480/10547] batch time: 0.309 trainign loss: 5.8738 avg training loss: 5.8687
batch: [7490/10547] batch time: 0.047 trainign loss: 0.5527 avg training loss: 5.8684
batch: [7500/10547] batch time: 0.054 trainign loss: 6.4553 avg training loss: 5.8683
batch: [7510/10547] batch time: 0.046 trainign loss: 6.2791 avg training loss: 5.8681
batch: [7520/10547] batch time: 0.046 trainign loss: 4.6244 avg training loss: 5.8680
batch: [7530/10547] batch time: 0.046 trainign loss: 6.0980 avg training loss: 5.8679
batch: [7540/10547] batch time: 0.047 trainign loss: 5.7312 avg training loss: 5.8678
batch: [7550/10547] batch time: 0.046 trainign loss: 1.8076 avg training loss: 5.8676
batch: [7560/10547] batch time: 0.046 trainign loss: 5.9259 avg training loss: 5.8675
batch: [7570/10547] batch time: 0.046 trainign loss: 5.4838 avg training loss: 5.8675
batch: [7580/10547] batch time: 0.047 trainign loss: 5.5980 avg training loss: 5.8673
batch: [7590/10547] batch time: 0.052 trainign loss: 5.6775 avg training loss: 5.8671
batch: [7600/10547] batch time: 0.053 trainign loss: 3.5816 avg training loss: 5.8670
batch: [7610/10547] batch time: 0.046 trainign loss: 5.9251 avg training loss: 5.8669
batch: [7620/10547] batch time: 0.054 trainign loss: 5.9812 avg training loss: 5.8668
batch: [7630/10547] batch time: 0.054 trainign loss: 3.7265 avg training loss: 5.8667
batch: [7640/10547] batch time: 0.054 trainign loss: 5.8551 avg training loss: 5.8665
batch: [7650/10547] batch time: 0.047 trainign loss: 6.4743 avg training loss: 5.8665
batch: [7660/10547] batch time: 0.046 trainign loss: 5.3558 avg training loss: 5.8665
batch: [7670/10547] batch time: 0.056 trainign loss: 3.9455 avg training loss: 5.8664
batch: [7680/10547] batch time: 0.046 trainign loss: 4.8341 avg training loss: 5.8663
batch: [7690/10547] batch time: 0.046 trainign loss: 5.0920 avg training loss: 5.8661
batch: [7700/10547] batch time: 0.047 trainign loss: 4.8613 avg training loss: 5.8660
batch: [7710/10547] batch time: 0.047 trainign loss: 6.4527 avg training loss: 5.8656
batch: [7720/10547] batch time: 0.046 trainign loss: 6.2860 avg training loss: 5.8655
batch: [7730/10547] batch time: 0.045 trainign loss: 4.5285 avg training loss: 5.8654
batch: [7740/10547] batch time: 0.046 trainign loss: 5.2556 avg training loss: 5.8653
batch: [7750/10547] batch time: 0.054 trainign loss: 5.3532 avg training loss: 5.8652
batch: [7760/10547] batch time: 0.049 trainign loss: 5.7991 avg training loss: 5.8651
batch: [7770/10547] batch time: 0.046 trainign loss: 5.5627 avg training loss: 5.8651
batch: [7780/10547] batch time: 0.049 trainign loss: 5.8176 avg training loss: 5.8650
batch: [7790/10547] batch time: 0.636 trainign loss: 5.2075 avg training loss: 5.8649
batch: [7800/10547] batch time: 0.046 trainign loss: 4.7973 avg training loss: 5.8647
batch: [7810/10547] batch time: 0.059 trainign loss: 0.1714 avg training loss: 5.8642
batch: [7820/10547] batch time: 0.047 trainign loss: 8.8593 avg training loss: 5.8637
batch: [7830/10547] batch time: 1.168 trainign loss: 6.8217 avg training loss: 5.8638
batch: [7840/10547] batch time: 0.047 trainign loss: 6.0042 avg training loss: 5.8638
batch: [7850/10547] batch time: 0.048 trainign loss: 2.7115 avg training loss: 5.8637
batch: [7860/10547] batch time: 0.042 trainign loss: 5.5212 avg training loss: 5.8635
batch: [7870/10547] batch time: 0.048 trainign loss: 5.5781 avg training loss: 5.8635
batch: [7880/10547] batch time: 0.047 trainign loss: 4.9251 avg training loss: 5.8633
batch: [7890/10547] batch time: 0.047 trainign loss: 5.6064 avg training loss: 5.8633
batch: [7900/10547] batch time: 0.329 trainign loss: 2.7909 avg training loss: 5.8631
batch: [7910/10547] batch time: 0.048 trainign loss: 5.2745 avg training loss: 5.8629
batch: [7920/10547] batch time: 0.220 trainign loss: 5.6136 avg training loss: 5.8628
batch: [7930/10547] batch time: 0.047 trainign loss: 5.1524 avg training loss: 5.8627
batch: [7940/10547] batch time: 0.047 trainign loss: 2.8905 avg training loss: 5.8625
batch: [7950/10547] batch time: 0.047 trainign loss: 6.9843 avg training loss: 5.8623
batch: [7960/10547] batch time: 0.044 trainign loss: 5.0001 avg training loss: 5.8622
batch: [7970/10547] batch time: 0.046 trainign loss: 6.0940 avg training loss: 5.8622
batch: [7980/10547] batch time: 0.047 trainign loss: 5.7579 avg training loss: 5.8621
batch: [7990/10547] batch time: 0.054 trainign loss: 5.5349 avg training loss: 5.8620
batch: [8000/10547] batch time: 0.042 trainign loss: 4.5742 avg training loss: 5.8619
batch: [8010/10547] batch time: 0.046 trainign loss: 5.1242 avg training loss: 5.8617
batch: [8020/10547] batch time: 0.046 trainign loss: 5.5680 avg training loss: 5.8616
batch: [8030/10547] batch time: 0.046 trainign loss: 6.1998 avg training loss: 5.8616
batch: [8040/10547] batch time: 0.047 trainign loss: 4.7899 avg training loss: 5.8615
batch: [8050/10547] batch time: 0.046 trainign loss: 4.6661 avg training loss: 5.8614
batch: [8060/10547] batch time: 0.050 trainign loss: 5.6578 avg training loss: 5.8613
batch: [8070/10547] batch time: 0.046 trainign loss: 4.6570 avg training loss: 5.8612
batch: [8080/10547] batch time: 0.045 trainign loss: 5.5179 avg training loss: 5.8611
batch: [8090/10547] batch time: 0.048 trainign loss: 4.6360 avg training loss: 5.8609
batch: [8100/10547] batch time: 0.048 trainign loss: 5.0715 avg training loss: 5.8606
batch: [8110/10547] batch time: 0.046 trainign loss: 4.9086 avg training loss: 5.8605
batch: [8120/10547] batch time: 0.603 trainign loss: 5.9935 avg training loss: 5.8605
batch: [8130/10547] batch time: 0.047 trainign loss: 5.0981 avg training loss: 5.8605
batch: [8140/10547] batch time: 0.047 trainign loss: 4.4863 avg training loss: 5.8604
batch: [8150/10547] batch time: 0.046 trainign loss: 5.2038 avg training loss: 5.8602
batch: [8160/10547] batch time: 0.046 trainign loss: 5.8742 avg training loss: 5.8602
batch: [8170/10547] batch time: 0.047 trainign loss: 3.4199 avg training loss: 5.8600
batch: [8180/10547] batch time: 0.055 trainign loss: 5.5763 avg training loss: 5.8599
batch: [8190/10547] batch time: 0.046 trainign loss: 5.2743 avg training loss: 5.8598
batch: [8200/10547] batch time: 0.046 trainign loss: 4.0710 avg training loss: 5.8597
batch: [8210/10547] batch time: 0.046 trainign loss: 1.4224 avg training loss: 5.8592
batch: [8220/10547] batch time: 0.046 trainign loss: 5.9956 avg training loss: 5.8590
batch: [8230/10547] batch time: 0.048 trainign loss: 5.6986 avg training loss: 5.8591
batch: [8240/10547] batch time: 0.048 trainign loss: 5.2679 avg training loss: 5.8589
batch: [8250/10547] batch time: 0.046 trainign loss: 5.3848 avg training loss: 5.8588
batch: [8260/10547] batch time: 0.046 trainign loss: 4.4394 avg training loss: 5.8587
batch: [8270/10547] batch time: 0.046 trainign loss: 5.2969 avg training loss: 5.8586
batch: [8280/10547] batch time: 0.048 trainign loss: 5.6009 avg training loss: 5.8586
batch: [8290/10547] batch time: 0.054 trainign loss: 3.9484 avg training loss: 5.8583
batch: [8300/10547] batch time: 0.148 trainign loss: 5.0801 avg training loss: 5.8582
batch: [8310/10547] batch time: 0.049 trainign loss: 5.2650 avg training loss: 5.8580
batch: [8320/10547] batch time: 0.833 trainign loss: 6.1899 avg training loss: 5.8580
batch: [8330/10547] batch time: 0.046 trainign loss: 5.4071 avg training loss: 5.8579
batch: [8340/10547] batch time: 0.048 trainign loss: 3.3253 avg training loss: 5.8577
batch: [8350/10547] batch time: 0.046 trainign loss: 0.0106 avg training loss: 5.8570
batch: [8360/10547] batch time: 0.046 trainign loss: 6.4389 avg training loss: 5.8571
batch: [8370/10547] batch time: 0.048 trainign loss: 6.1090 avg training loss: 5.8570
batch: [8380/10547] batch time: 0.046 trainign loss: 4.9684 avg training loss: 5.8569
batch: [8390/10547] batch time: 0.046 trainign loss: 3.0557 avg training loss: 5.8567
batch: [8400/10547] batch time: 0.047 trainign loss: 5.0700 avg training loss: 5.8565
batch: [8410/10547] batch time: 0.046 trainign loss: 5.2066 avg training loss: 5.8563
batch: [8420/10547] batch time: 0.046 trainign loss: 4.2337 avg training loss: 5.8558
batch: [8430/10547] batch time: 0.047 trainign loss: 3.8893 avg training loss: 5.8558
batch: [8440/10547] batch time: 0.047 trainign loss: 4.9674 avg training loss: 5.8557
batch: [8450/10547] batch time: 0.047 trainign loss: 6.2779 avg training loss: 5.8556
batch: [8460/10547] batch time: 0.048 trainign loss: 5.3968 avg training loss: 5.8555
batch: [8470/10547] batch time: 0.055 trainign loss: 6.2442 avg training loss: 5.8555
batch: [8480/10547] batch time: 0.046 trainign loss: 5.0689 avg training loss: 5.8555
batch: [8490/10547] batch time: 0.045 trainign loss: 5.0525 avg training loss: 5.8552
batch: [8500/10547] batch time: 0.048 trainign loss: 2.7825 avg training loss: 5.8550
batch: [8510/10547] batch time: 0.055 trainign loss: 4.9431 avg training loss: 5.8549
batch: [8520/10547] batch time: 0.054 trainign loss: 3.2659 avg training loss: 5.8547
batch: [8530/10547] batch time: 0.046 trainign loss: 2.1833 avg training loss: 5.8544
batch: [8540/10547] batch time: 0.047 trainign loss: 6.6077 avg training loss: 5.8545
batch: [8550/10547] batch time: 0.046 trainign loss: 5.3848 avg training loss: 5.8544
batch: [8560/10547] batch time: 0.046 trainign loss: 0.3885 avg training loss: 5.8540
batch: [8570/10547] batch time: 0.042 trainign loss: 6.7708 avg training loss: 5.8537
batch: [8580/10547] batch time: 0.046 trainign loss: 5.5606 avg training loss: 5.8536
batch: [8590/10547] batch time: 0.046 trainign loss: 6.6482 avg training loss: 5.8536
batch: [8600/10547] batch time: 0.047 trainign loss: 6.1429 avg training loss: 5.8536
batch: [8610/10547] batch time: 0.047 trainign loss: 5.8099 avg training loss: 5.8536
batch: [8620/10547] batch time: 0.054 trainign loss: 4.7270 avg training loss: 5.8535
batch: [8630/10547] batch time: 0.510 trainign loss: 2.9025 avg training loss: 5.8533
batch: [8640/10547] batch time: 0.047 trainign loss: 6.7863 avg training loss: 5.8532
batch: [8650/10547] batch time: 0.100 trainign loss: 5.4656 avg training loss: 5.8531
batch: [8660/10547] batch time: 0.046 trainign loss: 4.4235 avg training loss: 5.8530
batch: [8670/10547] batch time: 1.222 trainign loss: 5.0096 avg training loss: 5.8529
batch: [8680/10547] batch time: 0.054 trainign loss: 3.4731 avg training loss: 5.8527
batch: [8690/10547] batch time: 2.128 trainign loss: 0.0121 avg training loss: 5.8520
batch: [8700/10547] batch time: 0.055 trainign loss: 7.1239 avg training loss: 5.8518
batch: [8710/10547] batch time: 2.303 trainign loss: 5.6051 avg training loss: 5.8519
batch: [8720/10547] batch time: 0.047 trainign loss: 6.5186 avg training loss: 5.8520
batch: [8730/10547] batch time: 2.139 trainign loss: 5.7764 avg training loss: 5.8519
batch: [8740/10547] batch time: 0.046 trainign loss: 4.1482 avg training loss: 5.8518
batch: [8750/10547] batch time: 2.052 trainign loss: 5.1994 avg training loss: 5.8517
batch: [8760/10547] batch time: 0.046 trainign loss: 7.0310 avg training loss: 5.8516
batch: [8770/10547] batch time: 1.848 trainign loss: 6.1921 avg training loss: 5.8516
batch: [8780/10547] batch time: 0.047 trainign loss: 5.3870 avg training loss: 5.8515
batch: [8790/10547] batch time: 2.201 trainign loss: 5.9302 avg training loss: 5.8515
batch: [8800/10547] batch time: 0.047 trainign loss: 5.3656 avg training loss: 5.8515
batch: [8810/10547] batch time: 1.843 trainign loss: 5.4834 avg training loss: 5.8514
batch: [8820/10547] batch time: 0.046 trainign loss: 5.2380 avg training loss: 5.8514
batch: [8830/10547] batch time: 1.858 trainign loss: 5.4177 avg training loss: 5.8513
batch: [8840/10547] batch time: 0.046 trainign loss: 4.5953 avg training loss: 5.8511
batch: [8850/10547] batch time: 1.925 trainign loss: 5.1241 avg training loss: 5.8511
batch: [8860/10547] batch time: 0.046 trainign loss: 4.1905 avg training loss: 5.8510
batch: [8870/10547] batch time: 1.292 trainign loss: 5.3853 avg training loss: 5.8508
batch: [8880/10547] batch time: 0.046 trainign loss: 4.6913 avg training loss: 5.8508
batch: [8890/10547] batch time: 0.980 trainign loss: 4.5917 avg training loss: 5.8506
batch: [8900/10547] batch time: 0.046 trainign loss: 2.3094 avg training loss: 5.8504
batch: [8910/10547] batch time: 2.217 trainign loss: 4.9527 avg training loss: 5.8501
batch: [8920/10547] batch time: 0.046 trainign loss: 4.9393 avg training loss: 5.8500
batch: [8930/10547] batch time: 2.176 trainign loss: 6.4723 avg training loss: 5.8500
batch: [8940/10547] batch time: 0.046 trainign loss: 5.4093 avg training loss: 5.8500
batch: [8950/10547] batch time: 2.273 trainign loss: 4.8646 avg training loss: 5.8499
batch: [8960/10547] batch time: 0.054 trainign loss: 4.5756 avg training loss: 5.8498
batch: [8970/10547] batch time: 2.500 trainign loss: 3.4792 avg training loss: 5.8497
batch: [8980/10547] batch time: 0.056 trainign loss: 5.0470 avg training loss: 5.8495
batch: [8990/10547] batch time: 2.130 trainign loss: 5.2226 avg training loss: 5.8494
batch: [9000/10547] batch time: 0.055 trainign loss: 0.4997 avg training loss: 5.8491
batch: [9010/10547] batch time: 0.864 trainign loss: 6.1143 avg training loss: 5.8489
batch: [9020/10547] batch time: 0.048 trainign loss: 3.9746 avg training loss: 5.8488
batch: [9030/10547] batch time: 1.357 trainign loss: 4.3419 avg training loss: 5.8487
batch: [9040/10547] batch time: 0.046 trainign loss: 4.4547 avg training loss: 5.8487
batch: [9050/10547] batch time: 1.276 trainign loss: 4.7670 avg training loss: 5.8484
batch: [9060/10547] batch time: 0.046 trainign loss: 5.0216 avg training loss: 5.8483
batch: [9070/10547] batch time: 1.447 trainign loss: 3.2241 avg training loss: 5.8481
batch: [9080/10547] batch time: 0.047 trainign loss: 5.3528 avg training loss: 5.8480
batch: [9090/10547] batch time: 0.433 trainign loss: 3.9813 avg training loss: 5.8480
batch: [9100/10547] batch time: 0.047 trainign loss: 3.5958 avg training loss: 5.8478
batch: [9110/10547] batch time: 0.716 trainign loss: 5.6459 avg training loss: 5.8478
batch: [9120/10547] batch time: 0.046 trainign loss: 5.5972 avg training loss: 5.8477
batch: [9130/10547] batch time: 0.055 trainign loss: 0.0654 avg training loss: 5.8471
batch: [9140/10547] batch time: 0.047 trainign loss: 6.4438 avg training loss: 5.8470
batch: [9150/10547] batch time: 0.043 trainign loss: 6.0778 avg training loss: 5.8470
batch: [9160/10547] batch time: 0.051 trainign loss: 1.7424 avg training loss: 5.8468
batch: [9170/10547] batch time: 0.046 trainign loss: 0.0039 avg training loss: 5.8460
batch: [9180/10547] batch time: 0.047 trainign loss: 7.0512 avg training loss: 5.8462
batch: [9190/10547] batch time: 0.042 trainign loss: 5.1450 avg training loss: 5.8462
batch: [9200/10547] batch time: 0.047 trainign loss: 5.4188 avg training loss: 5.8460
batch: [9210/10547] batch time: 0.047 trainign loss: 4.8596 avg training loss: 5.8459
batch: [9220/10547] batch time: 0.046 trainign loss: 3.5839 avg training loss: 5.8458
batch: [9230/10547] batch time: 0.047 trainign loss: 5.7270 avg training loss: 5.8457
batch: [9240/10547] batch time: 0.048 trainign loss: 5.0155 avg training loss: 5.8456
batch: [9250/10547] batch time: 0.047 trainign loss: 0.0483 avg training loss: 5.8451
batch: [9260/10547] batch time: 0.049 trainign loss: 9.6117 avg training loss: 5.8445
batch: [9270/10547] batch time: 0.054 trainign loss: 6.2737 avg training loss: 5.8447
batch: [9280/10547] batch time: 0.046 trainign loss: 6.4502 avg training loss: 5.8447
batch: [9290/10547] batch time: 0.047 trainign loss: 5.9670 avg training loss: 5.8447
batch: [9300/10547] batch time: 0.047 trainign loss: 5.1568 avg training loss: 5.8446
batch: [9310/10547] batch time: 0.046 trainign loss: 4.9395 avg training loss: 5.8444
batch: [9320/10547] batch time: 0.051 trainign loss: 5.8005 avg training loss: 5.8442
batch: [9330/10547] batch time: 0.050 trainign loss: 5.6991 avg training loss: 5.8442
batch: [9340/10547] batch time: 0.047 trainign loss: 2.9488 avg training loss: 5.8440
batch: [9350/10547] batch time: 0.047 trainign loss: 5.6504 avg training loss: 5.8439
batch: [9360/10547] batch time: 0.047 trainign loss: 6.8219 avg training loss: 5.8437
batch: [9370/10547] batch time: 0.047 trainign loss: 5.9796 avg training loss: 5.8437
batch: [9380/10547] batch time: 0.046 trainign loss: 4.5668 avg training loss: 5.8437
batch: [9390/10547] batch time: 0.043 trainign loss: 5.4442 avg training loss: 5.8435
batch: [9400/10547] batch time: 0.049 trainign loss: 2.0425 avg training loss: 5.8433
batch: [9410/10547] batch time: 0.046 trainign loss: 5.1902 avg training loss: 5.8432
batch: [9420/10547] batch time: 0.048 trainign loss: 5.9595 avg training loss: 5.8432
batch: [9430/10547] batch time: 0.047 trainign loss: 4.5998 avg training loss: 5.8431
batch: [9440/10547] batch time: 0.046 trainign loss: 4.5067 avg training loss: 5.8430
batch: [9450/10547] batch time: 0.054 trainign loss: 3.2317 avg training loss: 5.8427
batch: [9460/10547] batch time: 0.046 trainign loss: 6.0779 avg training loss: 5.8427
batch: [9470/10547] batch time: 0.042 trainign loss: 3.3821 avg training loss: 5.8426
batch: [9480/10547] batch time: 0.046 trainign loss: 4.2492 avg training loss: 5.8425
batch: [9490/10547] batch time: 0.047 trainign loss: 6.2179 avg training loss: 5.8425
batch: [9500/10547] batch time: 0.054 trainign loss: 4.7005 avg training loss: 5.8425
batch: [9510/10547] batch time: 0.047 trainign loss: 4.6856 avg training loss: 5.8423
batch: [9520/10547] batch time: 0.511 trainign loss: 4.7098 avg training loss: 5.8422
batch: [9530/10547] batch time: 0.055 trainign loss: 6.4633 avg training loss: 5.8420
batch: [9540/10547] batch time: 0.579 trainign loss: 2.9340 avg training loss: 5.8419
batch: [9550/10547] batch time: 0.048 trainign loss: 5.1665 avg training loss: 5.8417
batch: [9560/10547] batch time: 1.869 trainign loss: 6.9238 avg training loss: 5.8417
batch: [9570/10547] batch time: 0.054 trainign loss: 6.4045 avg training loss: 5.8418
batch: [9580/10547] batch time: 1.076 trainign loss: 5.3981 avg training loss: 5.8417
batch: [9590/10547] batch time: 0.045 trainign loss: 3.4538 avg training loss: 5.8416
batch: [9600/10547] batch time: 1.928 trainign loss: 4.4415 avg training loss: 5.8415
batch: [9610/10547] batch time: 0.054 trainign loss: 5.6238 avg training loss: 5.8415
batch: [9620/10547] batch time: 2.060 trainign loss: 4.5158 avg training loss: 5.8414
batch: [9630/10547] batch time: 0.046 trainign loss: 5.7803 avg training loss: 5.8414
batch: [9640/10547] batch time: 1.969 trainign loss: 5.8142 avg training loss: 5.8413
batch: [9650/10547] batch time: 0.054 trainign loss: 5.3793 avg training loss: 5.8412
batch: [9660/10547] batch time: 2.508 trainign loss: 5.1944 avg training loss: 5.8411
batch: [9670/10547] batch time: 0.055 trainign loss: 5.2807 avg training loss: 5.8410
batch: [9680/10547] batch time: 2.271 trainign loss: 5.5277 avg training loss: 5.8410
batch: [9690/10547] batch time: 0.054 trainign loss: 4.9989 avg training loss: 5.8408
batch: [9700/10547] batch time: 2.225 trainign loss: 4.9005 avg training loss: 5.8408
batch: [9710/10547] batch time: 0.046 trainign loss: 5.8767 avg training loss: 5.8408
batch: [9720/10547] batch time: 2.229 trainign loss: 5.2887 avg training loss: 5.8407
batch: [9730/10547] batch time: 0.045 trainign loss: 0.1799 avg training loss: 5.8402
batch: [9740/10547] batch time: 2.272 trainign loss: 5.7205 avg training loss: 5.8401
batch: [9750/10547] batch time: 0.046 trainign loss: 5.6391 avg training loss: 5.8401
batch: [9760/10547] batch time: 2.186 trainign loss: 5.9524 avg training loss: 5.8400
batch: [9770/10547] batch time: 1.319 trainign loss: 4.8430 avg training loss: 5.8399
batch: [9780/10547] batch time: 0.888 trainign loss: 6.2025 avg training loss: 5.8399
batch: [9790/10547] batch time: 1.222 trainign loss: 5.1309 avg training loss: 5.8398
batch: [9800/10547] batch time: 0.945 trainign loss: 5.2601 avg training loss: 5.8396
batch: [9810/10547] batch time: 0.772 trainign loss: 5.9584 avg training loss: 5.8396
batch: [9820/10547] batch time: 0.569 trainign loss: 6.1048 avg training loss: 5.8395
batch: [9830/10547] batch time: 1.976 trainign loss: 4.9168 avg training loss: 5.8394
batch: [9840/10547] batch time: 0.046 trainign loss: 0.2930 avg training loss: 5.8390
batch: [9850/10547] batch time: 2.470 trainign loss: 0.0268 avg training loss: 5.8385
batch: [9860/10547] batch time: 0.045 trainign loss: 6.2588 avg training loss: 5.8386
batch: [9870/10547] batch time: 2.684 trainign loss: 3.9883 avg training loss: 5.8385
batch: [9880/10547] batch time: 0.047 trainign loss: 5.0277 avg training loss: 5.8383
batch: [9890/10547] batch time: 2.325 trainign loss: 6.3853 avg training loss: 5.8383
batch: [9900/10547] batch time: 0.046 trainign loss: 5.1282 avg training loss: 5.8383
batch: [9910/10547] batch time: 1.916 trainign loss: 1.3684 avg training loss: 5.8380
batch: [9920/10547] batch time: 0.047 trainign loss: 4.2213 avg training loss: 5.8378
batch: [9930/10547] batch time: 2.430 trainign loss: 5.4137 avg training loss: 5.8376
batch: [9940/10547] batch time: 0.054 trainign loss: 6.4171 avg training loss: 5.8373
batch: [9950/10547] batch time: 2.483 trainign loss: 6.6258 avg training loss: 5.8372
batch: [9960/10547] batch time: 0.046 trainign loss: 4.1940 avg training loss: 5.8373
batch: [9970/10547] batch time: 2.375 trainign loss: 6.9330 avg training loss: 5.8369
batch: [9980/10547] batch time: 0.050 trainign loss: 6.6070 avg training loss: 5.8370
batch: [9990/10547] batch time: 2.572 trainign loss: 5.2789 avg training loss: 5.8370
batch: [10000/10547] batch time: 0.051 trainign loss: 5.8671 avg training loss: 5.8369
batch: [10010/10547] batch time: 2.003 trainign loss: 4.5090 avg training loss: 5.8368
batch: [10020/10547] batch time: 0.041 trainign loss: 5.9777 avg training loss: 5.8366
batch: [10030/10547] batch time: 2.496 trainign loss: 5.7994 avg training loss: 5.8366
batch: [10040/10547] batch time: 0.047 trainign loss: 3.7198 avg training loss: 5.8365
batch: [10050/10547] batch time: 1.744 trainign loss: 3.9095 avg training loss: 5.8364
batch: [10060/10547] batch time: 0.935 trainign loss: 7.1589 avg training loss: 5.8360
batch: [10070/10547] batch time: 1.241 trainign loss: 6.0280 avg training loss: 5.8360
batch: [10080/10547] batch time: 0.738 trainign loss: 4.3268 avg training loss: 5.8359
batch: [10090/10547] batch time: 1.188 trainign loss: 5.9199 avg training loss: 5.8357
batch: [10100/10547] batch time: 1.417 trainign loss: 3.7257 avg training loss: 5.8356
batch: [10110/10547] batch time: 1.693 trainign loss: 5.0356 avg training loss: 5.8355
batch: [10120/10547] batch time: 1.240 trainign loss: 4.4275 avg training loss: 5.8354
batch: [10130/10547] batch time: 0.861 trainign loss: 4.4049 avg training loss: 5.8351
batch: [10140/10547] batch time: 1.146 trainign loss: 3.3861 avg training loss: 5.8349
batch: [10150/10547] batch time: 1.362 trainign loss: 4.2407 avg training loss: 5.8349
batch: [10160/10547] batch time: 0.489 trainign loss: 0.0332 avg training loss: 5.8342
batch: [10170/10547] batch time: 1.552 trainign loss: 7.2943 avg training loss: 5.8340
batch: [10180/10547] batch time: 0.815 trainign loss: 5.9999 avg training loss: 5.8341
batch: [10190/10547] batch time: 1.842 trainign loss: 1.3368 avg training loss: 5.8338
batch: [10200/10547] batch time: 0.534 trainign loss: 5.6036 avg training loss: 5.8339
batch: [10210/10547] batch time: 1.749 trainign loss: 6.0146 avg training loss: 5.8338
batch: [10220/10547] batch time: 0.046 trainign loss: 4.7811 avg training loss: 5.8337
batch: [10230/10547] batch time: 2.552 trainign loss: 5.9877 avg training loss: 5.8336
batch: [10240/10547] batch time: 0.046 trainign loss: 5.1325 avg training loss: 5.8336
batch: [10250/10547] batch time: 2.437 trainign loss: 4.9746 avg training loss: 5.8335
batch: [10260/10547] batch time: 0.046 trainign loss: 5.0660 avg training loss: 5.8333
batch: [10270/10547] batch time: 2.329 trainign loss: 5.8713 avg training loss: 5.8333
batch: [10280/10547] batch time: 0.046 trainign loss: 6.3819 avg training loss: 5.8334
batch: [10290/10547] batch time: 1.923 trainign loss: 5.0743 avg training loss: 5.8333
batch: [10300/10547] batch time: 0.051 trainign loss: 5.4867 avg training loss: 5.8333
batch: [10310/10547] batch time: 2.188 trainign loss: 5.3341 avg training loss: 5.8332
batch: [10320/10547] batch time: 0.047 trainign loss: 6.0298 avg training loss: 5.8331
batch: [10330/10547] batch time: 2.286 trainign loss: 4.2347 avg training loss: 5.8330
batch: [10340/10547] batch time: 0.053 trainign loss: 5.5086 avg training loss: 5.8328
batch: [10350/10547] batch time: 2.239 trainign loss: 5.4105 avg training loss: 5.8328
batch: [10360/10547] batch time: 0.051 trainign loss: 4.6720 avg training loss: 5.8325
batch: [10370/10547] batch time: 2.714 trainign loss: 0.7423 avg training loss: 5.8322
batch: [10380/10547] batch time: 0.047 trainign loss: 3.6010 avg training loss: 5.8319
batch: [10390/10547] batch time: 2.648 trainign loss: 6.4562 avg training loss: 5.8319
batch: [10400/10547] batch time: 0.046 trainign loss: 5.8935 avg training loss: 5.8319
batch: [10410/10547] batch time: 2.165 trainign loss: 5.8824 avg training loss: 5.8319
batch: [10420/10547] batch time: 0.046 trainign loss: 5.4740 avg training loss: 5.8318
batch: [10430/10547] batch time: 1.969 trainign loss: 4.8745 avg training loss: 5.8318
batch: [10440/10547] batch time: 0.046 trainign loss: 4.5583 avg training loss: 5.8315
batch: [10450/10547] batch time: 2.498 trainign loss: 4.2863 avg training loss: 5.8313
batch: [10460/10547] batch time: 0.046 trainign loss: 5.7679 avg training loss: 5.8312
batch: [10470/10547] batch time: 2.570 trainign loss: 5.0657 avg training loss: 5.8312
batch: [10480/10547] batch time: 0.054 trainign loss: 2.4269 avg training loss: 5.8310
batch: [10490/10547] batch time: 2.389 trainign loss: 5.6410 avg training loss: 5.8308
batch: [10500/10547] batch time: 0.047 trainign loss: 5.8685 avg training loss: 5.8308
batch: [10510/10547] batch time: 2.363 trainign loss: 5.8161 avg training loss: 5.8307
batch: [10520/10547] batch time: 0.047 trainign loss: 4.2461 avg training loss: 5.8306
batch: [10530/10547] batch time: 2.867 trainign loss: 5.2409 avg training loss: 5.8304
batch: [10540/10547] batch time: 0.047 trainign loss: 4.5644 avg training loss: 5.8303
