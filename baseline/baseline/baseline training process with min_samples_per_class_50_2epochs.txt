total batches: 10547
Epoch: 1
----------------------------------------------------------------------
batch: [0/10547] batch time: 0.566 trainign loss: 8.8638 avg training loss: 8.8638
batch: [10/10547] batch time: 0.056 trainign loss: 8.8224 avg training loss: 8.8070
batch: [20/10547] batch time: 0.220 trainign loss: 8.7730 avg training loss: 8.8126
batch: [30/10547] batch time: 0.058 trainign loss: 8.7989 avg training loss: 8.8215
batch: [40/10547] batch time: 0.230 trainign loss: 8.6978 avg training loss: 8.8267
batch: [50/10547] batch time: 0.056 trainign loss: 8.8095 avg training loss: 8.8267
batch: [60/10547] batch time: 0.124 trainign loss: 9.0112 avg training loss: 8.8274
batch: [70/10547] batch time: 0.056 trainign loss: 8.8271 avg training loss: 8.8341
batch: [80/10547] batch time: 0.136 trainign loss: 8.8673 avg training loss: 8.8344
batch: [90/10547] batch time: 0.056 trainign loss: 8.8577 avg training loss: 8.8345
batch: [100/10547] batch time: 0.058 trainign loss: 8.6891 avg training loss: 8.8271
batch: [110/10547] batch time: 0.057 trainign loss: 8.1791 avg training loss: 8.7914
batch: [120/10547] batch time: 0.093 trainign loss: 8.7919 avg training loss: 8.7878
batch: [130/10547] batch time: 0.056 trainign loss: 8.7429 avg training loss: 8.7888
batch: [140/10547] batch time: 0.143 trainign loss: 8.7155 avg training loss: 8.7907
batch: [150/10547] batch time: 0.057 trainign loss: 8.6944 avg training loss: 8.7892
batch: [160/10547] batch time: 0.111 trainign loss: 8.8121 avg training loss: 8.7880
batch: [170/10547] batch time: 0.056 trainign loss: 8.8266 avg training loss: 8.7900
batch: [180/10547] batch time: 0.130 trainign loss: 8.7627 avg training loss: 8.7909
batch: [190/10547] batch time: 0.057 trainign loss: 8.9195 avg training loss: 8.7901
batch: [200/10547] batch time: 0.186 trainign loss: 8.7869 avg training loss: 8.7928
batch: [210/10547] batch time: 0.056 trainign loss: 8.8397 avg training loss: 8.7906
batch: [220/10547] batch time: 0.156 trainign loss: 8.8528 avg training loss: 8.7913
batch: [230/10547] batch time: 0.056 trainign loss: 8.8333 avg training loss: 8.7910
batch: [240/10547] batch time: 0.161 trainign loss: 8.8677 avg training loss: 8.7916
batch: [250/10547] batch time: 0.056 trainign loss: 8.8144 avg training loss: 8.7932
batch: [260/10547] batch time: 0.111 trainign loss: 8.9079 avg training loss: 8.7929
batch: [270/10547] batch time: 0.057 trainign loss: 8.7491 avg training loss: 8.7911
batch: [280/10547] batch time: 0.151 trainign loss: 8.7131 avg training loss: 8.7927
batch: [290/10547] batch time: 0.057 trainign loss: 8.8067 avg training loss: 8.7901
batch: [300/10547] batch time: 0.145 trainign loss: 8.7932 avg training loss: 8.7902
batch: [310/10547] batch time: 0.056 trainign loss: 8.7221 avg training loss: 8.7885
batch: [320/10547] batch time: 0.164 trainign loss: 8.9652 avg training loss: 8.7893
batch: [330/10547] batch time: 0.056 trainign loss: 8.7404 avg training loss: 8.7914
batch: [340/10547] batch time: 0.175 trainign loss: 8.8403 avg training loss: 8.7899
batch: [350/10547] batch time: 0.056 trainign loss: 8.8471 avg training loss: 8.7854
batch: [360/10547] batch time: 0.168 trainign loss: 8.8252 avg training loss: 8.7858
batch: [370/10547] batch time: 0.056 trainign loss: 8.9276 avg training loss: 8.7857
batch: [380/10547] batch time: 0.256 trainign loss: 8.7607 avg training loss: 8.7874
batch: [390/10547] batch time: 0.059 trainign loss: 8.8953 avg training loss: 8.7889
batch: [400/10547] batch time: 0.252 trainign loss: 8.8473 avg training loss: 8.7876
batch: [410/10547] batch time: 0.056 trainign loss: 8.8712 avg training loss: 8.7886
batch: [420/10547] batch time: 0.146 trainign loss: 8.9118 avg training loss: 8.7896
batch: [430/10547] batch time: 0.056 trainign loss: 8.8123 avg training loss: 8.7899
batch: [440/10547] batch time: 0.123 trainign loss: 8.6067 avg training loss: 8.7897
batch: [450/10547] batch time: 0.056 trainign loss: 8.7999 avg training loss: 8.7885
batch: [460/10547] batch time: 0.057 trainign loss: 8.8178 avg training loss: 8.7894
batch: [470/10547] batch time: 0.056 trainign loss: 8.2307 avg training loss: 8.7875
batch: [480/10547] batch time: 0.057 trainign loss: 8.9088 avg training loss: 8.7560
batch: [490/10547] batch time: 0.056 trainign loss: 8.9686 avg training loss: 8.7616
batch: [500/10547] batch time: 0.058 trainign loss: 8.9842 avg training loss: 8.7624
batch: [510/10547] batch time: 0.057 trainign loss: 8.7719 avg training loss: 8.7656
batch: [520/10547] batch time: 0.059 trainign loss: 8.8438 avg training loss: 8.7663
batch: [530/10547] batch time: 0.057 trainign loss: 7.8120 avg training loss: 8.7628
batch: [540/10547] batch time: 0.058 trainign loss: 8.8173 avg training loss: 8.7469
batch: [550/10547] batch time: 0.058 trainign loss: 8.8033 avg training loss: 8.7484
batch: [560/10547] batch time: 0.058 trainign loss: 8.9813 avg training loss: 8.7520
batch: [570/10547] batch time: 0.057 trainign loss: 8.8461 avg training loss: 8.7552
batch: [580/10547] batch time: 0.058 trainign loss: 8.8895 avg training loss: 8.7577
batch: [590/10547] batch time: 0.057 trainign loss: 8.8254 avg training loss: 8.7581
batch: [600/10547] batch time: 0.058 trainign loss: 8.6743 avg training loss: 8.7590
batch: [610/10547] batch time: 0.057 trainign loss: 8.8502 avg training loss: 8.7602
batch: [620/10547] batch time: 0.059 trainign loss: 8.7613 avg training loss: 8.7608
batch: [630/10547] batch time: 0.057 trainign loss: 8.7879 avg training loss: 8.7611
batch: [640/10547] batch time: 0.058 trainign loss: 8.3762 avg training loss: 8.7605
batch: [650/10547] batch time: 0.057 trainign loss: 8.5365 avg training loss: 8.7588
batch: [660/10547] batch time: 0.059 trainign loss: 8.8628 avg training loss: 8.7578
batch: [670/10547] batch time: 0.056 trainign loss: 8.7495 avg training loss: 8.7531
batch: [680/10547] batch time: 0.058 trainign loss: 8.6794 avg training loss: 8.7529
batch: [690/10547] batch time: 0.057 trainign loss: 8.5585 avg training loss: 8.7517
batch: [700/10547] batch time: 0.058 trainign loss: 8.5343 avg training loss: 8.7495
batch: [710/10547] batch time: 0.057 trainign loss: 8.7572 avg training loss: 8.7491
batch: [720/10547] batch time: 0.059 trainign loss: 8.2805 avg training loss: 8.7481
batch: [730/10547] batch time: 0.057 trainign loss: 8.5953 avg training loss: 8.7099
batch: [740/10547] batch time: 0.080 trainign loss: 8.0668 avg training loss: 8.7133
batch: [750/10547] batch time: 0.057 trainign loss: 8.4727 avg training loss: 8.7141
batch: [760/10547] batch time: 0.058 trainign loss: 6.6606 avg training loss: 8.7029
batch: [770/10547] batch time: 0.056 trainign loss: 8.9169 avg training loss: 8.7044
batch: [780/10547] batch time: 0.058 trainign loss: 8.9359 avg training loss: 8.7058
batch: [790/10547] batch time: 0.056 trainign loss: 8.7443 avg training loss: 8.7076
batch: [800/10547] batch time: 0.115 trainign loss: 8.7835 avg training loss: 8.7078
batch: [810/10547] batch time: 0.056 trainign loss: 8.6586 avg training loss: 8.7081
batch: [820/10547] batch time: 0.924 trainign loss: 8.7539 avg training loss: 8.7075
batch: [830/10547] batch time: 0.047 trainign loss: 8.7942 avg training loss: 8.7083
batch: [840/10547] batch time: 1.403 trainign loss: 8.7247 avg training loss: 8.7087
batch: [850/10547] batch time: 0.046 trainign loss: 8.7008 avg training loss: 8.7093
batch: [860/10547] batch time: 0.686 trainign loss: 8.7735 avg training loss: 8.7083
batch: [870/10547] batch time: 1.426 trainign loss: 8.8146 avg training loss: 8.7094
batch: [880/10547] batch time: 0.057 trainign loss: 8.7005 avg training loss: 8.7097
batch: [890/10547] batch time: 0.217 trainign loss: 8.4136 avg training loss: 8.7057
batch: [900/10547] batch time: 0.445 trainign loss: 8.7257 avg training loss: 8.7061
batch: [910/10547] batch time: 0.057 trainign loss: 8.8614 avg training loss: 8.7082
batch: [920/10547] batch time: 0.944 trainign loss: 8.8220 avg training loss: 8.7073
batch: [930/10547] batch time: 0.055 trainign loss: 8.6772 avg training loss: 8.7078
batch: [940/10547] batch time: 1.741 trainign loss: 8.8054 avg training loss: 8.7079
batch: [950/10547] batch time: 0.056 trainign loss: 8.9534 avg training loss: 8.7084
batch: [960/10547] batch time: 0.057 trainign loss: 6.8903 avg training loss: 8.7032
batch: [970/10547] batch time: 0.801 trainign loss: 8.9902 avg training loss: 8.7021
batch: [980/10547] batch time: 0.056 trainign loss: 8.9612 avg training loss: 8.6986
batch: [990/10547] batch time: 0.057 trainign loss: 6.6517 avg training loss: 8.6900
batch: [1000/10547] batch time: 0.043 trainign loss: 10.6478 avg training loss: 8.6489
batch: [1010/10547] batch time: 0.057 trainign loss: 8.9512 avg training loss: 8.6635
batch: [1020/10547] batch time: 0.056 trainign loss: 2.5455 avg training loss: 8.6359
batch: [1030/10547] batch time: 0.057 trainign loss: 0.0101 avg training loss: 8.5563
batch: [1040/10547] batch time: 0.046 trainign loss: 15.6559 avg training loss: 8.5239
batch: [1050/10547] batch time: 0.997 trainign loss: 10.1587 avg training loss: 8.5541
batch: [1060/10547] batch time: 0.056 trainign loss: 8.6318 avg training loss: 8.5598
batch: [1070/10547] batch time: 0.334 trainign loss: 8.7713 avg training loss: 8.5618
batch: [1080/10547] batch time: 0.043 trainign loss: 7.9698 avg training loss: 8.5623
batch: [1090/10547] batch time: 1.328 trainign loss: 8.6240 avg training loss: 8.5621
batch: [1100/10547] batch time: 0.184 trainign loss: 8.1364 avg training loss: 8.5606
batch: [1110/10547] batch time: 0.092 trainign loss: 8.8359 avg training loss: 8.5554
batch: [1120/10547] batch time: 0.056 trainign loss: 8.8665 avg training loss: 8.5514
batch: [1130/10547] batch time: 1.276 trainign loss: 8.9895 avg training loss: 8.5487
batch: [1140/10547] batch time: 0.811 trainign loss: 8.9310 avg training loss: 8.5511
batch: [1150/10547] batch time: 1.329 trainign loss: 8.7815 avg training loss: 8.5527
batch: [1160/10547] batch time: 0.047 trainign loss: 8.6135 avg training loss: 8.5533
batch: [1170/10547] batch time: 2.348 trainign loss: 8.6618 avg training loss: 8.5546
batch: [1180/10547] batch time: 0.047 trainign loss: 8.7733 avg training loss: 8.5566
batch: [1190/10547] batch time: 2.444 trainign loss: 8.8126 avg training loss: 8.5575
batch: [1200/10547] batch time: 0.045 trainign loss: 8.3675 avg training loss: 8.5588
batch: [1210/10547] batch time: 0.232 trainign loss: 8.6712 avg training loss: 8.5599
batch: [1220/10547] batch time: 0.056 trainign loss: 8.8112 avg training loss: 8.5599
batch: [1230/10547] batch time: 2.139 trainign loss: 8.6267 avg training loss: 8.5604
batch: [1240/10547] batch time: 0.056 trainign loss: 8.8103 avg training loss: 8.5618
batch: [1250/10547] batch time: 1.268 trainign loss: 8.9425 avg training loss: 8.5626
batch: [1260/10547] batch time: 0.055 trainign loss: 3.8853 avg training loss: 8.5491
batch: [1270/10547] batch time: 1.899 trainign loss: 9.2020 avg training loss: 8.5496
batch: [1280/10547] batch time: 0.043 trainign loss: 6.7439 avg training loss: 8.5477
batch: [1290/10547] batch time: 1.642 trainign loss: 8.8946 avg training loss: 8.5492
batch: [1300/10547] batch time: 0.044 trainign loss: 8.6674 avg training loss: 8.5499
batch: [1310/10547] batch time: 0.689 trainign loss: 8.3305 avg training loss: 8.5510
batch: [1320/10547] batch time: 0.057 trainign loss: 7.8216 avg training loss: 8.5497
batch: [1330/10547] batch time: 0.478 trainign loss: 8.7735 avg training loss: 8.5497
batch: [1340/10547] batch time: 0.056 trainign loss: 8.7407 avg training loss: 8.5510
batch: [1350/10547] batch time: 0.183 trainign loss: 8.7628 avg training loss: 8.5521
batch: [1360/10547] batch time: 0.048 trainign loss: 8.6849 avg training loss: 8.5533
batch: [1370/10547] batch time: 0.056 trainign loss: 8.3254 avg training loss: 8.5541
batch: [1380/10547] batch time: 0.057 trainign loss: 8.8227 avg training loss: 8.5528
batch: [1390/10547] batch time: 0.249 trainign loss: 9.2485 avg training loss: 8.5425
batch: [1400/10547] batch time: 0.535 trainign loss: 8.5512 avg training loss: 8.5465
batch: [1410/10547] batch time: 0.235 trainign loss: 8.7001 avg training loss: 8.5455
batch: [1420/10547] batch time: 1.369 trainign loss: 8.8468 avg training loss: 8.5466
batch: [1430/10547] batch time: 0.056 trainign loss: 8.6951 avg training loss: 8.5468
batch: [1440/10547] batch time: 0.540 trainign loss: 8.7668 avg training loss: 8.5479
batch: [1450/10547] batch time: 0.056 trainign loss: 6.7963 avg training loss: 8.5450
batch: [1460/10547] batch time: 0.157 trainign loss: 8.0567 avg training loss: 8.5397
batch: [1470/10547] batch time: 0.055 trainign loss: 8.8797 avg training loss: 8.5425
batch: [1480/10547] batch time: 0.044 trainign loss: 8.7391 avg training loss: 8.5380
batch: [1490/10547] batch time: 0.055 trainign loss: 8.7623 avg training loss: 8.5403
batch: [1500/10547] batch time: 0.044 trainign loss: 8.7688 avg training loss: 8.5417
batch: [1510/10547] batch time: 0.049 trainign loss: 8.7623 avg training loss: 8.5425
batch: [1520/10547] batch time: 0.789 trainign loss: 8.5838 avg training loss: 8.5429
batch: [1530/10547] batch time: 0.057 trainign loss: 8.9102 avg training loss: 8.5441
batch: [1540/10547] batch time: 0.056 trainign loss: 5.2360 avg training loss: 8.5391
batch: [1550/10547] batch time: 0.048 trainign loss: 9.0495 avg training loss: 8.5413
batch: [1560/10547] batch time: 0.319 trainign loss: 8.9233 avg training loss: 8.5409
batch: [1570/10547] batch time: 0.354 trainign loss: 8.6326 avg training loss: 8.5419
batch: [1580/10547] batch time: 0.047 trainign loss: 8.8813 avg training loss: 8.5422
batch: [1590/10547] batch time: 0.043 trainign loss: 7.1398 avg training loss: 8.5407
batch: [1600/10547] batch time: 0.056 trainign loss: 8.6633 avg training loss: 8.5422
batch: [1610/10547] batch time: 0.056 trainign loss: 5.8340 avg training loss: 8.5356
batch: [1620/10547] batch time: 0.055 trainign loss: 8.9617 avg training loss: 8.5374
batch: [1630/10547] batch time: 0.055 trainign loss: 6.2794 avg training loss: 8.5350
batch: [1640/10547] batch time: 0.048 trainign loss: 8.8138 avg training loss: 8.5316
batch: [1650/10547] batch time: 1.399 trainign loss: 8.4589 avg training loss: 8.5325
batch: [1660/10547] batch time: 0.785 trainign loss: 8.3983 avg training loss: 8.5305
batch: [1670/10547] batch time: 0.054 trainign loss: 6.9156 avg training loss: 8.5286
batch: [1680/10547] batch time: 1.211 trainign loss: 8.6668 avg training loss: 8.5288
batch: [1690/10547] batch time: 0.055 trainign loss: 8.5359 avg training loss: 8.5294
batch: [1700/10547] batch time: 1.994 trainign loss: 8.9046 avg training loss: 8.5271
batch: [1710/10547] batch time: 0.049 trainign loss: 8.8683 avg training loss: 8.5288
batch: [1720/10547] batch time: 0.057 trainign loss: 8.7259 avg training loss: 8.5293
batch: [1730/10547] batch time: 0.047 trainign loss: 8.8331 avg training loss: 8.5297
batch: [1740/10547] batch time: 0.050 trainign loss: 8.7126 avg training loss: 8.5309
batch: [1750/10547] batch time: 0.056 trainign loss: 8.7844 avg training loss: 8.5319
batch: [1760/10547] batch time: 0.055 trainign loss: 7.0556 avg training loss: 8.5292
batch: [1770/10547] batch time: 0.056 trainign loss: 8.7041 avg training loss: 8.5308
batch: [1780/10547] batch time: 0.050 trainign loss: 8.7151 avg training loss: 8.5291
batch: [1790/10547] batch time: 0.055 trainign loss: 8.4970 avg training loss: 8.5296
batch: [1800/10547] batch time: 2.825 trainign loss: 8.4010 avg training loss: 8.5300
batch: [1810/10547] batch time: 0.054 trainign loss: 8.6319 avg training loss: 8.5309
batch: [1820/10547] batch time: 1.023 trainign loss: 8.7724 avg training loss: 8.5317
batch: [1830/10547] batch time: 0.046 trainign loss: 8.4835 avg training loss: 8.5322
batch: [1840/10547] batch time: 0.857 trainign loss: 8.8700 avg training loss: 8.5340
batch: [1850/10547] batch time: 0.057 trainign loss: 8.4392 avg training loss: 8.5330
batch: [1860/10547] batch time: 0.222 trainign loss: 8.2106 avg training loss: 8.5332
batch: [1870/10547] batch time: 1.496 trainign loss: 8.8019 avg training loss: 8.5341
batch: [1880/10547] batch time: 0.046 trainign loss: 8.4967 avg training loss: 8.5350
batch: [1890/10547] batch time: 2.196 trainign loss: 8.7648 avg training loss: 8.5353
batch: [1900/10547] batch time: 0.057 trainign loss: 7.7153 avg training loss: 8.5354
batch: [1910/10547] batch time: 0.568 trainign loss: 1.8214 avg training loss: 8.5088
batch: [1920/10547] batch time: 0.661 trainign loss: 8.8256 avg training loss: 8.5167
batch: [1930/10547] batch time: 0.756 trainign loss: 8.5480 avg training loss: 8.5178
batch: [1940/10547] batch time: 0.054 trainign loss: 8.7268 avg training loss: 8.5190
batch: [1950/10547] batch time: 0.316 trainign loss: 8.7157 avg training loss: 8.5200
batch: [1960/10547] batch time: 0.948 trainign loss: 8.7329 avg training loss: 8.5194
batch: [1970/10547] batch time: 0.050 trainign loss: 8.5282 avg training loss: 8.5201
batch: [1980/10547] batch time: 0.159 trainign loss: 8.6607 avg training loss: 8.5202
batch: [1990/10547] batch time: 0.058 trainign loss: 3.0484 avg training loss: 8.5107
batch: [2000/10547] batch time: 0.680 trainign loss: 8.8661 avg training loss: 8.5044
batch: [2010/10547] batch time: 0.052 trainign loss: 8.8573 avg training loss: 8.5050
batch: [2020/10547] batch time: 2.387 trainign loss: 8.6488 avg training loss: 8.5053
batch: [2030/10547] batch time: 0.049 trainign loss: 8.5965 avg training loss: 8.5061
batch: [2040/10547] batch time: 1.881 trainign loss: 8.8409 avg training loss: 8.5044
batch: [2050/10547] batch time: 0.047 trainign loss: 8.9164 avg training loss: 8.5061
batch: [2060/10547] batch time: 0.375 trainign loss: 7.7038 avg training loss: 8.5064
batch: [2070/10547] batch time: 0.057 trainign loss: 7.7019 avg training loss: 8.5065
batch: [2080/10547] batch time: 0.056 trainign loss: 8.8625 avg training loss: 8.5031
batch: [2090/10547] batch time: 0.057 trainign loss: 8.6208 avg training loss: 8.5034
batch: [2100/10547] batch time: 0.204 trainign loss: 1.1219 avg training loss: 8.4881
batch: [2110/10547] batch time: 0.056 trainign loss: 8.5562 avg training loss: 8.4940
batch: [2120/10547] batch time: 0.043 trainign loss: 7.7955 avg training loss: 8.4937
batch: [2130/10547] batch time: 0.043 trainign loss: 8.6308 avg training loss: 8.4948
batch: [2140/10547] batch time: 0.042 trainign loss: 8.6974 avg training loss: 8.4955
batch: [2150/10547] batch time: 0.057 trainign loss: 7.9135 avg training loss: 8.4961
batch: [2160/10547] batch time: 0.047 trainign loss: 8.6744 avg training loss: 8.4939
batch: [2170/10547] batch time: 0.260 trainign loss: 8.7808 avg training loss: 8.4939
batch: [2180/10547] batch time: 0.595 trainign loss: 8.7768 avg training loss: 8.4950
batch: [2190/10547] batch time: 0.056 trainign loss: 2.7404 avg training loss: 8.4864
batch: [2200/10547] batch time: 1.438 trainign loss: 8.4998 avg training loss: 8.4826
batch: [2210/10547] batch time: 0.057 trainign loss: 7.5064 avg training loss: 8.4831
batch: [2220/10547] batch time: 1.027 trainign loss: 9.7208 avg training loss: 8.4777
batch: [2230/10547] batch time: 0.057 trainign loss: 7.8703 avg training loss: 8.4780
batch: [2240/10547] batch time: 0.533 trainign loss: 9.0726 avg training loss: 8.4776
batch: [2250/10547] batch time: 0.056 trainign loss: 8.8487 avg training loss: 8.4800
batch: [2260/10547] batch time: 0.226 trainign loss: 8.7025 avg training loss: 8.4810
batch: [2270/10547] batch time: 0.951 trainign loss: 8.7202 avg training loss: 8.4815
batch: [2280/10547] batch time: 0.055 trainign loss: 7.9966 avg training loss: 8.4824
batch: [2290/10547] batch time: 1.182 trainign loss: 8.9362 avg training loss: 8.4828
batch: [2300/10547] batch time: 0.047 trainign loss: 8.5119 avg training loss: 8.4841
batch: [2310/10547] batch time: 0.553 trainign loss: 8.7555 avg training loss: 8.4833
batch: [2320/10547] batch time: 0.046 trainign loss: 9.0513 avg training loss: 8.4838
batch: [2330/10547] batch time: 1.893 trainign loss: 8.7466 avg training loss: 8.4844
batch: [2340/10547] batch time: 0.051 trainign loss: 8.7795 avg training loss: 8.4855
batch: [2350/10547] batch time: 0.693 trainign loss: 8.5500 avg training loss: 8.4858
batch: [2360/10547] batch time: 0.047 trainign loss: 8.5685 avg training loss: 8.4836
batch: [2370/10547] batch time: 0.698 trainign loss: 8.1728 avg training loss: 8.4851
batch: [2380/10547] batch time: 0.055 trainign loss: 7.6537 avg training loss: 8.4851
batch: [2390/10547] batch time: 1.412 trainign loss: 10.6805 avg training loss: 8.4715
batch: [2400/10547] batch time: 0.047 trainign loss: 8.9648 avg training loss: 8.4762
batch: [2410/10547] batch time: 0.240 trainign loss: 8.2320 avg training loss: 8.4768
batch: [2420/10547] batch time: 0.153 trainign loss: 8.8391 avg training loss: 8.4766
batch: [2430/10547] batch time: 0.534 trainign loss: 9.0099 avg training loss: 8.4762
batch: [2440/10547] batch time: 0.623 trainign loss: 8.5856 avg training loss: 8.4774
batch: [2450/10547] batch time: 0.556 trainign loss: 8.6323 avg training loss: 8.4777
batch: [2460/10547] batch time: 0.254 trainign loss: 5.1218 avg training loss: 8.4747
batch: [2470/10547] batch time: 0.542 trainign loss: 12.8898 avg training loss: 8.4608
batch: [2480/10547] batch time: 0.240 trainign loss: 8.0551 avg training loss: 8.4656
batch: [2490/10547] batch time: 1.717 trainign loss: 8.8623 avg training loss: 8.4637
batch: [2500/10547] batch time: 0.989 trainign loss: 9.0870 avg training loss: 8.4647
batch: [2510/10547] batch time: 0.240 trainign loss: 3.4572 avg training loss: 8.4591
batch: [2520/10547] batch time: 0.377 trainign loss: 10.5567 avg training loss: 8.4562
batch: [2530/10547] batch time: 0.205 trainign loss: 8.5675 avg training loss: 8.4580
batch: [2540/10547] batch time: 0.460 trainign loss: 8.7766 avg training loss: 8.4591
batch: [2550/10547] batch time: 0.055 trainign loss: 8.7573 avg training loss: 8.4584
batch: [2560/10547] batch time: 0.047 trainign loss: 8.8513 avg training loss: 8.4598
batch: [2570/10547] batch time: 0.438 trainign loss: 8.5153 avg training loss: 8.4607
batch: [2580/10547] batch time: 0.048 trainign loss: 8.6637 avg training loss: 8.4617
batch: [2590/10547] batch time: 1.635 trainign loss: 8.8453 avg training loss: 8.4626
batch: [2600/10547] batch time: 0.048 trainign loss: 6.6726 avg training loss: 8.4622
batch: [2610/10547] batch time: 0.846 trainign loss: 8.9751 avg training loss: 8.4630
batch: [2620/10547] batch time: 0.055 trainign loss: 8.8304 avg training loss: 8.4638
batch: [2630/10547] batch time: 0.891 trainign loss: 8.8306 avg training loss: 8.4648
batch: [2640/10547] batch time: 0.045 trainign loss: 7.1829 avg training loss: 8.4646
batch: [2650/10547] batch time: 0.108 trainign loss: 8.9430 avg training loss: 8.4650
batch: [2660/10547] batch time: 0.047 trainign loss: 9.0997 avg training loss: 8.4658
batch: [2670/10547] batch time: 1.189 trainign loss: 8.5323 avg training loss: 8.4666
batch: [2680/10547] batch time: 0.053 trainign loss: 8.7824 avg training loss: 8.4674
batch: [2690/10547] batch time: 0.056 trainign loss: 8.3417 avg training loss: 8.4680
batch: [2700/10547] batch time: 0.047 trainign loss: 8.6804 avg training loss: 8.4677
batch: [2710/10547] batch time: 0.043 trainign loss: 8.5369 avg training loss: 8.4685
batch: [2720/10547] batch time: 0.253 trainign loss: 8.6950 avg training loss: 8.4697
batch: [2730/10547] batch time: 0.058 trainign loss: 8.8388 avg training loss: 8.4710
batch: [2740/10547] batch time: 0.991 trainign loss: 8.6387 avg training loss: 8.4682
batch: [2750/10547] batch time: 0.048 trainign loss: 8.5533 avg training loss: 8.4699
batch: [2760/10547] batch time: 0.574 trainign loss: 8.6809 avg training loss: 8.4701
batch: [2770/10547] batch time: 0.205 trainign loss: 4.3599 avg training loss: 8.4661
batch: [2780/10547] batch time: 1.496 trainign loss: 8.7805 avg training loss: 8.4667
batch: [2790/10547] batch time: 0.044 trainign loss: 8.7781 avg training loss: 8.4672
batch: [2800/10547] batch time: 0.374 trainign loss: 8.7394 avg training loss: 8.4670
batch: [2810/10547] batch time: 0.646 trainign loss: 8.2968 avg training loss: 8.4676
batch: [2820/10547] batch time: 0.047 trainign loss: 8.8153 avg training loss: 8.4685
batch: [2830/10547] batch time: 0.042 trainign loss: 8.5815 avg training loss: 8.4691
batch: [2840/10547] batch time: 0.057 trainign loss: 8.6989 avg training loss: 8.4686
batch: [2850/10547] batch time: 0.048 trainign loss: 9.0823 avg training loss: 8.4691
batch: [2860/10547] batch time: 1.148 trainign loss: 8.7404 avg training loss: 8.4702
batch: [2870/10547] batch time: 0.049 trainign loss: 8.6623 avg training loss: 8.4708
batch: [2880/10547] batch time: 0.056 trainign loss: 8.9340 avg training loss: 8.4715
batch: [2890/10547] batch time: 0.047 trainign loss: 8.8388 avg training loss: 8.4725
batch: [2900/10547] batch time: 0.650 trainign loss: 8.6969 avg training loss: 8.4735
batch: [2910/10547] batch time: 0.049 trainign loss: 8.9287 avg training loss: 8.4745
batch: [2920/10547] batch time: 0.057 trainign loss: 8.1803 avg training loss: 8.4751
batch: [2930/10547] batch time: 0.056 trainign loss: 8.0706 avg training loss: 8.4740
batch: [2940/10547] batch time: 0.047 trainign loss: 8.9154 avg training loss: 8.4740
batch: [2950/10547] batch time: 0.056 trainign loss: 8.1034 avg training loss: 8.4732
batch: [2960/10547] batch time: 0.049 trainign loss: 8.6947 avg training loss: 8.4735
batch: [2970/10547] batch time: 0.057 trainign loss: 8.6371 avg training loss: 8.4743
batch: [2980/10547] batch time: 0.057 trainign loss: 7.7937 avg training loss: 8.4741
batch: [2990/10547] batch time: 0.285 trainign loss: 8.6942 avg training loss: 8.4730
batch: [3000/10547] batch time: 0.096 trainign loss: 8.7968 avg training loss: 8.4735
batch: [3010/10547] batch time: 0.057 trainign loss: 8.4579 avg training loss: 8.4726
batch: [3020/10547] batch time: 0.935 trainign loss: 8.7804 avg training loss: 8.4726
batch: [3030/10547] batch time: 0.053 trainign loss: 9.1221 avg training loss: 8.4734
batch: [3040/10547] batch time: 0.154 trainign loss: 8.2121 avg training loss: 8.4739
batch: [3050/10547] batch time: 0.048 trainign loss: 8.8524 avg training loss: 8.4738
batch: [3060/10547] batch time: 1.174 trainign loss: 8.6924 avg training loss: 8.4747
batch: [3070/10547] batch time: 0.046 trainign loss: 8.7764 avg training loss: 8.4744
batch: [3080/10547] batch time: 1.462 trainign loss: 8.9087 avg training loss: 8.4754
batch: [3090/10547] batch time: 0.051 trainign loss: 8.5879 avg training loss: 8.4764
batch: [3100/10547] batch time: 2.442 trainign loss: 8.7394 avg training loss: 8.4770
batch: [3110/10547] batch time: 0.051 trainign loss: 8.5739 avg training loss: 8.4776
batch: [3120/10547] batch time: 0.239 trainign loss: 8.8606 avg training loss: 8.4781
batch: [3130/10547] batch time: 1.356 trainign loss: 8.5703 avg training loss: 8.4787
batch: [3140/10547] batch time: 0.330 trainign loss: 8.7455 avg training loss: 8.4792
batch: [3150/10547] batch time: 0.613 trainign loss: 8.8515 avg training loss: 8.4796
batch: [3160/10547] batch time: 0.055 trainign loss: 8.8241 avg training loss: 8.4798
batch: [3170/10547] batch time: 0.219 trainign loss: 8.5905 avg training loss: 8.4803
batch: [3180/10547] batch time: 0.055 trainign loss: 8.6757 avg training loss: 8.4807
batch: [3190/10547] batch time: 0.647 trainign loss: 7.2601 avg training loss: 8.4787
batch: [3200/10547] batch time: 0.049 trainign loss: 8.8030 avg training loss: 8.4799
batch: [3210/10547] batch time: 0.277 trainign loss: 9.0213 avg training loss: 8.4806
batch: [3220/10547] batch time: 0.494 trainign loss: 8.6102 avg training loss: 8.4812
batch: [3230/10547] batch time: 0.141 trainign loss: 8.5714 avg training loss: 8.4814
batch: [3240/10547] batch time: 0.056 trainign loss: 8.0451 avg training loss: 8.4820
batch: [3250/10547] batch time: 0.362 trainign loss: 8.6669 avg training loss: 8.4821
batch: [3260/10547] batch time: 0.050 trainign loss: 8.4983 avg training loss: 8.4827
batch: [3270/10547] batch time: 0.239 trainign loss: 8.5223 avg training loss: 8.4836
batch: [3280/10547] batch time: 0.055 trainign loss: 9.0412 avg training loss: 8.4839
batch: [3290/10547] batch time: 0.056 trainign loss: 8.6605 avg training loss: 8.4846
batch: [3300/10547] batch time: 0.989 trainign loss: 8.8890 avg training loss: 8.4852
batch: [3310/10547] batch time: 0.057 trainign loss: 8.3453 avg training loss: 8.4854
batch: [3320/10547] batch time: 0.055 trainign loss: 8.5025 avg training loss: 8.4857
batch: [3330/10547] batch time: 0.506 trainign loss: 8.7711 avg training loss: 8.4865
batch: [3340/10547] batch time: 0.507 trainign loss: 8.6107 avg training loss: 8.4872
batch: [3350/10547] batch time: 0.046 trainign loss: 8.7445 avg training loss: 8.4879
batch: [3360/10547] batch time: 0.655 trainign loss: 8.8815 avg training loss: 8.4886
batch: [3370/10547] batch time: 0.056 trainign loss: 6.3397 avg training loss: 8.4876
batch: [3380/10547] batch time: 0.055 trainign loss: 9.3279 avg training loss: 8.4868
batch: [3390/10547] batch time: 1.226 trainign loss: 8.8463 avg training loss: 8.4864
batch: [3400/10547] batch time: 0.044 trainign loss: 7.9219 avg training loss: 8.4866
batch: [3410/10547] batch time: 2.898 trainign loss: 8.9209 avg training loss: 8.4875
batch: [3420/10547] batch time: 0.056 trainign loss: 8.7595 avg training loss: 8.4883
batch: [3430/10547] batch time: 0.254 trainign loss: 7.8267 avg training loss: 8.4886
batch: [3440/10547] batch time: 0.143 trainign loss: 8.6813 avg training loss: 8.4890
batch: [3450/10547] batch time: 0.448 trainign loss: 7.7370 avg training loss: 8.4884
batch: [3460/10547] batch time: 0.046 trainign loss: 8.9519 avg training loss: 8.4896
batch: [3470/10547] batch time: 0.047 trainign loss: 8.6981 avg training loss: 8.4902
batch: [3480/10547] batch time: 0.049 trainign loss: 8.7455 avg training loss: 8.4911
batch: [3490/10547] batch time: 0.628 trainign loss: 8.8961 avg training loss: 8.4914
batch: [3500/10547] batch time: 0.051 trainign loss: 8.5987 avg training loss: 8.4923
batch: [3510/10547] batch time: 0.053 trainign loss: 8.0576 avg training loss: 8.4927
batch: [3520/10547] batch time: 0.044 trainign loss: 8.6887 avg training loss: 8.4919
batch: [3530/10547] batch time: 0.049 trainign loss: 8.8557 avg training loss: 8.4924
batch: [3540/10547] batch time: 0.046 trainign loss: 8.2445 avg training loss: 8.4929
batch: [3550/10547] batch time: 0.043 trainign loss: 8.6917 avg training loss: 8.4932
batch: [3560/10547] batch time: 0.042 trainign loss: 7.5673 avg training loss: 8.4937
batch: [3570/10547] batch time: 0.056 trainign loss: 9.0612 avg training loss: 8.4939
batch: [3580/10547] batch time: 0.077 trainign loss: 8.8808 avg training loss: 8.4945
batch: [3590/10547] batch time: 0.056 trainign loss: 8.6813 avg training loss: 8.4949
batch: [3600/10547] batch time: 1.222 trainign loss: 8.7957 avg training loss: 8.4955
batch: [3610/10547] batch time: 0.053 trainign loss: 8.8053 avg training loss: 8.4958
batch: [3620/10547] batch time: 0.354 trainign loss: 8.7635 avg training loss: 8.4964
batch: [3630/10547] batch time: 0.048 trainign loss: 8.7642 avg training loss: 8.4967
batch: [3640/10547] batch time: 0.254 trainign loss: 7.8633 avg training loss: 8.4969
batch: [3650/10547] batch time: 0.044 trainign loss: 8.8326 avg training loss: 8.4967
batch: [3660/10547] batch time: 0.152 trainign loss: 5.8313 avg training loss: 8.4954
batch: [3670/10547] batch time: 0.056 trainign loss: 8.9827 avg training loss: 8.4965
batch: [3680/10547] batch time: 0.091 trainign loss: 7.5694 avg training loss: 8.4967
batch: [3690/10547] batch time: 0.045 trainign loss: 6.6247 avg training loss: 8.4945
batch: [3700/10547] batch time: 2.272 trainign loss: 8.9943 avg training loss: 8.4956
batch: [3710/10547] batch time: 0.047 trainign loss: 8.8221 avg training loss: 8.4961
batch: [3720/10547] batch time: 1.474 trainign loss: 8.7577 avg training loss: 8.4966
batch: [3730/10547] batch time: 0.055 trainign loss: 8.7598 avg training loss: 8.4970
batch: [3740/10547] batch time: 1.204 trainign loss: 8.7043 avg training loss: 8.4973
batch: [3750/10547] batch time: 1.670 trainign loss: 8.6071 avg training loss: 8.4979
batch: [3760/10547] batch time: 1.612 trainign loss: 8.7073 avg training loss: 8.4984
batch: [3770/10547] batch time: 0.240 trainign loss: 7.8037 avg training loss: 8.4987
batch: [3780/10547] batch time: 1.347 trainign loss: 8.9192 avg training loss: 8.4993
batch: [3790/10547] batch time: 0.049 trainign loss: 8.4015 avg training loss: 8.4999
batch: [3800/10547] batch time: 0.701 trainign loss: 7.1145 avg training loss: 8.4994
batch: [3810/10547] batch time: 0.056 trainign loss: 7.0940 avg training loss: 8.4994
batch: [3820/10547] batch time: 0.761 trainign loss: 9.3599 avg training loss: 8.4968
batch: [3830/10547] batch time: 0.055 trainign loss: 8.8883 avg training loss: 8.4980
batch: [3840/10547] batch time: 0.972 trainign loss: 8.7933 avg training loss: 8.4985
batch: [3850/10547] batch time: 0.043 trainign loss: 8.6703 avg training loss: 8.4990
batch: [3860/10547] batch time: 0.566 trainign loss: 8.7817 avg training loss: 8.4994
batch: [3870/10547] batch time: 0.050 trainign loss: 8.7897 avg training loss: 8.5003
batch: [3880/10547] batch time: 0.684 trainign loss: 8.6068 avg training loss: 8.5007
batch: [3890/10547] batch time: 0.056 trainign loss: 8.5436 avg training loss: 8.5015
batch: [3900/10547] batch time: 0.867 trainign loss: 6.6513 avg training loss: 8.5001
batch: [3910/10547] batch time: 0.046 trainign loss: 8.6454 avg training loss: 8.5010
batch: [3920/10547] batch time: 0.047 trainign loss: 8.7542 avg training loss: 8.5018
batch: [3930/10547] batch time: 0.056 trainign loss: 8.8877 avg training loss: 8.5026
batch: [3940/10547] batch time: 0.056 trainign loss: 8.6209 avg training loss: 8.5025
batch: [3950/10547] batch time: 0.269 trainign loss: 8.7089 avg training loss: 8.5032
batch: [3960/10547] batch time: 0.057 trainign loss: 5.4992 avg training loss: 8.5016
batch: [3970/10547] batch time: 0.055 trainign loss: 8.7605 avg training loss: 8.5002
batch: [3980/10547] batch time: 2.717 trainign loss: 8.7800 avg training loss: 8.5004
batch: [3990/10547] batch time: 0.056 trainign loss: 8.8334 avg training loss: 8.5009
batch: [4000/10547] batch time: 2.150 trainign loss: 8.6918 avg training loss: 8.5014
batch: [4010/10547] batch time: 0.048 trainign loss: 8.8955 avg training loss: 8.5018
batch: [4020/10547] batch time: 0.890 trainign loss: 8.9335 avg training loss: 8.5026
batch: [4030/10547] batch time: 0.220 trainign loss: 8.2182 avg training loss: 8.5030
batch: [4040/10547] batch time: 1.728 trainign loss: 8.7888 avg training loss: 8.5037
batch: [4050/10547] batch time: 1.143 trainign loss: 8.1589 avg training loss: 8.5041
batch: [4060/10547] batch time: 0.231 trainign loss: 7.7402 avg training loss: 8.5044
batch: [4070/10547] batch time: 1.462 trainign loss: 8.7238 avg training loss: 8.5048
batch: [4080/10547] batch time: 0.202 trainign loss: 8.7798 avg training loss: 8.5054
batch: [4090/10547] batch time: 0.254 trainign loss: 8.7594 avg training loss: 8.5061
batch: [4100/10547] batch time: 0.056 trainign loss: 8.6526 avg training loss: 8.5064
batch: [4110/10547] batch time: 1.735 trainign loss: 8.5286 avg training loss: 8.5066
batch: [4120/10547] batch time: 0.057 trainign loss: 7.4520 avg training loss: 8.5065
batch: [4130/10547] batch time: 0.207 trainign loss: 3.0487 avg training loss: 8.5021
batch: [4140/10547] batch time: 0.057 trainign loss: 8.4524 avg training loss: 8.5024
batch: [4150/10547] batch time: 0.056 trainign loss: 8.9957 avg training loss: 8.5003
batch: [4160/10547] batch time: 0.056 trainign loss: 8.2059 avg training loss: 8.5005
batch: [4170/10547] batch time: 0.043 trainign loss: 8.4902 avg training loss: 8.5007
batch: [4180/10547] batch time: 0.042 trainign loss: 8.0945 avg training loss: 8.5007
batch: [4190/10547] batch time: 0.046 trainign loss: 8.8887 avg training loss: 8.5010
batch: [4200/10547] batch time: 0.042 trainign loss: 8.8923 avg training loss: 8.5015
batch: [4210/10547] batch time: 0.046 trainign loss: 8.6742 avg training loss: 8.5017
batch: [4220/10547] batch time: 0.102 trainign loss: 8.6670 avg training loss: 8.5023
batch: [4230/10547] batch time: 0.051 trainign loss: 8.9623 avg training loss: 8.5024
batch: [4240/10547] batch time: 1.263 trainign loss: 8.8300 avg training loss: 8.5029
batch: [4250/10547] batch time: 0.045 trainign loss: 8.7322 avg training loss: 8.5026
batch: [4260/10547] batch time: 0.055 trainign loss: 8.0131 avg training loss: 8.5030
batch: [4270/10547] batch time: 0.057 trainign loss: 8.4788 avg training loss: 8.5032
batch: [4280/10547] batch time: 0.057 trainign loss: 3.5869 avg training loss: 8.4998
batch: [4290/10547] batch time: 0.057 trainign loss: 0.0006 avg training loss: 8.4810
batch: [4300/10547] batch time: 0.044 trainign loss: 3.2999 avg training loss: 8.4621
batch: [4310/10547] batch time: 0.055 trainign loss: 7.9227 avg training loss: 8.4698
batch: [4320/10547] batch time: 0.055 trainign loss: 8.5802 avg training loss: 8.4703
batch: [4330/10547] batch time: 0.047 trainign loss: 7.9756 avg training loss: 8.4705
batch: [4340/10547] batch time: 0.048 trainign loss: 8.7964 avg training loss: 8.4708
batch: [4350/10547] batch time: 0.048 trainign loss: 8.6968 avg training loss: 8.4716
batch: [4360/10547] batch time: 0.048 trainign loss: 8.7483 avg training loss: 8.4719
batch: [4370/10547] batch time: 0.048 trainign loss: 8.6287 avg training loss: 8.4728
batch: [4380/10547] batch time: 0.042 trainign loss: 8.7318 avg training loss: 8.4733
batch: [4390/10547] batch time: 0.052 trainign loss: 8.5945 avg training loss: 8.4741
batch: [4400/10547] batch time: 0.057 trainign loss: 8.7961 avg training loss: 8.4736
batch: [4410/10547] batch time: 0.047 trainign loss: 8.4890 avg training loss: 8.4742
batch: [4420/10547] batch time: 0.057 trainign loss: 8.5221 avg training loss: 8.4748
batch: [4430/10547] batch time: 0.465 trainign loss: 8.8347 avg training loss: 8.4749
batch: [4440/10547] batch time: 0.042 trainign loss: 8.1053 avg training loss: 8.4718
batch: [4450/10547] batch time: 1.216 trainign loss: 8.7344 avg training loss: 8.4729
batch: [4460/10547] batch time: 0.053 trainign loss: 8.8430 avg training loss: 8.4727
batch: [4470/10547] batch time: 0.241 trainign loss: 8.9650 avg training loss: 8.4733
batch: [4480/10547] batch time: 0.057 trainign loss: 8.8874 avg training loss: 8.4733
batch: [4490/10547] batch time: 0.508 trainign loss: 8.8519 avg training loss: 8.4737
batch: [4500/10547] batch time: 0.057 trainign loss: 7.5987 avg training loss: 8.4740
batch: [4510/10547] batch time: 0.238 trainign loss: 8.7594 avg training loss: 8.4731
batch: [4520/10547] batch time: 0.056 trainign loss: 9.1879 avg training loss: 8.4736
batch: [4530/10547] batch time: 0.200 trainign loss: 8.3639 avg training loss: 8.4742
batch: [4540/10547] batch time: 0.047 trainign loss: 8.8002 avg training loss: 8.4738
batch: [4550/10547] batch time: 0.970 trainign loss: 8.8399 avg training loss: 8.4746
batch: [4560/10547] batch time: 0.047 trainign loss: 8.6410 avg training loss: 8.4751
batch: [4570/10547] batch time: 0.963 trainign loss: 8.8446 avg training loss: 8.4754
batch: [4580/10547] batch time: 0.055 trainign loss: 8.6731 avg training loss: 8.4762
batch: [4590/10547] batch time: 1.333 trainign loss: 8.6933 avg training loss: 8.4764
batch: [4600/10547] batch time: 0.046 trainign loss: 8.6804 avg training loss: 8.4761
batch: [4610/10547] batch time: 0.549 trainign loss: 8.8222 avg training loss: 8.4766
batch: [4620/10547] batch time: 0.055 trainign loss: 8.8947 avg training loss: 8.4772
batch: [4630/10547] batch time: 0.050 trainign loss: 8.6336 avg training loss: 8.4777
batch: [4640/10547] batch time: 0.866 trainign loss: 8.8497 avg training loss: 8.4782
batch: [4650/10547] batch time: 0.057 trainign loss: 8.8010 avg training loss: 8.4787
batch: [4660/10547] batch time: 0.043 trainign loss: 8.9497 avg training loss: 8.4787
batch: [4670/10547] batch time: 0.248 trainign loss: 8.2847 avg training loss: 8.4779
batch: [4680/10547] batch time: 0.874 trainign loss: 8.6463 avg training loss: 8.4787
batch: [4690/10547] batch time: 0.049 trainign loss: 8.9091 avg training loss: 8.4784
batch: [4700/10547] batch time: 0.621 trainign loss: 8.8672 avg training loss: 8.4788
batch: [4710/10547] batch time: 0.104 trainign loss: 8.9015 avg training loss: 8.4786
batch: [4720/10547] batch time: 0.118 trainign loss: 8.8786 avg training loss: 8.4786
batch: [4730/10547] batch time: 0.356 trainign loss: 8.7471 avg training loss: 8.4781
batch: [4740/10547] batch time: 0.043 trainign loss: 8.7845 avg training loss: 8.4784
batch: [4750/10547] batch time: 0.050 trainign loss: 8.7729 avg training loss: 8.4791
batch: [4760/10547] batch time: 0.057 trainign loss: 8.8109 avg training loss: 8.4794
batch: [4770/10547] batch time: 0.052 trainign loss: 8.9356 avg training loss: 8.4797
batch: [4780/10547] batch time: 2.177 trainign loss: 8.7871 avg training loss: 8.4804
batch: [4790/10547] batch time: 0.056 trainign loss: 8.8561 avg training loss: 8.4809
batch: [4800/10547] batch time: 1.455 trainign loss: 9.0449 avg training loss: 8.4814
batch: [4810/10547] batch time: 0.997 trainign loss: 9.0055 avg training loss: 8.4821
batch: [4820/10547] batch time: 1.080 trainign loss: 8.5029 avg training loss: 8.4822
batch: [4830/10547] batch time: 0.139 trainign loss: 8.8018 avg training loss: 8.4828
batch: [4840/10547] batch time: 0.056 trainign loss: 8.6765 avg training loss: 8.4831
batch: [4850/10547] batch time: 0.057 trainign loss: 7.6916 avg training loss: 8.4826
batch: [4860/10547] batch time: 0.044 trainign loss: 8.7183 avg training loss: 8.4827
batch: [4870/10547] batch time: 0.047 trainign loss: 8.7580 avg training loss: 8.4831
batch: [4880/10547] batch time: 0.190 trainign loss: 8.4450 avg training loss: 8.4835
batch: [4890/10547] batch time: 0.048 trainign loss: 8.8553 avg training loss: 8.4838
batch: [4900/10547] batch time: 0.048 trainign loss: 8.7529 avg training loss: 8.4845
batch: [4910/10547] batch time: 0.056 trainign loss: 8.8613 avg training loss: 8.4849
batch: [4920/10547] batch time: 0.055 trainign loss: 8.8536 avg training loss: 8.4857
batch: [4930/10547] batch time: 0.055 trainign loss: 8.6597 avg training loss: 8.4860
batch: [4940/10547] batch time: 0.144 trainign loss: 8.8503 avg training loss: 8.4866
batch: [4950/10547] batch time: 0.705 trainign loss: 8.8374 avg training loss: 8.4861
batch: [4960/10547] batch time: 0.055 trainign loss: 8.7134 avg training loss: 8.4867
batch: [4970/10547] batch time: 2.114 trainign loss: 8.8476 avg training loss: 8.4873
batch: [4980/10547] batch time: 0.050 trainign loss: 8.7839 avg training loss: 8.4880
batch: [4990/10547] batch time: 0.044 trainign loss: 8.6980 avg training loss: 8.4883
batch: [5000/10547] batch time: 0.043 trainign loss: 8.3364 avg training loss: 8.4889
batch: [5010/10547] batch time: 0.057 trainign loss: 8.4599 avg training loss: 8.4890
batch: [5020/10547] batch time: 0.055 trainign loss: 8.9001 avg training loss: 8.4892
batch: [5030/10547] batch time: 1.284 trainign loss: 8.8732 avg training loss: 8.4896
batch: [5040/10547] batch time: 0.048 trainign loss: 8.8779 avg training loss: 8.4899
batch: [5050/10547] batch time: 0.057 trainign loss: 8.9635 avg training loss: 8.4903
batch: [5060/10547] batch time: 1.553 trainign loss: 9.6123 avg training loss: 8.4888
batch: [5070/10547] batch time: 0.047 trainign loss: 8.7863 avg training loss: 8.4899
batch: [5080/10547] batch time: 0.231 trainign loss: 8.8411 avg training loss: 8.4899
batch: [5090/10547] batch time: 0.047 trainign loss: 8.8585 avg training loss: 8.4902
batch: [5100/10547] batch time: 0.218 trainign loss: 7.3267 avg training loss: 8.4904
batch: [5110/10547] batch time: 0.048 trainign loss: 8.9334 avg training loss: 8.4907
batch: [5120/10547] batch time: 0.229 trainign loss: 7.4077 avg training loss: 8.4910
batch: [5130/10547] batch time: 0.055 trainign loss: 8.2128 avg training loss: 8.4911
batch: [5140/10547] batch time: 1.666 trainign loss: 8.8576 avg training loss: 8.4914
batch: [5150/10547] batch time: 0.379 trainign loss: 8.8691 avg training loss: 8.4921
batch: [5160/10547] batch time: 0.206 trainign loss: 4.6118 avg training loss: 8.4902
batch: [5170/10547] batch time: 0.959 trainign loss: 9.1034 avg training loss: 8.4909
batch: [5180/10547] batch time: 0.680 trainign loss: 8.8802 avg training loss: 8.4913
batch: [5190/10547] batch time: 1.312 trainign loss: 8.9964 avg training loss: 8.4914
batch: [5200/10547] batch time: 0.214 trainign loss: 7.6520 avg training loss: 8.4917
batch: [5210/10547] batch time: 0.184 trainign loss: 8.7826 avg training loss: 8.4921
batch: [5220/10547] batch time: 0.047 trainign loss: 9.0484 avg training loss: 8.4924
batch: [5230/10547] batch time: 0.057 trainign loss: 8.2602 avg training loss: 8.4926
batch: [5240/10547] batch time: 0.056 trainign loss: 6.8847 avg training loss: 8.4912
batch: [5250/10547] batch time: 0.056 trainign loss: 8.8559 avg training loss: 8.4920
batch: [5260/10547] batch time: 0.055 trainign loss: 8.6933 avg training loss: 8.4925
batch: [5270/10547] batch time: 0.049 trainign loss: 8.8523 avg training loss: 8.4928
batch: [5280/10547] batch time: 0.043 trainign loss: 8.9765 avg training loss: 8.4932
batch: [5290/10547] batch time: 0.054 trainign loss: 8.7578 avg training loss: 8.4936
batch: [5300/10547] batch time: 0.055 trainign loss: 5.9723 avg training loss: 8.4926
batch: [5310/10547] batch time: 0.056 trainign loss: 9.0891 avg training loss: 8.4929
batch: [5320/10547] batch time: 1.491 trainign loss: 8.6481 avg training loss: 8.4936
batch: [5330/10547] batch time: 0.049 trainign loss: 8.9249 avg training loss: 8.4941
batch: [5340/10547] batch time: 0.868 trainign loss: 8.0663 avg training loss: 8.4944
batch: [5350/10547] batch time: 0.148 trainign loss: 8.7530 avg training loss: 8.4949
batch: [5360/10547] batch time: 0.049 trainign loss: 8.4378 avg training loss: 8.4950
batch: [5370/10547] batch time: 0.075 trainign loss: 8.3575 avg training loss: 8.4956
batch: [5380/10547] batch time: 0.057 trainign loss: 8.9049 avg training loss: 8.4956
batch: [5390/10547] batch time: 0.102 trainign loss: 7.7693 avg training loss: 8.4960
batch: [5400/10547] batch time: 0.056 trainign loss: 8.8690 avg training loss: 8.4961
batch: [5410/10547] batch time: 0.815 trainign loss: 8.9176 avg training loss: 8.4967
batch: [5420/10547] batch time: 0.052 trainign loss: 8.9049 avg training loss: 8.4972
batch: [5430/10547] batch time: 0.049 trainign loss: 8.7868 avg training loss: 8.4977
batch: [5440/10547] batch time: 0.994 trainign loss: 8.9325 avg training loss: 8.4984
batch: [5450/10547] batch time: 0.057 trainign loss: 8.7423 avg training loss: 8.4987
batch: [5460/10547] batch time: 0.626 trainign loss: 8.7444 avg training loss: 8.4988
batch: [5470/10547] batch time: 0.057 trainign loss: 6.2580 avg training loss: 8.4984
batch: [5480/10547] batch time: 0.067 trainign loss: 8.9743 avg training loss: 8.4973
batch: [5490/10547] batch time: 0.047 trainign loss: 8.8586 avg training loss: 8.4975
batch: [5500/10547] batch time: 0.181 trainign loss: 8.8438 avg training loss: 8.4981
batch: [5510/10547] batch time: 0.403 trainign loss: 8.8217 avg training loss: 8.4986
batch: [5520/10547] batch time: 0.599 trainign loss: 8.7626 avg training loss: 8.4970
batch: [5530/10547] batch time: 0.057 trainign loss: 8.6686 avg training loss: 8.4981
batch: [5540/10547] batch time: 0.268 trainign loss: 7.2214 avg training loss: 8.4981
batch: [5550/10547] batch time: 0.196 trainign loss: 8.9478 avg training loss: 8.4979
batch: [5560/10547] batch time: 1.436 trainign loss: 8.7107 avg training loss: 8.4986
batch: [5570/10547] batch time: 0.538 trainign loss: 8.9071 avg training loss: 8.4991
batch: [5580/10547] batch time: 0.114 trainign loss: 8.6340 avg training loss: 8.4995
batch: [5590/10547] batch time: 1.767 trainign loss: 9.3717 avg training loss: 8.4960
batch: [5600/10547] batch time: 0.376 trainign loss: 8.6392 avg training loss: 8.4981
batch: [5610/10547] batch time: 1.296 trainign loss: 8.9399 avg training loss: 8.4987
batch: [5620/10547] batch time: 0.055 trainign loss: 8.8247 avg training loss: 8.4993
batch: [5630/10547] batch time: 0.096 trainign loss: 8.6448 avg training loss: 8.4994
batch: [5640/10547] batch time: 0.171 trainign loss: 9.2117 avg training loss: 8.4991
batch: [5650/10547] batch time: 0.173 trainign loss: 8.8954 avg training loss: 8.4997
batch: [5660/10547] batch time: 0.043 trainign loss: 8.5432 avg training loss: 8.5002
batch: [5670/10547] batch time: 0.056 trainign loss: 7.9668 avg training loss: 8.5005
batch: [5680/10547] batch time: 0.047 trainign loss: 8.5775 avg training loss: 8.5007
batch: [5690/10547] batch time: 1.062 trainign loss: 8.8121 avg training loss: 8.5013
batch: [5700/10547] batch time: 0.043 trainign loss: 8.8458 avg training loss: 8.5018
batch: [5710/10547] batch time: 0.253 trainign loss: 8.5417 avg training loss: 8.5020
batch: [5720/10547] batch time: 0.043 trainign loss: 8.7895 avg training loss: 8.5026
batch: [5730/10547] batch time: 0.047 trainign loss: 8.7209 avg training loss: 8.5033
batch: [5740/10547] batch time: 0.056 trainign loss: 8.9387 avg training loss: 8.5039
batch: [5750/10547] batch time: 0.057 trainign loss: 8.2873 avg training loss: 8.5041
batch: [5760/10547] batch time: 0.047 trainign loss: 8.8411 avg training loss: 8.5041
batch: [5770/10547] batch time: 1.574 trainign loss: 8.8619 avg training loss: 8.5047
batch: [5780/10547] batch time: 0.056 trainign loss: 8.7473 avg training loss: 8.5051
batch: [5790/10547] batch time: 0.058 trainign loss: 0.9404 avg training loss: 8.4993
batch: [5800/10547] batch time: 0.050 trainign loss: 15.7012 avg training loss: 8.4934
batch: [5810/10547] batch time: 0.058 trainign loss: 8.8732 avg training loss: 8.4955
batch: [5820/10547] batch time: 0.044 trainign loss: 9.3219 avg training loss: 8.4951
batch: [5830/10547] batch time: 0.052 trainign loss: 8.7278 avg training loss: 8.4951
batch: [5840/10547] batch time: 0.057 trainign loss: 8.0508 avg training loss: 8.4948
batch: [5850/10547] batch time: 1.118 trainign loss: 9.5917 avg training loss: 8.4932
batch: [5860/10547] batch time: 0.179 trainign loss: 8.5163 avg training loss: 8.4939
batch: [5870/10547] batch time: 0.047 trainign loss: 8.1773 avg training loss: 8.4941
batch: [5880/10547] batch time: 0.055 trainign loss: 7.7587 avg training loss: 8.4937
batch: [5890/10547] batch time: 1.764 trainign loss: 8.5611 avg training loss: 8.4943
batch: [5900/10547] batch time: 0.047 trainign loss: 8.5588 avg training loss: 8.4948
batch: [5910/10547] batch time: 1.201 trainign loss: 8.8401 avg training loss: 8.4954
batch: [5920/10547] batch time: 0.047 trainign loss: 8.7978 avg training loss: 8.4960
batch: [5930/10547] batch time: 0.265 trainign loss: 7.3907 avg training loss: 8.4961
batch: [5940/10547] batch time: 0.057 trainign loss: 8.3878 avg training loss: 8.4944
batch: [5950/10547] batch time: 0.055 trainign loss: 7.8852 avg training loss: 8.4951
batch: [5960/10547] batch time: 0.044 trainign loss: 8.7855 avg training loss: 8.4957
batch: [5970/10547] batch time: 0.057 trainign loss: 9.1274 avg training loss: 8.4950
batch: [5980/10547] batch time: 0.043 trainign loss: 8.7342 avg training loss: 8.4950
batch: [5990/10547] batch time: 0.806 trainign loss: 9.0160 avg training loss: 8.4956
batch: [6000/10547] batch time: 0.046 trainign loss: 8.8107 avg training loss: 8.4962
batch: [6010/10547] batch time: 0.081 trainign loss: 8.8558 avg training loss: 8.4968
batch: [6020/10547] batch time: 0.051 trainign loss: 8.9191 avg training loss: 8.4975
batch: [6030/10547] batch time: 0.111 trainign loss: 8.6353 avg training loss: 8.4977
batch: [6040/10547] batch time: 0.047 trainign loss: 8.9333 avg training loss: 8.4981
batch: [6050/10547] batch time: 0.992 trainign loss: 8.9631 avg training loss: 8.4988
batch: [6060/10547] batch time: 0.056 trainign loss: 8.7546 avg training loss: 8.4994
batch: [6070/10547] batch time: 0.046 trainign loss: 8.8240 avg training loss: 8.4998
batch: [6080/10547] batch time: 0.055 trainign loss: 8.8037 avg training loss: 8.5004
batch: [6090/10547] batch time: 0.047 trainign loss: 9.0095 avg training loss: 8.5009
batch: [6100/10547] batch time: 0.056 trainign loss: 8.3088 avg training loss: 8.5012
batch: [6110/10547] batch time: 0.223 trainign loss: 9.0147 avg training loss: 8.5017
batch: [6120/10547] batch time: 0.047 trainign loss: 8.7529 avg training loss: 8.5022
batch: [6130/10547] batch time: 0.056 trainign loss: 8.2970 avg training loss: 8.5027
batch: [6140/10547] batch time: 0.051 trainign loss: 9.0216 avg training loss: 8.5029
batch: [6150/10547] batch time: 0.763 trainign loss: 8.8051 avg training loss: 8.5036
batch: [6160/10547] batch time: 0.055 trainign loss: 8.9879 avg training loss: 8.5040
batch: [6170/10547] batch time: 0.223 trainign loss: 4.1458 avg training loss: 8.5021
batch: [6180/10547] batch time: 1.442 trainign loss: 10.4305 avg training loss: 8.4994
batch: [6190/10547] batch time: 0.051 trainign loss: 8.8161 avg training loss: 8.4997
batch: [6200/10547] batch time: 1.370 trainign loss: 8.9769 avg training loss: 8.4999
batch: [6210/10547] batch time: 0.079 trainign loss: 8.8298 avg training loss: 8.5006
batch: [6220/10547] batch time: 1.914 trainign loss: 8.7474 avg training loss: 8.5012
batch: [6230/10547] batch time: 0.056 trainign loss: 8.2683 avg training loss: 8.5015
batch: [6240/10547] batch time: 1.325 trainign loss: 8.6508 avg training loss: 8.5017
batch: [6250/10547] batch time: 0.056 trainign loss: 8.7787 avg training loss: 8.5024
batch: [6260/10547] batch time: 0.344 trainign loss: 8.6559 avg training loss: 8.5026
batch: [6270/10547] batch time: 0.205 trainign loss: 7.6940 avg training loss: 8.5026
batch: [6280/10547] batch time: 0.056 trainign loss: 8.7815 avg training loss: 8.5028
batch: [6290/10547] batch time: 1.131 trainign loss: 8.8580 avg training loss: 8.5031
batch: [6300/10547] batch time: 0.050 trainign loss: 8.8327 avg training loss: 8.5035
batch: [6310/10547] batch time: 0.223 trainign loss: 7.4747 avg training loss: 8.5036
batch: [6320/10547] batch time: 0.056 trainign loss: 8.8625 avg training loss: 8.5043
batch: [6330/10547] batch time: 0.938 trainign loss: 8.8261 avg training loss: 8.5049
batch: [6340/10547] batch time: 0.054 trainign loss: 8.9387 avg training loss: 8.5053
batch: [6350/10547] batch time: 1.790 trainign loss: 8.8449 avg training loss: 8.5059
batch: [6360/10547] batch time: 0.057 trainign loss: 8.8944 avg training loss: 8.5065
batch: [6370/10547] batch time: 1.855 trainign loss: 8.8191 avg training loss: 8.5061
batch: [6380/10547] batch time: 0.052 trainign loss: 7.4818 avg training loss: 8.5065
batch: [6390/10547] batch time: 0.224 trainign loss: 8.8221 avg training loss: 8.5068
batch: [6400/10547] batch time: 1.005 trainign loss: 8.1820 avg training loss: 8.5069
batch: [6410/10547] batch time: 0.162 trainign loss: 8.8832 avg training loss: 8.5073
batch: [6420/10547] batch time: 0.043 trainign loss: 8.9484 avg training loss: 8.5070
batch: [6430/10547] batch time: 0.046 trainign loss: 8.8466 avg training loss: 8.5076
batch: [6440/10547] batch time: 0.055 trainign loss: 6.7886 avg training loss: 8.5069
batch: [6450/10547] batch time: 0.055 trainign loss: 9.3681 avg training loss: 8.5070
batch: [6460/10547] batch time: 0.056 trainign loss: 8.4674 avg training loss: 8.5077
batch: [6470/10547] batch time: 0.049 trainign loss: 9.0691 avg training loss: 8.5072
batch: [6480/10547] batch time: 0.091 trainign loss: 9.0198 avg training loss: 8.5078
batch: [6490/10547] batch time: 0.092 trainign loss: 8.6362 avg training loss: 8.5082
batch: [6500/10547] batch time: 0.058 trainign loss: 2.8991 avg training loss: 8.5052
batch: [6510/10547] batch time: 0.099 trainign loss: 0.0004 avg training loss: 8.4927
batch: [6520/10547] batch time: 0.058 trainign loss: 0.0000 avg training loss: 8.4796
batch: [6530/10547] batch time: 0.594 trainign loss: 9.5006 avg training loss: 8.4769
batch: [6540/10547] batch time: 0.188 trainign loss: 8.9786 avg training loss: 8.4774
batch: [6550/10547] batch time: 0.047 trainign loss: 8.5366 avg training loss: 8.4779
batch: [6560/10547] batch time: 0.047 trainign loss: 8.5766 avg training loss: 8.4786
batch: [6570/10547] batch time: 0.055 trainign loss: 7.1014 avg training loss: 8.4786
batch: [6580/10547] batch time: 0.054 trainign loss: 9.1118 avg training loss: 8.4788
batch: [6590/10547] batch time: 0.565 trainign loss: 9.0418 avg training loss: 8.4795
batch: [6600/10547] batch time: 0.043 trainign loss: 7.8452 avg training loss: 8.4797
batch: [6610/10547] batch time: 0.045 trainign loss: 7.8622 avg training loss: 8.4799
batch: [6620/10547] batch time: 0.046 trainign loss: 8.9281 avg training loss: 8.4805
batch: [6630/10547] batch time: 0.056 trainign loss: 8.8903 avg training loss: 8.4811
batch: [6640/10547] batch time: 0.047 trainign loss: 9.0390 avg training loss: 8.4811
batch: [6650/10547] batch time: 0.054 trainign loss: 7.6619 avg training loss: 8.4812
batch: [6660/10547] batch time: 1.050 trainign loss: 8.9785 avg training loss: 8.4815
batch: [6670/10547] batch time: 0.050 trainign loss: 8.9217 avg training loss: 8.4821
batch: [6680/10547] batch time: 0.849 trainign loss: 8.9868 avg training loss: 8.4822
batch: [6690/10547] batch time: 0.052 trainign loss: 9.0566 avg training loss: 8.4826
batch: [6700/10547] batch time: 0.046 trainign loss: 9.0062 avg training loss: 8.4831
batch: [6710/10547] batch time: 0.046 trainign loss: 8.9731 avg training loss: 8.4837
batch: [6720/10547] batch time: 0.055 trainign loss: 8.8783 avg training loss: 8.4841
batch: [6730/10547] batch time: 0.044 trainign loss: 9.1015 avg training loss: 8.4847
batch: [6740/10547] batch time: 0.050 trainign loss: 8.6354 avg training loss: 8.4854
batch: [6750/10547] batch time: 1.869 trainign loss: 8.7970 avg training loss: 8.4856
batch: [6760/10547] batch time: 0.057 trainign loss: 8.8690 avg training loss: 8.4861
batch: [6770/10547] batch time: 1.729 trainign loss: 8.9079 avg training loss: 8.4865
batch: [6780/10547] batch time: 0.046 trainign loss: 8.7286 avg training loss: 8.4867
batch: [6790/10547] batch time: 2.677 trainign loss: 8.7889 avg training loss: 8.4873
batch: [6800/10547] batch time: 0.057 trainign loss: 8.9207 avg training loss: 8.4878
batch: [6810/10547] batch time: 1.552 trainign loss: 8.6915 avg training loss: 8.4882
batch: [6820/10547] batch time: 0.057 trainign loss: 6.1425 avg training loss: 8.4864
batch: [6830/10547] batch time: 0.404 trainign loss: 8.0409 avg training loss: 8.4867
batch: [6840/10547] batch time: 0.182 trainign loss: 8.3022 avg training loss: 8.4872
batch: [6850/10547] batch time: 0.222 trainign loss: 7.1734 avg training loss: 8.4872
batch: [6860/10547] batch time: 0.057 trainign loss: 8.9039 avg training loss: 8.4872
batch: [6870/10547] batch time: 1.154 trainign loss: 9.0572 avg training loss: 8.4872
batch: [6880/10547] batch time: 0.046 trainign loss: 8.4677 avg training loss: 8.4877
batch: [6890/10547] batch time: 0.049 trainign loss: 9.0013 avg training loss: 8.4883
batch: [6900/10547] batch time: 0.056 trainign loss: 9.1096 avg training loss: 8.4885
batch: [6910/10547] batch time: 0.056 trainign loss: 8.9187 avg training loss: 8.4887
batch: [6920/10547] batch time: 0.118 trainign loss: 8.8391 avg training loss: 8.4889
batch: [6930/10547] batch time: 0.172 trainign loss: 8.9166 avg training loss: 8.4891
batch: [6940/10547] batch time: 2.009 trainign loss: 8.9075 avg training loss: 8.4897
batch: [6950/10547] batch time: 0.364 trainign loss: 8.4228 avg training loss: 8.4900
batch: [6960/10547] batch time: 1.116 trainign loss: 9.0014 avg training loss: 8.4899
batch: [6970/10547] batch time: 0.323 trainign loss: 8.9198 avg training loss: 8.4905
batch: [6980/10547] batch time: 0.721 trainign loss: 8.8410 avg training loss: 8.4911
batch: [6990/10547] batch time: 0.057 trainign loss: 8.7364 avg training loss: 8.4917
batch: [7000/10547] batch time: 1.129 trainign loss: 9.1252 avg training loss: 8.4916
batch: [7010/10547] batch time: 0.187 trainign loss: 7.3089 avg training loss: 8.4918
batch: [7020/10547] batch time: 0.047 trainign loss: 9.0594 avg training loss: 8.4914
batch: [7030/10547] batch time: 0.540 trainign loss: 8.8667 avg training loss: 8.4920
batch: [7040/10547] batch time: 0.909 trainign loss: 9.0324 avg training loss: 8.4923
batch: [7050/10547] batch time: 0.133 trainign loss: 8.9968 avg training loss: 8.4928
batch: [7060/10547] batch time: 0.332 trainign loss: 9.0629 avg training loss: 8.4934
batch: [7070/10547] batch time: 0.044 trainign loss: 9.1857 avg training loss: 8.4924
batch: [7080/10547] batch time: 1.167 trainign loss: 8.8700 avg training loss: 8.4933
batch: [7090/10547] batch time: 0.048 trainign loss: 9.0254 avg training loss: 8.4937
batch: [7100/10547] batch time: 1.401 trainign loss: 8.9919 avg training loss: 8.4943
batch: [7110/10547] batch time: 0.046 trainign loss: 8.9482 avg training loss: 8.4947
batch: [7120/10547] batch time: 2.071 trainign loss: 9.0813 avg training loss: 8.4952
batch: [7130/10547] batch time: 0.056 trainign loss: 8.5331 avg training loss: 8.4957
batch: [7140/10547] batch time: 1.970 trainign loss: 8.7254 avg training loss: 8.4956
batch: [7150/10547] batch time: 0.056 trainign loss: 4.8477 avg training loss: 8.4947
batch: [7160/10547] batch time: 1.905 trainign loss: 0.0012 avg training loss: 8.4841
batch: [7170/10547] batch time: 0.056 trainign loss: 0.0000 avg training loss: 8.4723
batch: [7180/10547] batch time: 1.915 trainign loss: 0.0000 avg training loss: 8.4605
batch: [7190/10547] batch time: 0.045 trainign loss: 0.0000 avg training loss: 8.4487
batch: [7200/10547] batch time: 1.915 trainign loss: 0.0000 avg training loss: 8.4370
batch: [7210/10547] batch time: 0.049 trainign loss: 0.0000 avg training loss: 8.4253
batch: [7220/10547] batch time: 1.970 trainign loss: 0.0000 avg training loss: 8.4136
batch: [7230/10547] batch time: 0.046 trainign loss: 0.0000 avg training loss: 8.4020
batch: [7240/10547] batch time: 2.507 trainign loss: 0.0000 avg training loss: 8.3904
batch: [7250/10547] batch time: 0.043 trainign loss: 8.9231 avg training loss: 8.3936
batch: [7260/10547] batch time: 2.300 trainign loss: 8.7966 avg training loss: 8.3942
batch: [7270/10547] batch time: 0.046 trainign loss: 8.5855 avg training loss: 8.3948
batch: [7280/10547] batch time: 0.276 trainign loss: 9.1162 avg training loss: 8.3956
batch: [7290/10547] batch time: 0.532 trainign loss: 9.1726 avg training loss: 8.3957
batch: [7300/10547] batch time: 0.057 trainign loss: 3.2583 avg training loss: 8.3937
batch: [7310/10547] batch time: 0.167 trainign loss: 12.8972 avg training loss: 8.3892
batch: [7320/10547] batch time: 1.792 trainign loss: 9.0051 avg training loss: 8.3899
batch: [7330/10547] batch time: 0.906 trainign loss: 8.7119 avg training loss: 8.3905
batch: [7340/10547] batch time: 0.210 trainign loss: 8.9337 avg training loss: 8.3913
batch: [7350/10547] batch time: 1.185 trainign loss: 8.9270 avg training loss: 8.3919
batch: [7360/10547] batch time: 0.843 trainign loss: 8.9283 avg training loss: 8.3926
batch: [7370/10547] batch time: 1.122 trainign loss: 8.8764 avg training loss: 8.3933
batch: [7380/10547] batch time: 0.763 trainign loss: 8.9388 avg training loss: 8.3940
batch: [7390/10547] batch time: 1.768 trainign loss: 9.0345 avg training loss: 8.3947
batch: [7400/10547] batch time: 0.058 trainign loss: 8.9612 avg training loss: 8.3950
batch: [7410/10547] batch time: 2.327 trainign loss: 8.8806 avg training loss: 8.3956
batch: [7420/10547] batch time: 0.057 trainign loss: 8.8364 avg training loss: 8.3962
batch: [7430/10547] batch time: 0.214 trainign loss: 8.7238 avg training loss: 8.3962
batch: [7440/10547] batch time: 0.624 trainign loss: 8.6110 avg training loss: 8.3967
batch: [7450/10547] batch time: 0.055 trainign loss: 8.9395 avg training loss: 8.3973
batch: [7460/10547] batch time: 0.457 trainign loss: 8.9107 avg training loss: 8.3979
batch: [7470/10547] batch time: 0.056 trainign loss: 9.0733 avg training loss: 8.3982
batch: [7480/10547] batch time: 0.055 trainign loss: 8.9107 avg training loss: 8.3988
batch: [7490/10547] batch time: 0.056 trainign loss: 5.0574 avg training loss: 8.3980
batch: [7500/10547] batch time: 0.056 trainign loss: 9.0173 avg training loss: 8.3982
batch: [7510/10547] batch time: 0.049 trainign loss: 8.9786 avg training loss: 8.3981
batch: [7520/10547] batch time: 0.044 trainign loss: 8.6753 avg training loss: 8.3984
batch: [7530/10547] batch time: 0.117 trainign loss: 9.0096 avg training loss: 8.3991
batch: [7540/10547] batch time: 0.247 trainign loss: 8.8384 avg training loss: 8.3994
batch: [7550/10547] batch time: 0.043 trainign loss: 7.1163 avg training loss: 8.3995
batch: [7560/10547] batch time: 0.340 trainign loss: 8.7753 avg training loss: 8.4002
batch: [7570/10547] batch time: 0.052 trainign loss: 8.8658 avg training loss: 8.4009
batch: [7580/10547] batch time: 0.056 trainign loss: 8.9970 avg training loss: 8.4011
batch: [7590/10547] batch time: 0.726 trainign loss: 8.9965 avg training loss: 8.4015
batch: [7600/10547] batch time: 0.056 trainign loss: 8.0983 avg training loss: 8.4018
batch: [7610/10547] batch time: 0.599 trainign loss: 8.9654 avg training loss: 8.4023
batch: [7620/10547] batch time: 0.055 trainign loss: 9.0408 avg training loss: 8.4029
batch: [7630/10547] batch time: 0.056 trainign loss: 8.8005 avg training loss: 8.4035
batch: [7640/10547] batch time: 0.056 trainign loss: 9.0241 avg training loss: 8.4037
batch: [7650/10547] batch time: 0.976 trainign loss: 9.1337 avg training loss: 8.4046
batch: [7660/10547] batch time: 0.047 trainign loss: 9.0103 avg training loss: 8.4052
batch: [7670/10547] batch time: 0.055 trainign loss: 8.3576 avg training loss: 8.4058
batch: [7680/10547] batch time: 0.046 trainign loss: 8.7989 avg training loss: 8.4063
batch: [7690/10547] batch time: 0.046 trainign loss: 8.8175 avg training loss: 8.4069
batch: [7700/10547] batch time: 0.043 trainign loss: 9.1161 avg training loss: 8.4073
batch: [7710/10547] batch time: 0.057 trainign loss: 9.0585 avg training loss: 8.4061
batch: [7720/10547] batch time: 1.005 trainign loss: 9.1053 avg training loss: 8.4069
batch: [7730/10547] batch time: 0.047 trainign loss: 8.7034 avg training loss: 8.4074
batch: [7740/10547] batch time: 1.443 trainign loss: 8.7641 avg training loss: 8.4081
batch: [7750/10547] batch time: 0.047 trainign loss: 9.0047 avg training loss: 8.4087
batch: [7760/10547] batch time: 1.946 trainign loss: 8.9561 avg training loss: 8.4095
batch: [7770/10547] batch time: 0.047 trainign loss: 9.1611 avg training loss: 8.4102
batch: [7780/10547] batch time: 1.954 trainign loss: 9.0215 avg training loss: 8.4108
batch: [7790/10547] batch time: 0.055 trainign loss: 9.0575 avg training loss: 8.4115
batch: [7800/10547] batch time: 1.293 trainign loss: 8.8638 avg training loss: 8.4120
batch: [7810/10547] batch time: 0.056 trainign loss: 5.0131 avg training loss: 8.4110
batch: [7820/10547] batch time: 0.319 trainign loss: 14.2960 avg training loss: 8.4054
batch: [7830/10547] batch time: 0.046 trainign loss: 8.9692 avg training loss: 8.4067
batch: [7840/10547] batch time: 0.530 trainign loss: 9.0835 avg training loss: 8.4074
batch: [7850/10547] batch time: 0.055 trainign loss: 7.8038 avg training loss: 8.4078
batch: [7860/10547] batch time: 1.623 trainign loss: 8.9619 avg training loss: 8.4084
batch: [7870/10547] batch time: 0.047 trainign loss: 8.9368 avg training loss: 8.4090
batch: [7880/10547] batch time: 0.056 trainign loss: 8.9435 avg training loss: 8.4096
batch: [7890/10547] batch time: 0.857 trainign loss: 9.0020 avg training loss: 8.4102
batch: [7900/10547] batch time: 0.043 trainign loss: 7.5115 avg training loss: 8.4105
batch: [7910/10547] batch time: 0.570 trainign loss: 8.9812 avg training loss: 8.4107
batch: [7920/10547] batch time: 0.047 trainign loss: 8.9104 avg training loss: 8.4113
batch: [7930/10547] batch time: 0.969 trainign loss: 8.8734 avg training loss: 8.4120
batch: [7940/10547] batch time: 0.056 trainign loss: 8.1820 avg training loss: 8.4124
batch: [7950/10547] batch time: 0.606 trainign loss: 9.6366 avg training loss: 8.4115
batch: [7960/10547] batch time: 0.056 trainign loss: 8.7419 avg training loss: 8.4122
batch: [7970/10547] batch time: 0.361 trainign loss: 9.1574 avg training loss: 8.4128
batch: [7980/10547] batch time: 0.048 trainign loss: 9.1690 avg training loss: 8.4136
batch: [7990/10547] batch time: 0.047 trainign loss: 9.0111 avg training loss: 8.4143
batch: [8000/10547] batch time: 0.051 trainign loss: 8.9368 avg training loss: 8.4147
batch: [8010/10547] batch time: 0.048 trainign loss: 8.9059 avg training loss: 8.4153
batch: [8020/10547] batch time: 0.047 trainign loss: 8.9829 avg training loss: 8.4157
batch: [8030/10547] batch time: 0.044 trainign loss: 8.9504 avg training loss: 8.4163
batch: [8040/10547] batch time: 0.316 trainign loss: 8.9112 avg training loss: 8.4169
batch: [8050/10547] batch time: 0.046 trainign loss: 8.8097 avg training loss: 8.4176
batch: [8060/10547] batch time: 0.049 trainign loss: 9.0097 avg training loss: 8.4182
batch: [8070/10547] batch time: 0.056 trainign loss: 8.9010 avg training loss: 8.4189
batch: [8080/10547] batch time: 1.098 trainign loss: 8.9314 avg training loss: 8.4195
batch: [8090/10547] batch time: 0.278 trainign loss: 8.9034 avg training loss: 8.4197
batch: [8100/10547] batch time: 0.545 trainign loss: 8.7528 avg training loss: 8.4199
batch: [8110/10547] batch time: 0.798 trainign loss: 8.8017 avg training loss: 8.4203
batch: [8120/10547] batch time: 1.063 trainign loss: 8.9335 avg training loss: 8.4209
batch: [8130/10547] batch time: 0.055 trainign loss: 8.6975 avg training loss: 8.4215
batch: [8140/10547] batch time: 0.055 trainign loss: 8.7562 avg training loss: 8.4221
batch: [8150/10547] batch time: 0.280 trainign loss: 8.9446 avg training loss: 8.4226
batch: [8160/10547] batch time: 0.047 trainign loss: 9.1404 avg training loss: 8.4232
batch: [8170/10547] batch time: 0.043 trainign loss: 8.0172 avg training loss: 8.4235
batch: [8180/10547] batch time: 1.028 trainign loss: 8.9964 avg training loss: 8.4240
batch: [8190/10547] batch time: 0.226 trainign loss: 8.7726 avg training loss: 8.4246
batch: [8200/10547] batch time: 0.056 trainign loss: 8.7983 avg training loss: 8.4251
batch: [8210/10547] batch time: 0.265 trainign loss: 6.3455 avg training loss: 8.4241
batch: [8220/10547] batch time: 0.050 trainign loss: 8.8448 avg training loss: 8.4245
batch: [8230/10547] batch time: 0.321 trainign loss: 8.7649 avg training loss: 8.4251
batch: [8240/10547] batch time: 0.051 trainign loss: 8.8842 avg training loss: 8.4256
batch: [8250/10547] batch time: 1.227 trainign loss: 9.0523 avg training loss: 8.4260
batch: [8260/10547] batch time: 0.055 trainign loss: 8.7099 avg training loss: 8.4267
batch: [8270/10547] batch time: 2.471 trainign loss: 8.9459 avg training loss: 8.4273
batch: [8280/10547] batch time: 0.055 trainign loss: 9.0850 avg training loss: 8.4279
batch: [8290/10547] batch time: 0.230 trainign loss: 8.7467 avg training loss: 8.4279
batch: [8300/10547] batch time: 0.110 trainign loss: 8.8082 avg training loss: 8.4282
batch: [8310/10547] batch time: 0.939 trainign loss: 8.9159 avg training loss: 8.4287
batch: [8320/10547] batch time: 0.051 trainign loss: 9.0447 avg training loss: 8.4292
batch: [8330/10547] batch time: 0.386 trainign loss: 9.0487 avg training loss: 8.4295
batch: [8340/10547] batch time: 0.057 trainign loss: 8.3897 avg training loss: 8.4301
batch: [8350/10547] batch time: 0.057 trainign loss: 0.3057 avg training loss: 8.4248
batch: [8360/10547] batch time: 0.056 trainign loss: 8.9481 avg training loss: 8.4256
batch: [8370/10547] batch time: 0.055 trainign loss: 9.1887 avg training loss: 8.4258
batch: [8380/10547] batch time: 0.086 trainign loss: 8.4738 avg training loss: 8.4260
batch: [8390/10547] batch time: 0.052 trainign loss: 7.1801 avg training loss: 8.4263
batch: [8400/10547] batch time: 0.334 trainign loss: 9.2062 avg training loss: 8.4263
batch: [8410/10547] batch time: 0.044 trainign loss: 8.9620 avg training loss: 8.4266
batch: [8420/10547] batch time: 0.443 trainign loss: 5.6090 avg training loss: 8.4234
batch: [8430/10547] batch time: 0.051 trainign loss: 7.8254 avg training loss: 8.4250
batch: [8440/10547] batch time: 0.057 trainign loss: 9.0073 avg training loss: 8.4253
batch: [8450/10547] batch time: 0.044 trainign loss: 9.1328 avg training loss: 8.4254
batch: [8460/10547] batch time: 0.047 trainign loss: 8.9470 avg training loss: 8.4255
batch: [8470/10547] batch time: 0.046 trainign loss: 9.0479 avg training loss: 8.4261
batch: [8480/10547] batch time: 0.053 trainign loss: 8.6823 avg training loss: 8.4267
batch: [8490/10547] batch time: 0.057 trainign loss: 8.9367 avg training loss: 8.4265
batch: [8500/10547] batch time: 0.044 trainign loss: 8.0906 avg training loss: 8.4269
batch: [8510/10547] batch time: 0.044 trainign loss: 8.9333 avg training loss: 8.4274
batch: [8520/10547] batch time: 0.056 trainign loss: 8.1839 avg training loss: 8.4277
batch: [8530/10547] batch time: 0.043 trainign loss: 7.4867 avg training loss: 8.4279
batch: [8540/10547] batch time: 1.844 trainign loss: 9.0054 avg training loss: 8.4287
batch: [8550/10547] batch time: 0.057 trainign loss: 9.1573 avg training loss: 8.4291
batch: [8560/10547] batch time: 0.058 trainign loss: 5.9820 avg training loss: 8.4288
batch: [8570/10547] batch time: 0.648 trainign loss: 10.6326 avg training loss: 8.4268
batch: [8580/10547] batch time: 0.056 trainign loss: 8.7755 avg training loss: 8.4264
batch: [8590/10547] batch time: 0.047 trainign loss: 9.3006 avg training loss: 8.4271
batch: [8600/10547] batch time: 0.044 trainign loss: 8.9193 avg training loss: 8.4278
batch: [8610/10547] batch time: 0.047 trainign loss: 9.0126 avg training loss: 8.4283
batch: [8620/10547] batch time: 0.047 trainign loss: 8.8110 avg training loss: 8.4289
batch: [8630/10547] batch time: 0.055 trainign loss: 8.0375 avg training loss: 8.4293
batch: [8640/10547] batch time: 0.057 trainign loss: 9.1143 avg training loss: 8.4298
batch: [8650/10547] batch time: 0.056 trainign loss: 9.1183 avg training loss: 8.4303
batch: [8660/10547] batch time: 0.057 trainign loss: 8.8919 avg training loss: 8.4309
batch: [8670/10547] batch time: 0.044 trainign loss: 8.8289 avg training loss: 8.4315
batch: [8680/10547] batch time: 0.055 trainign loss: 8.7619 avg training loss: 8.4320
batch: [8690/10547] batch time: 0.055 trainign loss: 0.5013 avg training loss: 8.4276
batch: [8700/10547] batch time: 0.055 trainign loss: 9.1930 avg training loss: 8.4272
batch: [8710/10547] batch time: 0.043 trainign loss: 8.4620 avg training loss: 8.4277
batch: [8720/10547] batch time: 0.050 trainign loss: 9.2222 avg training loss: 8.4284
batch: [8730/10547] batch time: 0.051 trainign loss: 9.1368 avg training loss: 8.4290
batch: [8740/10547] batch time: 0.056 trainign loss: 8.7878 avg training loss: 8.4296
batch: [8750/10547] batch time: 0.056 trainign loss: 8.8946 avg training loss: 8.4301
batch: [8760/10547] batch time: 0.626 trainign loss: 9.1926 avg training loss: 8.4303
batch: [8770/10547] batch time: 0.573 trainign loss: 9.0954 avg training loss: 8.4310
batch: [8780/10547] batch time: 0.055 trainign loss: 8.9268 avg training loss: 8.4315
batch: [8790/10547] batch time: 1.021 trainign loss: 9.1850 avg training loss: 8.4322
batch: [8800/10547] batch time: 0.046 trainign loss: 9.0904 avg training loss: 8.4329
batch: [8810/10547] batch time: 1.121 trainign loss: 9.0958 avg training loss: 8.4335
batch: [8820/10547] batch time: 0.052 trainign loss: 8.9999 avg training loss: 8.4342
batch: [8830/10547] batch time: 0.969 trainign loss: 9.0252 avg training loss: 8.4348
batch: [8840/10547] batch time: 0.052 trainign loss: 8.8240 avg training loss: 8.4353
batch: [8850/10547] batch time: 0.044 trainign loss: 9.1273 avg training loss: 8.4359
batch: [8860/10547] batch time: 0.044 trainign loss: 8.6486 avg training loss: 8.4364
batch: [8870/10547] batch time: 0.047 trainign loss: 8.9284 avg training loss: 8.4365
batch: [8880/10547] batch time: 0.045 trainign loss: 8.3181 avg training loss: 8.4369
batch: [8890/10547] batch time: 0.047 trainign loss: 9.0011 avg training loss: 8.4374
batch: [8900/10547] batch time: 0.089 trainign loss: 7.0477 avg training loss: 8.4375
batch: [8910/10547] batch time: 0.057 trainign loss: 9.3658 avg training loss: 8.4372
batch: [8920/10547] batch time: 0.056 trainign loss: 8.6330 avg training loss: 8.4379
batch: [8930/10547] batch time: 0.829 trainign loss: 9.0807 avg training loss: 8.4380
batch: [8940/10547] batch time: 1.861 trainign loss: 8.9292 avg training loss: 8.4386
batch: [8950/10547] batch time: 0.048 trainign loss: 8.7181 avg training loss: 8.4392
batch: [8960/10547] batch time: 2.487 trainign loss: 8.8734 avg training loss: 8.4398
batch: [8970/10547] batch time: 0.056 trainign loss: 8.5328 avg training loss: 8.4403
batch: [8980/10547] batch time: 0.181 trainign loss: 8.9393 avg training loss: 8.4401
batch: [8990/10547] batch time: 0.500 trainign loss: 8.8659 avg training loss: 8.4407
batch: [9000/10547] batch time: 0.056 trainign loss: 5.2375 avg training loss: 8.4401
batch: [9010/10547] batch time: 0.057 trainign loss: 9.3080 avg training loss: 8.4394
batch: [9020/10547] batch time: 0.043 trainign loss: 7.9387 avg training loss: 8.4395
batch: [9030/10547] batch time: 0.043 trainign loss: 8.4262 avg training loss: 8.4401
batch: [9040/10547] batch time: 0.057 trainign loss: 9.0198 avg training loss: 8.4407
batch: [9050/10547] batch time: 0.240 trainign loss: 8.9263 avg training loss: 8.4408
batch: [9060/10547] batch time: 0.056 trainign loss: 8.8735 avg training loss: 8.4411
batch: [9070/10547] batch time: 0.224 trainign loss: 7.8368 avg training loss: 8.4414
batch: [9080/10547] batch time: 0.542 trainign loss: 9.0364 avg training loss: 8.4417
batch: [9090/10547] batch time: 0.219 trainign loss: 8.3593 avg training loss: 8.4423
batch: [9100/10547] batch time: 0.056 trainign loss: 8.3504 avg training loss: 8.4427
batch: [9110/10547] batch time: 0.056 trainign loss: 9.0979 avg training loss: 8.4432
batch: [9120/10547] batch time: 0.054 trainign loss: 9.1045 avg training loss: 8.4437
batch: [9130/10547] batch time: 0.057 trainign loss: 3.6604 avg training loss: 8.4421
batch: [9140/10547] batch time: 0.241 trainign loss: 9.0843 avg training loss: 8.4407
batch: [9150/10547] batch time: 0.056 trainign loss: 9.0070 avg training loss: 8.4412
batch: [9160/10547] batch time: 0.057 trainign loss: 7.7983 avg training loss: 8.4415
batch: [9170/10547] batch time: 0.043 trainign loss: 0.1074 avg training loss: 8.4359
batch: [9180/10547] batch time: 0.307 trainign loss: 9.0428 avg training loss: 8.4381
batch: [9190/10547] batch time: 0.047 trainign loss: 9.0564 avg training loss: 8.4385
batch: [9200/10547] batch time: 0.595 trainign loss: 8.8857 avg training loss: 8.4387
batch: [9210/10547] batch time: 0.048 trainign loss: 8.9526 avg training loss: 8.4392
batch: [9220/10547] batch time: 0.046 trainign loss: 8.0902 avg training loss: 8.4397
batch: [9230/10547] batch time: 0.054 trainign loss: 9.2297 avg training loss: 8.4397
batch: [9240/10547] batch time: 0.577 trainign loss: 8.8906 avg training loss: 8.4404
batch: [9250/10547] batch time: 0.058 trainign loss: 3.3847 avg training loss: 8.4388
batch: [9260/10547] batch time: 0.687 trainign loss: 15.2438 avg training loss: 8.4335
batch: [9270/10547] batch time: 0.046 trainign loss: 9.1492 avg training loss: 8.4349
batch: [9280/10547] batch time: 0.477 trainign loss: 9.0492 avg training loss: 8.4355
batch: [9290/10547] batch time: 0.056 trainign loss: 9.0202 avg training loss: 8.4360
batch: [9300/10547] batch time: 0.049 trainign loss: 9.1149 avg training loss: 8.4365
batch: [9310/10547] batch time: 0.058 trainign loss: 9.1076 avg training loss: 8.4368
batch: [9320/10547] batch time: 0.047 trainign loss: 9.1468 avg training loss: 8.4372
batch: [9330/10547] batch time: 1.053 trainign loss: 9.0672 avg training loss: 8.4376
batch: [9340/10547] batch time: 0.055 trainign loss: 7.3519 avg training loss: 8.4378
batch: [9350/10547] batch time: 1.178 trainign loss: 9.1143 avg training loss: 8.4382
batch: [9360/10547] batch time: 0.047 trainign loss: 9.1937 avg training loss: 8.4382
batch: [9370/10547] batch time: 2.169 trainign loss: 8.9680 avg training loss: 8.4389
batch: [9380/10547] batch time: 0.047 trainign loss: 8.8811 avg training loss: 8.4394
batch: [9390/10547] batch time: 2.007 trainign loss: 9.0771 avg training loss: 8.4396
batch: [9400/10547] batch time: 0.043 trainign loss: 6.3100 avg training loss: 8.4395
batch: [9410/10547] batch time: 1.228 trainign loss: 8.9279 avg training loss: 8.4400
batch: [9420/10547] batch time: 0.046 trainign loss: 9.1886 avg training loss: 8.4406
batch: [9430/10547] batch time: 0.402 trainign loss: 8.8712 avg training loss: 8.4411
batch: [9440/10547] batch time: 0.056 trainign loss: 8.8594 avg training loss: 8.4416
batch: [9450/10547] batch time: 0.054 trainign loss: 8.4804 avg training loss: 8.4417
batch: [9460/10547] batch time: 0.046 trainign loss: 9.1193 avg training loss: 8.4424
batch: [9470/10547] batch time: 0.054 trainign loss: 7.9451 avg training loss: 8.4428
batch: [9480/10547] batch time: 0.042 trainign loss: 8.4540 avg training loss: 8.4433
batch: [9490/10547] batch time: 0.056 trainign loss: 9.2023 avg training loss: 8.4439
batch: [9500/10547] batch time: 0.057 trainign loss: 8.9198 avg training loss: 8.4445
batch: [9510/10547] batch time: 0.046 trainign loss: 9.0283 avg training loss: 8.4448
batch: [9520/10547] batch time: 0.056 trainign loss: 8.8172 avg training loss: 8.4452
batch: [9530/10547] batch time: 0.050 trainign loss: 9.1560 avg training loss: 8.4454
batch: [9540/10547] batch time: 0.056 trainign loss: 7.1840 avg training loss: 8.4456
batch: [9550/10547] batch time: 0.056 trainign loss: 9.2186 avg training loss: 8.4458
batch: [9560/10547] batch time: 0.730 trainign loss: 9.1209 avg training loss: 8.4464
batch: [9570/10547] batch time: 0.049 trainign loss: 9.0757 avg training loss: 8.4470
batch: [9580/10547] batch time: 0.709 trainign loss: 9.1429 avg training loss: 8.4475
batch: [9590/10547] batch time: 0.057 trainign loss: 7.8622 avg training loss: 8.4479
batch: [9600/10547] batch time: 1.934 trainign loss: 8.8052 avg training loss: 8.4483
batch: [9610/10547] batch time: 0.051 trainign loss: 8.8927 avg training loss: 8.4489
batch: [9620/10547] batch time: 0.042 trainign loss: 8.8394 avg training loss: 8.4494
batch: [9630/10547] batch time: 0.047 trainign loss: 8.9581 avg training loss: 8.4500
batch: [9640/10547] batch time: 0.055 trainign loss: 9.1286 avg training loss: 8.4506
batch: [9650/10547] batch time: 0.047 trainign loss: 9.1418 avg training loss: 8.4511
batch: [9660/10547] batch time: 0.050 trainign loss: 9.0095 avg training loss: 8.4516
batch: [9670/10547] batch time: 0.047 trainign loss: 9.0105 avg training loss: 8.4521
batch: [9680/10547] batch time: 0.056 trainign loss: 9.0160 avg training loss: 8.4527
batch: [9690/10547] batch time: 1.544 trainign loss: 8.8735 avg training loss: 8.4530
batch: [9700/10547] batch time: 0.056 trainign loss: 8.6476 avg training loss: 8.4535
batch: [9710/10547] batch time: 0.470 trainign loss: 9.0318 avg training loss: 8.4541
batch: [9720/10547] batch time: 0.412 trainign loss: 9.0096 avg training loss: 8.4546
batch: [9730/10547] batch time: 0.256 trainign loss: 4.5153 avg training loss: 8.4537
batch: [9740/10547] batch time: 0.055 trainign loss: 9.2383 avg training loss: 8.4527
batch: [9750/10547] batch time: 0.305 trainign loss: 8.9720 avg training loss: 8.4533
batch: [9760/10547] batch time: 0.046 trainign loss: 9.0915 avg training loss: 8.4538
batch: [9770/10547] batch time: 0.815 trainign loss: 8.9573 avg training loss: 8.4544
batch: [9780/10547] batch time: 0.054 trainign loss: 9.1031 avg training loss: 8.4550
batch: [9790/10547] batch time: 0.302 trainign loss: 9.1237 avg training loss: 8.4555
batch: [9800/10547] batch time: 0.046 trainign loss: 9.1080 avg training loss: 8.4557
batch: [9810/10547] batch time: 0.057 trainign loss: 9.1670 avg training loss: 8.4564
batch: [9820/10547] batch time: 0.047 trainign loss: 9.1999 avg training loss: 8.4570
batch: [9830/10547] batch time: 0.546 trainign loss: 9.1094 avg training loss: 8.4576
batch: [9840/10547] batch time: 0.057 trainign loss: 4.5959 avg training loss: 8.4566
batch: [9850/10547] batch time: 0.056 trainign loss: 3.6720 avg training loss: 8.4548
batch: [9860/10547] batch time: 0.056 trainign loss: 8.7608 avg training loss: 8.4553
batch: [9870/10547] batch time: 0.057 trainign loss: 8.1903 avg training loss: 8.4553
batch: [9880/10547] batch time: 0.840 trainign loss: 9.1745 avg training loss: 8.4556
batch: [9890/10547] batch time: 0.055 trainign loss: 9.1534 avg training loss: 8.4563
batch: [9900/10547] batch time: 0.048 trainign loss: 8.9832 avg training loss: 8.4567
batch: [9910/10547] batch time: 0.057 trainign loss: 6.3967 avg training loss: 8.4566
batch: [9920/10547] batch time: 0.057 trainign loss: 9.5391 avg training loss: 8.4557
batch: [9930/10547] batch time: 0.597 trainign loss: 9.0452 avg training loss: 8.4560
batch: [9940/10547] batch time: 0.051 trainign loss: 9.0866 avg training loss: 8.4556
batch: [9950/10547] batch time: 1.751 trainign loss: 9.1449 avg training loss: 8.4562
batch: [9960/10547] batch time: 0.056 trainign loss: 8.4497 avg training loss: 8.4567
batch: [9970/10547] batch time: 1.457 trainign loss: 9.5701 avg training loss: 8.4551
batch: [9980/10547] batch time: 0.162 trainign loss: 9.2786 avg training loss: 8.4563
batch: [9990/10547] batch time: 2.144 trainign loss: 8.9645 avg training loss: 8.4568
batch: [10000/10547] batch time: 0.054 trainign loss: 9.1918 avg training loss: 8.4573
batch: [10010/10547] batch time: 2.200 trainign loss: 9.0624 avg training loss: 8.4576
batch: [10020/10547] batch time: 0.044 trainign loss: 9.3012 avg training loss: 8.4580
batch: [10030/10547] batch time: 2.299 trainign loss: 9.2352 avg training loss: 8.4586
batch: [10040/10547] batch time: 0.047 trainign loss: 8.3541 avg training loss: 8.4590
batch: [10050/10547] batch time: 2.793 trainign loss: 8.7954 avg training loss: 8.4596
batch: [10060/10547] batch time: 0.046 trainign loss: 9.1815 avg training loss: 8.4587
batch: [10070/10547] batch time: 2.292 trainign loss: 9.1274 avg training loss: 8.4594
batch: [10080/10547] batch time: 0.047 trainign loss: 8.8205 avg training loss: 8.4597
batch: [10090/10547] batch time: 2.354 trainign loss: 9.2540 avg training loss: 8.4597
batch: [10100/10547] batch time: 0.047 trainign loss: 8.2779 avg training loss: 8.4603
batch: [10110/10547] batch time: 2.114 trainign loss: 8.9445 avg training loss: 8.4606
batch: [10120/10547] batch time: 0.056 trainign loss: 8.5132 avg training loss: 8.4611
batch: [10130/10547] batch time: 2.053 trainign loss: 8.0164 avg training loss: 8.4608
batch: [10140/10547] batch time: 0.048 trainign loss: 8.4080 avg training loss: 8.4612
batch: [10150/10547] batch time: 1.914 trainign loss: 8.9270 avg training loss: 8.4618
batch: [10160/10547] batch time: 0.055 trainign loss: 1.5718 avg training loss: 8.4589
batch: [10170/10547] batch time: 1.740 trainign loss: 10.6643 avg training loss: 8.4568
batch: [10180/10547] batch time: 0.046 trainign loss: 9.1563 avg training loss: 8.4575
batch: [10190/10547] batch time: 2.348 trainign loss: 6.4167 avg training loss: 8.4572
batch: [10200/10547] batch time: 0.050 trainign loss: 9.2745 avg training loss: 8.4579
batch: [10210/10547] batch time: 1.921 trainign loss: 8.9382 avg training loss: 8.4581
batch: [10220/10547] batch time: 0.056 trainign loss: 8.8816 avg training loss: 8.4586
batch: [10230/10547] batch time: 1.847 trainign loss: 9.0593 avg training loss: 8.4592
batch: [10240/10547] batch time: 0.047 trainign loss: 9.0347 avg training loss: 8.4597
batch: [10250/10547] batch time: 2.174 trainign loss: 8.9097 avg training loss: 8.4602
batch: [10260/10547] batch time: 0.049 trainign loss: 8.9993 avg training loss: 8.4607
batch: [10270/10547] batch time: 2.303 trainign loss: 8.9753 avg training loss: 8.4613
batch: [10280/10547] batch time: 0.056 trainign loss: 9.2319 avg training loss: 8.4620
batch: [10290/10547] batch time: 2.286 trainign loss: 8.9394 avg training loss: 8.4625
batch: [10300/10547] batch time: 0.057 trainign loss: 9.1314 avg training loss: 8.4631
batch: [10310/10547] batch time: 2.232 trainign loss: 9.1405 avg training loss: 8.4635
batch: [10320/10547] batch time: 0.047 trainign loss: 9.1593 avg training loss: 8.4642
batch: [10330/10547] batch time: 2.179 trainign loss: 8.7337 avg training loss: 8.4645
batch: [10340/10547] batch time: 0.056 trainign loss: 9.1105 avg training loss: 8.4650
batch: [10350/10547] batch time: 1.352 trainign loss: 9.0796 avg training loss: 8.4656
batch: [10360/10547] batch time: 0.047 trainign loss: 8.9195 avg training loss: 8.4659
batch: [10370/10547] batch time: 0.629 trainign loss: 5.9751 avg training loss: 8.4657
batch: [10380/10547] batch time: 0.047 trainign loss: 8.6694 avg training loss: 8.4656
batch: [10390/10547] batch time: 0.915 trainign loss: 9.1574 avg training loss: 8.4658
batch: [10400/10547] batch time: 0.056 trainign loss: 9.0628 avg training loss: 8.4663
batch: [10410/10547] batch time: 0.157 trainign loss: 8.9647 avg training loss: 8.4668
batch: [10420/10547] batch time: 0.049 trainign loss: 9.1001 avg training loss: 8.4673
batch: [10430/10547] batch time: 0.047 trainign loss: 8.8681 avg training loss: 8.4678
batch: [10440/10547] batch time: 0.056 trainign loss: 8.7957 avg training loss: 8.4681
batch: [10450/10547] batch time: 0.048 trainign loss: 9.1070 avg training loss: 8.4684
batch: [10460/10547] batch time: 0.048 trainign loss: 8.9482 avg training loss: 8.4686
batch: [10470/10547] batch time: 0.048 trainign loss: 8.8751 avg training loss: 8.4692
batch: [10480/10547] batch time: 0.047 trainign loss: 7.9499 avg training loss: 8.4695
batch: [10490/10547] batch time: 0.047 trainign loss: 9.2342 avg training loss: 8.4694
batch: [10500/10547] batch time: 0.048 trainign loss: 9.0712 avg training loss: 8.4698
batch: [10510/10547] batch time: 0.056 trainign loss: 9.1269 avg training loss: 8.4704
batch: [10520/10547] batch time: 0.047 trainign loss: 8.9105 avg training loss: 8.4709
batch: [10530/10547] batch time: 0.679 trainign loss: 9.0663 avg training loss: 8.4709
batch: [10540/10547] batch time: 0.569 trainign loss: 9.1042 avg training loss: 8.4713
Epoch: 2
----------------------------------------------------------------------
batch: [0/10547] batch time: 2.500 trainign loss: 8.7206 avg training loss: 8.4716
batch: [10/10547] batch time: 0.048 trainign loss: 8.8725 avg training loss: 8.4705
batch: [20/10547] batch time: 1.973 trainign loss: 8.8777 avg training loss: 8.4710
batch: [30/10547] batch time: 0.326 trainign loss: 8.6465 avg training loss: 8.4709
batch: [40/10547] batch time: 0.916 trainign loss: 8.6588 avg training loss: 8.4710
batch: [50/10547] batch time: 0.051 trainign loss: 8.7399 avg training loss: 8.4710
batch: [60/10547] batch time: 1.119 trainign loss: 8.1886 avg training loss: 8.4710
batch: [70/10547] batch time: 0.049 trainign loss: 8.6638 avg training loss: 8.4708
batch: [80/10547] batch time: 0.344 trainign loss: 8.6550 avg training loss: 8.4706
batch: [90/10547] batch time: 0.635 trainign loss: 8.4051 avg training loss: 8.4707
batch: [100/10547] batch time: 0.400 trainign loss: 7.0217 avg training loss: 8.4704
batch: [110/10547] batch time: 0.741 trainign loss: 0.0664 avg training loss: 8.4651
batch: [120/10547] batch time: 0.046 trainign loss: 8.6521 avg training loss: 8.4661
batch: [130/10547] batch time: 0.457 trainign loss: 8.5985 avg training loss: 8.4662
batch: [140/10547] batch time: 0.046 trainign loss: 8.4574 avg training loss: 8.4662
batch: [150/10547] batch time: 0.506 trainign loss: 7.6403 avg training loss: 8.4660
batch: [160/10547] batch time: 0.056 trainign loss: 8.5986 avg training loss: 8.4656
batch: [170/10547] batch time: 0.049 trainign loss: 8.0654 avg training loss: 8.4655
batch: [180/10547] batch time: 0.049 trainign loss: 8.4545 avg training loss: 8.4654
batch: [190/10547] batch time: 0.056 trainign loss: 8.4449 avg training loss: 8.4653
batch: [200/10547] batch time: 0.043 trainign loss: 8.3964 avg training loss: 8.4652
batch: [210/10547] batch time: 0.203 trainign loss: 8.4063 avg training loss: 8.4648
batch: [220/10547] batch time: 0.051 trainign loss: 8.5503 avg training loss: 8.4649
batch: [230/10547] batch time: 0.046 trainign loss: 8.5877 avg training loss: 8.4647
batch: [240/10547] batch time: 0.047 trainign loss: 8.5561 avg training loss: 8.4643
batch: [250/10547] batch time: 0.052 trainign loss: 8.5016 avg training loss: 8.4644
batch: [260/10547] batch time: 0.351 trainign loss: 8.5629 avg training loss: 8.4643
batch: [270/10547] batch time: 0.666 trainign loss: 8.2504 avg training loss: 8.4639
batch: [280/10547] batch time: 0.046 trainign loss: 7.9022 avg training loss: 8.4638
batch: [290/10547] batch time: 0.938 trainign loss: 8.0920 avg training loss: 8.4633
batch: [300/10547] batch time: 0.057 trainign loss: 8.0450 avg training loss: 8.4630
batch: [310/10547] batch time: 1.050 trainign loss: 8.3526 avg training loss: 8.4626
batch: [320/10547] batch time: 0.046 trainign loss: 8.5728 avg training loss: 8.4627
batch: [330/10547] batch time: 1.635 trainign loss: 8.2908 avg training loss: 8.4626
batch: [340/10547] batch time: 0.047 trainign loss: 8.3381 avg training loss: 8.4621
batch: [350/10547] batch time: 1.354 trainign loss: 8.6186 avg training loss: 8.4613
batch: [360/10547] batch time: 0.046 trainign loss: 8.5787 avg training loss: 8.4611
batch: [370/10547] batch time: 1.214 trainign loss: 8.5937 avg training loss: 8.4609
batch: [380/10547] batch time: 0.055 trainign loss: 8.5460 avg training loss: 8.4610
batch: [390/10547] batch time: 2.062 trainign loss: 8.5859 avg training loss: 8.4611
batch: [400/10547] batch time: 0.049 trainign loss: 8.5983 avg training loss: 8.4608
batch: [410/10547] batch time: 2.479 trainign loss: 8.3304 avg training loss: 8.4607
batch: [420/10547] batch time: 0.046 trainign loss: 8.4403 avg training loss: 8.4605
batch: [430/10547] batch time: 2.025 trainign loss: 8.4417 avg training loss: 8.4603
batch: [440/10547] batch time: 0.046 trainign loss: 6.9124 avg training loss: 8.4598
batch: [450/10547] batch time: 1.892 trainign loss: 8.6090 avg training loss: 8.4595
batch: [460/10547] batch time: 0.047 trainign loss: 8.2369 avg training loss: 8.4593
batch: [470/10547] batch time: 2.404 trainign loss: 4.6330 avg training loss: 8.4582
batch: [480/10547] batch time: 0.056 trainign loss: 14.0146 avg training loss: 8.4534
batch: [490/10547] batch time: 2.342 trainign loss: 8.6031 avg training loss: 8.4552
batch: [500/10547] batch time: 0.056 trainign loss: 8.7184 avg training loss: 8.4550
batch: [510/10547] batch time: 2.191 trainign loss: 8.1118 avg training loss: 8.4551
batch: [520/10547] batch time: 0.046 trainign loss: 8.5386 avg training loss: 8.4550
batch: [530/10547] batch time: 2.074 trainign loss: 3.2969 avg training loss: 8.4533
batch: [540/10547] batch time: 0.047 trainign loss: 9.3817 avg training loss: 8.4514
batch: [550/10547] batch time: 2.639 trainign loss: 8.3496 avg training loss: 8.4511
batch: [560/10547] batch time: 0.055 trainign loss: 8.6388 avg training loss: 8.4510
batch: [570/10547] batch time: 1.996 trainign loss: 8.3058 avg training loss: 8.4510
batch: [580/10547] batch time: 0.056 trainign loss: 8.3967 avg training loss: 8.4511
batch: [590/10547] batch time: 2.448 trainign loss: 8.5786 avg training loss: 8.4508
batch: [600/10547] batch time: 0.046 trainign loss: 7.4477 avg training loss: 8.4505
batch: [610/10547] batch time: 2.074 trainign loss: 8.4247 avg training loss: 8.4504
batch: [620/10547] batch time: 0.053 trainign loss: 7.8482 avg training loss: 8.4502
batch: [630/10547] batch time: 1.628 trainign loss: 8.2627 avg training loss: 8.4498
batch: [640/10547] batch time: 0.046 trainign loss: 5.7076 avg training loss: 8.4490
batch: [650/10547] batch time: 1.446 trainign loss: 7.6911 avg training loss: 8.4483
batch: [660/10547] batch time: 0.046 trainign loss: 6.9777 avg training loss: 8.4475
batch: [670/10547] batch time: 0.428 trainign loss: 7.6464 avg training loss: 8.4459
batch: [680/10547] batch time: 0.043 trainign loss: 7.6966 avg training loss: 8.4454
batch: [690/10547] batch time: 0.046 trainign loss: 7.2769 avg training loss: 8.4445
batch: [700/10547] batch time: 0.043 trainign loss: 7.2434 avg training loss: 8.4435
batch: [710/10547] batch time: 0.048 trainign loss: 7.7295 avg training loss: 8.4428
batch: [720/10547] batch time: 0.047 trainign loss: 5.2256 avg training loss: 8.4415
batch: [730/10547] batch time: 0.047 trainign loss: 11.8351 avg training loss: 8.4363
batch: [740/10547] batch time: 0.045 trainign loss: 7.1760 avg training loss: 8.4373
batch: [750/10547] batch time: 0.056 trainign loss: 7.2385 avg training loss: 8.4369
batch: [760/10547] batch time: 0.049 trainign loss: 3.8904 avg training loss: 8.4348
batch: [770/10547] batch time: 0.056 trainign loss: 7.8452 avg training loss: 8.4348
batch: [780/10547] batch time: 0.047 trainign loss: 8.0802 avg training loss: 8.4342
batch: [790/10547] batch time: 0.047 trainign loss: 7.7641 avg training loss: 8.4338
batch: [800/10547] batch time: 0.047 trainign loss: 7.9428 avg training loss: 8.4333
batch: [810/10547] batch time: 0.047 trainign loss: 6.3781 avg training loss: 8.4323
batch: [820/10547] batch time: 0.046 trainign loss: 7.6394 avg training loss: 8.4315
batch: [830/10547] batch time: 0.048 trainign loss: 8.0973 avg training loss: 8.4309
batch: [840/10547] batch time: 0.048 trainign loss: 7.1601 avg training loss: 8.4301
batch: [850/10547] batch time: 0.055 trainign loss: 7.4650 avg training loss: 8.4294
batch: [860/10547] batch time: 0.048 trainign loss: 7.4660 avg training loss: 8.4280
batch: [870/10547] batch time: 0.046 trainign loss: 7.6309 avg training loss: 8.4275
batch: [880/10547] batch time: 0.055 trainign loss: 6.8665 avg training loss: 8.4268
batch: [890/10547] batch time: 0.055 trainign loss: 6.4422 avg training loss: 8.4243
batch: [900/10547] batch time: 0.052 trainign loss: 7.2657 avg training loss: 8.4239
batch: [910/10547] batch time: 0.047 trainign loss: 6.5019 avg training loss: 8.4232
batch: [920/10547] batch time: 0.056 trainign loss: 8.0734 avg training loss: 8.4220
batch: [930/10547] batch time: 0.047 trainign loss: 6.6093 avg training loss: 8.4211
batch: [940/10547] batch time: 0.046 trainign loss: 7.8330 avg training loss: 8.4201
batch: [950/10547] batch time: 0.056 trainign loss: 7.1473 avg training loss: 8.4192
batch: [960/10547] batch time: 0.047 trainign loss: 2.0156 avg training loss: 8.4164
batch: [970/10547] batch time: 0.051 trainign loss: 7.8213 avg training loss: 8.4162
batch: [980/10547] batch time: 0.048 trainign loss: 6.5066 avg training loss: 8.4142
batch: [990/10547] batch time: 0.047 trainign loss: 3.2992 avg training loss: 8.4113
batch: [1000/10547] batch time: 0.044 trainign loss: 11.9268 avg training loss: 8.4063
batch: [1010/10547] batch time: 0.047 trainign loss: 8.0373 avg training loss: 8.4070
batch: [1020/10547] batch time: 0.046 trainign loss: 0.4404 avg training loss: 8.4033
batch: [1030/10547] batch time: 0.051 trainign loss: 0.0005 avg training loss: 8.3961
batch: [1040/10547] batch time: 0.046 trainign loss: 17.1751 avg training loss: 8.3938
batch: [1050/10547] batch time: 0.047 trainign loss: 8.4194 avg training loss: 8.3956
batch: [1060/10547] batch time: 0.048 trainign loss: 8.0568 avg training loss: 8.3955
batch: [1070/10547] batch time: 0.047 trainign loss: 8.2506 avg training loss: 8.3952
batch: [1080/10547] batch time: 0.048 trainign loss: 5.5904 avg training loss: 8.3941
batch: [1090/10547] batch time: 0.054 trainign loss: 7.5692 avg training loss: 8.3933
batch: [1100/10547] batch time: 0.054 trainign loss: 5.8243 avg training loss: 8.3917
batch: [1110/10547] batch time: 0.047 trainign loss: 7.1879 avg training loss: 8.3896
batch: [1120/10547] batch time: 0.047 trainign loss: 7.1734 avg training loss: 8.3876
batch: [1130/10547] batch time: 0.055 trainign loss: 7.5039 avg training loss: 8.3860
batch: [1140/10547] batch time: 0.047 trainign loss: 7.5995 avg training loss: 8.3853
batch: [1150/10547] batch time: 0.049 trainign loss: 7.1722 avg training loss: 8.3845
batch: [1160/10547] batch time: 0.047 trainign loss: 6.9513 avg training loss: 8.3834
batch: [1170/10547] batch time: 0.047 trainign loss: 7.4336 avg training loss: 8.3826
batch: [1180/10547] batch time: 0.051 trainign loss: 7.6205 avg training loss: 8.3820
batch: [1190/10547] batch time: 0.047 trainign loss: 7.4449 avg training loss: 8.3813
batch: [1200/10547] batch time: 0.044 trainign loss: 6.2925 avg training loss: 8.3802
batch: [1210/10547] batch time: 0.047 trainign loss: 6.0324 avg training loss: 8.3791
batch: [1220/10547] batch time: 0.044 trainign loss: 7.0761 avg training loss: 8.3779
batch: [1230/10547] batch time: 0.047 trainign loss: 7.2470 avg training loss: 8.3769
batch: [1240/10547] batch time: 0.047 trainign loss: 7.5730 avg training loss: 8.3761
batch: [1250/10547] batch time: 0.051 trainign loss: 7.3999 avg training loss: 8.3750
batch: [1260/10547] batch time: 0.047 trainign loss: 0.5807 avg training loss: 8.3711
batch: [1270/10547] batch time: 0.056 trainign loss: 8.0765 avg training loss: 8.3711
batch: [1280/10547] batch time: 0.046 trainign loss: 5.5692 avg training loss: 8.3703
batch: [1290/10547] batch time: 0.048 trainign loss: 8.1310 avg training loss: 8.3699
batch: [1300/10547] batch time: 0.047 trainign loss: 7.4405 avg training loss: 8.3691
batch: [1310/10547] batch time: 0.047 trainign loss: 6.7168 avg training loss: 8.3682
batch: [1320/10547] batch time: 0.045 trainign loss: 5.5341 avg training loss: 8.3665
batch: [1330/10547] batch time: 0.050 trainign loss: 7.1243 avg training loss: 8.3650
batch: [1340/10547] batch time: 0.048 trainign loss: 7.8795 avg training loss: 8.3641
batch: [1350/10547] batch time: 0.048 trainign loss: 7.7692 avg training loss: 8.3635
batch: [1360/10547] batch time: 0.045 trainign loss: 7.6546 avg training loss: 8.3630
batch: [1370/10547] batch time: 0.047 trainign loss: 5.8986 avg training loss: 8.3621
batch: [1380/10547] batch time: 0.056 trainign loss: 6.5434 avg training loss: 8.3607
batch: [1390/10547] batch time: 0.047 trainign loss: 9.5237 avg training loss: 8.3576
batch: [1400/10547] batch time: 0.056 trainign loss: 7.8829 avg training loss: 8.3573
batch: [1410/10547] batch time: 0.047 trainign loss: 7.6243 avg training loss: 8.3562
batch: [1420/10547] batch time: 0.047 trainign loss: 6.8843 avg training loss: 8.3555
batch: [1430/10547] batch time: 0.056 trainign loss: 7.0158 avg training loss: 8.3542
batch: [1440/10547] batch time: 0.047 trainign loss: 6.8732 avg training loss: 8.3531
batch: [1450/10547] batch time: 0.047 trainign loss: 3.4491 avg training loss: 8.3507
batch: [1460/10547] batch time: 0.055 trainign loss: 5.3548 avg training loss: 8.3486
batch: [1470/10547] batch time: 0.055 trainign loss: 7.2039 avg training loss: 8.3479
batch: [1480/10547] batch time: 0.047 trainign loss: 7.7308 avg training loss: 8.3461
batch: [1490/10547] batch time: 0.049 trainign loss: 6.9680 avg training loss: 8.3453
batch: [1500/10547] batch time: 0.049 trainign loss: 7.5876 avg training loss: 8.3445
batch: [1510/10547] batch time: 0.047 trainign loss: 7.4070 avg training loss: 8.3433
batch: [1520/10547] batch time: 0.046 trainign loss: 6.7080 avg training loss: 8.3422
batch: [1530/10547] batch time: 0.046 trainign loss: 7.0954 avg training loss: 8.3412
batch: [1540/10547] batch time: 0.047 trainign loss: 1.6784 avg training loss: 8.3385
batch: [1550/10547] batch time: 0.048 trainign loss: 6.1348 avg training loss: 8.3378
batch: [1560/10547] batch time: 0.047 trainign loss: 8.0591 avg training loss: 8.3365
batch: [1570/10547] batch time: 0.055 trainign loss: 6.6929 avg training loss: 8.3356
batch: [1580/10547] batch time: 0.043 trainign loss: 7.7062 avg training loss: 8.3344
batch: [1590/10547] batch time: 0.046 trainign loss: 4.2537 avg training loss: 8.3327
batch: [1600/10547] batch time: 0.043 trainign loss: 6.3115 avg training loss: 8.3316
batch: [1610/10547] batch time: 0.047 trainign loss: 2.7217 avg training loss: 8.3287
batch: [1620/10547] batch time: 0.051 trainign loss: 6.6924 avg training loss: 8.3276
batch: [1630/10547] batch time: 0.046 trainign loss: 3.0128 avg training loss: 8.3254
batch: [1640/10547] batch time: 0.056 trainign loss: 6.5954 avg training loss: 8.3242
batch: [1650/10547] batch time: 0.055 trainign loss: 6.5851 avg training loss: 8.3227
batch: [1660/10547] batch time: 0.047 trainign loss: 6.3344 avg training loss: 8.3208
batch: [1670/10547] batch time: 0.046 trainign loss: 3.7679 avg training loss: 8.3185
batch: [1680/10547] batch time: 0.048 trainign loss: 6.8304 avg training loss: 8.3171
batch: [1690/10547] batch time: 0.046 trainign loss: 7.3710 avg training loss: 8.3162
batch: [1700/10547] batch time: 0.046 trainign loss: 7.4577 avg training loss: 8.3147
batch: [1710/10547] batch time: 0.047 trainign loss: 7.1324 avg training loss: 8.3137
batch: [1720/10547] batch time: 0.440 trainign loss: 7.1648 avg training loss: 8.3125
batch: [1730/10547] batch time: 0.228 trainign loss: 7.6894 avg training loss: 8.3115
batch: [1740/10547] batch time: 0.055 trainign loss: 7.2658 avg training loss: 8.3107
batch: [1750/10547] batch time: 0.843 trainign loss: 6.7462 avg training loss: 8.3097
batch: [1760/10547] batch time: 0.055 trainign loss: 3.6016 avg training loss: 8.3071
batch: [1770/10547] batch time: 0.714 trainign loss: 7.2797 avg training loss: 8.3066
batch: [1780/10547] batch time: 0.047 trainign loss: 7.3647 avg training loss: 8.3048
batch: [1790/10547] batch time: 0.858 trainign loss: 6.1931 avg training loss: 8.3039
batch: [1800/10547] batch time: 0.055 trainign loss: 6.7017 avg training loss: 8.3026
batch: [1810/10547] batch time: 0.315 trainign loss: 7.3168 avg training loss: 8.3018
batch: [1820/10547] batch time: 0.046 trainign loss: 7.3194 avg training loss: 8.3008
batch: [1830/10547] batch time: 0.306 trainign loss: 6.4478 avg training loss: 8.2995
batch: [1840/10547] batch time: 0.050 trainign loss: 7.4463 avg training loss: 8.2986
batch: [1850/10547] batch time: 0.951 trainign loss: 6.5066 avg training loss: 8.2967
batch: [1860/10547] batch time: 0.056 trainign loss: 5.5945 avg training loss: 8.2953
batch: [1870/10547] batch time: 1.231 trainign loss: 7.5784 avg training loss: 8.2943
batch: [1880/10547] batch time: 0.047 trainign loss: 6.5834 avg training loss: 8.2935
batch: [1890/10547] batch time: 2.026 trainign loss: 7.4616 avg training loss: 8.2925
batch: [1900/10547] batch time: 0.056 trainign loss: 3.4223 avg training loss: 8.2908
batch: [1910/10547] batch time: 1.347 trainign loss: 1.6878 avg training loss: 8.2847
batch: [1920/10547] batch time: 0.047 trainign loss: 8.5911 avg training loss: 8.2862
batch: [1930/10547] batch time: 0.241 trainign loss: 7.5446 avg training loss: 8.2861
batch: [1940/10547] batch time: 0.045 trainign loss: 7.3430 avg training loss: 8.2854
batch: [1950/10547] batch time: 0.047 trainign loss: 7.8051 avg training loss: 8.2848
batch: [1960/10547] batch time: 0.047 trainign loss: 7.3605 avg training loss: 8.2832
batch: [1970/10547] batch time: 0.048 trainign loss: 6.2330 avg training loss: 8.2822
batch: [1980/10547] batch time: 0.043 trainign loss: 7.1439 avg training loss: 8.2809
batch: [1990/10547] batch time: 0.200 trainign loss: 0.3430 avg training loss: 8.2773
batch: [2000/10547] batch time: 0.046 trainign loss: 9.3132 avg training loss: 8.2751
batch: [2010/10547] batch time: 0.048 trainign loss: 7.9690 avg training loss: 8.2748
batch: [2020/10547] batch time: 0.056 trainign loss: 7.4190 avg training loss: 8.2743
batch: [2030/10547] batch time: 0.049 trainign loss: 5.4553 avg training loss: 8.2731
batch: [2040/10547] batch time: 0.047 trainign loss: 7.3058 avg training loss: 8.2712
batch: [2050/10547] batch time: 0.055 trainign loss: 7.3978 avg training loss: 8.2705
batch: [2060/10547] batch time: 0.043 trainign loss: 5.6857 avg training loss: 8.2696
batch: [2070/10547] batch time: 0.047 trainign loss: 4.4648 avg training loss: 8.2684
batch: [2080/10547] batch time: 0.046 trainign loss: 6.0841 avg training loss: 8.2663
batch: [2090/10547] batch time: 0.046 trainign loss: 6.4802 avg training loss: 8.2649
batch: [2100/10547] batch time: 0.045 trainign loss: 0.1342 avg training loss: 8.2606
batch: [2110/10547] batch time: 0.055 trainign loss: 7.6845 avg training loss: 8.2609
batch: [2120/10547] batch time: 0.048 trainign loss: 6.3416 avg training loss: 8.2602
batch: [2130/10547] batch time: 0.049 trainign loss: 7.0911 avg training loss: 8.2596
batch: [2140/10547] batch time: 0.047 trainign loss: 6.8107 avg training loss: 8.2583
batch: [2150/10547] batch time: 0.048 trainign loss: 5.1551 avg training loss: 8.2571
batch: [2160/10547] batch time: 0.056 trainign loss: 7.5501 avg training loss: 8.2550
batch: [2170/10547] batch time: 0.047 trainign loss: 7.7312 avg training loss: 8.2536
batch: [2180/10547] batch time: 0.050 trainign loss: 7.6453 avg training loss: 8.2532
batch: [2190/10547] batch time: 0.325 trainign loss: 0.5327 avg training loss: 8.2500
batch: [2200/10547] batch time: 0.046 trainign loss: 8.7235 avg training loss: 8.2489
batch: [2210/10547] batch time: 0.047 trainign loss: 6.0032 avg training loss: 8.2484
batch: [2220/10547] batch time: 0.046 trainign loss: 8.6677 avg training loss: 8.2467
batch: [2230/10547] batch time: 0.188 trainign loss: 5.6576 avg training loss: 8.2455
batch: [2240/10547] batch time: 0.054 trainign loss: 7.6079 avg training loss: 8.2440
batch: [2250/10547] batch time: 0.362 trainign loss: 6.7540 avg training loss: 8.2434
batch: [2260/10547] batch time: 0.055 trainign loss: 7.0445 avg training loss: 8.2424
batch: [2270/10547] batch time: 1.046 trainign loss: 7.3125 avg training loss: 8.2414
batch: [2280/10547] batch time: 0.050 trainign loss: 4.7931 avg training loss: 8.2402
batch: [2290/10547] batch time: 1.952 trainign loss: 7.8810 avg training loss: 8.2390
batch: [2300/10547] batch time: 0.046 trainign loss: 5.6807 avg training loss: 8.2380
batch: [2310/10547] batch time: 2.153 trainign loss: 8.0352 avg training loss: 8.2368
batch: [2320/10547] batch time: 0.047 trainign loss: 7.9301 avg training loss: 8.2356
batch: [2330/10547] batch time: 2.275 trainign loss: 7.4145 avg training loss: 8.2345
batch: [2340/10547] batch time: 0.057 trainign loss: 6.6563 avg training loss: 8.2337
batch: [2350/10547] batch time: 2.333 trainign loss: 6.4106 avg training loss: 8.2322
batch: [2360/10547] batch time: 0.056 trainign loss: 7.5167 avg training loss: 8.2296
batch: [2370/10547] batch time: 1.976 trainign loss: 6.1538 avg training loss: 8.2288
batch: [2380/10547] batch time: 0.048 trainign loss: 4.5919 avg training loss: 8.2275
batch: [2390/10547] batch time: 2.065 trainign loss: 11.8595 avg training loss: 8.2235
batch: [2400/10547] batch time: 0.056 trainign loss: 7.5177 avg training loss: 8.2238
batch: [2410/10547] batch time: 2.191 trainign loss: 6.5259 avg training loss: 8.2231
batch: [2420/10547] batch time: 0.055 trainign loss: 7.8172 avg training loss: 8.2221
batch: [2430/10547] batch time: 1.965 trainign loss: 6.8600 avg training loss: 8.2207
batch: [2440/10547] batch time: 0.697 trainign loss: 6.3611 avg training loss: 8.2195
batch: [2450/10547] batch time: 2.186 trainign loss: 7.3262 avg training loss: 8.2180
batch: [2460/10547] batch time: 0.055 trainign loss: 1.9617 avg training loss: 8.2158
batch: [2470/10547] batch time: 2.095 trainign loss: 11.4311 avg training loss: 8.2122
batch: [2480/10547] batch time: 0.054 trainign loss: 6.1692 avg training loss: 8.2119
batch: [2490/10547] batch time: 2.370 trainign loss: 7.7961 avg training loss: 8.2105
batch: [2500/10547] batch time: 0.054 trainign loss: 7.0925 avg training loss: 8.2095
batch: [2510/10547] batch time: 2.183 trainign loss: 1.3191 avg training loss: 8.2070
batch: [2520/10547] batch time: 0.047 trainign loss: 9.4680 avg training loss: 8.2057
batch: [2530/10547] batch time: 2.077 trainign loss: 7.3934 avg training loss: 8.2054
batch: [2540/10547] batch time: 0.046 trainign loss: 7.5596 avg training loss: 8.2049
batch: [2550/10547] batch time: 2.146 trainign loss: 6.9576 avg training loss: 8.2030
batch: [2560/10547] batch time: 0.055 trainign loss: 6.7945 avg training loss: 8.2022
batch: [2570/10547] batch time: 2.076 trainign loss: 6.7935 avg training loss: 8.2013
batch: [2580/10547] batch time: 0.052 trainign loss: 6.9757 avg training loss: 8.2006
batch: [2590/10547] batch time: 2.258 trainign loss: 7.3689 avg training loss: 8.1998
batch: [2600/10547] batch time: 0.048 trainign loss: 3.4652 avg training loss: 8.1982
batch: [2610/10547] batch time: 1.414 trainign loss: 7.6244 avg training loss: 8.1974
batch: [2620/10547] batch time: 0.799 trainign loss: 7.2522 avg training loss: 8.1962
batch: [2630/10547] batch time: 1.460 trainign loss: 7.8294 avg training loss: 8.1956
batch: [2640/10547] batch time: 1.410 trainign loss: 4.0819 avg training loss: 8.1942
batch: [2650/10547] batch time: 1.281 trainign loss: 7.3135 avg training loss: 8.1928
batch: [2660/10547] batch time: 0.856 trainign loss: 7.6733 avg training loss: 8.1916
batch: [2670/10547] batch time: 1.538 trainign loss: 6.6034 avg training loss: 8.1905
batch: [2680/10547] batch time: 0.892 trainign loss: 6.7514 avg training loss: 8.1894
batch: [2690/10547] batch time: 1.736 trainign loss: 5.7608 avg training loss: 8.1881
batch: [2700/10547] batch time: 0.175 trainign loss: 7.3226 avg training loss: 8.1862
batch: [2710/10547] batch time: 2.217 trainign loss: 6.5379 avg training loss: 8.1852
batch: [2720/10547] batch time: 0.056 trainign loss: 7.3841 avg training loss: 8.1844
batch: [2730/10547] batch time: 2.308 trainign loss: 7.4085 avg training loss: 8.1839
batch: [2740/10547] batch time: 0.213 trainign loss: 8.3412 avg training loss: 8.1812
batch: [2750/10547] batch time: 2.314 trainign loss: 6.5910 avg training loss: 8.1806
batch: [2760/10547] batch time: 0.115 trainign loss: 7.2051 avg training loss: 8.1794
batch: [2770/10547] batch time: 2.260 trainign loss: 0.6332 avg training loss: 8.1763
batch: [2780/10547] batch time: 0.047 trainign loss: 7.1752 avg training loss: 8.1762
batch: [2790/10547] batch time: 2.032 trainign loss: 7.0083 avg training loss: 8.1755
batch: [2800/10547] batch time: 0.053 trainign loss: 6.8127 avg training loss: 8.1743
batch: [2810/10547] batch time: 2.208 trainign loss: 5.8296 avg training loss: 8.1728
batch: [2820/10547] batch time: 0.046 trainign loss: 7.2152 avg training loss: 8.1717
batch: [2830/10547] batch time: 2.143 trainign loss: 6.7896 avg training loss: 8.1707
batch: [2840/10547] batch time: 0.047 trainign loss: 6.6763 avg training loss: 8.1689
batch: [2850/10547] batch time: 1.990 trainign loss: 7.0514 avg training loss: 8.1675
batch: [2860/10547] batch time: 0.246 trainign loss: 6.9753 avg training loss: 8.1667
batch: [2870/10547] batch time: 2.411 trainign loss: 7.0136 avg training loss: 8.1657
batch: [2880/10547] batch time: 0.047 trainign loss: 7.1136 avg training loss: 8.1645
batch: [2890/10547] batch time: 2.317 trainign loss: 6.1086 avg training loss: 8.1633
batch: [2900/10547] batch time: 0.047 trainign loss: 6.3091 avg training loss: 8.1619
batch: [2910/10547] batch time: 2.604 trainign loss: 6.9342 avg training loss: 8.1607
batch: [2920/10547] batch time: 0.046 trainign loss: 4.3716 avg training loss: 8.1593
batch: [2930/10547] batch time: 1.840 trainign loss: 4.1250 avg training loss: 8.1571
batch: [2940/10547] batch time: 0.047 trainign loss: 6.5239 avg training loss: 8.1554
batch: [2950/10547] batch time: 2.450 trainign loss: 6.2672 avg training loss: 8.1540
batch: [2960/10547] batch time: 0.047 trainign loss: 7.1551 avg training loss: 8.1526
batch: [2970/10547] batch time: 2.168 trainign loss: 6.8506 avg training loss: 8.1519
batch: [2980/10547] batch time: 0.047 trainign loss: 4.4720 avg training loss: 8.1502
batch: [2990/10547] batch time: 2.214 trainign loss: 7.5906 avg training loss: 8.1483
batch: [3000/10547] batch time: 0.050 trainign loss: 6.8701 avg training loss: 8.1470
batch: [3010/10547] batch time: 2.257 trainign loss: 6.4226 avg training loss: 8.1451
batch: [3020/10547] batch time: 0.047 trainign loss: 6.8015 avg training loss: 8.1436
batch: [3030/10547] batch time: 2.321 trainign loss: 7.4223 avg training loss: 8.1423
batch: [3040/10547] batch time: 0.047 trainign loss: 3.9266 avg training loss: 8.1407
batch: [3050/10547] batch time: 2.296 trainign loss: 6.8728 avg training loss: 8.1392
batch: [3060/10547] batch time: 0.046 trainign loss: 7.3429 avg training loss: 8.1384
batch: [3070/10547] batch time: 2.032 trainign loss: 8.2150 avg training loss: 8.1371
batch: [3080/10547] batch time: 0.047 trainign loss: 7.3608 avg training loss: 8.1364
batch: [3090/10547] batch time: 2.254 trainign loss: 6.1779 avg training loss: 8.1354
batch: [3100/10547] batch time: 0.045 trainign loss: 6.8675 avg training loss: 8.1341
batch: [3110/10547] batch time: 2.380 trainign loss: 6.2924 avg training loss: 8.1330
batch: [3120/10547] batch time: 0.048 trainign loss: 7.4998 avg training loss: 8.1315
batch: [3130/10547] batch time: 2.137 trainign loss: 5.8536 avg training loss: 8.1301
batch: [3140/10547] batch time: 0.053 trainign loss: 7.1687 avg training loss: 8.1286
batch: [3150/10547] batch time: 1.864 trainign loss: 6.7237 avg training loss: 8.1272
batch: [3160/10547] batch time: 0.047 trainign loss: 6.9089 avg training loss: 8.1253
batch: [3170/10547] batch time: 2.107 trainign loss: 5.3765 avg training loss: 8.1238
batch: [3180/10547] batch time: 0.047 trainign loss: 5.7556 avg training loss: 8.1223
batch: [3190/10547] batch time: 2.275 trainign loss: 4.1196 avg training loss: 8.1195
batch: [3200/10547] batch time: 0.056 trainign loss: 7.0202 avg training loss: 8.1187
batch: [3210/10547] batch time: 1.832 trainign loss: 7.1207 avg training loss: 8.1178
batch: [3220/10547] batch time: 0.047 trainign loss: 6.1228 avg training loss: 8.1166
batch: [3230/10547] batch time: 0.681 trainign loss: 6.5370 avg training loss: 8.1147
batch: [3240/10547] batch time: 0.055 trainign loss: 4.2933 avg training loss: 8.1132
batch: [3250/10547] batch time: 0.673 trainign loss: 6.7500 avg training loss: 8.1118
batch: [3260/10547] batch time: 0.055 trainign loss: 6.2911 avg training loss: 8.1106
batch: [3270/10547] batch time: 0.620 trainign loss: 5.1023 avg training loss: 8.1094
batch: [3280/10547] batch time: 0.046 trainign loss: 6.0875 avg training loss: 8.1082
batch: [3290/10547] batch time: 1.569 trainign loss: 5.3728 avg training loss: 8.1065
batch: [3300/10547] batch time: 0.055 trainign loss: 6.5312 avg training loss: 8.1051
batch: [3310/10547] batch time: 1.008 trainign loss: 5.0051 avg training loss: 8.1033
batch: [3320/10547] batch time: 0.050 trainign loss: 5.8029 avg training loss: 8.1017
batch: [3330/10547] batch time: 0.962 trainign loss: 6.8590 avg training loss: 8.1006
batch: [3340/10547] batch time: 0.053 trainign loss: 6.0395 avg training loss: 8.0993
batch: [3350/10547] batch time: 2.287 trainign loss: 6.4738 avg training loss: 8.0981
batch: [3360/10547] batch time: 0.056 trainign loss: 7.2051 avg training loss: 8.0969
batch: [3370/10547] batch time: 2.044 trainign loss: 1.1442 avg training loss: 8.0943
batch: [3380/10547] batch time: 0.048 trainign loss: 6.1973 avg training loss: 8.0933
batch: [3390/10547] batch time: 1.696 trainign loss: 7.6063 avg training loss: 8.0917
batch: [3400/10547] batch time: 0.055 trainign loss: 4.0951 avg training loss: 8.0900
batch: [3410/10547] batch time: 1.228 trainign loss: 7.2219 avg training loss: 8.0892
batch: [3420/10547] batch time: 0.048 trainign loss: 6.7136 avg training loss: 8.0882
batch: [3430/10547] batch time: 1.478 trainign loss: 3.2468 avg training loss: 8.0863
batch: [3440/10547] batch time: 0.085 trainign loss: 7.0774 avg training loss: 8.0853
batch: [3450/10547] batch time: 1.924 trainign loss: 4.5854 avg training loss: 8.0832
batch: [3460/10547] batch time: 0.047 trainign loss: 7.2481 avg training loss: 8.0825
batch: [3470/10547] batch time: 2.259 trainign loss: 5.7912 avg training loss: 8.0813
batch: [3480/10547] batch time: 0.055 trainign loss: 6.1888 avg training loss: 8.0804
batch: [3490/10547] batch time: 2.047 trainign loss: 7.5715 avg training loss: 8.0787
batch: [3500/10547] batch time: 0.423 trainign loss: 5.9963 avg training loss: 8.0776
batch: [3510/10547] batch time: 1.854 trainign loss: 4.5193 avg training loss: 8.0761
batch: [3520/10547] batch time: 0.055 trainign loss: 7.1563 avg training loss: 8.0738
batch: [3530/10547] batch time: 2.350 trainign loss: 6.5491 avg training loss: 8.0725
batch: [3540/10547] batch time: 0.056 trainign loss: 5.3394 avg training loss: 8.0712
batch: [3550/10547] batch time: 2.701 trainign loss: 6.9863 avg training loss: 8.0701
batch: [3560/10547] batch time: 0.046 trainign loss: 3.4443 avg training loss: 8.0687
batch: [3570/10547] batch time: 2.565 trainign loss: 6.4651 avg training loss: 8.0673
batch: [3580/10547] batch time: 0.047 trainign loss: 7.0874 avg training loss: 8.0657
batch: [3590/10547] batch time: 2.010 trainign loss: 5.6883 avg training loss: 8.0643
batch: [3600/10547] batch time: 0.046 trainign loss: 7.4360 avg training loss: 8.0632
batch: [3610/10547] batch time: 2.342 trainign loss: 6.9988 avg training loss: 8.0618
batch: [3620/10547] batch time: 0.047 trainign loss: 5.6731 avg training loss: 8.0606
batch: [3630/10547] batch time: 2.387 trainign loss: 6.8155 avg training loss: 8.0591
batch: [3640/10547] batch time: 0.048 trainign loss: 3.7647 avg training loss: 8.0575
batch: [3650/10547] batch time: 2.060 trainign loss: 7.3712 avg training loss: 8.0561
batch: [3660/10547] batch time: 0.055 trainign loss: 1.2066 avg training loss: 8.0536
batch: [3670/10547] batch time: 2.323 trainign loss: 7.0549 avg training loss: 8.0534
batch: [3680/10547] batch time: 0.047 trainign loss: 3.8298 avg training loss: 8.0520
batch: [3690/10547] batch time: 2.187 trainign loss: 3.4325 avg training loss: 8.0501
batch: [3700/10547] batch time: 0.050 trainign loss: 7.3224 avg training loss: 8.0493
batch: [3710/10547] batch time: 2.325 trainign loss: 7.0737 avg training loss: 8.0484
batch: [3720/10547] batch time: 0.051 trainign loss: 6.6517 avg training loss: 8.0473
batch: [3730/10547] batch time: 2.376 trainign loss: 6.0388 avg training loss: 8.0459
batch: [3740/10547] batch time: 0.046 trainign loss: 6.5278 avg training loss: 8.0448
batch: [3750/10547] batch time: 2.483 trainign loss: 5.6452 avg training loss: 8.0436
batch: [3760/10547] batch time: 0.056 trainign loss: 6.2482 avg training loss: 8.0422
batch: [3770/10547] batch time: 2.312 trainign loss: 3.6381 avg training loss: 8.0407
batch: [3780/10547] batch time: 0.047 trainign loss: 6.8267 avg training loss: 8.0397
batch: [3790/10547] batch time: 2.865 trainign loss: 5.6909 avg training loss: 8.0385
batch: [3800/10547] batch time: 0.047 trainign loss: 3.4003 avg training loss: 8.0367
batch: [3810/10547] batch time: 2.087 trainign loss: 2.4322 avg training loss: 8.0353
batch: [3820/10547] batch time: 0.046 trainign loss: 8.2802 avg training loss: 8.0328
batch: [3830/10547] batch time: 2.177 trainign loss: 7.5822 avg training loss: 8.0325
batch: [3840/10547] batch time: 0.047 trainign loss: 7.0987 avg training loss: 8.0316
batch: [3850/10547] batch time: 2.036 trainign loss: 7.1174 avg training loss: 8.0309
batch: [3860/10547] batch time: 0.056 trainign loss: 6.9902 avg training loss: 8.0298
batch: [3870/10547] batch time: 2.514 trainign loss: 6.8400 avg training loss: 8.0291
batch: [3880/10547] batch time: 0.048 trainign loss: 6.2017 avg training loss: 8.0278
batch: [3890/10547] batch time: 2.205 trainign loss: 5.1344 avg training loss: 8.0269
batch: [3900/10547] batch time: 0.047 trainign loss: 2.5015 avg training loss: 8.0243
batch: [3910/10547] batch time: 1.870 trainign loss: 6.9546 avg training loss: 8.0240
batch: [3920/10547] batch time: 0.055 trainign loss: 7.0398 avg training loss: 8.0236
batch: [3930/10547] batch time: 0.863 trainign loss: 6.6715 avg training loss: 8.0228
batch: [3940/10547] batch time: 0.055 trainign loss: 5.7888 avg training loss: 8.0212
batch: [3950/10547] batch time: 1.596 trainign loss: 5.9616 avg training loss: 8.0203
batch: [3960/10547] batch time: 0.055 trainign loss: 1.5446 avg training loss: 8.0180
batch: [3970/10547] batch time: 1.474 trainign loss: 7.1209 avg training loss: 8.0167
batch: [3980/10547] batch time: 0.049 trainign loss: 7.3079 avg training loss: 8.0162
batch: [3990/10547] batch time: 0.842 trainign loss: 6.1878 avg training loss: 8.0153
batch: [4000/10547] batch time: 0.047 trainign loss: 6.0403 avg training loss: 8.0143
batch: [4010/10547] batch time: 0.352 trainign loss: 7.5069 avg training loss: 8.0133
batch: [4020/10547] batch time: 0.053 trainign loss: 6.9571 avg training loss: 8.0125
batch: [4030/10547] batch time: 1.289 trainign loss: 4.9094 avg training loss: 8.0115
batch: [4040/10547] batch time: 0.056 trainign loss: 7.1120 avg training loss: 8.0109
batch: [4050/10547] batch time: 1.109 trainign loss: 5.3522 avg training loss: 8.0096
batch: [4060/10547] batch time: 0.054 trainign loss: 3.7561 avg training loss: 8.0085
batch: [4070/10547] batch time: 0.545 trainign loss: 6.3194 avg training loss: 8.0074
batch: [4080/10547] batch time: 0.046 trainign loss: 6.9623 avg training loss: 8.0066
batch: [4090/10547] batch time: 0.412 trainign loss: 6.4536 avg training loss: 8.0059
batch: [4100/10547] batch time: 0.047 trainign loss: 5.9281 avg training loss: 8.0048
batch: [4110/10547] batch time: 0.988 trainign loss: 5.7589 avg training loss: 8.0035
batch: [4120/10547] batch time: 0.047 trainign loss: 3.7869 avg training loss: 8.0019
batch: [4130/10547] batch time: 0.231 trainign loss: 0.1086 avg training loss: 7.9983
batch: [4140/10547] batch time: 0.047 trainign loss: 6.8728 avg training loss: 7.9980
batch: [4150/10547] batch time: 0.766 trainign loss: 7.4873 avg training loss: 7.9963
batch: [4160/10547] batch time: 0.047 trainign loss: 5.4880 avg training loss: 7.9952
batch: [4170/10547] batch time: 0.055 trainign loss: 5.3759 avg training loss: 7.9941
batch: [4180/10547] batch time: 0.055 trainign loss: 5.3850 avg training loss: 7.9928
batch: [4190/10547] batch time: 1.158 trainign loss: 7.5798 avg training loss: 7.9918
batch: [4200/10547] batch time: 0.047 trainign loss: 7.1096 avg training loss: 7.9910
batch: [4210/10547] batch time: 1.598 trainign loss: 6.5042 avg training loss: 7.9901
batch: [4220/10547] batch time: 0.047 trainign loss: 6.4591 avg training loss: 7.9893
batch: [4230/10547] batch time: 1.396 trainign loss: 7.5384 avg training loss: 7.9880
batch: [4240/10547] batch time: 0.047 trainign loss: 6.7460 avg training loss: 7.9867
batch: [4250/10547] batch time: 0.625 trainign loss: 7.2993 avg training loss: 7.9853
batch: [4260/10547] batch time: 0.047 trainign loss: 4.8623 avg training loss: 7.9843
batch: [4270/10547] batch time: 1.302 trainign loss: 5.8681 avg training loss: 7.9833
batch: [4280/10547] batch time: 0.055 trainign loss: 0.1118 avg training loss: 7.9800
batch: [4290/10547] batch time: 1.212 trainign loss: 0.0003 avg training loss: 7.9746
batch: [4300/10547] batch time: 0.056 trainign loss: 2.6546 avg training loss: 7.9694
batch: [4310/10547] batch time: 1.644 trainign loss: 7.4967 avg training loss: 7.9716
batch: [4320/10547] batch time: 0.047 trainign loss: 6.6927 avg training loss: 7.9714
batch: [4330/10547] batch time: 1.722 trainign loss: 4.1777 avg training loss: 7.9701
batch: [4340/10547] batch time: 0.047 trainign loss: 6.7094 avg training loss: 7.9689
batch: [4350/10547] batch time: 1.394 trainign loss: 6.9651 avg training loss: 7.9684
batch: [4360/10547] batch time: 0.055 trainign loss: 7.2088 avg training loss: 7.9675
batch: [4370/10547] batch time: 1.935 trainign loss: 6.4472 avg training loss: 7.9671
batch: [4380/10547] batch time: 0.054 trainign loss: 6.2304 avg training loss: 7.9659
batch: [4390/10547] batch time: 1.345 trainign loss: 6.0602 avg training loss: 7.9651
batch: [4400/10547] batch time: 0.046 trainign loss: 7.1229 avg training loss: 7.9634
batch: [4410/10547] batch time: 1.760 trainign loss: 6.5254 avg training loss: 7.9626
batch: [4420/10547] batch time: 0.046 trainign loss: 6.5820 avg training loss: 7.9622
batch: [4430/10547] batch time: 1.836 trainign loss: 7.3770 avg training loss: 7.9612
batch: [4440/10547] batch time: 0.051 trainign loss: 7.0011 avg training loss: 7.9582
batch: [4450/10547] batch time: 0.917 trainign loss: 7.2568 avg training loss: 7.9583
batch: [4460/10547] batch time: 0.046 trainign loss: 7.4236 avg training loss: 7.9573
batch: [4470/10547] batch time: 2.195 trainign loss: 6.9422 avg training loss: 7.9562
batch: [4480/10547] batch time: 0.046 trainign loss: 7.6626 avg training loss: 7.9549
batch: [4490/10547] batch time: 1.768 trainign loss: 7.2528 avg training loss: 7.9539
batch: [4500/10547] batch time: 0.055 trainign loss: 3.9042 avg training loss: 7.9527
batch: [4510/10547] batch time: 0.985 trainign loss: 7.8493 avg training loss: 7.9508
batch: [4520/10547] batch time: 0.049 trainign loss: 7.1654 avg training loss: 7.9497
batch: [4530/10547] batch time: 0.880 trainign loss: 5.3870 avg training loss: 7.9489
batch: [4540/10547] batch time: 0.049 trainign loss: 6.8201 avg training loss: 7.9477
batch: [4550/10547] batch time: 1.006 trainign loss: 6.3364 avg training loss: 7.9468
batch: [4560/10547] batch time: 0.048 trainign loss: 5.9067 avg training loss: 7.9457
batch: [4570/10547] batch time: 1.682 trainign loss: 6.1357 avg training loss: 7.9443
batch: [4580/10547] batch time: 0.047 trainign loss: 5.2392 avg training loss: 7.9432
batch: [4590/10547] batch time: 1.841 trainign loss: 5.9578 avg training loss: 7.9419
batch: [4600/10547] batch time: 0.047 trainign loss: 6.7321 avg training loss: 7.9402
batch: [4610/10547] batch time: 1.580 trainign loss: 6.5911 avg training loss: 7.9389
batch: [4620/10547] batch time: 0.078 trainign loss: 6.2416 avg training loss: 7.9379
batch: [4630/10547] batch time: 0.523 trainign loss: 5.5640 avg training loss: 7.9367
batch: [4640/10547] batch time: 0.779 trainign loss: 7.3818 avg training loss: 7.9358
batch: [4650/10547] batch time: 0.790 trainign loss: 5.1170 avg training loss: 7.9348
batch: [4660/10547] batch time: 0.047 trainign loss: 7.2195 avg training loss: 7.9335
batch: [4670/10547] batch time: 0.052 trainign loss: 6.0416 avg training loss: 7.9315
batch: [4680/10547] batch time: 0.835 trainign loss: 6.6976 avg training loss: 7.9308
batch: [4690/10547] batch time: 0.047 trainign loss: 6.5182 avg training loss: 7.9294
batch: [4700/10547] batch time: 0.261 trainign loss: 7.1568 avg training loss: 7.9281
batch: [4710/10547] batch time: 0.056 trainign loss: 6.9149 avg training loss: 7.9270
batch: [4720/10547] batch time: 0.047 trainign loss: 7.3967 avg training loss: 7.9255
batch: [4730/10547] batch time: 0.046 trainign loss: 7.7902 avg training loss: 7.9238
batch: [4740/10547] batch time: 0.047 trainign loss: 6.9793 avg training loss: 7.9226
batch: [4750/10547] batch time: 0.047 trainign loss: 7.1714 avg training loss: 7.9222
batch: [4760/10547] batch time: 0.047 trainign loss: 6.7021 avg training loss: 7.9212
batch: [4770/10547] batch time: 0.047 trainign loss: 7.9388 avg training loss: 7.9201
batch: [4780/10547] batch time: 0.048 trainign loss: 7.3239 avg training loss: 7.9197
batch: [4790/10547] batch time: 0.045 trainign loss: 6.4641 avg training loss: 7.9189
batch: [4800/10547] batch time: 0.047 trainign loss: 7.5176 avg training loss: 7.9181
batch: [4810/10547] batch time: 0.047 trainign loss: 7.0527 avg training loss: 7.9170
batch: [4820/10547] batch time: 0.048 trainign loss: 5.8842 avg training loss: 7.9156
batch: [4830/10547] batch time: 0.055 trainign loss: 6.6408 avg training loss: 7.9148
batch: [4840/10547] batch time: 0.046 trainign loss: 6.4506 avg training loss: 7.9135
batch: [4850/10547] batch time: 0.047 trainign loss: 4.5106 avg training loss: 7.9117
batch: [4860/10547] batch time: 0.046 trainign loss: 5.2712 avg training loss: 7.9100
batch: [4870/10547] batch time: 0.047 trainign loss: 6.7624 avg training loss: 7.9092
batch: [4880/10547] batch time: 0.047 trainign loss: 5.9047 avg training loss: 7.9084
batch: [4890/10547] batch time: 1.017 trainign loss: 6.8591 avg training loss: 7.9073
batch: [4900/10547] batch time: 0.056 trainign loss: 6.5468 avg training loss: 7.9065
batch: [4910/10547] batch time: 1.035 trainign loss: 7.4173 avg training loss: 7.9053
batch: [4920/10547] batch time: 0.048 trainign loss: 6.7999 avg training loss: 7.9046
batch: [4930/10547] batch time: 0.562 trainign loss: 5.3948 avg training loss: 7.9031
batch: [4940/10547] batch time: 0.049 trainign loss: 5.3285 avg training loss: 7.9020
batch: [4950/10547] batch time: 0.565 trainign loss: 7.2640 avg training loss: 7.9003
batch: [4960/10547] batch time: 0.047 trainign loss: 7.2561 avg training loss: 7.8994
batch: [4970/10547] batch time: 0.048 trainign loss: 7.5640 avg training loss: 7.8992
batch: [4980/10547] batch time: 0.046 trainign loss: 6.4805 avg training loss: 7.8986
batch: [4990/10547] batch time: 0.047 trainign loss: 6.1550 avg training loss: 7.8974
batch: [5000/10547] batch time: 0.047 trainign loss: 4.1947 avg training loss: 7.8963
batch: [5010/10547] batch time: 0.047 trainign loss: 5.3457 avg training loss: 7.8949
batch: [5020/10547] batch time: 0.047 trainign loss: 6.9493 avg training loss: 7.8935
batch: [5030/10547] batch time: 0.047 trainign loss: 7.0861 avg training loss: 7.8923
batch: [5040/10547] batch time: 0.048 trainign loss: 6.8731 avg training loss: 7.8912
batch: [5050/10547] batch time: 0.048 trainign loss: 4.9399 avg training loss: 7.8899
batch: [5060/10547] batch time: 0.050 trainign loss: 8.2032 avg training loss: 7.8877
batch: [5070/10547] batch time: 0.046 trainign loss: 7.3909 avg training loss: 7.8874
batch: [5080/10547] batch time: 0.055 trainign loss: 6.0521 avg training loss: 7.8864
batch: [5090/10547] batch time: 0.054 trainign loss: 6.7015 avg training loss: 7.8851
batch: [5100/10547] batch time: 0.047 trainign loss: 4.0332 avg training loss: 7.8842
batch: [5110/10547] batch time: 0.044 trainign loss: 6.4090 avg training loss: 7.8832
batch: [5120/10547] batch time: 0.046 trainign loss: 2.7648 avg training loss: 7.8817
batch: [5130/10547] batch time: 0.044 trainign loss: 4.2013 avg training loss: 7.8805
batch: [5140/10547] batch time: 0.057 trainign loss: 6.8077 avg training loss: 7.8792
batch: [5150/10547] batch time: 0.043 trainign loss: 7.3082 avg training loss: 7.8788
batch: [5160/10547] batch time: 0.046 trainign loss: 0.4342 avg training loss: 7.8764
batch: [5170/10547] batch time: 0.056 trainign loss: 6.9589 avg training loss: 7.8760
batch: [5180/10547] batch time: 0.046 trainign loss: 7.3652 avg training loss: 7.8753
batch: [5190/10547] batch time: 0.048 trainign loss: 7.2827 avg training loss: 7.8741
batch: [5200/10547] batch time: 0.047 trainign loss: 2.8981 avg training loss: 7.8725
batch: [5210/10547] batch time: 0.042 trainign loss: 6.3264 avg training loss: 7.8715
batch: [5220/10547] batch time: 0.047 trainign loss: 7.2532 avg training loss: 7.8703
batch: [5230/10547] batch time: 0.055 trainign loss: 5.2055 avg training loss: 7.8689
batch: [5240/10547] batch time: 0.047 trainign loss: 3.3298 avg training loss: 7.8663
batch: [5250/10547] batch time: 0.047 trainign loss: 7.4885 avg training loss: 7.8658
batch: [5260/10547] batch time: 0.054 trainign loss: 6.5280 avg training loss: 7.8654
batch: [5270/10547] batch time: 0.046 trainign loss: 5.5036 avg training loss: 7.8640
batch: [5280/10547] batch time: 0.047 trainign loss: 7.5295 avg training loss: 7.8631
batch: [5290/10547] batch time: 0.047 trainign loss: 7.0019 avg training loss: 7.8626
batch: [5300/10547] batch time: 0.048 trainign loss: 1.6048 avg training loss: 7.8603
batch: [5310/10547] batch time: 0.048 trainign loss: 7.2292 avg training loss: 7.8598
batch: [5320/10547] batch time: 0.047 trainign loss: 6.9198 avg training loss: 7.8594
batch: [5330/10547] batch time: 0.046 trainign loss: 6.0792 avg training loss: 7.8585
batch: [5340/10547] batch time: 0.046 trainign loss: 4.6945 avg training loss: 7.8571
batch: [5350/10547] batch time: 0.048 trainign loss: 6.6077 avg training loss: 7.8562
batch: [5360/10547] batch time: 0.055 trainign loss: 6.6118 avg training loss: 7.8551
batch: [5370/10547] batch time: 0.047 trainign loss: 5.0857 avg training loss: 7.8545
batch: [5380/10547] batch time: 0.047 trainign loss: 7.1349 avg training loss: 7.8534
batch: [5390/10547] batch time: 0.056 trainign loss: 4.2075 avg training loss: 7.8522
batch: [5400/10547] batch time: 0.177 trainign loss: 6.2714 avg training loss: 7.8513
batch: [5410/10547] batch time: 0.047 trainign loss: 7.5695 avg training loss: 7.8504
batch: [5420/10547] batch time: 0.193 trainign loss: 6.7770 avg training loss: 7.8498
batch: [5430/10547] batch time: 0.840 trainign loss: 7.0787 avg training loss: 7.8485
batch: [5440/10547] batch time: 0.056 trainign loss: 6.7094 avg training loss: 7.8479
batch: [5450/10547] batch time: 0.646 trainign loss: 6.8126 avg training loss: 7.8468
batch: [5460/10547] batch time: 0.047 trainign loss: 6.5815 avg training loss: 7.8456
batch: [5470/10547] batch time: 0.651 trainign loss: 1.8547 avg training loss: 7.8439
batch: [5480/10547] batch time: 0.047 trainign loss: 4.2968 avg training loss: 7.8425
batch: [5490/10547] batch time: 0.995 trainign loss: 7.4565 avg training loss: 7.8416
batch: [5500/10547] batch time: 0.056 trainign loss: 6.0220 avg training loss: 7.8410
batch: [5510/10547] batch time: 0.210 trainign loss: 6.5744 avg training loss: 7.8402
batch: [5520/10547] batch time: 0.052 trainign loss: 8.2077 avg training loss: 7.8377
batch: [5530/10547] batch time: 0.378 trainign loss: 5.6807 avg training loss: 7.8370
batch: [5540/10547] batch time: 0.047 trainign loss: 3.6064 avg training loss: 7.8359
batch: [5550/10547] batch time: 1.402 trainign loss: 6.5639 avg training loss: 7.8346
batch: [5560/10547] batch time: 0.046 trainign loss: 5.9701 avg training loss: 7.8335
batch: [5570/10547] batch time: 1.429 trainign loss: 7.4523 avg training loss: 7.8330
batch: [5580/10547] batch time: 0.047 trainign loss: 5.4604 avg training loss: 7.8320
batch: [5590/10547] batch time: 1.899 trainign loss: 10.3861 avg training loss: 7.8290
batch: [5600/10547] batch time: 0.047 trainign loss: 6.9237 avg training loss: 7.8293
batch: [5610/10547] batch time: 1.590 trainign loss: 7.8465 avg training loss: 7.8291
batch: [5620/10547] batch time: 0.047 trainign loss: 5.7261 avg training loss: 7.8285
batch: [5630/10547] batch time: 0.935 trainign loss: 4.4744 avg training loss: 7.8271
batch: [5640/10547] batch time: 0.047 trainign loss: 6.6272 avg training loss: 7.8258
batch: [5650/10547] batch time: 1.351 trainign loss: 7.1155 avg training loss: 7.8249
batch: [5660/10547] batch time: 0.055 trainign loss: 5.8082 avg training loss: 7.8243
batch: [5670/10547] batch time: 1.044 trainign loss: 3.9036 avg training loss: 7.8233
batch: [5680/10547] batch time: 0.055 trainign loss: 5.7143 avg training loss: 7.8221
batch: [5690/10547] batch time: 0.282 trainign loss: 6.9075 avg training loss: 7.8212
batch: [5700/10547] batch time: 0.057 trainign loss: 7.0788 avg training loss: 7.8203
batch: [5710/10547] batch time: 1.192 trainign loss: 6.0888 avg training loss: 7.8193
batch: [5720/10547] batch time: 0.051 trainign loss: 6.4774 avg training loss: 7.8186
batch: [5730/10547] batch time: 0.941 trainign loss: 6.2442 avg training loss: 7.8179
batch: [5740/10547] batch time: 0.055 trainign loss: 5.4030 avg training loss: 7.8168
batch: [5750/10547] batch time: 0.260 trainign loss: 4.2314 avg training loss: 7.8154
batch: [5760/10547] batch time: 0.045 trainign loss: 5.8610 avg training loss: 7.8140
batch: [5770/10547] batch time: 0.445 trainign loss: 6.8855 avg training loss: 7.8132
batch: [5780/10547] batch time: 0.055 trainign loss: 4.4501 avg training loss: 7.8122
batch: [5790/10547] batch time: 1.061 trainign loss: 0.0157 avg training loss: 7.8081
batch: [5800/10547] batch time: 0.047 trainign loss: 11.6661 avg training loss: 7.8058
batch: [5810/10547] batch time: 1.429 trainign loss: 8.1389 avg training loss: 7.8063
batch: [5820/10547] batch time: 0.047 trainign loss: 7.5239 avg training loss: 7.8057
batch: [5830/10547] batch time: 1.633 trainign loss: 6.0604 avg training loss: 7.8044
batch: [5840/10547] batch time: 0.047 trainign loss: 4.1817 avg training loss: 7.8029
batch: [5850/10547] batch time: 1.799 trainign loss: 8.1383 avg training loss: 7.8014
batch: [5860/10547] batch time: 0.047 trainign loss: 7.2458 avg training loss: 7.8011
batch: [5870/10547] batch time: 2.024 trainign loss: 5.4452 avg training loss: 7.8006
batch: [5880/10547] batch time: 0.055 trainign loss: 3.5772 avg training loss: 7.7991
batch: [5890/10547] batch time: 1.544 trainign loss: 6.0620 avg training loss: 7.7983
batch: [5900/10547] batch time: 0.048 trainign loss: 6.1611 avg training loss: 7.7975
batch: [5910/10547] batch time: 1.317 trainign loss: 7.0907 avg training loss: 7.7970
batch: [5920/10547] batch time: 0.056 trainign loss: 6.2146 avg training loss: 7.7962
batch: [5930/10547] batch time: 2.304 trainign loss: 3.0551 avg training loss: 7.7947
batch: [5940/10547] batch time: 0.055 trainign loss: 6.4634 avg training loss: 7.7923
batch: [5950/10547] batch time: 2.180 trainign loss: 4.8625 avg training loss: 7.7914
batch: [5960/10547] batch time: 0.048 trainign loss: 6.2473 avg training loss: 7.7913
batch: [5970/10547] batch time: 1.673 trainign loss: 8.6825 avg training loss: 7.7894
batch: [5980/10547] batch time: 0.051 trainign loss: 6.9120 avg training loss: 7.7879
batch: [5990/10547] batch time: 2.782 trainign loss: 8.0058 avg training loss: 7.7879
batch: [6000/10547] batch time: 0.047 trainign loss: 6.7671 avg training loss: 7.7873
batch: [6010/10547] batch time: 2.381 trainign loss: 6.5470 avg training loss: 7.7864
batch: [6020/10547] batch time: 0.056 trainign loss: 6.7555 avg training loss: 7.7857
batch: [6030/10547] batch time: 2.425 trainign loss: 5.3164 avg training loss: 7.7843
batch: [6040/10547] batch time: 0.047 trainign loss: 7.3827 avg training loss: 7.7835
batch: [6050/10547] batch time: 2.479 trainign loss: 7.0164 avg training loss: 7.7829
batch: [6060/10547] batch time: 0.052 trainign loss: 6.1667 avg training loss: 7.7821
batch: [6070/10547] batch time: 2.410 trainign loss: 6.3312 avg training loss: 7.7809
batch: [6080/10547] batch time: 0.051 trainign loss: 5.6868 avg training loss: 7.7800
batch: [6090/10547] batch time: 2.190 trainign loss: 7.3611 avg training loss: 7.7793
batch: [6100/10547] batch time: 0.056 trainign loss: 5.2806 avg training loss: 7.7784
batch: [6110/10547] batch time: 1.516 trainign loss: 7.1247 avg training loss: 7.7774
batch: [6120/10547] batch time: 0.049 trainign loss: 6.5272 avg training loss: 7.7765
batch: [6130/10547] batch time: 1.636 trainign loss: 4.5702 avg training loss: 7.7757
batch: [6140/10547] batch time: 0.046 trainign loss: 7.0294 avg training loss: 7.7750
batch: [6150/10547] batch time: 1.624 trainign loss: 6.7542 avg training loss: 7.7742
batch: [6160/10547] batch time: 0.047 trainign loss: 7.6956 avg training loss: 7.7735
batch: [6170/10547] batch time: 1.481 trainign loss: 0.1461 avg training loss: 7.7709
batch: [6180/10547] batch time: 0.046 trainign loss: 10.7857 avg training loss: 7.7699
batch: [6190/10547] batch time: 1.147 trainign loss: 7.7267 avg training loss: 7.7697
batch: [6200/10547] batch time: 0.055 trainign loss: 7.5143 avg training loss: 7.7694
batch: [6210/10547] batch time: 1.307 trainign loss: 6.8014 avg training loss: 7.7690
batch: [6220/10547] batch time: 0.047 trainign loss: 6.1702 avg training loss: 7.7685
batch: [6230/10547] batch time: 1.091 trainign loss: 4.3460 avg training loss: 7.7674
batch: [6240/10547] batch time: 0.047 trainign loss: 5.7038 avg training loss: 7.7662
batch: [6250/10547] batch time: 0.904 trainign loss: 5.6761 avg training loss: 7.7652
batch: [6260/10547] batch time: 0.047 trainign loss: 6.7829 avg training loss: 7.7641
batch: [6270/10547] batch time: 1.444 trainign loss: 3.5646 avg training loss: 7.7627
batch: [6280/10547] batch time: 0.047 trainign loss: 6.3799 avg training loss: 7.7618
batch: [6290/10547] batch time: 0.357 trainign loss: 6.9893 avg training loss: 7.7609
batch: [6300/10547] batch time: 0.048 trainign loss: 7.3352 avg training loss: 7.7604
batch: [6310/10547] batch time: 0.339 trainign loss: 3.0798 avg training loss: 7.7591
batch: [6320/10547] batch time: 0.047 trainign loss: 6.3414 avg training loss: 7.7585
batch: [6330/10547] batch time: 0.048 trainign loss: 6.4864 avg training loss: 7.7579
batch: [6340/10547] batch time: 0.047 trainign loss: 7.1478 avg training loss: 7.7571
batch: [6350/10547] batch time: 0.043 trainign loss: 6.5780 avg training loss: 7.7564
batch: [6360/10547] batch time: 0.047 trainign loss: 5.9851 avg training loss: 7.7557
batch: [6370/10547] batch time: 0.212 trainign loss: 7.5813 avg training loss: 7.7542
batch: [6380/10547] batch time: 0.047 trainign loss: 2.6935 avg training loss: 7.7529
batch: [6390/10547] batch time: 0.072 trainign loss: 6.3820 avg training loss: 7.7525
batch: [6400/10547] batch time: 0.057 trainign loss: 5.8480 avg training loss: 7.7515
batch: [6410/10547] batch time: 0.273 trainign loss: 6.2997 avg training loss: 7.7508
batch: [6420/10547] batch time: 0.047 trainign loss: 7.0857 avg training loss: 7.7492
batch: [6430/10547] batch time: 0.260 trainign loss: 7.5950 avg training loss: 7.7488
batch: [6440/10547] batch time: 0.050 trainign loss: 4.1191 avg training loss: 7.7476
batch: [6450/10547] batch time: 0.046 trainign loss: 8.3107 avg training loss: 7.7465
batch: [6460/10547] batch time: 0.050 trainign loss: 6.4649 avg training loss: 7.7463
batch: [6470/10547] batch time: 0.052 trainign loss: 8.4841 avg training loss: 7.7454
batch: [6480/10547] batch time: 0.050 trainign loss: 7.5652 avg training loss: 7.7448
batch: [6490/10547] batch time: 0.045 trainign loss: 5.9298 avg training loss: 7.7442
batch: [6500/10547] batch time: 0.047 trainign loss: 0.0234 avg training loss: 7.7407
batch: [6510/10547] batch time: 0.051 trainign loss: 0.0006 avg training loss: 7.7362
batch: [6520/10547] batch time: 0.046 trainign loss: 0.0001 avg training loss: 7.7317
batch: [6530/10547] batch time: 0.047 trainign loss: 13.3016 avg training loss: 7.7307
batch: [6540/10547] batch time: 0.047 trainign loss: 8.2082 avg training loss: 7.7313
batch: [6550/10547] batch time: 0.047 trainign loss: 6.6632 avg training loss: 7.7312
batch: [6560/10547] batch time: 0.046 trainign loss: 6.6420 avg training loss: 7.7310
batch: [6570/10547] batch time: 0.045 trainign loss: 2.9532 avg training loss: 7.7298
batch: [6580/10547] batch time: 0.056 trainign loss: 5.9417 avg training loss: 7.7290
batch: [6590/10547] batch time: 0.056 trainign loss: 7.1205 avg training loss: 7.7283
batch: [6600/10547] batch time: 0.947 trainign loss: 3.9214 avg training loss: 7.7272
batch: [6610/10547] batch time: 0.046 trainign loss: 4.4459 avg training loss: 7.7263
batch: [6620/10547] batch time: 0.959 trainign loss: 7.6133 avg training loss: 7.7259
batch: [6630/10547] batch time: 0.055 trainign loss: 6.5223 avg training loss: 7.7255
batch: [6640/10547] batch time: 0.099 trainign loss: 6.2441 avg training loss: 7.7242
batch: [6650/10547] batch time: 0.056 trainign loss: 4.2177 avg training loss: 7.7230
batch: [6660/10547] batch time: 0.054 trainign loss: 7.0745 avg training loss: 7.7223
batch: [6670/10547] batch time: 0.047 trainign loss: 7.5076 avg training loss: 7.7219
batch: [6680/10547] batch time: 0.047 trainign loss: 7.1430 avg training loss: 7.7209
batch: [6690/10547] batch time: 0.047 trainign loss: 6.4160 avg training loss: 7.7200
batch: [6700/10547] batch time: 0.055 trainign loss: 7.0180 avg training loss: 7.7195
batch: [6710/10547] batch time: 0.047 trainign loss: 7.1402 avg training loss: 7.7191
batch: [6720/10547] batch time: 0.047 trainign loss: 6.5520 avg training loss: 7.7182
batch: [6730/10547] batch time: 0.056 trainign loss: 7.2871 avg training loss: 7.7175
batch: [6740/10547] batch time: 0.048 trainign loss: 6.4601 avg training loss: 7.7172
batch: [6750/10547] batch time: 0.045 trainign loss: 6.3842 avg training loss: 7.7162
batch: [6760/10547] batch time: 0.345 trainign loss: 5.4468 avg training loss: 7.7155
batch: [6770/10547] batch time: 0.048 trainign loss: 6.8803 avg training loss: 7.7145
batch: [6780/10547] batch time: 0.748 trainign loss: 6.6145 avg training loss: 7.7134
batch: [6790/10547] batch time: 0.046 trainign loss: 6.7308 avg training loss: 7.7130
batch: [6800/10547] batch time: 0.617 trainign loss: 7.2760 avg training loss: 7.7127
batch: [6810/10547] batch time: 0.047 trainign loss: 6.5079 avg training loss: 7.7120
batch: [6820/10547] batch time: 0.824 trainign loss: 3.5635 avg training loss: 7.7091
batch: [6830/10547] batch time: 0.056 trainign loss: 6.6122 avg training loss: 7.7092
batch: [6840/10547] batch time: 0.047 trainign loss: 5.4470 avg training loss: 7.7090
batch: [6850/10547] batch time: 0.054 trainign loss: 1.9551 avg training loss: 7.7076
batch: [6860/10547] batch time: 0.705 trainign loss: 6.4155 avg training loss: 7.7068
batch: [6870/10547] batch time: 0.046 trainign loss: 6.9086 avg training loss: 7.7056
batch: [6880/10547] batch time: 1.090 trainign loss: 6.5048 avg training loss: 7.7050
batch: [6890/10547] batch time: 0.054 trainign loss: 6.8188 avg training loss: 7.7047
batch: [6900/10547] batch time: 2.060 trainign loss: 5.2648 avg training loss: 7.7036
batch: [6910/10547] batch time: 0.047 trainign loss: 5.6385 avg training loss: 7.7023
batch: [6920/10547] batch time: 2.204 trainign loss: 5.4665 avg training loss: 7.7011
batch: [6930/10547] batch time: 0.055 trainign loss: 7.2227 avg training loss: 7.7002
batch: [6940/10547] batch time: 1.581 trainign loss: 6.3961 avg training loss: 7.6997
batch: [6950/10547] batch time: 0.047 trainign loss: 5.4059 avg training loss: 7.6988
batch: [6960/10547] batch time: 2.197 trainign loss: 7.1429 avg training loss: 7.6973
batch: [6970/10547] batch time: 0.046 trainign loss: 6.9292 avg training loss: 7.6966
batch: [6980/10547] batch time: 2.524 trainign loss: 6.8293 avg training loss: 7.6961
batch: [6990/10547] batch time: 0.055 trainign loss: 6.0073 avg training loss: 7.6954
batch: [7000/10547] batch time: 2.110 trainign loss: 8.2249 avg training loss: 7.6943
batch: [7010/10547] batch time: 0.047 trainign loss: 2.7116 avg training loss: 7.6932
batch: [7020/10547] batch time: 2.214 trainign loss: 7.5660 avg training loss: 7.6924
batch: [7030/10547] batch time: 0.049 trainign loss: 8.0146 avg training loss: 7.6924
batch: [7040/10547] batch time: 2.420 trainign loss: 7.0205 avg training loss: 7.6919
batch: [7050/10547] batch time: 0.047 trainign loss: 6.9915 avg training loss: 7.6912
batch: [7060/10547] batch time: 1.870 trainign loss: 6.9229 avg training loss: 7.6907
batch: [7070/10547] batch time: 0.057 trainign loss: 9.3641 avg training loss: 7.6890
batch: [7080/10547] batch time: 2.524 trainign loss: 7.2523 avg training loss: 7.6887
batch: [7090/10547] batch time: 0.046 trainign loss: 8.0283 avg training loss: 7.6885
batch: [7100/10547] batch time: 2.445 trainign loss: 7.3977 avg training loss: 7.6880
batch: [7110/10547] batch time: 0.047 trainign loss: 6.3528 avg training loss: 7.6869
batch: [7120/10547] batch time: 2.097 trainign loss: 7.2933 avg training loss: 7.6863
batch: [7130/10547] batch time: 0.049 trainign loss: 6.3824 avg training loss: 7.6859
batch: [7140/10547] batch time: 2.116 trainign loss: 6.4081 avg training loss: 7.6847
batch: [7150/10547] batch time: 0.056 trainign loss: 0.1792 avg training loss: 7.6826
batch: [7160/10547] batch time: 1.919 trainign loss: 0.0003 avg training loss: 7.6783
batch: [7170/10547] batch time: 0.056 trainign loss: 0.0000 avg training loss: 7.6740
batch: [7180/10547] batch time: 1.799 trainign loss: 0.0000 avg training loss: 7.6696
batch: [7190/10547] batch time: 0.055 trainign loss: 0.0000 avg training loss: 7.6653
batch: [7200/10547] batch time: 1.850 trainign loss: 0.0000 avg training loss: 7.6610
batch: [7210/10547] batch time: 0.056 trainign loss: 0.0000 avg training loss: 7.6567
batch: [7220/10547] batch time: 1.895 trainign loss: 0.0000 avg training loss: 7.6524
batch: [7230/10547] batch time: 0.047 trainign loss: 0.0000 avg training loss: 7.6481
batch: [7240/10547] batch time: 2.510 trainign loss: 0.0000 avg training loss: 7.6438
batch: [7250/10547] batch time: 0.047 trainign loss: 8.4970 avg training loss: 7.6454
batch: [7260/10547] batch time: 2.419 trainign loss: 7.2993 avg training loss: 7.6454
batch: [7270/10547] batch time: 0.047 trainign loss: 6.0638 avg training loss: 7.6451
batch: [7280/10547] batch time: 2.450 trainign loss: 5.0569 avg training loss: 7.6445
batch: [7290/10547] batch time: 0.047 trainign loss: 8.6394 avg training loss: 7.6433
batch: [7300/10547] batch time: 2.490 trainign loss: 0.1326 avg training loss: 7.6409
batch: [7310/10547] batch time: 0.047 trainign loss: 11.7463 avg training loss: 7.6390
batch: [7320/10547] batch time: 2.168 trainign loss: 7.9977 avg training loss: 7.6395
batch: [7330/10547] batch time: 0.052 trainign loss: 6.8800 avg training loss: 7.6391
batch: [7340/10547] batch time: 2.520 trainign loss: 7.2951 avg training loss: 7.6390
batch: [7350/10547] batch time: 0.047 trainign loss: 7.3843 avg training loss: 7.6384
batch: [7360/10547] batch time: 2.256 trainign loss: 7.1483 avg training loss: 7.6381
batch: [7370/10547] batch time: 0.047 trainign loss: 6.3826 avg training loss: 7.6376
batch: [7380/10547] batch time: 2.224 trainign loss: 7.6351 avg training loss: 7.6373
batch: [7390/10547] batch time: 0.055 trainign loss: 7.2128 avg training loss: 7.6371
batch: [7400/10547] batch time: 2.151 trainign loss: 7.2490 avg training loss: 7.6361
batch: [7410/10547] batch time: 0.047 trainign loss: 7.1066 avg training loss: 7.6356
batch: [7420/10547] batch time: 1.996 trainign loss: 7.1397 avg training loss: 7.6354
batch: [7430/10547] batch time: 0.053 trainign loss: 6.3330 avg training loss: 7.6341
batch: [7440/10547] batch time: 1.577 trainign loss: 6.5407 avg training loss: 7.6334
batch: [7450/10547] batch time: 0.052 trainign loss: 6.7050 avg training loss: 7.6330
batch: [7460/10547] batch time: 0.198 trainign loss: 6.8962 avg training loss: 7.6326
batch: [7470/10547] batch time: 0.047 trainign loss: 7.2366 avg training loss: 7.6314
batch: [7480/10547] batch time: 0.613 trainign loss: 7.1387 avg training loss: 7.6308
batch: [7490/10547] batch time: 0.047 trainign loss: 0.8438 avg training loss: 7.6293
batch: [7500/10547] batch time: 0.165 trainign loss: 7.8146 avg training loss: 7.6290
batch: [7510/10547] batch time: 0.047 trainign loss: 7.8694 avg training loss: 7.6284
batch: [7520/10547] batch time: 0.047 trainign loss: 6.0761 avg training loss: 7.6274
batch: [7530/10547] batch time: 0.047 trainign loss: 7.2678 avg training loss: 7.6273
batch: [7540/10547] batch time: 0.414 trainign loss: 7.2440 avg training loss: 7.6262
batch: [7550/10547] batch time: 0.047 trainign loss: 2.0440 avg training loss: 7.6248
batch: [7560/10547] batch time: 1.471 trainign loss: 7.1399 avg training loss: 7.6246
batch: [7570/10547] batch time: 0.056 trainign loss: 7.1528 avg training loss: 7.6245
batch: [7580/10547] batch time: 0.048 trainign loss: 7.4151 avg training loss: 7.6233
batch: [7590/10547] batch time: 0.056 trainign loss: 7.3296 avg training loss: 7.6224
batch: [7600/10547] batch time: 0.046 trainign loss: 4.1268 avg training loss: 7.6214
batch: [7610/10547] batch time: 0.048 trainign loss: 7.1954 avg training loss: 7.6207
batch: [7620/10547] batch time: 0.043 trainign loss: 7.8341 avg training loss: 7.6203
batch: [7630/10547] batch time: 0.047 trainign loss: 4.5255 avg training loss: 7.6196
batch: [7640/10547] batch time: 0.043 trainign loss: 6.4733 avg training loss: 7.6186
batch: [7650/10547] batch time: 0.049 trainign loss: 7.6677 avg training loss: 7.6182
batch: [7660/10547] batch time: 0.047 trainign loss: 6.9248 avg training loss: 7.6177
batch: [7670/10547] batch time: 0.047 trainign loss: 4.2894 avg training loss: 7.6169
batch: [7680/10547] batch time: 0.043 trainign loss: 5.7123 avg training loss: 7.6162
batch: [7690/10547] batch time: 0.055 trainign loss: 6.4972 avg training loss: 7.6154
batch: [7700/10547] batch time: 0.047 trainign loss: 7.1718 avg training loss: 7.6144
batch: [7710/10547] batch time: 0.047 trainign loss: 8.5125 avg training loss: 7.6125
batch: [7720/10547] batch time: 0.048 trainign loss: 7.9515 avg training loss: 7.6119
batch: [7730/10547] batch time: 0.046 trainign loss: 6.2654 avg training loss: 7.6113
batch: [7740/10547] batch time: 0.056 trainign loss: 6.3266 avg training loss: 7.6110
batch: [7750/10547] batch time: 0.056 trainign loss: 7.0781 avg training loss: 7.6103
batch: [7760/10547] batch time: 0.047 trainign loss: 7.4574 avg training loss: 7.6101
batch: [7770/10547] batch time: 0.047 trainign loss: 6.9483 avg training loss: 7.6097
batch: [7780/10547] batch time: 0.043 trainign loss: 6.9508 avg training loss: 7.6091
batch: [7790/10547] batch time: 0.046 trainign loss: 6.4613 avg training loss: 7.6084
batch: [7800/10547] batch time: 0.043 trainign loss: 6.2795 avg training loss: 7.6076
batch: [7810/10547] batch time: 0.047 trainign loss: 0.1249 avg training loss: 7.6051
batch: [7820/10547] batch time: 0.046 trainign loss: 12.9434 avg training loss: 7.6024
batch: [7830/10547] batch time: 1.041 trainign loss: 8.5791 avg training loss: 7.6036
batch: [7840/10547] batch time: 0.047 trainign loss: 7.8587 avg training loss: 7.6039
batch: [7850/10547] batch time: 1.416 trainign loss: 3.1853 avg training loss: 7.6031
batch: [7860/10547] batch time: 0.055 trainign loss: 6.9092 avg training loss: 7.6025
batch: [7870/10547] batch time: 0.047 trainign loss: 7.6663 avg training loss: 7.6023
batch: [7880/10547] batch time: 0.045 trainign loss: 6.6038 avg training loss: 7.6018
batch: [7890/10547] batch time: 0.048 trainign loss: 6.4404 avg training loss: 7.6013
batch: [7900/10547] batch time: 0.045 trainign loss: 3.4279 avg training loss: 7.6001
batch: [7910/10547] batch time: 0.047 trainign loss: 7.0911 avg training loss: 7.5992
batch: [7920/10547] batch time: 0.046 trainign loss: 7.0696 avg training loss: 7.5984
batch: [7930/10547] batch time: 0.047 trainign loss: 6.7737 avg training loss: 7.5981
batch: [7940/10547] batch time: 0.049 trainign loss: 3.3080 avg training loss: 7.5971
batch: [7950/10547] batch time: 0.047 trainign loss: 9.3212 avg training loss: 7.5960
batch: [7960/10547] batch time: 0.054 trainign loss: 6.7085 avg training loss: 7.5954
batch: [7970/10547] batch time: 0.363 trainign loss: 7.6088 avg training loss: 7.5951
batch: [7980/10547] batch time: 0.049 trainign loss: 7.1523 avg training loss: 7.5946
batch: [7990/10547] batch time: 0.918 trainign loss: 6.9969 avg training loss: 7.5940
batch: [8000/10547] batch time: 0.047 trainign loss: 6.0404 avg training loss: 7.5930
batch: [8010/10547] batch time: 1.545 trainign loss: 6.1955 avg training loss: 7.5922
batch: [8020/10547] batch time: 0.046 trainign loss: 7.1592 avg training loss: 7.5913
batch: [8030/10547] batch time: 1.978 trainign loss: 7.0230 avg training loss: 7.5910
batch: [8040/10547] batch time: 0.049 trainign loss: 6.5214 avg training loss: 7.5905
batch: [8050/10547] batch time: 2.120 trainign loss: 6.4082 avg training loss: 7.5901
batch: [8060/10547] batch time: 0.055 trainign loss: 7.2889 avg training loss: 7.5896
batch: [8070/10547] batch time: 1.758 trainign loss: 6.1555 avg training loss: 7.5889
batch: [8080/10547] batch time: 0.047 trainign loss: 6.5389 avg training loss: 7.5884
batch: [8090/10547] batch time: 1.188 trainign loss: 6.8693 avg training loss: 7.5872
batch: [8100/10547] batch time: 0.047 trainign loss: 6.4149 avg training loss: 7.5859
batch: [8110/10547] batch time: 0.293 trainign loss: 6.4587 avg training loss: 7.5852
batch: [8120/10547] batch time: 0.046 trainign loss: 7.0639 avg training loss: 7.5849
batch: [8130/10547] batch time: 0.042 trainign loss: 6.1182 avg training loss: 7.5844
batch: [8140/10547] batch time: 0.048 trainign loss: 5.4755 avg training loss: 7.5839
batch: [8150/10547] batch time: 0.048 trainign loss: 6.7525 avg training loss: 7.5831
batch: [8160/10547] batch time: 0.046 trainign loss: 7.1639 avg training loss: 7.5824
batch: [8170/10547] batch time: 0.045 trainign loss: 3.5227 avg training loss: 7.5814
batch: [8180/10547] batch time: 0.045 trainign loss: 6.9173 avg training loss: 7.5806
batch: [8190/10547] batch time: 0.049 trainign loss: 6.8487 avg training loss: 7.5804
batch: [8200/10547] batch time: 0.049 trainign loss: 4.6664 avg training loss: 7.5797
batch: [8210/10547] batch time: 0.047 trainign loss: 1.5849 avg training loss: 7.5772
batch: [8220/10547] batch time: 0.047 trainign loss: 7.0682 avg training loss: 7.5762
batch: [8230/10547] batch time: 0.555 trainign loss: 6.8214 avg training loss: 7.5760
batch: [8240/10547] batch time: 0.047 trainign loss: 6.6615 avg training loss: 7.5752
batch: [8250/10547] batch time: 0.047 trainign loss: 6.6082 avg training loss: 7.5743
batch: [8260/10547] batch time: 0.055 trainign loss: 5.4765 avg training loss: 7.5738
batch: [8270/10547] batch time: 0.043 trainign loss: 6.6982 avg training loss: 7.5734
batch: [8280/10547] batch time: 0.046 trainign loss: 6.6417 avg training loss: 7.5728
batch: [8290/10547] batch time: 0.049 trainign loss: 4.3473 avg training loss: 7.5712
batch: [8300/10547] batch time: 0.047 trainign loss: 6.1234 avg training loss: 7.5703
batch: [8310/10547] batch time: 0.291 trainign loss: 7.3893 avg training loss: 7.5699
batch: [8320/10547] batch time: 0.398 trainign loss: 6.2438 avg training loss: 7.5695
batch: [8330/10547] batch time: 0.056 trainign loss: 7.7493 avg training loss: 7.5685
batch: [8340/10547] batch time: 0.766 trainign loss: 3.2633 avg training loss: 7.5676
batch: [8350/10547] batch time: 0.055 trainign loss: 0.0018 avg training loss: 7.5638
batch: [8360/10547] batch time: 1.738 trainign loss: 8.6364 avg training loss: 7.5649
batch: [8370/10547] batch time: 0.056 trainign loss: 7.5855 avg training loss: 7.5647
batch: [8380/10547] batch time: 2.128 trainign loss: 5.8243 avg training loss: 7.5635
batch: [8390/10547] batch time: 0.046 trainign loss: 2.8581 avg training loss: 7.5626
batch: [8400/10547] batch time: 2.280 trainign loss: 6.6664 avg training loss: 7.5615
batch: [8410/10547] batch time: 0.048 trainign loss: 5.8263 avg training loss: 7.5607
batch: [8420/10547] batch time: 1.844 trainign loss: 5.4819 avg training loss: 7.5580
batch: [8430/10547] batch time: 0.054 trainign loss: 5.5348 avg training loss: 7.5584
batch: [8440/10547] batch time: 1.630 trainign loss: 5.8343 avg training loss: 7.5578
batch: [8450/10547] batch time: 0.053 trainign loss: 6.2298 avg training loss: 7.5569
batch: [8460/10547] batch time: 1.122 trainign loss: 7.3968 avg training loss: 7.5559
batch: [8470/10547] batch time: 0.047 trainign loss: 7.2471 avg training loss: 7.5557
batch: [8480/10547] batch time: 2.507 trainign loss: 5.9696 avg training loss: 7.5553
batch: [8490/10547] batch time: 0.056 trainign loss: 6.9565 avg training loss: 7.5537
batch: [8500/10547] batch time: 2.110 trainign loss: 4.0069 avg training loss: 7.5527
batch: [8510/10547] batch time: 0.051 trainign loss: 6.3336 avg training loss: 7.5521
batch: [8520/10547] batch time: 2.154 trainign loss: 4.1471 avg training loss: 7.5509
batch: [8530/10547] batch time: 0.052 trainign loss: 3.0063 avg training loss: 7.5495
batch: [8540/10547] batch time: 2.138 trainign loss: 7.4970 avg training loss: 7.5494
batch: [8550/10547] batch time: 0.047 trainign loss: 7.4375 avg training loss: 7.5490
batch: [8560/10547] batch time: 2.370 trainign loss: 0.3694 avg training loss: 7.5468
batch: [8570/10547] batch time: 0.047 trainign loss: 9.5143 avg training loss: 7.5459
batch: [8580/10547] batch time: 2.391 trainign loss: 7.2704 avg training loss: 7.5458
batch: [8590/10547] batch time: 0.046 trainign loss: 7.6981 avg training loss: 7.5456
batch: [8600/10547] batch time: 2.009 trainign loss: 7.2176 avg training loss: 7.5455
batch: [8610/10547] batch time: 0.047 trainign loss: 7.4994 avg training loss: 7.5450
batch: [8620/10547] batch time: 2.431 trainign loss: 5.8633 avg training loss: 7.5446
batch: [8630/10547] batch time: 0.047 trainign loss: 3.6353 avg training loss: 7.5432
batch: [8640/10547] batch time: 1.536 trainign loss: 7.1021 avg training loss: 7.5427
batch: [8650/10547] batch time: 0.046 trainign loss: 6.9211 avg training loss: 7.5420
batch: [8660/10547] batch time: 0.885 trainign loss: 6.7984 avg training loss: 7.5417
batch: [8670/10547] batch time: 0.047 trainign loss: 6.4119 avg training loss: 7.5414
batch: [8680/10547] batch time: 1.314 trainign loss: 3.9755 avg training loss: 7.5403
batch: [8690/10547] batch time: 0.046 trainign loss: 0.0039 avg training loss: 7.5368
batch: [8700/10547] batch time: 0.864 trainign loss: 8.2208 avg training loss: 7.5365
batch: [8710/10547] batch time: 0.047 trainign loss: 5.7971 avg training loss: 7.5362
batch: [8720/10547] batch time: 2.077 trainign loss: 7.6637 avg training loss: 7.5360
batch: [8730/10547] batch time: 0.049 trainign loss: 6.9802 avg training loss: 7.5355
batch: [8740/10547] batch time: 2.268 trainign loss: 5.5167 avg training loss: 7.5349
batch: [8750/10547] batch time: 0.047 trainign loss: 6.7204 avg training loss: 7.5344
batch: [8760/10547] batch time: 2.366 trainign loss: 8.1994 avg training loss: 7.5335
batch: [8770/10547] batch time: 0.047 trainign loss: 7.2215 avg training loss: 7.5332
batch: [8780/10547] batch time: 2.784 trainign loss: 6.9696 avg training loss: 7.5326
batch: [8790/10547] batch time: 0.047 trainign loss: 6.9268 avg training loss: 7.5323
batch: [8800/10547] batch time: 2.019 trainign loss: 6.0288 avg training loss: 7.5318
batch: [8810/10547] batch time: 0.046 trainign loss: 6.3558 avg training loss: 7.5311
batch: [8820/10547] batch time: 2.282 trainign loss: 6.1618 avg training loss: 7.5306
batch: [8830/10547] batch time: 0.046 trainign loss: 6.4380 avg training loss: 7.5298
batch: [8840/10547] batch time: 1.338 trainign loss: 6.1039 avg training loss: 7.5290
batch: [8850/10547] batch time: 1.489 trainign loss: 6.5249 avg training loss: 7.5286
batch: [8860/10547] batch time: 0.845 trainign loss: 4.2769 avg training loss: 7.5279
batch: [8870/10547] batch time: 1.618 trainign loss: 6.5336 avg training loss: 7.5270
batch: [8880/10547] batch time: 1.200 trainign loss: 5.4308 avg training loss: 7.5263
batch: [8890/10547] batch time: 0.854 trainign loss: 6.5389 avg training loss: 7.5257
batch: [8900/10547] batch time: 0.963 trainign loss: 2.3254 avg training loss: 7.5246
batch: [8910/10547] batch time: 0.936 trainign loss: 7.1633 avg training loss: 7.5235
batch: [8920/10547] batch time: 1.610 trainign loss: 6.1837 avg training loss: 7.5232
batch: [8930/10547] batch time: 0.668 trainign loss: 7.6627 avg training loss: 7.5228
batch: [8940/10547] batch time: 1.805 trainign loss: 6.0175 avg training loss: 7.5222
batch: [8950/10547] batch time: 0.969 trainign loss: 6.1992 avg training loss: 7.5218
batch: [8960/10547] batch time: 1.191 trainign loss: 5.7586 avg training loss: 7.5212
batch: [8970/10547] batch time: 0.486 trainign loss: 3.6546 avg training loss: 7.5204
batch: [8980/10547] batch time: 1.481 trainign loss: 6.0740 avg training loss: 7.5192
batch: [8990/10547] batch time: 1.207 trainign loss: 7.0695 avg training loss: 7.5188
batch: [9000/10547] batch time: 0.802 trainign loss: 0.4683 avg training loss: 7.5172
batch: [9010/10547] batch time: 2.125 trainign loss: 8.4618 avg training loss: 7.5169
batch: [9020/10547] batch time: 0.329 trainign loss: 5.2165 avg training loss: 7.5165
batch: [9030/10547] batch time: 1.810 trainign loss: 5.3499 avg training loss: 7.5159
batch: [9040/10547] batch time: 0.050 trainign loss: 4.8577 avg training loss: 7.5155
batch: [9050/10547] batch time: 2.434 trainign loss: 5.6669 avg training loss: 7.5143
batch: [9060/10547] batch time: 0.047 trainign loss: 5.8352 avg training loss: 7.5134
batch: [9070/10547] batch time: 2.221 trainign loss: 3.9757 avg training loss: 7.5127
batch: [9080/10547] batch time: 0.055 trainign loss: 6.5255 avg training loss: 7.5122
batch: [9090/10547] batch time: 2.185 trainign loss: 4.9498 avg training loss: 7.5117
batch: [9100/10547] batch time: 0.056 trainign loss: 4.7415 avg training loss: 7.5111
batch: [9110/10547] batch time: 2.070 trainign loss: 5.9543 avg training loss: 7.5106
batch: [9120/10547] batch time: 0.171 trainign loss: 7.6227 avg training loss: 7.5099
batch: [9130/10547] batch time: 1.775 trainign loss: 0.0267 avg training loss: 7.5073
batch: [9140/10547] batch time: 0.055 trainign loss: 9.8073 avg training loss: 7.5074
batch: [9150/10547] batch time: 2.330 trainign loss: 8.1142 avg training loss: 7.5078
batch: [9160/10547] batch time: 0.047 trainign loss: 2.6848 avg training loss: 7.5070
batch: [9170/10547] batch time: 2.370 trainign loss: 0.0022 avg training loss: 7.5034
batch: [9180/10547] batch time: 0.047 trainign loss: 8.4505 avg training loss: 7.5046
batch: [9190/10547] batch time: 1.885 trainign loss: 6.6646 avg training loss: 7.5046
batch: [9200/10547] batch time: 0.047 trainign loss: 6.9619 avg training loss: 7.5039
batch: [9210/10547] batch time: 0.988 trainign loss: 6.4213 avg training loss: 7.5033
batch: [9220/10547] batch time: 0.050 trainign loss: 3.5198 avg training loss: 7.5025
batch: [9230/10547] batch time: 0.603 trainign loss: 7.1564 avg training loss: 7.5018
batch: [9240/10547] batch time: 0.056 trainign loss: 6.8892 avg training loss: 7.5014
batch: [9250/10547] batch time: 0.448 trainign loss: 0.0182 avg training loss: 7.4988
batch: [9260/10547] batch time: 0.052 trainign loss: 13.8273 avg training loss: 7.4965
batch: [9270/10547] batch time: 1.408 trainign loss: 8.4264 avg training loss: 7.4975
batch: [9280/10547] batch time: 0.055 trainign loss: 7.7178 avg training loss: 7.4976
batch: [9290/10547] batch time: 1.681 trainign loss: 7.4096 avg training loss: 7.4973
batch: [9300/10547] batch time: 0.051 trainign loss: 5.7829 avg training loss: 7.4967
batch: [9310/10547] batch time: 1.825 trainign loss: 5.6245 avg training loss: 7.4957
batch: [9320/10547] batch time: 0.056 trainign loss: 7.2463 avg training loss: 7.4949
batch: [9330/10547] batch time: 1.533 trainign loss: 7.0696 avg training loss: 7.4943
batch: [9340/10547] batch time: 0.052 trainign loss: 3.4479 avg training loss: 7.4934
batch: [9350/10547] batch time: 0.833 trainign loss: 6.8773 avg training loss: 7.4929
batch: [9360/10547] batch time: 0.047 trainign loss: 9.0821 avg training loss: 7.4918
batch: [9370/10547] batch time: 0.432 trainign loss: 7.7285 avg training loss: 7.4919
batch: [9380/10547] batch time: 0.046 trainign loss: 6.8956 avg training loss: 7.4920
batch: [9390/10547] batch time: 0.960 trainign loss: 7.1426 avg training loss: 7.4912
batch: [9400/10547] batch time: 0.047 trainign loss: 0.7785 avg training loss: 7.4897
batch: [9410/10547] batch time: 0.056 trainign loss: 6.6867 avg training loss: 7.4897
batch: [9420/10547] batch time: 0.046 trainign loss: 7.3015 avg training loss: 7.4896
batch: [9430/10547] batch time: 0.047 trainign loss: 6.1799 avg training loss: 7.4890
batch: [9440/10547] batch time: 0.043 trainign loss: 5.2172 avg training loss: 7.4886
batch: [9450/10547] batch time: 0.047 trainign loss: 5.3815 avg training loss: 7.4874
batch: [9460/10547] batch time: 0.047 trainign loss: 7.0997 avg training loss: 7.4872
batch: [9470/10547] batch time: 0.047 trainign loss: 4.3324 avg training loss: 7.4867
batch: [9480/10547] batch time: 0.047 trainign loss: 4.9391 avg training loss: 7.4862
batch: [9490/10547] batch time: 0.047 trainign loss: 7.5069 avg training loss: 7.4859
batch: [9500/10547] batch time: 0.047 trainign loss: 5.3729 avg training loss: 7.4855
batch: [9510/10547] batch time: 0.049 trainign loss: 6.3959 avg training loss: 7.4846
batch: [9520/10547] batch time: 0.047 trainign loss: 4.7618 avg training loss: 7.4839
batch: [9530/10547] batch time: 0.056 trainign loss: 7.3786 avg training loss: 7.4832
batch: [9540/10547] batch time: 0.044 trainign loss: 3.0078 avg training loss: 7.4824
batch: [9550/10547] batch time: 0.047 trainign loss: 6.8971 avg training loss: 7.4815
batch: [9560/10547] batch time: 0.047 trainign loss: 7.9045 avg training loss: 7.4812
batch: [9570/10547] batch time: 0.056 trainign loss: 7.3686 avg training loss: 7.4812
batch: [9580/10547] batch time: 0.050 trainign loss: 6.8654 avg training loss: 7.4807
batch: [9590/10547] batch time: 0.056 trainign loss: 3.1613 avg training loss: 7.4799
batch: [9600/10547] batch time: 0.408 trainign loss: 5.7359 avg training loss: 7.4792
batch: [9610/10547] batch time: 0.049 trainign loss: 6.4217 avg training loss: 7.4787
batch: [9620/10547] batch time: 0.051 trainign loss: 6.2095 avg training loss: 7.4782
batch: [9630/10547] batch time: 0.989 trainign loss: 6.3044 avg training loss: 7.4779
batch: [9640/10547] batch time: 0.046 trainign loss: 7.0544 avg training loss: 7.4773
batch: [9650/10547] batch time: 0.686 trainign loss: 6.1157 avg training loss: 7.4768
batch: [9660/10547] batch time: 0.047 trainign loss: 6.5927 avg training loss: 7.4761
batch: [9670/10547] batch time: 0.730 trainign loss: 6.5097 avg training loss: 7.4753
batch: [9680/10547] batch time: 0.057 trainign loss: 5.8353 avg training loss: 7.4748
batch: [9690/10547] batch time: 1.028 trainign loss: 6.7097 avg training loss: 7.4740
batch: [9700/10547] batch time: 0.047 trainign loss: 5.5455 avg training loss: 7.4736
batch: [9710/10547] batch time: 0.771 trainign loss: 7.3254 avg training loss: 7.4733
batch: [9720/10547] batch time: 0.049 trainign loss: 7.0275 avg training loss: 7.4730
batch: [9730/10547] batch time: 0.569 trainign loss: 0.0988 avg training loss: 7.4709
batch: [9740/10547] batch time: 0.047 trainign loss: 7.1899 avg training loss: 7.4703
batch: [9750/10547] batch time: 1.378 trainign loss: 7.8818 avg training loss: 7.4707
batch: [9760/10547] batch time: 0.056 trainign loss: 7.6832 avg training loss: 7.4705
batch: [9770/10547] batch time: 2.080 trainign loss: 6.4967 avg training loss: 7.4701
batch: [9780/10547] batch time: 0.056 trainign loss: 7.2560 avg training loss: 7.4700
batch: [9790/10547] batch time: 0.816 trainign loss: 6.7108 avg training loss: 7.4692
batch: [9800/10547] batch time: 0.054 trainign loss: 6.9377 avg training loss: 7.4682
batch: [9810/10547] batch time: 0.841 trainign loss: 6.6993 avg training loss: 7.4680
batch: [9820/10547] batch time: 0.056 trainign loss: 7.5794 avg training loss: 7.4675
batch: [9830/10547] batch time: 1.883 trainign loss: 6.6726 avg training loss: 7.4672
batch: [9840/10547] batch time: 0.048 trainign loss: 0.7555 avg training loss: 7.4656
batch: [9850/10547] batch time: 1.539 trainign loss: 0.0389 avg training loss: 7.4634
batch: [9860/10547] batch time: 0.055 trainign loss: 7.6247 avg training loss: 7.4642
batch: [9870/10547] batch time: 0.735 trainign loss: 6.3263 avg training loss: 7.4640
batch: [9880/10547] batch time: 0.467 trainign loss: 6.3550 avg training loss: 7.4634
batch: [9890/10547] batch time: 0.418 trainign loss: 7.7427 avg training loss: 7.4633
batch: [9900/10547] batch time: 1.324 trainign loss: 7.4368 avg training loss: 7.4631
batch: [9910/10547] batch time: 0.046 trainign loss: 1.8735 avg training loss: 7.4621
batch: [9920/10547] batch time: 1.095 trainign loss: 6.7997 avg training loss: 7.4612
batch: [9930/10547] batch time: 0.809 trainign loss: 7.7244 avg training loss: 7.4607
batch: [9940/10547] batch time: 0.221 trainign loss: 7.8963 avg training loss: 7.4598
batch: [9950/10547] batch time: 0.047 trainign loss: 7.2279 avg training loss: 7.4593
batch: [9960/10547] batch time: 0.050 trainign loss: 6.4967 avg training loss: 7.4593
batch: [9970/10547] batch time: 0.046 trainign loss: 8.9718 avg training loss: 7.4580
batch: [9980/10547] batch time: 0.349 trainign loss: 7.4500 avg training loss: 7.4581
batch: [9990/10547] batch time: 0.055 trainign loss: 7.2055 avg training loss: 7.4578
batch: [10000/10547] batch time: 0.653 trainign loss: 7.4795 avg training loss: 7.4577
batch: [10010/10547] batch time: 0.047 trainign loss: 5.6170 avg training loss: 7.4569
batch: [10020/10547] batch time: 1.510 trainign loss: 7.6400 avg training loss: 7.4562
batch: [10030/10547] batch time: 0.046 trainign loss: 6.8498 avg training loss: 7.4558
batch: [10040/10547] batch time: 0.799 trainign loss: 4.7666 avg training loss: 7.4551
batch: [10050/10547] batch time: 0.048 trainign loss: 4.3912 avg training loss: 7.4546
batch: [10060/10547] batch time: 0.044 trainign loss: 10.1959 avg training loss: 7.4531
batch: [10070/10547] batch time: 0.047 trainign loss: 7.1651 avg training loss: 7.4529
batch: [10080/10547] batch time: 0.306 trainign loss: 5.6031 avg training loss: 7.4524
batch: [10090/10547] batch time: 0.050 trainign loss: 7.9072 avg training loss: 7.4518
batch: [10100/10547] batch time: 0.959 trainign loss: 3.9335 avg training loss: 7.4510
batch: [10110/10547] batch time: 0.056 trainign loss: 6.9897 avg training loss: 7.4504
batch: [10120/10547] batch time: 1.311 trainign loss: 5.7934 avg training loss: 7.4499
batch: [10130/10547] batch time: 0.047 trainign loss: 5.8988 avg training loss: 7.4486
batch: [10140/10547] batch time: 2.368 trainign loss: 4.6478 avg training loss: 7.4477
batch: [10150/10547] batch time: 0.048 trainign loss: 5.3426 avg training loss: 7.4476
batch: [10160/10547] batch time: 1.910 trainign loss: 0.0085 avg training loss: 7.4446
batch: [10170/10547] batch time: 0.050 trainign loss: 11.0977 avg training loss: 7.4440
batch: [10180/10547] batch time: 1.897 trainign loss: 8.0728 avg training loss: 7.4445
batch: [10190/10547] batch time: 0.648 trainign loss: 2.4000 avg training loss: 7.4435
batch: [10200/10547] batch time: 0.993 trainign loss: 5.2023 avg training loss: 7.4436
batch: [10210/10547] batch time: 0.452 trainign loss: 7.9559 avg training loss: 7.4430
batch: [10220/10547] batch time: 1.278 trainign loss: 6.7286 avg training loss: 7.4427
batch: [10230/10547] batch time: 2.011 trainign loss: 6.9246 avg training loss: 7.4426
batch: [10240/10547] batch time: 1.006 trainign loss: 6.4064 avg training loss: 7.4420
batch: [10250/10547] batch time: 1.057 trainign loss: 6.3206 avg training loss: 7.4415
batch: [10260/10547] batch time: 0.309 trainign loss: 6.4783 avg training loss: 7.4408
batch: [10270/10547] batch time: 0.584 trainign loss: 6.5940 avg training loss: 7.4405
batch: [10280/10547] batch time: 0.684 trainign loss: 7.5893 avg training loss: 7.4404
batch: [10290/10547] batch time: 0.757 trainign loss: 5.8896 avg training loss: 7.4400
batch: [10300/10547] batch time: 0.912 trainign loss: 6.8931 avg training loss: 7.4395
batch: [10310/10547] batch time: 0.732 trainign loss: 6.7246 avg training loss: 7.4389
batch: [10320/10547] batch time: 1.032 trainign loss: 7.4607 avg training loss: 7.4387
batch: [10330/10547] batch time: 0.871 trainign loss: 5.5214 avg training loss: 7.4378
batch: [10340/10547] batch time: 0.951 trainign loss: 6.8191 avg training loss: 7.4373
batch: [10350/10547] batch time: 0.753 trainign loss: 6.6342 avg training loss: 7.4369
batch: [10360/10547] batch time: 0.051 trainign loss: 6.6180 avg training loss: 7.4359
batch: [10370/10547] batch time: 2.115 trainign loss: 0.6860 avg training loss: 7.4344
batch: [10380/10547] batch time: 0.047 trainign loss: 4.5725 avg training loss: 7.4331
batch: [10390/10547] batch time: 1.609 trainign loss: 7.1555 avg training loss: 7.4326
batch: [10400/10547] batch time: 0.049 trainign loss: 6.7667 avg training loss: 7.4324
batch: [10410/10547] batch time: 1.292 trainign loss: 6.8333 avg training loss: 7.4319
batch: [10420/10547] batch time: 0.047 trainign loss: 6.9538 avg training loss: 7.4314
batch: [10430/10547] batch time: 0.377 trainign loss: 5.6840 avg training loss: 7.4310
batch: [10440/10547] batch time: 0.047 trainign loss: 6.0445 avg training loss: 7.4299
batch: [10450/10547] batch time: 0.641 trainign loss: 5.1106 avg training loss: 7.4290
batch: [10460/10547] batch time: 0.054 trainign loss: 6.7247 avg training loss: 7.4282
batch: [10470/10547] batch time: 0.796 trainign loss: 7.3972 avg training loss: 7.4283
batch: [10480/10547] batch time: 0.056 trainign loss: 3.4744 avg training loss: 7.4278
batch: [10490/10547] batch time: 1.816 trainign loss: 6.8483 avg training loss: 7.4272
batch: [10500/10547] batch time: 0.054 trainign loss: 7.3651 avg training loss: 7.4266
batch: [10510/10547] batch time: 2.064 trainign loss: 7.0683 avg training loss: 7.4264
batch: [10520/10547] batch time: 0.052 trainign loss: 5.5381 avg training loss: 7.4260
batch: [10530/10547] batch time: 1.529 trainign loss: 7.0584 avg training loss: 7.4248
batch: [10540/10547] batch time: 0.049 trainign loss: 5.9180 avg training loss: 7.4241
